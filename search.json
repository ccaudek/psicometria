[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analisi dei dati per psicologi",
    "section": "",
    "text": "Benvenuti\nQuesto sito web è dedicato al materiale didattico dell’insegnamento di Psicometria (A.A. 2024/2025), rivolto agli studenti del primo anno del Corso di Laurea in Scienze e Tecniche Psicologiche dell’Università degli Studi di Firenze.\nIl corso è strutturato per fornire agli studenti una formazione teorica e pratica approfondita nell’inferenza statistica, enfatizzando particolarmente le applicazioni pratiche attraverso la programmazione. Attraverso esercitazioni guidate, gli studenti impareranno a manipolare e analizzare dati psicologici utilizzando Python, acquisendo così le competenze necessarie per prendere decisioni informate e realizzare interpretazioni precise nei loro progetti di modellazione.\nIl programma copre un ampio spettro di tecniche, partendo dall’analisi descrittiva per arrivare fino ai modelli gerarchici avanzati. Si pone un forte accento sull’inferenza causale, approcciata da una prospettiva bayesiana, includendo l’uso di Grafi Aciclici Diretti (DAG) per esplorare in modo approfondito le relazioni causali. L’intento è di andare oltre i limiti della modellazione lineare tradizionale, mostrando come integrare efficacemente i modelli psicologici avanzati nell’analisi statistica.",
    "crumbs": [
      "Benvenuti"
    ]
  },
  {
    "objectID": "index.html#informazioni-sullinsegnamento",
    "href": "index.html#informazioni-sullinsegnamento",
    "title": "Analisi dei dati per psicologi",
    "section": "Informazioni sull’insegnamento",
    "text": "Informazioni sull’insegnamento\nCodice: B000286 - PSICOMETRIA  Modulo: B000286 - PSICOMETRIA (Cognomi L-Z)  Corso di laurea: Scienze e Tecniche Psicologiche  Anno Accademico: 2024-2025  Materiali didattici: È sufficiente disporre di un laptop/computer funzionante. Tutti i materiali didattici e il software necessario sono forniti gratuitamente a tutti gli studenti, senza richiedere alcun acquisto. Calendario: Il corso si terrà dal 3 marzo al 31 maggio 2025. Orario delle lezioni: Le lezioni si svolgeranno il lunedì e il martedì dalle 8:30 alle 10:30 e il giovedì dalle 11:30 alle 13:30. Luogo: Le lezioni si terranno presso il Plesso didattico La Torretta. Modalità di svolgimento della didattica: Le lezioni ed esercitazioni saranno svolte in modalità frontale.\n\n\n\n\n\n\nIl presente sito web costituisce l’unica fonte ufficiale da consultare per ottenere informazioni sul programma dell’insegnamento B000286 - PSICOMETRIA (Cognomi L-Z) A.A. 2024-2025 e sulle modalità d’esame.",
    "crumbs": [
      "Benvenuti"
    ]
  },
  {
    "objectID": "chapters/chapter_2/00_scientific_method.html",
    "href": "chapters/chapter_2/00_scientific_method.html",
    "title": "1  La scienza dei dati e il metodo scientifico",
    "section": "",
    "text": "1.1 Metodo Scientifico\nIniziamo introducendo alcuni concetti di base relativi al metodo scientifico, ponendo particolare enfasi sui concetti di teorie scientifiche, modelli scientifici e la pratica della previsione, nonché sul ruolo indispensabile della data science nella scienza contemporanea.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>La scienza dei dati e il metodo scientifico</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/00_scientific_method.html#metodo-scientifico",
    "href": "chapters/chapter_2/00_scientific_method.html#metodo-scientifico",
    "title": "1  La scienza dei dati e il metodo scientifico",
    "section": "",
    "text": "1.1.1 Teorie Scientifiche\nLe teorie scientifiche rappresentano il fulcro del metodo scientifico, offrendo spiegazioni astratte e universali per i fenomeni del mondo naturale. Queste teorie sono sostenute da una solida base di prove empiriche e devono resistere a rigorosi test di verifica per essere accettate all’interno della comunità scientifica. Importante è notare che le teorie scientifiche non sono limitate alla quantificazione; infatti, alcune delle più influenti sono di natura qualitativa, come la teoria dei germi delle malattie, la teoria dell’evoluzione di Darwin e la teoria delle cellule. Queste teorie sono essenziali per il progresso scientifico, in quanto forniscono un quadro concettuale per la comprensione e l’interpretazione dei fenomeni.\n\n\n1.1.2 Modelli Scientifici\nParallelamente alle teorie, i modelli scientifici svolgono un ruolo vitale nel metodo scientifico. Spesso formulati come rappresentazioni matematiche, i modelli permettono agli scienziati di prevedere, spiegare o ragionare su specifici fenomeni. Benché possano essere radicati in teorie consolidate, i modelli possono anche emergere da osservazioni empiriche dirette o da principi di senso comune. A differenza delle teorie, che cercano di offrire spiegazioni universali, i modelli tendono ad avere un ambito più ristretto e un carattere pragmatico. La presenza di prove contrarie a un modello non ne decreta necessariamente la fine, ma può semplicemente circoscrivere il suo campo di applicabilità. In questo contesto, è possibile che modelli differenti coesistano all’interno di uno stesso dominio, anche in presenza di apparenti contraddizioni.\n\n\n1.1.3 Il Ruolo della Predizione nella Scienza\nLa predizione è centrale nella scienza e ha diverse finalità:\n\nFalsificazione e conferma delle ipotesi: La capacità di generare previsioni che possano essere confrontate con i dati empirici è essenziale per il processo di falsificazione. Quando le previsioni si rivelano incoerenti con le evidenze sperimentali, ciò consente di scartare o mettere in discussione specifiche ipotesi, teorie o modelli scientifici. D’altra parte, le previsioni che trovano conferma nei dati e nelle osservazioni future contribuiscono a consolidare la validità delle teorie e dei modelli proposti.\nFornire uno standard di confronto: L’accuratezza predittiva dei modelli fornisce uno standard per confrontare modelli scientifici che è basato sulle prestazioni effettive nella previsione di fenomeni reali e indipendente dal funzionamento dei modelli stessi.\nGuidare gli esperimenti: Le predizioni possono indicarci su quale aspetto di un fenomeno concentrarci e condurre esperimenti. Se i nostri modelli o teorie fanno predizioni interessanti per casi limite, possiamo verificarle attraverso esperimenti.\nVerifica della prevedibilità: Comprendere i limiti e le possibilità della previsione è estremamente importante. Se possiamo prevedere in modo consistente un fenomeno basandoci su un insieme di informazioni, significa che tale insieme contiene le informazioni rilevanti per determinare lo stato del fenomeno. Tuttavia, se non possiamo, questo potrebbe significare che tali informazioni sono insufficienti.\n\n\n\n1.1.4 Data Science: Fondamentale per la Predizione Scientifica\nNell’epoca contemporanea, la data science e la statistica rivestono un ruolo fondamentale nel panorama scientifico, permeando ogni fase del processo di ricerca. Queste discipline forniscono gli strumenti metodologici e analitici necessari per gestire e interpretare grandi quantità di dati, facilitando così la formulazione e la verifica delle previsioni che costituiscono il cuore del metodo scientifico. Attraverso l’impiego di tecniche avanzate di analisi dati, la data science consente agli scienziati di estrarre insight significativi dai dati, contribuendo in tal modo alla validazione o alla revisione delle teorie e dei modelli scientifici.\nIn conclusione, il metodo scientifico si configura come un processo dinamico e interconnesso, nel quale teorie, modelli e previsioni interagiscono in modo complesso per approfondire la nostra comprensione del mondo. La data science, grazie ai suoi approcci metodologici avanzati, assume un ruolo imprescindibile in questo processo, consentendo la verifica empirica delle teorie e dei modelli.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>La scienza dei dati e il metodo scientifico</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html",
    "href": "chapters/chapter_2/01_key_notions.html",
    "title": "2  Concetti chiave",
    "section": "",
    "text": "2.1 Introduzione\nL’analisi dei dati si colloca all’intersezione tra statistica, teoria della probabilità e informatica. Questa disciplina multidisciplinare richiede una solida comprensione dei concetti fondamentali provenienti da ciascuna di queste tre aree.\nLa statistica fornisce gli strumenti e le tecniche per raccogliere, analizzare e interpretare i dati. Attraverso metodi descrittivi e inferenziali, la statistica permette di trarre conclusioni dai dati e di prendere decisioni informate.\nLa teoria della probabilità costituisce la base matematica della statistica. Essa consente di modellare l’incertezza e di comprendere i fenomeni aleatori, fornendo i fondamenti per sviluppare metodi statistici rigorosi.\nL’informatica gioca un ruolo cruciale nell’analisi dei dati, offrendo gli strumenti necessari per la gestione, l’elaborazione e la visualizzazione dei dati su larga scala. Conoscere i principi dell’informatica è essenziale per sfruttare appieno le tecnologie moderne, come il machine learning e l’intelligenza artificiale, nell’analisi dei dati. Ad esempio, l’uso di linguaggi di programmazione come Python e R, insieme a librerie specializzate, permette di eseguire analisi complesse e di visualizzare i dati in modo efficace.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#popolazioni-e-campioni",
    "href": "chapters/chapter_2/01_key_notions.html#popolazioni-e-campioni",
    "title": "2  Concetti chiave",
    "section": "2.2 Popolazioni e Campioni",
    "text": "2.2 Popolazioni e Campioni\nPer iniziare l’analisi dei dati, è fondamentale individuare le unità che contengono le informazioni rilevanti per il fenomeno di interesse. Questo insieme di unità costituisce la popolazione o universo, che rappresenta l’insieme completo di entità capaci di fornire informazioni per l’indagine statistica in questione. Possiamo rappresentare la popolazione come $ N $ o $ $ nel caso di popolazioni finite o infinite, rispettivamente. Le singole unità dell’insieme sono chiamate unità statistiche.\nNella ricerca psicologica, sia nelle ricerche sperimentali che in quelle osservazionali, l’obiettivo principale è studiare i fenomeni psicologici all’interno di una specifica popolazione. Pertanto, è essenziale definire con chiarezza la popolazione di interesse, ovvero l’insieme di individui ai quali verranno applicati i risultati della ricerca. Tale popolazione può essere reale, come ad esempio tutte le persone sopravvissute per un anno dopo il bombardamento atomico di Hiroshima, o ipotetica, come ad esempio tutte le persone depresse che potrebbero beneficiare di un intervento psicologico. Il ricercatore deve sempre essere in grado di identificare se un individuo specifico appartiene o meno alla popolazione in questione.\n\n2.2.1 Sotto-popolazioni e Campioni\nUna sotto-popolazione è un sottoinsieme di individui che possiedono proprietà specifiche ben definite. Ad esempio, potremmo essere interessati alla sotto-popolazione degli uomini di età inferiore ai 30 anni o alla sotto-popolazione dei pazienti depressi che hanno ricevuto uno specifico intervento psicologico. Molte questioni scientifiche cercano di descrivere le differenze tra sotto-popolazioni, come ad esempio il confronto tra un gruppo di pazienti sottoposti a psicoterapia e un gruppo di controllo per valutare l’efficacia di un trattamento.\nIl campione è un sottoinsieme della popolazione composto da un insieme di elementi, ognuno dei quali rappresenta un’unità statistica (abbreviata con u.s.) portatrice delle informazioni che verranno rilevate tramite un’operazione di misurazione. Il campione viene utilizzato per ottenere informazioni sulla popolazione di riferimento.\n\n\n2.2.2 Metodi di Campionamento\nIl campionamento può avvenire in diversi modi. Il campionamento casuale consente al ricercatore di trarre conclusioni sulla popolazione e di quantificare l’incertezza dei risultati. Un esempio di campione casuale è quello utilizzato in un sondaggio. Tuttavia, esistono anche altre forme di campionamento, come il campione di convenienza, in cui si seleziona una coorte di studenti da un unico istituto, o il campionamento stratificato, dove la popolazione viene divisa in gruppi o strati, e vengono selezionati campioni proporzionali da ciascuno strato.\nIl ricercatore, indipendentemente dal metodo di acquisizione dei dati, deve sempre considerare la questione della rappresentatività statistica del campione, ovvero se il campione scelto è in grado di riflettere in modo accurato e privo di distorsioni le caratteristiche di interesse della popolazione. Selezionare le unità statistiche in modo casuale rappresenta il metodo più semplice per garantire la rappresentatività del campione. Tuttavia, in molti casi, soprattutto in psicologia, i ricercatori possono non avere a disposizione le risorse necessarie, inclusi i fondi, per utilizzare la tecnica del campionamento casuale nelle loro ricerche. In tali situazioni, possono ricorrere ad altri metodi di campionamento, come il campionamento di convenienza, a seconda delle esigenze e delle risorse disponibili.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#variabili-e-costanti",
    "href": "chapters/chapter_2/01_key_notions.html#variabili-e-costanti",
    "title": "2  Concetti chiave",
    "section": "2.3 Variabili e Costanti",
    "text": "2.3 Variabili e Costanti\nNell’ambito dell’analisi statistica, le variabili rappresentano concetti centrali, denotando le caratteristiche o gli attributi che possono assumere una varietà di valori, numerici o categoriali. Essi incarnano gli elementi quantificabili o osservabili ai quali le unità statistiche danno riscontro. Ad esempio, ponendo la domanda “Qual è l’età di questo partecipante?” e ottenendo come risposta “19 anni”, si identifica “età” come la variabile, e “19” come il valore corrispondente.\n\n2.3.1 Tipi di Variabili\nIn pratica, nel contesto della ricerca empirica, una variabile rappresenta un insieme di osservazioni relative alla stessa misurazione, come ad esempio il punteggio di “nevrosismo” ottenuto da interviste condotte su 744 bambini. Descrivere una variabile significa essere in grado di prendere tali osservazioni e comunicare chiaramente il loro significato, senza obbligare chi legge a dover esaminare ciascuno dei 744 punteggi di nevrosismo separatamente. Questo compito non è affatto semplice.\nPossiamo distinguere varie classi di variabili:\n\nVariabili continue: Possono assumere valori in un intervallo potenzialmente infinito.\nVariabili di conteggio: Rappresentano la frequenza con cui si verificano eventi o la quantità di oggetti in una categoria specifica.\nVariabili ordinali: Caratterizzate da un ordine intrinseco tra i loro valori, ma non esiste una scala standard per quantificare la differenza tra essi.\nVariabili categoriche: Utilizzate per assegnare categorie o classi a un’osservazione, indicando a quale categoria appartiene.\nVariabili binarie: Una sottocategoria di variabili categoriche che possono assumere solo due valori distinti.\n\nLe modalità descrivono le diverse forme che una variabile statistica può assumere. L’insieme delle modalità di una variabile è rappresentato dall’insieme $ $, che include tutte le possibili manifestazioni della variabile. Le modalità presenti nel campione vengono etichettate come dati.\nIn statistica, la nozione di “variabile” si distingue da quella di “costante”, che rimane immutabile attraverso tutte le unità statistiche.\n\nEsempio 1: In uno studio relativo all’intelligenza degli adulti italiani, la variabile di interesse è il punteggio nel test WAIS-IV, con modalità quali 112, 92, 121 ecc. Questa variabile è classificata come quantitativa discreta.\nEsempio 2: Nell’analisi del compito Stroop, focalizzata su bambini di età 6-8 anni, la variabile in esame è l’inverso dei tempi di reazione, misurati in secondi, con modalità come 1.93, 2.35, 1.32 ecc. Questa variabile è classificata come quantitativa continua.\nEsempio 3: In uno studio del disturbo di personalità condotto tra i detenuti nelle carceri italiane, la variabile scrutinata è l’assessment del disturbo di personalità, valutato attraverso interviste cliniche strutturate. Le modalità sono i Cluster A, Cluster B, Cluster C, secondo la classificazione del DSM-V, e questa variabile è classificata come qualitativa.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#la-distribuzione-delle-variabili",
    "href": "chapters/chapter_2/01_key_notions.html#la-distribuzione-delle-variabili",
    "title": "2  Concetti chiave",
    "section": "2.4 La Distribuzione delle Variabili",
    "text": "2.4 La Distribuzione delle Variabili\nUna volta compreso il tipo di variabile con cui lavoriamo, è fondamentale esaminare la distribuzione di tale variabile. La distribuzione di una variabile rappresenta la frequenza con cui si verificano i diversi valori. Nel caso delle variabili discrete, la distribuzione è semplicemente un elenco delle modalità (valori distinti) e delle relative frequenze. Ad esempio, se consideriamo la variabile “genere” all’interno di un campione di studenti, possiamo affermare che l’82% sono donne e il 18% sono uomini.\nLe distribuzioni delle variabili sono descritte in termini di probabilità. Le frequenze relative possono essere interpretate come “probabilità empiriche”. Nell’esempio precedente, la probabilità di “essere una donna” nel campione specifico è 0.82, mentre la probabilità di “essere un uomo” è 0.18. La probabilità che una variabile $ X $ (come il genere) assuma un valore specifico $ x $ (ad esempio, “donna”) viene indicata come $ P(X = x) $, o più semplicemente $ P(x) $.\nLe distribuzioni delle variabili continue sono più complesse da descrivere. Per le variabili continue, non descriviamo la probabilità che la variabile assuma un valore specifico, ma la probabilità che essa cada in un intervallo vicino a quel valore specifico. In futuro, esploreremo come rappresentare graficamente la distribuzione di una variabile continua utilizzando un istogramma o un grafico della densità di probabilità chiamato “Kernel Density Plot”.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#la-matrice-dei-dati",
    "href": "chapters/chapter_2/01_key_notions.html#la-matrice-dei-dati",
    "title": "2  Concetti chiave",
    "section": "2.5 La Matrice dei Dati",
    "text": "2.5 La Matrice dei Dati\nNell’ambito dell’analisi statistica, la matrice dei dati svolge un ruolo fondamentale nell’organizzazione delle informazioni relative alle variabili. Si tratta di una tabella strutturata con righe e colonne, dove ogni riga individua un’unità statistica specifica e ogni colonna rappresenta una diversa variabile statistica in esame.\nVa enfatizzato che, all’interno della matrice dei dati, le unità statistiche non seguono generalmente un ordine progressivo o gerarchico. L’indice attribuito a ciascuna unità statistica indica semplicemente la posizione che essa occupa all’interno della tabella, senza implicare un valore intrinseco o una relazione ordinale. Tale strutturazione metodica offre un mezzo efficace per raccogliere, visualizzare e analizzare le informazioni ottenute durante lo studio statistico, permettendo una gestione chiara e sistematica dei dati raccolti.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#capire-i-dati",
    "href": "chapters/chapter_2/01_key_notions.html#capire-i-dati",
    "title": "2  Concetti chiave",
    "section": "2.6 Capire i dati",
    "text": "2.6 Capire i dati\nA scopo illustrativo, prendiamo in considerazione il dataset contenuto nel file STAR.csv. Questi dati sono parte integrante del progetto STAR (Student-Teacher Achievement Ratio), un esperimento pedagogico sviluppato negli Stati Uniti nel periodo tra il 1985 e il 1990. La finalità centrale di questo studio era indagare l’impatto delle dimensioni delle classi sulle performance accademiche degli studenti. In questo contesto, gli studenti venivano distribuiti casualmente in classi di dimensioni ridotte (13-17 studenti) o più ampie (22-25 studenti).\nDopo aver preparato l’ambiente di lavoro caricando i pacchetti necessari, è possibile procedere all’importazione dei dati in Python. Si può fare ciò utilizzando il seguente codice, prestando attenzione al fatto che l’argomento di read_csv() deve specificare il percorso relativo del file rispetto alla directory in cui è situato lo script .ipynb:\n\ndf_star = pd.read_csv(\"../data/STAR.csv\")\n\nQuesto codice importa i dati dal file STAR.csv e li memorizza in un DataFrame di pandas. Questo passo è fondamentale per consentire un’analisi e una manipolazione efficiente delle informazioni relative all’esperimento STAR. In Python, il DataFrame rappresenta la struttura dati principale per la gestione e l’elaborazione dei dati. Il DataFrame attualizza il concetto di “matrice di dati” che abbiamo introdotto in precedenza.\n\ndf_star.shape\n\n(1274, 4)\n\n\nDato che il DataFrame è troppo grande (1274 righe e 4 colonne), stampiamo sullo schermo le prime 5 righe.\n\ndf_star.head()\n\n\n\n\n\n\n\n\n\nclasstype\nreading\nmath\ngraduated\n\n\n\n\n0\nsmall\n578\n610\n1\n\n\n1\nregular\n612\n612\n1\n\n\n2\nregular\n583\n606\n1\n\n\n3\nsmall\n661\n648\n1\n\n\n4\nsmall\n614\n636\n1\n\n\n\n\n\n\n\n\nNella terminologia statistica, l’osservazione è l’informazione raccolta da un individuo o un’entità specifica che partecipa allo studio. Considerando il dataset STAR, l’unità di osservazione è costituita dagli studenti. Pertanto, nel DataFrame denominato df_star, ogni riga simboleggia uno studente distinto coinvolto nell’indagine.\nLe variabili, d’altro canto, sono espressioni delle diverse caratteristiche degli individui o delle entità analizzate. Nel contesto del progetto STAR, questo concetto si traduce in:\n\nOgni colonna di df_star rappresenta una variabile che incarna una particolare proprietà condivisa da tutti gli studenti partecipanti.\nLe variabili sono identificate attraverso etichette collegate alle colonne, come classtype (il tipo di classe assegnata, con modalità small e regular), reading (il punteggio nel test di lettura standardizzato), math (il punteggio nel test di matematica standardizzato) e graduated (indicazione se lo studente ha conseguito o meno il diploma di scuola superiore, con “1” o “0” rispettivamente).\n\nPer rappresentare un’osservazione singola della variabile generica \\(X\\), si utilizza la notazione \\(X_i\\), dove \\(i\\) rappresenta l’indice dell’osservazione. Questo indice significa che abbiamo un valore differente di \\(X\\) per ogni valore distinto di \\(i\\). Ad esempio, nel caso di 1274 osservazioni, \\(i\\) può variare da 1 a 1274. Pertanto, per simboleggiare la seconda osservazione (quella con \\(i=2\\)), useremo la notazione \\(X_2\\). È fondamentale tener presente che, mentre in Python gli indici iniziano da 0, nella notazione matematica tradizionale, come quella rappresentata da \\(X_i\\), l’indice ha inizio da 1. Questa differenza tra le convenzioni di indicizzazione può essere un aspetto cruciale da considerare durante l’analisi dei dati.\n\ndf_star[\"reading\"][1]\n\n612\n\n\nUna delle prime cose da fare, quando esaminiamo un dataset, è capire che tipo di variabili sono incluse.\n\ndf_star.dtypes\n\nclasstype    object\nreading       int64\nmath          int64\ngraduated     int64\ndtype: object\n\n\nNel caso specifico, notiamo che la variabile classtype è di tipo object, quindi è una variabile qualitativa, mentre le altre variabili sono numeriche, rappresentate come numeri interi (int64). Se elenchiamo le modalità presenti in classtype utilizzando il metodo unique(), scopriamo che corrispondono a “small” e “regular”.\n\ndf_star[\"classtype\"].unique()\n\narray(['small', 'regular'], dtype=object)\n\n\nCon l’istruzione seguente verifichiamo che la variabile graduated sia una variabile binaria.\n\ndf_star[\"graduated\"].unique()\n\narray([1, 0])",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#variabili-indipendenti-e-variabili-dipendenti",
    "href": "chapters/chapter_2/01_key_notions.html#variabili-indipendenti-e-variabili-dipendenti",
    "title": "2  Concetti chiave",
    "section": "2.7 Variabili Indipendenti e Variabili Dipendenti",
    "text": "2.7 Variabili Indipendenti e Variabili Dipendenti\nNell’ambito della ricerca statistica, è fondamentale distinguere tra variabili indipendenti e dipendenti. Questa distinzione si basa sulla domanda di ricerca e sulla comprensione del fenomeno che si sta studiando.\n\n2.7.1 Variabili Indipendenti\nLe variabili indipendenti, a volte chiamate variabili predittive, rappresentano i fattori che si ipotizza influenzino l’esito di interesse. Esse sono spesso manipolate o controllate dal ricercatore.\n\n\n2.7.2 Variabili Dipendenti\nLe variabili dipendenti, d’altra parte, rappresentano l’esito o il risultato che si sta cercando di spiegare o prevedere. Esse sono ciò che il ricercatore sta cercando di capire e sono influenzate dalle variabili indipendenti.\nIn molti studi, è abbastanza chiaro quali sono le variabili indipendenti e dipendenti. Tuttavia, in alcuni casi, la relazione può essere più sfumata o complessa. Ad esempio, nell’analizzare la correlazione tra l’esercizio fisico e l’insonnia, può non essere immediatamente evidente quale sia la causa e quale l’effetto. In tali circostanze, una comprensione delle relazioni causali inerenti il fenomeno considerato è necessaria per una corretta distinzione tra variabili indipendenti (cause) e dipendenti (effetti).\nEsempio 5 Prendiamo in considerazione un esperimento in cui uno psicologo ha chiamato 120 studenti universitari per un test di memoria. Prima di iniziare, metà dei partecipanti è stata informata che il compito era particolarmente difficile, mentre all’altra metà non è stata fornita alcuna indicazione sulla difficoltà. Successivamente, è stato misurato il punteggio ottenuto nella prova di memoria da ciascun partecipante.\nIn questo esperimento: - La variabile indipendente è l’informazione sulla difficoltà del compito, che è stata manipolata dallo sperimentatore attraverso l’assegnazione casuale dei soggetti alle due diverse condizioni (“informazione fornita” e “informazione non fornita”). - La variabile dipendente è il punteggio ottenuto nella prova di memoria, ovvero l’outcome che lo sperimentatore sta cercando di capire e che potrebbe essere influenzato dalla variabile indipendente.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#effetto",
    "href": "chapters/chapter_2/01_key_notions.html#effetto",
    "title": "2  Concetti chiave",
    "section": "2.8 Effetto",
    "text": "2.8 Effetto\nIl concetto di “effetto” è fondamentale nell’analisi dei dati e nella statistica, poiché rappresenta una misura del cambiamento o dell’influenza tra le variabili. Ad esempio, consideriamo uno studio sulla memoria che indaga l’effetto delle mnemotecniche sul miglioramento della memoria. In questo studio, un gruppo riceve un intervento relativo al rilassamento, mentre un altro partecipa a un workshop di riorganizzazione mnemonica. Alla fine, i partecipanti vengono sottoposti a test di memoria e l’effetto delle mnemotecniche è determinato dalla differenza tra i punteggi medi dei due gruppi. Se il gruppo che ha seguito il workshop mostra un punteggio medio superiore, si può affermare che le mnemotecniche hanno un effetto positivo sulla memoria.\nL’effetto viene misurato attraverso diverse statistiche, come la differenza di medie, il rapporto di probabilità, ecc. Quando si analizzano i dati con metodi statistici, l’obiettivo è determinare se l’effetto osservato è credibile, ovvero se ci sono evidenze che l’effetto sia generalizzabile alla popolazione nel suo insieme. Questo aiuta a valutare l’importanza dell’effetto e a trarre conclusioni sulle relazioni tra le variabili nello studio.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#variabili-casuali",
    "href": "chapters/chapter_2/01_key_notions.html#variabili-casuali",
    "title": "2  Concetti chiave",
    "section": "2.9 Variabili Casuali",
    "text": "2.9 Variabili Casuali\nIl concetto di variabile nella statistica trova un corrispondente nella teoria delle probabilità, dove è chiamato variabile casuale. Quando ci occupiamo di studi come quelli sugli interventi psicologici, le variabili casuali vengono utilizzate per rappresentare e misurare i risultati di tali interventi. In sostanza, una variabile casuale descrive una caratteristica specifica degli individui all’interno di una popolazione, e i suoi valori possono variare tra gli individui. Teoricamente, una variabile casuale può assumere una gamma di valori possibili, ma in pratica si osserva un valore specifico per ogni individuo.\nUtilizziamo notazioni particolari per riferirci alle variabili casuali e ai loro valori specifici. Ad esempio, le lettere maiuscole come $ X $ e $ Y $ denotano le variabili casuali, mentre le lettere minuscole come $ x $ e $ y $ si riferiscono ai valori che queste variabili possono assumere in circostanze specifiche.\n\n2.9.1 Differenza tra Variabili Casuali e Variabili Statistiche\nLa chiave per comprendere la differenza tra questi due concetti risiede nell’incertezza epistemica che il ricercatore affronta. Immaginiamo un esperimento casuale, come il lancio di un dado. Supponiamo che la variabile di interesse $ X $ rappresenti l’esito del lancio. Prima del lancio, $ X $ è una variabile casuale, poiché conosciamo i possibili valori che può assumere (da 1 a 6), ma non sappiamo quale valore specifico si manifesterà. In questo stadio, $ X $ rappresenta una quantità incognita, suscettibile di variazione casuale.\nDopo il lancio, supponiamo che l’esito sia 5. A questo punto, la variabile $ X $ diventa una variabile statistica, poiché rappresenta un dato osservato e concreto all’interno del campione di osservazioni. L’incertezza è risolta, e il valore di $ X $ è ora noto e fisso.\nIn sintesi, una variabile casuale rappresenta una quantità che può assumere diversi valori con una certa probabilità, mentre una variabile statistica è una realizzazione specifica di quella quantità incognita. La transizione da una variabile casuale a una variabile statistica avviene attraverso l’osservazione e la misurazione, che trasformano un’incertezza teorica in una certezza empirica.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#stima-e-inferenza",
    "href": "chapters/chapter_2/01_key_notions.html#stima-e-inferenza",
    "title": "2  Concetti chiave",
    "section": "2.10 Stima e Inferenza",
    "text": "2.10 Stima e Inferenza\n\n2.10.1 Stima\nLa stima è un concetto centrale in statistica che si riferisce al processo attraverso il quale si ottengono informazioni sulle caratteristiche di una popolazione intera basandosi sui dati di un campione estratto da essa. Ad esempio, calcolando la media o la mediana dei dati all’interno del campione, possiamo derivare stime per la media o la mediana della popolazione complessiva. Le caratteristiche che vogliamo conoscere della popolazione sono spesso chiamate “parametri”, e la stima può riguardare sia questi parametri sia la distribuzione di una variabile casuale nella popolazione.\n\n\n2.10.2 Inferenza Statistica\nDopo aver ottenuto queste stime, si passa al passaggio successivo: l’inferenza statistica. Questo processo va oltre la semplice stima e ci permette di trarre conclusioni più ampie sulla popolazione. L’inferenza statistica riguarda la valutazione di specifiche ipotesi o risposte a domande di ricerca relative alla popolazione, utilizzando le stime ottenute dal campione.\nAd esempio, se abbiamo stimato la media dei redditi in un campione di famiglie, possiamo utilizzare l’inferenza statistica per testare se c’è una differenza significativa nei redditi tra diverse regioni o gruppi demografici all’interno della popolazione. In questo modo, l’inferenza statistica ci fornisce gli strumenti per fare previsioni e trarre conclusioni riguardanti la popolazione intera.\n\n\n2.10.3 Approcci all’Inferenza Statistica\nEsistono vari approcci e metodologie per condurre l’inferenza statistica, tra cui due dei più comuni sono l’inferenza bayesiana e l’approccio frequentista. L’inferenza bayesiana si basa sull’uso di probabilità a priori e a posteriori, mentre l’approccio frequentista si basa su tecniche come i test d’ipotesi e gli intervalli di confidenza.\nIn sintesi, la stima e l’inferenza statistica sono due fasi cruciali nell’analisi statistica. La stima ci permette di utilizzare i dati del campione per ottenere informazioni su specifiche caratteristiche della popolazione, mentre l’inferenza statistica ci consente di utilizzare quelle stime per fare affermazioni più generali e valutare ipotesi sulla popolazione nel suo insieme. Entrambi questi processi sono fondamentali per comprendere e interpretare i fenomeni che stiamo studiando.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#modelli-psicologici",
    "href": "chapters/chapter_2/01_key_notions.html#modelli-psicologici",
    "title": "2  Concetti chiave",
    "section": "2.11 Modelli Psicologici",
    "text": "2.11 Modelli Psicologici\n\n2.11.1 Concetto di Modello\nIn ambito statistico e nel campo della data science, un “modello” rappresenta una formulazione matematica semplificata di un fenomeno reale che si desidera studiare. Si tratta di un insieme di equazioni e ipotesi che delineano la struttura probabilistica e le relazioni tra le variabili, cercando di catturare gli aspetti essenziali del fenomeno senza rappresentarlo in ogni dettaglio. La scelta del modello specifico può dipendere dai dati disponibili, dalla domanda di ricerca e dall’obiettivo dell’analisi. Poiché spesso esistono diversi modelli che possono essere applicati allo stesso problema, la data science si pone il problema dell’identificazione del modello che meglio si adatta ai dati e che soddisfa certi criteri di validità e bontà.\n\n\n2.11.2 Modelli in Psicologia\nNel contesto della psicologia, la modellazione assume un ruolo particolare, in quanto i modelli sono utilizzati per descrivere e prevedere il comportamento umano o il funzionamento mentale. Un modello psicologico efficace deve soddisfare diverse caratteristiche: deve fornire una descrizione coerente del fenomeno studiato, essere in grado di formulare predizioni sulle manifestazioni future di tale fenomeno, essere supportato da prove empiriche e, crucialmente, deve essere falsificabile, cioè soggetto a verifica o confutazione attraverso l’osservazione e l’esperimento.\nL’analisi dei dati, attraverso l’applicazione di tecniche statistiche, è il mezzo attraverso il quale un modello psicologico viene valutato. Oltre a determinare se il modello è in grado di spiegare i dati osservati, l’analisi può anche verificare la capacità del modello di fare previsioni accurate su dati non ancora osservati. In questo modo, la modellazione diventa uno strumento potente non solo per comprendere i fenomeni psicologici ma anche per prevedere e, in alcuni casi, influenzare il comportamento e le dinamiche mentali.\nIn sintesi, un modello, sia in statistica che in psicologia, è un costrutto teorico che cerca di rappresentare un fenomeno complesso in una forma semplificata ma informativa, guidando la comprensione, la previsione e, in ultima analisi, l’intervento efficace su quel fenomeno. La scelta e la valutazione del modello giusto sono fondamentali per garantire che le conclusioni derivanti dall’analisi siano valide e utili nel contesto specifico.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#statistica",
    "href": "chapters/chapter_2/01_key_notions.html#statistica",
    "title": "2  Concetti chiave",
    "section": "2.12 Statistica",
    "text": "2.12 Statistica\n\n2.12.1 Disciplina Statistica\nIl termine “statistica” può assumere diversi significati, a seconda del contesto in cui viene utilizzato. Nel primo senso, la statistica è una scienza e una disciplina che si occupa dello studio e dell’applicazione di metodi e tecniche per la raccolta, l’organizzazione, l’analisi, l’interpretazione e la presentazione di dati. Include una vasta gamma di approcci e strumenti utilizzati per comprendere le tendenze e i modelli nei dati, permettendo di trarre conclusioni informate e fare previsioni su una popolazione a partire da un campione.\n\n\n2.12.2 Statistica come Misura\nNel secondo senso, il termine “statistica” si riferisce a una singola misura o un valore numerico che è stato calcolato a partire da un campione di dati. Questo tipo di statistica rappresenta una caratteristica specifica del campione, come la tendenza centrale, la dispersione o la relazione tra le variabili. Esempi comuni di statistiche in questo senso includono la media campionaria, la deviazione standard campionaria o il coefficiente di correlazione campionario. Queste statistiche sono utilizzate per sintetizzare le informazioni contenute nel campione e fornire una rappresentazione semplificata di aspetti particolari dei dati.\nIn sintesi, il termine “statistica” può riferirsi sia alla disciplina nel suo complesso che riguarda l’analisi dei dati, sia a una specifica misura calcolata da un insieme di dati. Entrambi gli aspetti sono fondamentali nell’ambito della ricerca, dell’analisi dei dati e della presa di decisioni basata su prove empiriche.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_2/01_key_notions.html#informazioni-sullambiente-di-sviluppo",
    "title": "2  Concetti chiave",
    "section": "2.13 Informazioni sull’Ambiente di Sviluppo",
    "text": "2.13 Informazioni sull’Ambiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/02_measurement.html",
    "href": "chapters/chapter_2/02_measurement.html",
    "title": "3  La misurazione in psicologia",
    "section": "",
    "text": "3.1 Introduzione\nIl problema dello scaling psicologico riguarda la trasformazione dei dati osservati in misure o punteggi che rappresentino accuratamente le caratteristiche psicologiche misurate. Quando conduciamo ricerche in psicologia, spesso vogliamo assegnare numeri ai comportamenti o alle risposte degli individui per analizzarli in modo oggettivo. Tuttavia, questa trasformazione è complessa e richiede attenzione a diverse considerazioni.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/02_measurement.html#introduzione",
    "href": "chapters/chapter_2/02_measurement.html#introduzione",
    "title": "3  La misurazione in psicologia",
    "section": "",
    "text": "3.1.1 Scaling di Guttman\nUno dei metodi di scaling più noti è il «Scaling di Guttman», utilizzato per rappresentare relazioni ordinate tra elementi di una scala. Per esempio, in un questionario sui sintomi di ansia, le domande sono ordinate per intensità crescente. Se un partecipante risponde “sì” a una domanda più intensa, dovrebbe aver risposto “sì” anche a tutte le domande meno intense precedenti, creando una scala ordinata di gravità dei sintomi.\n\n\n3.1.2 Scaling Thurstoniano\nLo «Scaling Thurstoniano» misura le preferenze o i giudizi soggettivi. Ad esempio, per valutare la preferenza per diversi tipi di cibi, i partecipanti confrontano due cibi alla volta e esprimono una preferenza. Queste risposte vengono usate per assegnare punteggi basati sulla preferenza media.\n\n\n3.1.3 Questionari Likert\nI questionari Likert richiedono ai partecipanti di esprimere il grado di accordo con affermazioni su una scala a più livelli (da «fortemente in disaccordo» a «fortemente d’accordo»). I punteggi ottenuti vengono sommati per rappresentare la posizione dell’individuo rispetto all’oggetto di studio.\n\n\n3.1.4 Metodi di Valutazione delle Scale Psicologiche\nPer valutare le proprietà delle scale psicologiche, si utilizzano vari metodi. Ad esempio, l’affidabilità delle misure può essere analizzata con il coefficiente alpha di Cronbach o il coefficiente Omega di McDonald, che misurano la coerenza interna delle risposte ai vari item del questionario. Inoltre, la validità delle scale può essere esaminata confrontando i risultati con misure simili o utilizzando analisi statistiche per verificare se la scala cattura accuratamente il costrutto che si intende misurare. La validità di costrutto è cruciale, poiché riguarda la capacità della scala di misurare effettivamente il concetto psicologico che si intende esaminare.\n\n\n3.1.5 Prospettive Moderne\nNegli ultimi anni, il dibattito sulla misurazione psicologica si è arricchito di nuove prospettive, grazie all’avvento di tecnologie avanzate e all’integrazione di approcci interdisciplinari. Ecco alcune delle tendenze più rilevanti:\n\n3.1.5.1 Teoria della Risposta agli Item (IRT)\nLa Teoria della Risposta agli Item (IRT) ha guadagnato popolarità per la sua capacità di fornire stime più precise delle abilità latenti rispetto ai modelli classici. La IRT considera la probabilità che un individuo risponda correttamente a un item in funzione della sua abilità e delle caratteristiche dell’item stesso, offrendo una visione più dettagliata delle proprietà psicometriche degli strumenti di misurazione.\n\n\n3.1.5.2 Approcci Bayesiani\nGli approcci bayesiani stanno rivoluzionando il campo della psicometria, permettendo di incorporare informazioni a priori nelle stime e di aggiornare le credenze sulla base di nuovi dati. Questi metodi sono particolarmente utili per affrontare la complessità e l’incertezza inerenti alla misurazione psicologica.\n\n\n3.1.5.3 Analisi di Rete\nL’analisi di rete è un’altra metodologia emergente che vede i costrutti psicologici non come variabili latenti indipendenti, ma come reti di sintomi interconnessi. Questo approccio può offrire nuove intuizioni sulla struttura delle psicopatologie e sulla dinamica dei sintomi.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/02_measurement.html#le-scale-di-misurazione",
    "href": "chapters/chapter_2/02_measurement.html#le-scale-di-misurazione",
    "title": "3  La misurazione in psicologia",
    "section": "3.2 Le scale di misurazione",
    "text": "3.2 Le scale di misurazione\nLe scale di misurazione sono strumenti fondamentali per assegnare numeri ai dati osservati, rappresentando le proprietà psicologiche. La teoria delle scale di Stevens {cite:p}stevens_46 identifica quattro tipi di scale di misurazione: nominali, ordinali, a intervalli e di rapporti. Ognuna di queste scale consente di effettuare operazioni aritmetiche diverse, poiché ciascuna di esse è in grado di “catturare” solo alcune delle proprietà dei fenomeni psicologici che si intende misurare.\n\n\n\nScale di misurazione.\n\n\n\n3.2.1 Scala nominale\nILa scala nominale è il livello di misurazione più semplice e corrisponde ad una tassonomia o classificazione delle categorie che utilizziamo per descrivere i fenomeni psicologici. I simboli o numeri che costituiscono questa scala rappresentano i nomi delle categorie e non hanno alcun valore numerico intrinseco. Con la scala nominale possiamo solo distinguere se una caratteristica psicologica è uguale o diversa da un’altra.\nI dati raccolti con la scala nominale sono suddivisi in categorie qualitative e mutuamente esclusive, in cui ogni dato appartiene ad una sola categoria. In questa scala, esiste solo la relazione di equivalenza tra le misure delle unità di studio: gli elementi del campione appartenenti a classi diverse sono differenti, mentre tutti quelli della stessa classe sono tra loro equivalenti.\nL’unica operazione algebrica consentita dalla scala nominale è quella di contare le unità di studio che appartengono ad ogni categoria e il numero totale di categorie. Di conseguenza, la descrizione dei dati avviene tramite le frequenze assolute e le frequenze relative.\nDalla scala nominale è possibile costruire altre scale nominali equivalenti alla prima, trasformando i valori della scala di partenza in modo tale da cambiare i nomi delle categorie, ma lasciando inalterata la suddivisione delle unità di studio nelle medesime classi di equivalenza. In altre parole, cambiando i nomi delle categorie di una variabile misurata su scala nominale, si ottiene una nuova variabile esattamente equivalente alla prima.\n\n\n3.2.2 Scala ordinale\nLa scala ordinale mantiene la caratteristica della scala nominale di classificare ogni unità di misura all’interno di una singola categoria, ma introduce la relazione di ordinamento tra le categorie. In quanto basata su una relazione di ordine, una scala ordinale descrive solo il rango di ordine tra le categorie e non fornisce informazioni sulla distanza tra di esse. Non ci dice, ad esempio, se la distanza tra le categorie \\(a\\) e \\(b\\) è uguale, maggiore o minore della distanza tra le categorie \\(b\\) e \\(c\\).\nUn esempio classico di scala ordinale è quello della scala Mohs per la determinazione della durezza dei minerali. Per stabilire la durezza dei minerali si usa il criterio empirico della scalfittura. Vengono stabiliti livelli di durezza crescente da 1 a 10 con riferimento a dieci minerali: talco, gesso, calcite, fluorite, apatite, ortoclasio, quarzo, topazio, corindone e diamante. Un minerale appartenente ad uno di questi livelli se scalfisce quello di livello inferiore ed è scalfito da quello di livello superiore.\n\n\n\nLa scala di durezza dei minerali di Mohs. Un oggetto è considerato più duro di X se graffia X. Sono incluse anche misure di durezza relativa utilizzando uno sclerometro, da cui emerge la non linearità della scala di Mohs (Burchard, 2004).\n\n\n\n\n3.2.3 Scala ad intervalli\nLa scala ad intervalli di misurazione include le proprietà della scala nominale e della scala ordinale e permette di misurare le distanze tra le coppie di unità statistiche in termini di un intervallo costante, chiamato “unità di misura”, a cui viene attribuito il valore “1”. L’origine della scala, ovvero il punto zero, è scelta arbitrariamente e non indica l’assenza della proprietà che si sta misurando. Ciò significa che la scala ad intervalli consente anche valori negativi e lo zero non viene attribuito all’unità statistica in cui la proprietà risulta assente.\nLa scala ad intervalli equivalenti consente l’esecuzione di operazioni algebriche basate sulla differenza tra i numeri associati ai diversi punti della scala, operazioni algebriche non possibili con le scale di misura nominale o ordinale. Tuttavia, il limite della scala ad intervalli è che non consente di calcolare il rapporto tra coppie di misure. È possibile affermare la differenza tra \\(a\\) e \\(b\\) come la metà della differenza tra \\(c\\) e \\(d\\) o che le due differenze sono uguali, ma non è possibile affermare che \\(a\\) abbia una proprietà misurata in quantità doppia rispetto a \\(b\\). In altre parole, non è possibile stabilire rapporti diretti tra le misure ottenute. Solo le differenze tra le modalità permettono tutte le operazioni aritmetiche, come la somma, l’elevazione a potenza o la divisione, che sono alla base della statistica inferenziale.\nNelle scale ad intervalli equivalenti, l’unità di misura è arbitraria e può essere cambiata attraverso una dilatazione, ovvero la moltiplicazione di tutti i valori della scala per una costante positiva. Inoltre, la traslazione, ovvero l’aggiunta di una costante a tutti i valori della scala, è ammessa poiché non altera le differenze tra i valori della scala. La scala rimane invariata rispetto a traslazioni e dilatazioni e dunque le uniche trasformazioni ammissibili sono le trasformazioni lineari:\n\\[\ny' = a + by, \\quad b &gt; 0.\n\\]\nInfatti, l’uguaglianza dei rapporti fra gli intervalli rimane invariata a seguito di una trasformazione lineare.\nEsempio di scala ad intervalli è la temperatura misurata in gradi Celsius o Fahrenheit, ma non Kelvin. Come per la scala nominale, è possibile stabilire se due modalità sono uguali o diverse: 30\\(^\\circ\\)C \\(\\neq\\) 20\\(^\\circ\\)C. Come per la scala ordinale è possibile mettere due modalità in una relazione d’ordine: 30\\(^\\circ\\)C \\(&gt;\\) 20\\(^\\circ\\)C. In aggiunta ai casi precedenti, però, è possibile definire una unità di misura per cui è possibile dire che tra 30\\(^\\circ\\)C e 20\\(^\\circ\\)C c’è una differenza di 30\\(^\\circ\\) - 20\\(^\\circ\\) = 10\\(^\\circ\\)C. I valori di temperatura, oltre a poter essere ordinati secondo l’intensità del fenomeno, godono della proprietà che le differenze tra loro sono direttamente confrontabili e quantificabili.\nIl limite della scala ad intervalli è quello di non consentire il calcolo del rapporto tra coppie di misure. Ad esempio, una temperatura di 80\\(^\\circ\\)C non è il doppio di una di 40\\(^\\circ\\)C. Se infatti esprimiamo le stesse temperature nei termini della scala Fahrenheit, allora i due valori non saranno in rapporto di 1 a 2 tra loro. Infatti, 20\\(^\\circ\\)C = 68\\(^\\circ\\)F e 40\\(^\\circ\\)C = 104\\(^\\circ\\)F. Questo significa che la relazione “il doppio di” che avevamo individuato in precedenza si applicava ai numeri della scala centigrada, ma non alla proprietà misurata (cioè la temperatura). La decisione di che scala usare (Centigrada vs. Fahrenheit) è arbitraria. Ma questa arbitrarietà non deve influenzare le inferenze che traiamo dai dati. Queste inferenze, infatti, devono dirci qualcosa a proposito della realtà empirica e non possono in nessun modo essere condizionate dalle nostre scelte arbitrarie che ci portano a scegliere la scala Centigrada piuttosto che quella Fahrenheit.\nConsideriamo ora l’aspetto invariante di una trasformazione lineare, ovvero l’uguaglianza dei rapporti fra intervalli. Prendiamo in esame, ad esempio, tre temperature: \\(20^\\circ C = 68^\\circ F\\), \\(15^\\circ C = 59^\\circ F\\), \\(10^\\circ C = 50 ^\\circ F\\).\nÈ facile rendersi conto del fatto che i rapporti fra intervalli restano costanti indipendentemente dall’unità di misura che è stata scelta:\n\\[\n  \\frac{20^\\circ C - 10^\\circ C}{20^\\circ C - 15^\\circ C} =\n  \\frac{68^\\circ F - 50^\\circ F}{68^\\circ F-59^\\circ F} = 2.\n\\]\n\n\n3.2.4 Scala di rapporti\nNella scala a rapporti equivalenti, lo zero non è arbitrario e rappresenta l’elemento che ha intensità nulla rispetto alla proprietà misurata. Per costruire questa scala, si associa il numero 0 all’elemento con intensità nulla e si sceglie un’unità di misura \\(u\\). Ad ogni elemento si assegna un numero \\(a\\) definito come \\(a=d/u\\), dove \\(d\\) rappresenta la distanza dall’origine. In questo modo, i numeri assegnati riflettono le differenze e i rapporti tra le intensità della proprietà misurata.\nIn questa scala, è possibile effettuare operazioni aritmetiche non solo sulle differenze tra i valori della scala, ma anche sui valori stessi della scala. L’unica scelta arbitraria è l’unità di misura, ma lo zero deve sempre rappresentare l’intensità nulla della proprietà considerata.\nLe trasformazioni ammissibili in questa scala sono chiamate trasformazioni di similarità e sono del tipo \\(y' = by\\), dove \\(b&gt;0\\). In questa scala, i rapporti tra i valori rimangono invariati dopo le trasformazioni. In altre parole, se rapportiamo due valori originali e due valori trasformati, il rapporto rimane lo stesso: \\(\\frac{y_i}{y_j} = \\frac{y'_i}{y'_j}\\).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/02_measurement.html#gerarchia-dei-livelli-delle-scale-di-misurazione",
    "href": "chapters/chapter_2/02_measurement.html#gerarchia-dei-livelli-delle-scale-di-misurazione",
    "title": "3  La misurazione in psicologia",
    "section": "3.3 Gerarchia dei livelli delle scale di misurazione",
    "text": "3.3 Gerarchia dei livelli delle scale di misurazione\nSecondo {cite:t}stevens_46, esiste una gerarchia dei livelli delle scale di misurazione, denominati “livelli di scala”. Questi livelli sono organizzati in modo gerarchico, in cui la scala nominale rappresenta il livello più basso della misurazione, mentre la scala a rapporti equivalenti rappresenta il livello più alto. - La scala nominale è il livello più elementare, in cui le categorie o le etichette vengono assegnate agli oggetti o agli individui senza alcuna valutazione di grandezza o ordine. - Al livello successivo si trova la scala ordinale, in cui le categorie sono ordinate in base a una qualche qualità o caratteristica. Qui, è possibile stabilire un ordine di preferenza o gerarchia tra le categorie, ma non è possibile quantificare la differenza tra di esse in modo preciso. - La scala intervallo rappresenta un livello successivo, in cui le categorie sono ordinate e la differenza tra di esse è quantificabile in modo preciso. In questa scala, è possibile effettuare operazioni matematiche come l’addizione e la sottrazione tra i valori, ma non è possibile stabilire un vero e proprio punto zero significativo. - Infine, la scala a rapporti equivalenti rappresenta il livello più alto. In questa scala, le categorie sono ordinate, la differenza tra di esse è quantificabile in modo preciso e esiste un punto zero assoluto che rappresenta l’assenza totale della grandezza misurata. Questo livello di scala permette di effettuare tutte le operazioni matematiche, compresa la moltiplicazione e la divisione.\nPassando da un livello di misurazione ad uno più alto aumenta il numero di operazioni aritmetiche che possono essere compiute sui valori della scala, come indicato nella figura seguente.\n\n\n\nRelazioni tra i livelli di misurazione.\n\n\nPer ciò che riguarda le trasformazioni ammissibili, più il livello di scala è basso, più le funzioni sono generali (sono minori cioè i vincoli per passare da una rappresentazione numerica ad un’altra equivalente). Salendo la gerarchia, la natura delle funzioni di trasformazione si fa più restrittiva.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/02_measurement.html#variabili-discrete-o-continue",
    "href": "chapters/chapter_2/02_measurement.html#variabili-discrete-o-continue",
    "title": "3  La misurazione in psicologia",
    "section": "3.4 Variabili discrete o continue",
    "text": "3.4 Variabili discrete o continue\nLe variabili possono essere classificate come variabili a livello di intervalli o di rapporti e possono essere sia discrete che continue. - Le variabili discrete assumono valori specifici ma non possono assumere valori intermedi. Una volta che l’elenco dei valori accettabili è stato definito, non vi sono casi che si trovano tra questi valori. In genere, le variabili discrete assumono valori interi, come il numero di eventi, il numero di persone o il numero di oggetti. - D’altra parte, le variabili continue possono assumere qualsiasi valore all’interno di un intervallo specificato. Teoricamente, ciò significa che è possibile utilizzare frazioni e decimali per ottenere qualsiasi grado di precisione.\n\n\n\nVariabili discrete e continue.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/02_measurement.html#comprendere-gli-errori-nella-misurazione",
    "href": "chapters/chapter_2/02_measurement.html#comprendere-gli-errori-nella-misurazione",
    "title": "3  La misurazione in psicologia",
    "section": "3.5 Comprendere gli errori nella misurazione",
    "text": "3.5 Comprendere gli errori nella misurazione\nGli errori di misurazione possono essere casuali o sistematici. Gli errori casuali sono fluttuazioni aleatorie, mentre gli errori sistematici sono costanti e derivano da problemi nel metodo di misurazione o negli strumenti.\n\n3.5.1 Precisione e Accuratezza\nLa precisione indica la coerenza tra misurazioni ripetute, mentre l’accuratezza si riferisce alla vicinanza del valore misurato al valore reale. Entrambi i concetti sono cruciali per l’assessment psicometrico.\nUtilizzando l’analogia del tiro al bersaglio, si può avere una serie di colpi vicini tra loro ma lontani dal centro (precisione senza accuratezza) oppure colpi distribuiti in modo sparso ma in media vicini al centro (accuratezza senza precisione).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/02_measurement.html#assessment-psicometrico",
    "href": "chapters/chapter_2/02_measurement.html#assessment-psicometrico",
    "title": "3  La misurazione in psicologia",
    "section": "3.6 Assessment psicometrico",
    "text": "3.6 Assessment psicometrico\nL’assessment psicometrico valuta la qualità delle misurazioni psicologiche, considerando la validità e l’affidabilità.\n\n3.6.1 Validità nella Misurazione Psicologica\nLa validità è una proprietà psicometrica fondamentale dei test psicologici. Secondo gli Standards for Educational and Psychological Testing (2014), la validità si riferisce al grado in cui evidenza e teoria supportano le interpretazioni dei punteggi dei test per gli usi proposti. Questo concetto evidenzia che la validità riguarda sia il significato dei punteggi sia il loro utilizzo, rendendola “la considerazione più fondamentale nello sviluppo e nella valutazione dei test”.\n\n\n3.6.2 Evoluzione del Concetto di Validità\nTradizionalmente, la validità era suddivisa in tre categorie:\n\nValidità di Contenuto: Si riferisce alla corrispondenza tra il contenuto degli item di un test e il dominio dell’attributo psicologico che il test intende misurare. È importante che gli item siano pertinenti e rappresentativi dell’attributo misurato.\nValidità di Criterio: Valuta il grado di concordanza tra i risultati ottenuti tramite lo strumento di misurazione e i risultati ottenuti da altri strumenti che misurano lo stesso costrutto o da un criterio esterno. Include validità concorrente e predittiva.\nValidità di Costrutto: Riguarda il grado in cui un test misura effettivamente il costrutto che si intende misurare. Si suddivide in validità convergente (accordo con strumenti che misurano lo stesso costrutto) e validità divergente (capacità di discriminare tra costrutti diversi).\n\nLa moderna teoria della validità non adotta più questa visione tripartita. Gli Standards del 2014 descrivono la validità come un concetto unitario, dove diverse forme di evidenza concorrono a supportare l’interpretazione dei punteggi del test per il loro utilizzo previsto.\n\n\n3.6.3 Tipologie di Prove di Validità\nGli Standards del 2014 identificano cinque categorie principali di prove di validità:\n\nProve Basate sul Contenuto del Test: Valutano quanto il contenuto del test rappresenti adeguatamente il dominio del costrutto da misurare.\nProve Basate sui Processi di Risposta: Analizzano se i processi cognitivi e comportamentali degli esaminandi riflettono il costrutto valutato.\nProve Basate sulla Struttura Interna: Esaminano la coerenza tra gli elementi del test e la struttura teorica del costrutto. L’analisi fattoriale è uno strumento chiave in questo contesto.\nProve Basate sulle Relazioni con Altre Variabili: Studiano la correlazione tra i punteggi del test e altre variabili teoricamente correlate, utilizzando metodi come la validità convergente e divergente.\nProve Basate sulle Conseguenze del Test: Considerano le implicazioni e gli effetti dell’uso del test, sia intenzionali che non intenzionali.\n\n\n\n3.6.4 Minacce alla Validità\nLa validità può essere compromessa quando un test non misura integralmente il costrutto di interesse (sotto-rappresentazione del costrutto) o quando include varianza estranea al costrutto. Inoltre, fattori esterni come l’ansia o la bassa motivazione degli esaminandi, e deviazioni nelle procedure di amministrazione e valutazione, possono influenzare negativamente la validità delle interpretazioni dei risultati.\n\n\n3.6.5 Integrazione delle Prove di Validità\nLa validità di un test si costruisce attraverso l’integrazione di diverse linee di evidenza. Ogni interpretazione o uso di un test deve essere validato specificamente, richiedendo una valutazione continua e accurata delle prove disponibili. Questo processo implica la costruzione di un argomento di validità che consideri attentamente la qualità tecnica del test e l’adeguatezza delle sue interpretazioni per gli scopi previsti.\nIn conclusione, la validità è un concetto complesso e integrato che richiede un’analisi continua e multidimensionale delle evidenze. La moderna teoria della validità enfatizza l’importanza di considerare diverse forme di evidenza per supportare le interpretazioni dei punteggi dei test, garantendo che siano utilizzati in modo appropriato e significativo. Gli sviluppatori e gli utilizzatori di test devono impegnarsi a valutare costantemente la validità per assicurare misurazioni psicologiche accurate e affidabili.\n\n\n3.6.6 Affidabilità\nL’affidabilità concerne la consistenza e stabilità delle misurazioni, verificata attraverso metodi come l’affidabilità test-retest, inter-rater, intra-rater e l’affidabilità interna.\n\nAffidabilità Test-Retest: Questa forma di affidabilità verifica la consistenza delle misurazioni nel tempo. Se un individuo viene testato in due momenti diversi, i risultati dovrebbero essere simili, assumendo che non ci siano stati cambiamenti significativi nel costrutto misurato.\nAffidabilità Inter-rater: In questo caso, l’affidabilità è determinata dalla concordanza tra le valutazioni di diversi esaminatori. Ad esempio, se più psicologi dovessero valutare un individuo utilizzando lo stesso strumento, le loro valutazioni dovrebbero essere simili.\nAffidabilità Intra-rater: Questa misura dell’affidabilità si riferisce alla consistenza delle valutazioni dello stesso esaminatore in momenti diversi.\nAffidabilità Interna: Si riferisce alla coerenza delle risposte all’interno dello stesso test. Ad esempio, se un test misura un costrutto come l’ansia, gli item che misurano l’ansia dovrebbero correlare positivamente l’uno con l’altro. Un modo comune per valutare l’affidabilità interna è utilizzare il coefficiente \\(\\omega\\) di McDonald.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/02_measurement.html#commenti-e-considerazioni-finali",
    "href": "chapters/chapter_2/02_measurement.html#commenti-e-considerazioni-finali",
    "title": "3  La misurazione in psicologia",
    "section": "3.7 Commenti e considerazioni finali",
    "text": "3.7 Commenti e considerazioni finali\nLa teoria della misurazione è fondamentale nella ricerca empirica per valutare l’attendibilità e la validità delle misurazioni. È cruciale valutare l’errore nella misurazione per garantire la precisione e l’accuratezza delle misure. L’assessment psicometrico si occupa di valutare la qualità delle misurazioni psicologiche, considerando l’affidabilità e la validità per garantire misure accurate dei costrutti teorici. Le moderne tecnologie e metodologie stanno continuamente arricchendo questo campo, offrendo strumenti sempre più raffinati per la comprensione delle caratteristiche psicologiche.\n\n\n\n\nStevens, Stanley Smith. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/03_freq_distr.html",
    "href": "chapters/chapter_2/03_freq_distr.html",
    "title": "4  Distribuzioni di Frequenze",
    "section": "",
    "text": "4.1 Introduzione\nIn questo capitolo, esploreremo le strategie per sintetizzare grandi quantità di dati, soffermandoci su concetti fondamentali come le distribuzioni di frequenza, i quantili e le tecniche di visualizzazione. Discuteremo sia il calcolo che l’interpretazione di queste misure, fornendo strumenti utili per rappresentare graficamente le sintesi di dati. Prima di procedere, è indispensabile leggere l’appendice {ref}sec-appendix-sums per comprendere appieno le operazioni descritte.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuzioni di Frequenze</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/03_freq_distr.html#i-dati-grezzi",
    "href": "chapters/chapter_2/03_freq_distr.html#i-dati-grezzi",
    "title": "4  Distribuzioni di Frequenze",
    "section": "4.2 I dati grezzi",
    "text": "4.2 I dati grezzi\nPer illustrare i principali strumenti della statistica descrittiva, analizzeremo i dati raccolti da Zetsche, Buerkner, and Renneberg (2019) in uno studio che indaga le aspettative negative come meccanismo chiave nel mantenimento della depressione. I ricercatori hanno confrontato 30 soggetti con episodi depressivi con un gruppo di controllo di 37 individui sani, utilizzando il Beck Depression Inventory (BDI-II) per misurare la depressione.\nCarichiamo quindi i dati dal file data.mood.csv.\n\ndf = pd.read_csv(\"../data/data.mood.csv\")\n\nPer conoscere le dimensioni del DataFrame utilizzo il metodo shape().\n\ndf.shape\n\n(1188, 44)\n\n\nIl DataFrame ha 1188 righe e 44 colonne. Visualizzo il nome delle colonne con il metodo .columns.\n\ndf.columns\n\nIndex(['Unnamed: 0', 'vpn_nr', 'esm_id', 'group', 'bildung', 'bdi',\n       'nr_of_episodes', 'nobs_mood', 'trigger_counter', 'form', 'traurig_re',\n       'niedergeschlagen_re', 'unsicher_re', 'nervos_re', 'glucklich_re',\n       'frohlich_re', 'mood_sad.5', 'mood_fearful.5', 'mood_neg.5',\n       'mood_happy.5', 'cesd_sum', 'rrs_sum', 'rrs_brood', 'rrs_reflect',\n       'forecast_sad', 'forecast_fear', 'forecast_neg', 'forecast_happy',\n       'recall_sad', 'recall_fear', 'recall_neg', 'recall_happy',\n       'diff_neg.fore.5', 'diff_sad.fore.5', 'diff_fear.fore.5',\n       'diff_happy.fore.5', 'diff_neg.retro.5', 'diff_sad.retro.5',\n       'diff_fear.retro.5', 'diff_happy.retro.5', 'mood_sad5_tm1',\n       'mood_neg5_tm1', 'mood_fearful5_tm1', 'mood_happy5_tm1'],\n      dtype='object')\n\n\nPer questo esercizio, ci concentriamo sulle colonne esm_id (il codice del soggetto), group (il gruppo) e bdi (il valore BDI-II).\n\ndf = df[[\"esm_id\", \"group\", \"bdi\"]]\ndf.head()\n\n\n\n\n\n\n\n\n\nesm_id\ngroup\nbdi\n\n\n\n\n0\n10\nmdd\n25.0\n\n\n1\n10\nmdd\n25.0\n\n\n2\n10\nmdd\n25.0\n\n\n3\n10\nmdd\n25.0\n\n\n4\n10\nmdd\n25.0\n\n\n\n\n\n\n\n\nRimuoviamo i duplicati per ottenere un unico valore BDI-II per ogni soggetto:\n\ndf = df.drop_duplicates(keep=\"first\")\n\nVerifichiamo di avere ottenuto il risultato desiderato.\n\ndf.shape\n\n(67, 3)\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nesm_id\ngroup\nbdi\n\n\n\n\n0\n10\nmdd\n25.0\n\n\n14\n9\nmdd\n30.0\n\n\n29\n6\nmdd\n26.0\n\n\n45\n7\nmdd\n35.0\n\n\n64\n12\nmdd\n44.0\n\n\n\n\n\n\n\n\nSi noti che il nuovo DataFrame (con 67 righe) conserva il “nome” delle righe (ovvero, l’indice di riga) del DataFrame originario (con 1188 righe). Per esempio, il secondo soggetto (con codice identificativo 9) si trova sulla seconda riga del DataFrame, ma il suo indice di riga è 15. Questo non ha nessuna conseguenza perché non useremo l’indice di riga nelle analisi seguenti.\nEliminiamo eventuali valori mancanti:\n\ndf = df[pd.notnull(df[\"bdi\"])]\n\nOtteniamo così il DataFrame finale per gli scopi presenti (66 righe e 3 colonne):\n\ndf.shape\n\n(66, 3)\n\n\nStampiamo i valori BDI-II presentandoli ordinati dal più piccolo al più grande:\n\nprint(df[\"bdi\"].sort_values())\n\n682     0.0\n455     0.0\n465     0.0\n485     0.0\n540     0.0\n       ... \n190    39.0\n810    41.0\n150    43.0\n135    43.0\n64     44.0\nName: bdi, Length: 66, dtype: float64\n\n\nÈ chiaro dall’elenco precedente che i dati grezzi non sono molto informativi. Nella sezione successiva vedremo come creare una rappresentazione sintetica e comprensibile di questi dati.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuzioni di Frequenze</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/03_freq_distr.html#distribuzioni-di-frequenze",
    "href": "chapters/chapter_2/03_freq_distr.html#distribuzioni-di-frequenze",
    "title": "4  Distribuzioni di Frequenze",
    "section": "4.3 Distribuzioni di frequenze",
    "text": "4.3 Distribuzioni di frequenze\nUna distribuzione di frequenze rappresenta il conteggio delle volte in cui i valori di una variabile si verificano all’interno di un intervallo. Per i nostri dati BDI-II, categorizziamo i punteggi in quattro classi:\n\n0–13: depressione minima\n14–19: depressione lieve-moderata\n20–28: depressione moderata-severa\n29–63: depressione severa\n\nOgni classe \\(\\Delta_i\\) rappresenta un intervallo di valori aperto a destra \\([a_i, b_i)\\) o aperto a sinistra \\((a_i, b_i]\\). Ad ogni classe \\(\\Delta_i\\), con limiti inferiori e superiori \\(a_i\\) e \\(b_i\\), vengono associati un’ampiezza \\(b_i - a_i\\) (che non è necessariamente uguale per ogni classe) e un valore centrale \\(\\bar{x}_i\\). Poiché ogni osservazione \\(x_i\\) appartiene a una sola classe \\(\\Delta_i\\), è possibile calcolare le seguenti quantità.\n\nLa frequenza assoluta \\(n_i\\) di ciascuna classe, ovvero il numero di osservazioni che ricadono nella classe \\(\\Delta_i\\).\n\nProprietà: \\(n_1 + n_2 + \\dots + n_m = n\\).\n\nLa frequenza relativa \\(f_i = n_i/n\\) di ciascuna classe.\n\nProprietà: \\(f_1+f_2+\\dots+f_m =1\\).\n\nLa frequenza cumulata \\(N_i\\), ovvero il numero totale delle osservazioni che ricadono nelle classi fino alla \\(i\\)-esima compresa: \\(N_i = \\sum_{i=1}^m n_i.\\)\nLa frequenza cumulata relativa \\(F_i\\), ovvero \\(F_i = f_1+f_2+\\dots+f_m = \\frac{N_i}{n} = \\frac{1}{n} \\sum_{i=1}^m f_i.\\)\n\n\n4.3.1 Frequenze Assolute e Relative\nPer ottenere la distribuzione di frequenza assoluta e relativa dei valori BDI-II nel dataset di zetsche_2019future, è necessario prima aggiungere al DataFrame df una colonna che contenga una variabile categoriale che classifichi ciascuna osservazione in una delle quattro classi che descrivono la gravità della depressione. Questo risultato si ottiene con il metodo pandas.cut().\nIn pandas.cut(), il primo argomento x è un array unidimensionale (lista python, numpy.ndarray o pandas.Series) che contiene i dati e il secondo argomento bins specifica gli intervalli delle classi. La funzione restituisce un array che specifica la classe di appartenenza di ogni elemento dell’array x. L’argomento include_lowest=True specifica classi chiuse a destra (nel nostro caso è irrilevante dato che nessuna osservazione coincide con il limite di una classe).\n\n4.3.1.1 Frequenze assolute\n\ndf[\"bdi_class\"] = pd.cut(df[\"bdi\"], bins=[0, 13.5, 19.5, 28.5, 63], include_lowest=True)\ndf[\"bdi_class\"].value_counts()\n\nbdi_class\n(-0.001, 13.5]    36\n(28.5, 63.0]      17\n(19.5, 28.5]      12\n(13.5, 19.5]       1\nName: count, dtype: int64\n\n\n\n\n4.3.1.2 Frequenze relative\n\nabs_freq = pd.crosstab(index=df[\"bdi_class\"], columns=[\"Abs. freq.\"])\nrel_freq = abs_freq / abs_freq.sum()\nrel_freq = rel_freq.round(2)\nrel_freq\n\n\n\n\n\n\n\n\ncol_0\nAbs. freq.\n\n\nbdi_class\n\n\n\n\n\n(-0.001, 13.5]\n0.55\n\n\n(13.5, 19.5]\n0.02\n\n\n(19.5, 28.5]\n0.18\n\n\n(28.5, 63.0]\n0.26\n\n\n\n\n\n\n\n\nControlliamo\n\nrel_freq.sum()\n\ncol_0\nAbs. freq.    1.01\ndtype: float64\n\n\n\ngrp_freq = pd.crosstab(index=df[\"group\"], columns=[\"Abs. freq.\"], colnames=[\"\"])\ngrp_freq\n\n\n\n\n\n\n\n\n\nAbs. freq.\n\n\ngroup\n\n\n\n\n\nctl\n36\n\n\nmdd\n30\n\n\n\n\n\n\n\n\nVolendo modificare tale ordine è possibile accedere al DataFrame tramite loc e specificando come secondo argomento una lista dei valori nell’ordine desiderato:\n\ngrp_freq.loc[[\"mdd\", \"ctl\"], :]\n\n\n\n\n\n\n\n\n\nAbs. freq.\n\n\ngroup\n\n\n\n\n\nmdd\n30\n\n\nctl\n36\n\n\n\n\n\n\n\n\nIn Python, il simbolo : utilizzato all’interno delle parentesi quadre permette di ottenere uno slicing corrispondente all’intera lista.\n\n\n\n4.3.2 Distribuzioni congiunte\nLe variabili possono anche essere analizzate insieme tramite le distribuzioni congiunte di frequenze. Queste distribuzioni rappresentano l’insieme delle frequenze assolute o relative ad ogni possibile combinazione di valori delle variabili. Ad esempio, se l’insieme di variabili \\(V\\) è composto da due variabili, \\(X\\) e \\(Y\\), ciascuna delle quali può assumere due valori, 1 e 2, allora una possibile distribuzione congiunta di frequenze relative per \\(V\\) potrebbe essere espressa come \\(f(X = 1, Y = 1) = 0.2\\), \\(f(X = 1, Y = 2) = 0.1\\), \\(f(X = 2, Y = 1) = 0.5\\), e \\(f(X = 2, Y = 2) = 0.2\\). Come nel caso delle distribuzioni di frequenze relative di una singola variabile, le frequenze relative di una distribuzione congiunta devono sommare a 1.\nPer i dati dell’esempio precedente, la funzione pd.crosstab può essere utilizzata anche per produrre questo tipo di tabella: basta indicare le serie corrispondenti alle variabili considerate come valori degli argomenti index e columns.\n\nbdi_group_abs_freq = pd.crosstab(index=df[\"bdi_class\"], columns=df[\"group\"])\nbdi_group_abs_freq\n\n\n\n\n\n\n\n\ngroup\nctl\nmdd\n\n\nbdi_class\n\n\n\n\n\n\n(-0.001, 13.5]\n36\n0\n\n\n(13.5, 19.5]\n0\n1\n\n\n(19.5, 28.5]\n0\n12\n\n\n(28.5, 63.0]\n0\n17\n\n\n\n\n\n\n\n\nOppure:\n\nbdi_group_rel_freq = pd.crosstab(index=df[\"bdi_class\"], columns=df[\"group\"], normalize=True)\nbdi_group_rel_freq\n\n\n\n\n\n\n\n\ngroup\nctl\nmdd\n\n\nbdi_class\n\n\n\n\n\n\n(-0.001, 13.5]\n0.545455\n0.000000\n\n\n(13.5, 19.5]\n0.000000\n0.015152\n\n\n(19.5, 28.5]\n0.000000\n0.181818\n\n\n(28.5, 63.0]\n0.000000\n0.257576\n\n\n\n\n\n\n\n\nInvocando il metodo plot.bar sulla tabella, otteniamo un grafico a barre nel quale le barre relative a uno stesso valore bdi_class risultino affiancate. Nel caso presente, le due distribuzioni sono completamente separate, quindi non abbiamo mai due barre affiancate:\n\nbdi_group_rel_freq.plot.bar();",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuzioni di Frequenze</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/03_freq_distr.html#istogramma",
    "href": "chapters/chapter_2/03_freq_distr.html#istogramma",
    "title": "4  Distribuzioni di Frequenze",
    "section": "4.4 Istogramma",
    "text": "4.4 Istogramma\nUn istogramma rappresenta graficamente una distribuzione di frequenze. Un istogramma mostra sulle ascisse i limiti delle classi \\(\\Delta_i\\) e sulle ordinate la densità della frequenza relativa della variabile \\(X\\) nella classe \\(\\Delta_i\\). La densità della frequenza relativa è misurata dalla funzione costante a tratti \\(\\varphi_n(x)= \\frac{f_i}{b_i-a_i}\\), dove \\(f_i\\) è la frequenza relativa della classe \\(\\Delta_i\\) e \\(b_i - a_i\\) rappresenta l’ampiezza della classe. In questo modo, l’area del rettangolo associato alla classe \\(\\Delta_i\\) sull’istogramma sarà proporzionale alla frequenza relativa \\(f_i\\). È importante notare che l’area totale dell’istogramma delle frequenze relative è uguale a 1.0, poiché rappresenta la somma delle aree dei singoli rettangoli.\nPer fare un esempio, costruiamo un istogramma per i valori BDI-II di {cite:t}zetsche_2019future. Con i quattro intervalli individuati dai cut-off del BDI-II creo una prima versione dell’istogramma – si notino le frequenze assolute sull’asse delle ordinate.\n\nplt.hist(df[\"bdi\"], bins=[0, 13.5, 19.5, 28.5, 63], density=True)\nplt.xlabel(\"BDI\")\nplt.ylabel(\"Density\")\nplt.title(\"Density Histogram of BDI Scores\")\nplt.show()\n\n\n\n\n\n\n\n\nAnche se nel caso presente è sensato usare ampiezze diverse per gli intervalli delle classi, in generale gli istogrammi si costruiscono utilizzando intervalli riportati sulle ascisse con un’ampiezza uguale.\n\nplt.hist(df[\"bdi\"], density=True)\nplt.xlabel(\"BDI\")\nplt.ylabel(\"Density\")\nplt.title(\"Density Histogram of BDI Scores\")\nplt.show()",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuzioni di Frequenze</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/03_freq_distr.html#kernel-density-plot",
    "href": "chapters/chapter_2/03_freq_distr.html#kernel-density-plot",
    "title": "4  Distribuzioni di Frequenze",
    "section": "4.5 Kernel density plot",
    "text": "4.5 Kernel density plot\nConfrontando le due figure precedenti, emerge chiaramente una limitazione dell’istogramma: la sua forma dipende dall’arbitrarietà con cui vengono scelti il numero e l’ampiezza delle classi, rendendo difficile interpretare correttamente la distribuzione dei dati.\nPer superare questa difficoltà, possiamo utilizzare una tecnica alternativa chiamata stima della densità kernel (KDE). Mentre l’istogramma utilizza barre per rappresentare i dati, la KDE crea un profilo smussato che fornisce una visione più continua e meno dipendente dall’arbitrarietà delle classi.\nImmaginiamo un istogramma con classi di ampiezza molto piccola, tanto da avere una curva continua invece di barre discrete. Questo è ciò che fa la KDE: smussa il profilo dell’istogramma per ottenere una rappresentazione continua dei dati. Invece di utilizzare barre, la KDE posiziona una piccola curva (detta kernel) su ogni osservazione nel dataset. Queste curve possono essere gaussiane (a forma di campana) o di altro tipo. Ogni kernel ha un’altezza e una larghezza determinate da parametri di smussamento (o bandwidth), che controllano quanto deve essere larga e alta la curva. Tutte le curve kernel vengono sommate per creare una singola curva complessiva. Questa curva rappresenta la densità dei dati, mostrando come i dati sono distribuiti lungo il range dei valori.\nLa curva risultante dal KDE mostra la proporzione di casi per ciascun intervallo di valori. L’area sotto la curva in un determinato intervallo rappresenta la proporzione di casi della distribuzione che ricadono in quell’intervallo. Per esempio, se un intervallo ha un’area maggiore sotto la curva rispetto ad altri, significa che in quell’intervallo c’è una maggiore concentrazione di dati.\nLa curva di densità ottenuta tramite KDE fornisce dunque un’idea chiara di come i dati sono distribuiti senza dipendere dall’arbitrarietà della scelta delle classi dell’istogramma.\nCrediamo un kernel density plot per ciascuno dei due gruppi di valori BDI-II riportati da {cite:t}zetsche_2019future.\n\nsns.kdeplot(data=df, x=\"bdi\", hue=\"group\", common_norm=False)\nplt.xlabel(\"BDI-II\")\nplt.ylabel(\"Density\")\nplt.title(\"Density Histogram of BDI-II Scores\")\nplt.show()",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuzioni di Frequenze</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/03_freq_distr.html#forma-di-una-distribuzione",
    "href": "chapters/chapter_2/03_freq_distr.html#forma-di-una-distribuzione",
    "title": "4  Distribuzioni di Frequenze",
    "section": "4.6 Forma di una distribuzione",
    "text": "4.6 Forma di una distribuzione\nIn generale, la forma di una distribuzione descrive come i dati si distribuiscono intorno ai valori centrali. Distinguiamo tra distribuzioni simmetriche e asimmetriche, e tra distribuzioni unimodali o multimodali. Un’illustrazione grafica è fornita nella figura seguente. Nel pannello 1 la distribuzione è unimodale con asimmetria negativa; nel pannello 2 la distribuzione è unimodale con asimmetria positiva; nel pannello 3 la distribuzione è simmetrica e unimodale; nel pannello 4 la distribuzione è bimodale.\nrprsdty ../images/shape_distribution.png :alt: shape :class: bg-primary mb-1 :width: 420px :align: center\n\nIl kernel density plot dei valori BDI-II nel campione di {cite:t}zetsche_2019future è bimodale. Ciò indica che le osservazioni della distribuzione si addensano in due cluster ben distinti: un gruppo di osservazioni tende ad avere valori BDI-II bassi, mentre l’altro gruppo tende ad avere BDI-II alti. Questi due cluster di osservazioni corrispondono al gruppo di controllo e al gruppo clinico nel campione di dati esaminato da {cite:t}zetsche_2019future.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuzioni di Frequenze</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/03_freq_distr.html#indici-di-posizione",
    "href": "chapters/chapter_2/03_freq_distr.html#indici-di-posizione",
    "title": "4  Distribuzioni di Frequenze",
    "section": "4.7 Indici di posizione",
    "text": "4.7 Indici di posizione\n\n4.7.1 Quantili\nLa distribuzione dei valori BDI-II di {cite:t}zetsche_2019future può essere sintetizzata attraverso l’uso dei quantili, che sono valori caratteristici che suddividono i dati in parti ugualmente numerose. I quartili sono tre quantili specifici: il primo quartile, \\(q_1\\), divide i dati in due parti, lasciando a sinistra il 25% del campione; il secondo quartile, \\(q_2\\), corrisponde alla mediana e divide i dati in due parti uguali; il terzo quartile lascia a sinistra il 75% del campione.\nInoltre, ci sono altri indici di posizione chiamati decili e percentili che suddividono i dati in parti di dimensioni uguali a 10% e 1%, rispettivamente.\nPer calcolare i quantili, i dati vengono prima ordinati in modo crescente e poi viene determinato il valore di \\(np\\), dove \\(n\\) è la dimensione del campione e \\(p\\) è l’ordine del quantile. Se \\(np\\) non è un intero, il valore del quantile corrisponde al valore del dato che si trova alla posizione successiva alla parte intera di \\(np\\). Se \\(np\\) è un intero, il valore del quantile corrisponde alla media dei dati nelle posizioni \\(k\\) e \\(k+1\\), dove \\(k\\) è la parte intera di \\(np\\).\nGli indici di posizione possono essere utilizzati per creare un box-plot, una rappresentazione grafica della distribuzione dei dati che è molto popolare e può essere utilizzata in alternativa ad un istogramma.\nAd esempio, per calcolare la mediana della distribuzione dei nove soggetti con un unico episodio di depressione maggiore del campione clinico di Zetsche et al. (2019), si determina il valore di \\(np = 9 \\cdot 0.5 = 4.5\\), che non è un intero. Pertanto, il valore del secondo quartile è pari al valore del dato che si trova alla posizione successiva alla parte intera di \\(np\\), ovvero \\(q_2 = x_{4 + 1} = 27\\). Per calcolare il quantile di ordine \\(2/3\\), si determina il valore di \\(np = 9 \\cdot 2/3 = 6\\), che è un intero. Quindi, il valore del quantile corrisponde alla media dei dati nelle posizioni \\(6\\) e \\(7\\), ovvero \\(q_{\\frac{2}{3}} = \\frac{1}{2} (x_{6} + x_{7}) = \\frac{1}{2} (33 + 33) = 33\\).\nUsiamo numpy per trovare la soluzione dell’esercizio precedente.\n\nx = [19, 26, 27, 28, 28, 33, 33, 41, 43]\nnp.quantile(x, 2 / 3)\n\n33.0",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuzioni di Frequenze</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/03_freq_distr.html#mostrare-i-dati",
    "href": "chapters/chapter_2/03_freq_distr.html#mostrare-i-dati",
    "title": "4  Distribuzioni di Frequenze",
    "section": "4.8 Mostrare i dati",
    "text": "4.8 Mostrare i dati\n\n4.8.1 Diagramma a scatola\nIl box plot è uno strumento grafico che visualizza la dispersione di una distribuzione. Per creare un box plot, si disegna un rettangolo (la “scatola”) di altezza arbitraria, basato sulla distanza interquartile (IQR), che corrisponde alla differenza tra il terzo quartile (\\(q_{0.75}\\)) e il primo quartile (\\(q_{0.25}\\)). La mediana (\\(q_{0.5}\\)) è rappresentata da una linea all’interno del rettangolo.\nAi lati della scatola, vengono tracciati due segmenti di retta, detti “baffi”, che rappresentano i valori adiacenti inferiore e superiore. Il valore adiacente inferiore è il valore più basso tra le osservazioni che è maggiore o uguale al primo quartile meno 1.5 volte la distanza interquartile. Il valore adiacente superiore è il valore più alto tra le osservazioni che è minore o uguale al terzo quartile più 1.5 volte la distanza interquartile.\nSe ci sono dei valori che cadono al di fuori dei valori adiacenti, vengono chiamati “valori anomali” e sono rappresentati individualmente nel box plot per evidenziare la loro presenza e posizione. In questo modo, il box plot fornisce una rappresentazione visiva della distribuzione dei dati, permettendo di individuare facilmente eventuali valori anomali e di comprendere la dispersione dei dati.\nrprsdty ../images/boxplot.png :alt: fishy :class: bg-primary mb-1 :width: 640px :align: center\n\nUtilizziamo un box-plot per rappresentare graficamente la distribuzione dei punteggi BDI-II nel gruppo dei pazienti e nel gruppo di controllo.\n\nsns.boxplot(x=\"group\", y=\"bdi\", data=df)\nplt.xlabel(\"Group\")\nplt.ylabel(\"BDI-II\")\nplt.title(\"Boxplot of BDI-II Scores by Group\")\nplt.show()\n\n\n\n\n\n\n\n\nUn risultato migliore si ottiene usando un violin plot e mostrando anche i dati grezzi.\n\n\n4.8.2 Grafico a violino\nI violin plot combinano box plot e KDE plot per una rappresentazione più dettagliata. Al grafico sono sovrapposti i dati grezzi.\n\nsns.violinplot(x=\"group\", y=\"bdi\", data=df, color=\"lightgray\")\nsns.stripplot(x=\"group\", y=\"bdi\", data=df, color=\"black\", size=5, jitter=True, alpha=0.3)\nplt.ylabel(\"BDI-II\")\nplt.xlabel(\"Group\")\nplt.title(\"Violin Plot with Overlay of Individual Data Points of BDI-II Scores by Group\")\nplt.show()",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuzioni di Frequenze</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/03_freq_distr.html#commenti-e-considerazioni-finali",
    "href": "chapters/chapter_2/03_freq_distr.html#commenti-e-considerazioni-finali",
    "title": "4  Distribuzioni di Frequenze",
    "section": "4.9 Commenti e considerazioni finali",
    "text": "4.9 Commenti e considerazioni finali\nAbbiamo esplorato diverse tecniche per sintetizzare e visualizzare i dati, includendo distribuzioni di frequenze, istogrammi e grafici di densità. Questi strumenti sono essenziali per comprendere meglio i dati e presentare risultati in modo chiaro e informativo.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuzioni di Frequenze</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/03_freq_distr.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_2/03_freq_distr.html#informazioni-sullambiente-di-sviluppo",
    "title": "4  Distribuzioni di Frequenze",
    "section": "4.10 Informazioni sull’Ambiente di Sviluppo",
    "text": "4.10 Informazioni sull’Ambiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m\n\nLast updated: Mon Jul 22 2024\n\nPython implementation: CPython\nPython version       : 3.12.4\nIPython version      : 8.26.0\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.5.0\nMachine     : arm64\nProcessor   : arm\nCPU cores   : 8\nArchitecture: 64bit\n\narviz     : 0.18.0\nnumpy     : 1.26.4\nseaborn   : 0.13.2\nmatplotlib: 3.9.1\npandas    : 2.2.2\n\nWatermark: 2.4.3\n\n\n\n\n\n\n\nZetsche, Ulrike, Paul-Christian Buerkner, and Babette Renneberg. 2019. “Future Expectations in Clinical Depression: Biased or Realistic?” Journal of Abnormal Psychology 128 (7): 678.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuzioni di Frequenze</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/04_loc_scale.html",
    "href": "chapters/chapter_2/04_loc_scale.html",
    "title": "5  Indici di posizione e di scala",
    "section": "",
    "text": "5.1 Introduzione\nLa visualizzazione grafica dei dati rappresenta il pilastro fondamentale di ogni analisi quantitativa. Grazie alle rappresentazioni grafiche adeguate, è possibile individuare importanti caratteristiche di una distribuzione, quali la simmetria o l’asimmetria, nonché la presenza di una o più mode. Successivamente, al fine di descrivere sinteticamente le principali caratteristiche dei dati, si rende necessario l’utilizzo di specifici indici numerici. In questo capitolo, verranno presentati i principali indicatori della statistica descrittiva.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Indici di posizione e di scala</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/04_loc_scale.html#indici-di-tendenza-centrale",
    "href": "chapters/chapter_2/04_loc_scale.html#indici-di-tendenza-centrale",
    "title": "5  Indici di posizione e di scala",
    "section": "5.2 Indici di tendenza centrale",
    "text": "5.2 Indici di tendenza centrale\nGli indici di tendenza centrale sono misure statistiche che cercano di rappresentare un valore tipico o centrale all’interno di un insieme di dati. Sono utilizzati per ottenere una comprensione immediata della distribuzione dei dati senza dover analizzare l’intero insieme. Gli indici di tendenza centrale sono fondamentali nell’analisi statistica, in quanto forniscono una sintesi semplice e comprensibile delle caratteristiche principali di un insieme di dati. I principali indici di tendenza centrale sono:\n\nMedia: La media è la somma di tutti i valori divisa per il numero totale di valori. È spesso utilizzata come misura generale di tendenza centrale, ma è sensibile agli estremi (valori molto alti o molto bassi).\nMediana: La mediana è il valore che divide l’insieme di dati in due parti uguali. A differenza della media, non è influenzata da valori estremi ed è quindi più robusta in presenza di outlier.\nModa: La moda è il valore che appare più frequentemente in un insieme di dati. In alcuni casi, può non essere presente o esserci più di una moda.\n\nLa scelta dell’indice di tendenza centrale appropriato dipende dalla natura dei dati e dall’obiettivo dell’analisi. Ad esempio, la mediana potrebbe essere preferita alla media se l’insieme di dati contiene valori anomali che potrebbero distorcere la rappresentazione centrale. La conoscenza e l’applicazione corretta di questi indici possono fornire una preziosa intuizione sulle caratteristiche centrali di una distribuzione di dati.\n\n5.2.1 Media\nLa media aritmetica di un insieme di valori rappresenta il punto centrale o il baricentro della distribuzione dei dati. È calcolata come la somma di tutti i valori divisa per il numero totale di valori, ed è espressa dalla formula:\n\\[\n\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i,\n\\tag{5.1}\\]\ndove \\(x_i\\) rappresenta i valori nell’insieme, \\(n\\) è il numero totale di valori, e \\(\\sum\\) indica la sommatoria.\n\n5.2.1.1 Proprietà della media\nUna proprietà fondamentale della media è che la somma degli scarti di ciascun valore dalla media è zero:\n\\[\n\\sum_{i=1}^n (x_i - \\bar{x}) = 0.\\notag\n\\tag{5.2}\\]\nInfatti,\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n (x_i - \\bar{x}) &= \\sum_i x_i - \\sum_i \\bar{x}\\notag\\\\\n&= \\sum_i x_i - n \\bar{x}\\notag\\\\\n&= \\sum_i x_i - \\sum_i x_i = 0.\\notag\n\\end{aligned}\n\\]\nQuesta proprietà implica che i dati sono equamente distribuiti intorno alla media.\n\n\n5.2.1.2 La media come centro di gravità dell’istogramma\nLa media aritmetica può essere interpretata come il centro di gravità o il punto di equilibrio della distribuzione dei dati. In termini fisici, il centro di gravità è il punto in cui la massa di un sistema è equilibrata o concentrata.\nIn termini statistici, possiamo considerare la media come il punto in cui la distribuzione dei dati è in equilibrio. Ogni valore dell’insieme di dati può essere visto come un punto materiale con una massa proporzionale al suo valore. Se immaginiamo questi punti disposti su una linea, con valori più grandi a destra e più piccoli a sinistra, la media corrisponderà esattamente al punto in cui la distribuzione sarebbe in equilibrio.\n\n\n5.2.1.3 Principio dei minimi quadrati\nLa posizione della media minimizza la somma delle distanze quadrate dai dati, un principio noto come “metodo dei minimi quadrati”. Matematicamente, questo si traduce nel fatto che la somma dei quadrati degli scarti tra ciascun valore e la media è minima. Questo principio è alla base dell’analisi statistica dei modelli di regressione e conferma l’interpretazione della media come centro di gravità dell’istogramma.\n\n\n5.2.1.4 Calcolo della media con NumPy\nPer calcolare la media di un piccolo numero di valori in Python, possiamo utilizzare la somma di questi valori e dividerla per il numero totale di elementi. Consideriamo ad esempio i valori 12, 44, 21, 62, 24:\n\n(12 + 44 + 21 + 62 + 24) / 5\n\n32.6\n\n\novvero\n\nx = np.array([12, 44, 21, 62, 24])\nnp.mean(x)\n\n32.6\n\n\n\nnp.average(x)\n\n32.6\n\n\n\n\n5.2.1.5 Le proporzioni sono medie\nSe una collezione consiste solo di uni e zeri, allora la somma della collezione è il numero di uni in essa, e la media della collezione è la proporzione di uni.\n\nzero_one = np.array([1, 1, 1, 0])\nresult = sum(zero_one)\nprint(result) \n\n3\n\n\n\nnp.mean(zero_one)\n\n0.75\n\n\nÈ possibile sostituire 1 con il valore booleano True e 0 con False:\n\nnp.mean(np.array([(True, True, True, False)]))\n\n0.75\n\n\n\n\n5.2.1.6 Limiti della media aritmetica\nLa media aritmetica, tuttavia, ha alcune limitazioni: non sempre è l’indice più adeguato per descrivere accuratamente la tendenza centrale della distribuzione, specialmente quando si verificano asimmetrie o valori anomali (outlier). In queste situazioni, è più indicato utilizzare la mediana o la media spuntata (come spiegheremo successivamente).\n\n\n5.2.1.7 Medie per gruppi\nMolto spesso però i nostri dati sono contenuti in file e inserire i dati manualmente non è fattibile. Per fare un esempio, considereremo i dati del Progetto STAR, contenuti nel file STAR.csv, che rappresentano un’importante indagine sulle prestazioni degli studenti in relazione alla dimensione delle classi. Negli anni ’80, i legislatori del Tennessee considerarono la possibilità di ridurre le dimensioni delle classi per migliorare il rendimento degli studenti. Al fine di prendere decisioni informate, commissionarono lo studio multimilionario “Progetto Student-Teacher Achievement Ratio” (Project STAR). Lo studio coinvolgeva bambini della scuola materna assegnati casualmente a classi piccole, con 13-17 studenti, o classi di dimensioni regolari, con 22-25 studenti, fino alla fine della terza elementare. I ricercatori hanno seguito il progresso degli studenti nel tempo, concentrandosi su variabili di risultato, come i punteggi dei test standardizzati di lettura (reading) e matematica (math) alla terza elementare, oltre ai tassi di diploma di scuola superiore (graduated, con valore 1 per sì e 0 per no).\nPoniamoci il problema di calcolare la media dei punteggi math calcolata separatamente per i due gruppi di studenti: coloro che hanno completato la scuola superiore e coloro che non l’hanno completata.\nProcediamo all’importazione dei dati per iniziare l’analisi.\n\ndf = pd.read_csv(\"../../data/STAR.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\nclasstype\nreading\nmath\ngraduated\n\n\n\n\n0\nsmall\n578\n610\n1\n\n\n1\nregular\n612\n612\n1\n\n\n2\nregular\n583\n606\n1\n\n\n3\nsmall\n661\n648\n1\n\n\n4\nsmall\n614\n636\n1\n\n\n\n\n\n\n\n\nEsaminiamo la numerosità di ciascun gruppo.\n\ndf.groupby(\"graduated\").size()\n\ngraduated\n0     166\n1    1108\ndtype: int64\n\n\nOra procediamo al calcolo delle medie dei punteggi math all’interno dei due gruppi. Per rendere la risposta più concisa, useremo la funzione round() per stampare solo 2 valori decimali.\n\ndf.groupby(\"graduated\")[\"math\"].mean().round(2)\n\ngraduated\n0    606.64\n1    635.33\nName: math, dtype: float64\n\n\nIn alternativa, possiamo usare il metodo .describe():\n\ndf.groupby(\"graduated\")[\"math\"].describe().round(1)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\ngraduated\n\n\n\n\n\n\n\n\n\n\n\n\n0\n166.0\n606.6\n34.1\n526.0\n580.5\n606.0\n629.0\n711.0\n\n\n1\n1108.0\n635.3\n38.1\n515.0\n609.5\n634.0\n659.0\n774.0\n\n\n\n\n\n\n\n\n\n\n\n5.2.2 Media spuntata\nLa media spuntata, indicata come \\(\\bar{x}_t\\) o trimmed mean, è un metodo di calcolo della media che prevede l’eliminazione di una determinata percentuale di dati estremi prima di effettuare la media aritmetica. Solitamente, viene eliminato il 10% dei dati, ovvero il 5% all’inizio e alla fine della distribuzione. Per ottenere la media spuntata, i dati vengono ordinati in modo crescente, \\(x_1 \\leq x_2 \\leq x_3 \\leq \\dots \\leq x_n\\), e quindi viene eliminato il primo 5% e l’ultimo 5% dei dati nella sequenza ordinata. Infine, la media spuntata è calcolata come la media aritmetica dei dati rimanenti. Questo approccio è utile quando ci sono valori anomali o quando la distribuzione è asimmetrica e la media aritmetica non rappresenta adeguatamente la tendenza centrale dei dati.\nA titolo di esempio, procediamo al calcolo della media spuntata dei valori math per i due gruppi definiti dalla variabile graduated, escludendo il 10% dei valori più estremi.\n\nnot_graduated = df[df[\"graduated\"] == 0].math\nstats.trim_mean(not_graduated, 0.10)\n\n605.6492537313433\n\n\n\ngraduated = df[df[\"graduated\"] == 1].math\nstats.trim_mean(graduated, 0.10)\n\n634.4403153153153\n\n\n\n\n5.2.3 Quantili\nIl quantile non interpolato di ordine \\(p\\) \\((0 &lt; p &lt; 1)\\) rappresenta il valore che divide la distribuzione dei dati in modo tale che una frazione \\(p\\) dei dati si trovi al di sotto di esso.\nLa formula per calcolare il quantile non interpolato è la seguente:\n\\[\n    q_p = x_{(k)},\n\\]\ndove \\(x_{(k)}\\) è l’elemento \\(k\\)-esimo nell’insieme di dati ordinato in modo crescente, e \\(k\\) è calcolato come:\n\\[\nk = \\lceil p \\cdot n \\rceil,\n\\]\ndove \\(n\\) è il numero totale di dati nel campione, e \\(\\lceil \\cdot \\rceil\\) rappresenta la funzione di arrotondamento all’intero successivo. In questa definizione, il quantile non interpolato corrisponde al valore effettivo nell’insieme di dati, senza effettuare alcuna interpolazione tra i valori circostanti.\nAd esempio, consideriamo il seguente insieme di dati: \\(\\{ 15, 20, 23, 25, 28, 30, 35, 40, 45, 50 \\}\\). Supponiamo di voler calcolare il quantile non interpolato di ordine \\(p = 0.3\\) (cioè il 30° percentile).\nOrdiniamo i dati in modo crescente: \\(\\{ 15, 20, 23, 25, 28, 30, 35, 40, 45, 50 \\}.\\) Calcoliamo \\(k\\) utilizzando la formula \\(k = \\lceil p \\cdot n \\rceil\\), dove \\(n\\) è il numero totale di dati nel campione. Nel nostro caso, \\(n = 10\\) e \\(p = 0.3\\):\n\\[\nk = \\lceil 0.3 \\cdot 10 \\rceil = \\lceil 3 \\rceil = 3.\n\\]\nIl quantile non interpolato corrisponde al valore \\(x_{(k)}\\), ovvero l’elemento \\(k\\)-esimo nell’insieme ordinato: \\(q_{0.3} = x_{(3)} = 23.\\)\nOltre al quantile non interpolato, esiste anche il concetto di quantile interpolato. A differenza del quantile non interpolato, il quantile interpolato può essere calcolato anche per percentili che non corrispondono esattamente a valori presenti nell’insieme di dati. Per ottenere il valore del quantile interpolato, viene utilizzato un procedimento di interpolazione lineare tra i valori adiacenti. In genere, il calcolo del quantile interpolato viene eseguito mediante l’uso di software dedicati.\nOra, procediamo al calcolo dei quantili di ordine 0.10 e 0.90 per i valori math all’interno dei due gruppi. I quantili sono dei valori che dividono la distribuzione dei dati in parti specifiche. Ad esempio, il quantile di ordine 0.10 corrisponde al valore al di sotto del quale si trova il 10% dei dati, mentre il quantile di ordine 0.90 rappresenta il valore al di sotto del quale si trova il 90% dei dati.\nCalcoliamo i quantili di ordine 0.1 e 0.9 della distribuzione dei punteggi math nei due gruppi definiti dalla variabile graduated.\n\n# Quantili di ordine 0.1 e 0.9 per il gruppo di studenti che hanno completato la scuola superiore\n[\n    df[df[\"graduated\"] == 1][\"math\"].quantile(0.1), \n    df[df[\"graduated\"] == 1][\"math\"].quantile(0.9)\n]\n\n[588.0, 684.0]\n\n\n\n# Quantili di ordine 0.1 e 0.9 per il gruppo di studenti che non hanno completato la scuola superiore\n[\n    df[df[\"graduated\"] == 0][\"math\"].quantile(0.1),\n    df[df[\"graduated\"] == 0][\"math\"].quantile(0.9),\n]\n\n[564.5, 651.0]\n\n\n\n\n5.2.4 Moda e mediana\nIn precedenza abbiamo già incontrato altri due popolari indici di tendenza centrale: la moda (Mo), che rappresenta il valore centrale della classe con la frequenza massima (in alcune distribuzioni può esserci più di una moda, rendendola multimodale e facendo perdere a questo indice il suo significato di indicatore di tendenza centrale); e la mediana (\\(\\tilde{x}\\)), che rappresenta il valore corrispondente al quantile di ordine 0.5 della distribuzione.\n\n\n5.2.5 Quando usare media, moda, mediana\nLa moda può essere utilizzata per dati a livello nominale o ordinale ed è l’unica tra le tre statistiche che può essere calcolata in questi casi.\nLa media, d’altra parte, è una buona misura di tendenza centrale solo se la distribuzione dei dati è simmetrica, ossia se i valori sono distribuiti uniformemente a sinistra e a destra della media. Tuttavia, se ci sono valori anomali o se la distribuzione è asimmetrica, la media può essere influenzata in modo significativo e, pertanto, potrebbe non essere la scelta migliore come misura di tendenza centrale.\nIn queste situazioni, la mediana può fornire una misura migliore di tendenza centrale rispetto alla media poiché è meno influenzata dai valori anomali e si basa esclusivamente sul valore centrale dell’insieme di dati. Di conseguenza, la scelta tra media e mediana dipende dal tipo di distribuzione dei dati e dagli obiettivi dell’analisi.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Indici di posizione e di scala</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/04_loc_scale.html#indici-di-dispersione",
    "href": "chapters/chapter_2/04_loc_scale.html#indici-di-dispersione",
    "title": "5  Indici di posizione e di scala",
    "section": "5.3 Indici di dispersione",
    "text": "5.3 Indici di dispersione\nLe misure di posizione descritte in precedenza, come le medie e gli indici di posizione, offrono una sintesi dei dati mettendo in evidenza la tendenza centrale delle osservazioni. Tuttavia, trascurano un aspetto importante della distribuzione dei dati: la variabilità dei valori numerici della variabile statistica. Pertanto, è essenziale completare la descrizione della distribuzione di una variabile statistica utilizzando anche indicatori che valutino la dispersione delle unità statistiche. In questo modo, otterremo una visione più completa e approfondita delle caratteristiche del campione analizzato.\n\n5.3.1 Indici basati sull’ordinamento dei dati\nPer valutare la variabilità dei dati, è possibile utilizzare indici basati sull’ordinamento dei dati. L’indice più semplice è l’intervallo di variazione, che corrisponde alla differenza tra il valore massimo e il valore minimo di una distribuzione di dati. Tuttavia, questo indice ha il limite di essere calcolato basandosi solo su due valori della distribuzione, e non tiene conto di tutte le informazioni disponibili. Inoltre, l’intervallo di variazione può essere fortemente influenzato dalla presenza di valori anomali.\nUn altro indice basato sull’ordinamento dei dati è la differenza interquartile, già incontrata in precedenza. Anche se questo indice utilizza più informazioni rispetto all’intervallo di variazione, presenta comunque il limite di essere calcolato basandosi solo su due valori della distribuzione, ossia il primo quartile \\(Q_1\\) e il terzo quartile \\(Q_3\\).\nPer valutare la variabilità in modo più completo, è necessario utilizzare altri indici di variabilità che tengano conto di tutti i dati disponibili. In questo modo, si otterrà una valutazione più accurata della dispersione dei valori nella distribuzione e si potranno individuare eventuali pattern o tendenze nascoste.\n\n\n5.3.2 Varianza\nDate le limitazioni delle statistiche descritte in precedenza, è più comune utilizzare una misura di variabilità che tenga conto della dispersione dei dati rispetto a un indice di tendenza centrale. La varianza è la misura di variabilità più utilizzata per valutare la variabilità di una variabile statistica. Essa è definita come la media dei quadrati degli scarti \\(x_i - \\bar{x}\\) tra ogni valore e la media della distribuzione, come segue:\n\\[\n\\begin{equation}\nS^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2.\n\\end{equation}\n\\tag{5.3}\\]\nLa varianza è una misura di dispersione più completa rispetto a quelle descritte in precedenza. Tuttavia, è appropriata solo nel caso di distribuzioni simmetriche ed è fortemente influenzata dai valori anomali, come altre misure di dispersione. Inoltre, la varianza è espressa in un’unità di misura che è il quadrato dell’unità di misura dei dati originali, pertanto, potrebbe non essere facilmente interpretata in modo intuitivo.\nCalcoliamo la varianza dei valori math per i dati del progetto STAR. Applicando l’equazione della varianza, otteniamo:\n\nsum((df[\"math\"] - np.mean(df[\"math\"])) ** 2) / len(df[\"math\"])\n\n1507.2328523125227\n\n\nPiù semplicemente, possiamo usare la funzione np.var():\n\nnp.var(df[\"math\"])\n\n1507.2328523125227\n\n\n\n5.3.2.1 Stima della varianza della popolazione\nSi noti il denominatore della formula della varianza. Nell’Equation 5.3, ho utilizzato \\(n\\) come denominatore (l’ampiezza campionaria, ovvero il numero di osservazioni nel campione). In questo modo, otteniamo la varianza come statistica descrittiva del campione. Tuttavia, è possibile utilizzare \\(n-1\\) come denominatore alternativo:\n\\[\n\\begin{equation}\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\n\\end{equation}\n\\tag{5.4}\\]\nIn questo secondo caso, otteniamo la varianza come stimatore della varianza della popolazione. Si può dimostrare che l’Equation 5.4 fornisce una stima corretta (ovvero, non distorta) della varianza della popolazione da cui abbiamo ottenuto il campione, mentre l’Equation 5.3 fornisce (in media) una stima troppo piccola della varianza della popolazione. Si presti attenzione alla notazione: \\(S^2\\) rappresenta la varianza come statistica descrittiva, mentre \\(s^2\\) rappresenta la varianza come stimatore.\nPer illustrare questo punto, svolgiamo una simulazione. Consideriamo la distribuzione dei punteggi del quoziente di intelligenza (QI). I valori del QI seguono una particolare distribuzione chiamata distribuzione normale, con media 100 e deviazione standard 15. La forma di questa distribuzione è illustrata nella figura seguente.\n\nx = np.arange(100 - 4 * 15, 100 + 4 * 15, 0.001)\n\nmu = 100\nsigma = 15\n\npdf = stats.norm.pdf(x, mu, sigma)\nplt.plot(x, pdf)\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.show()\n\n\n\n\n\n\n\n\nSupponiamo di estrarre un campione casuale di 4 osservazioni dalla popolazione del quoziente di intelligenza – in altre parole, supponiamo di misurare il quoziente di intelligenza di 4 persone prese a caso dalla popolazione.\n\nx = rng.normal(loc=100, scale=15, size=4)\nprint(x)\n\n[ 96.66036673 119.88488115  91.43544977 110.02373805]\n\n\nCalcoliamo la varianza usando \\(n\\) al denominatore. Si noti che la vera varianza del quoziente di intelligenza è \\(15^2\\) = 225.\n\nnp.var(x)\n\n134.65656223872708\n\n\nConsideriamo ora 10 campioni casuali del QI, ciascuno di ampiezza 4.\n\nmu = 100\nsigma = 15\nsize = 4\nniter = 10\nrandom_samples = []\n\nfor i in range(niter):\n    one_sample = rng.normal(loc=mu, scale=sigma, size=size)\n    random_samples.append(one_sample)\n\nIl primo campione è\n\nrandom_samples[0]\n\narray([ 70.73447217,  80.4673074 , 101.91760605,  95.25636111])\n\n\nIl decimo campione è\n\nrandom_samples[9]\n\narray([111.14881257, 108.14731402,  90.01735439, 103.48241985])\n\n\nStampiamo i valori di tutti i 10 campioni.\n\nrs = np.array(random_samples)\nrs\n\narray([[117.18071524, 108.81016091,  99.22835071,  72.32428097],\n       [ 95.39169462,  99.82359162, 109.53158888, 101.69933684],\n       [121.28194403, 111.99123812,  93.5863066 , 123.02032138],\n       [ 66.58961856, 104.23622977,  83.17674426, 119.51304508],\n       [107.69111686,  84.07010216,  98.52662251, 105.83838496],\n       [ 70.66964863, 105.06034379, 107.1614251 ,  83.45087842],\n       [ 92.51928633, 113.73691601,  90.51622696, 107.85154079],\n       [105.74034675, 102.67262674,  88.66272413,  87.55578622],\n       [108.18945322, 119.3681818 , 107.45218178,  81.44893886],\n       [ 84.56836494,  94.18203683,  97.52873515, 100.41911991]])\n\n\nPer ciascun campione (ovvero, per ciascuna riga della matrice precedente), calcoliamo la varianza usando la formula con \\(n\\) al denominatore. Otteniamo così 10 stime della varianza della popolazione del QI.\n\nx_var = np.var(rs, axis=1)  # applichiamo la funzione su ciascuna riga\nprint(x_var)\n\n[284.45704733  26.15452963 136.44568199 405.65618265  86.35524345\n 231.95644125  97.72682684  66.1097433  193.53695841  35.63101473]\n\n\nNotiamo due cose:\n\nle stime sono molto diverse tra loro; questo fenomeno è noto con il nome di variabilità campionaria;\nin media le stime sembrano troppo piccole.\n\nPer aumentare la sicurezza riguardo al secondo punto menzionato in precedenza, ripeteremo la simulazione utilizzando un numero di iterazioni maggiore.\n\nmu = 100\nsigma = 15\nsize = 4\nniter = 10000\nrandom_samples = []\n\nfor i in range(niter):\n    one_sample = rng.normal(loc=mu, scale=sigma, size=size)\n    random_samples.append(one_sample)\n\nrs = np.array(random_samples)\nx_var = np.var(rs, ddof=0, axis=1)\n\nEsaminiamo la distribuzione dei valori ottenuti.\n\nplt.hist(x_var, bins=10, color=\"skyblue\", edgecolor=\"black\")\nplt.xlabel(\"Varianza\")\nplt.ylabel(\"Frequenza\")\nplt.title(\"Varianza del QI in campioni di n = 4\")\nplt.show()\n\n\n\n\n\n\n\n\nLa stima più verosimile della varianza del QI è dato dalla media di questa distribuzione.\n\nnp.mean(x_var)\n\n170.04960311858687\n\n\nSi noti che il nostro spospetto è stato confermato: il valore medio della stima della varianza ottenuta con l’Equation 5.3 è troppo piccolo rispetto al valore corretto di \\(15^2 = 225\\).\nRipetiamo ora la simulazione usando la formula della varianza con \\(n-1\\) al denominatore.\n\nmu = 100\nsigma = 15\nsize = 4\nniter = 10000\nrandom_samples = []\n\nfor i in range(niter):\n    one_sample = rng.normal(loc=mu, scale=sigma, size=size)\n    random_samples.append(one_sample)\n\nrs = np.array(random_samples)\nx_var = np.var(rs, ddof=1, axis=1)\n\nnp.mean(x_var)\n\n222.53361717582456\n\n\nNel secondo caso, se utilizziamo \\(n-1\\) come denominatore per calcolare la stima della varianza, il valore atteso di questa stima è molto vicino al valore corretto di 225. Se il numero di campioni fosse infinito, i due valori sarebbero identici.\nIn conclusione, le due formule della varianza hanno scopi diversi. La formula della varianza con \\(n\\) al denominatore viene utilizzata come statistica descrittiva per descrivere la variabilità di un particolare campione di osservazioni. D’altro canto, la formula della varianza con \\(n-1\\) al denominatore viene utilizzata come stimatore per ottenere la migliore stima della varianza della popolazione da cui quel campione è stato estratto.\n\n\n\n5.3.3 Deviazione standard\nPer interpretare la varianza in modo più intuitivo, si può calcolare la deviazione standard (o scarto quadratico medio o scarto tipo) prendendo la radice quadrata della varianza. La deviazione standard è espressa nell’unità di misura originaria dei dati, a differenza della varianza che è espressa nel quadrato dell’unità di misura dei dati. La deviazione standard fornisce una misura della dispersione dei dati attorno alla media, rendendo più facile la comprensione della variabilità dei dati.\nLa deviazione standard (o scarto quadratico medio, o scarto tipo) è definita come:\n\\[\ns^2 = \\sqrt{(n-1)^{-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}.\n\\tag{5.5}\\]\nQuando tutte le osservazioni sono uguali, \\(s = 0\\), altrimenti \\(s &gt; 0\\).\n\n\n\n\n\n\nIl termine standard deviation è stato introdotto in statistica da Pearson nel 1894 assieme alla lettera greca \\(\\sigma\\) che lo rappresenta. Il termine italiano “deviazione standard” ne è la traduzione più utilizzata nel linguaggio comune; il termine dell’Ente Nazionale Italiano di Unificazione è tuttavia “scarto tipo”, definito come la radice quadrata positiva della varianza.\n\n\n\nLa deviazione standard \\(s\\) dovrebbe essere utilizzata solo quando la media è una misura appropriata per descrivere il centro della distribuzione, ad esempio nel caso di distribuzioni simmetriche. Tuttavia, è importante tener conto che, come la media \\(\\bar{x}\\), anche la deviazione standard è fortemente influenzata dalla presenza di dati anomali, ovvero pochi valori che si discostano notevolmente dalla media rispetto agli altri dati della distribuzione. In presenza di dati anomali, la deviazione standard può risultare ingannevole e non rappresentare accuratamente la variabilità complessiva della distribuzione. Pertanto, è fondamentale considerare attentamente il contesto e le caratteristiche dei dati prima di utilizzare la deviazione standard come misura di dispersione. In alcune situazioni, potrebbe essere più appropriato ricorrere a misure di dispersione robuste o ad altre statistiche descrittive per caratterizzare la variabilità dei dati in modo più accurato e affidabile.\nPer fare un esempio, calcoliamo la deviazione standard per i valori math del campione di dati del progetto STAR. Applicando l’Equation 5.5, per tutto il campione abbiamo\n\nnp.std(df.math)\n\n38.82309689234648\n\n\nPer ciascun gruppo, abbiamo:\n\ndf.groupby(\"graduated\")[\"math\"].std()\n\ngraduated\n0    34.105746\n1    38.130136\nName: math, dtype: float64\n\n\n\n5.3.3.1 Interpretazione\nLa deviazione standard può essere interpretata in modo semplice: essa rappresenta la dispersione dei dati rispetto alla media aritmetica. È simile allo scarto semplice medio campionario, cioè alla media aritmetica dei valori assoluti degli scarti tra ciascuna osservazione e la media, anche se non è identica. La deviazione standard ci fornisce un’indicazione di quanto, in media, le singole osservazioni si discostino dal centro della distribuzione.\nPer verificare l’interpretazione della deviazione standard, utilizziamo i valori math del campione di dati del progetto STAR.\n\nnp.std(df[\"math\"])\n\n38.82309689234648\n\n\nLa deviazione standard calcolata per questi dati è \\(\\approx 38.8\\). Questo valore ci indica che, in media, ogni osservazione si discosta di circa 38.8 punti dalla media aritmetica dei punteggi math. Maggiore è il valore della deviazione standard, maggiore è la dispersione dei dati attorno alla media, mentre un valore più piccolo indica che i dati sono più concentrati vicino alla media. La deviazione standard ci offre quindi una misura quantitativa della variabilità dei dati nella distribuzione.\nPer questi dati, lo scarto semplice medio campionario è\n\nnp.mean(np.abs(df.math - np.mean(df.math)))\n\n30.9682664274501\n\n\nSi noti che i due valori sono simili, ma non identici.\n\n\n\n5.3.4 Deviazione mediana assoluta\nUna misura robusta della dispersione statistica di un campione è la deviazione mediana assoluta (Median Absolute Deviation, MAD) definita come la mediana del valore assoluto delle deviazioni dei dati dalla mediana. Matematicamente, la formula per calcolare la MAD è:\n\\[\n\\text{MAD} = \\text{median} \\left( |X_i - \\text{median}(X)| \\right)\n\\tag{5.6}\\]\nLa deviazione mediana assoluta è particolarmente utile quando si affrontano distribuzioni con presenza di dati anomali o asimmetrie, poiché è meno influenzata da questi valori estremi rispetto alla deviazione standard.\nQuando i dati seguono una distribuzione gaussiana (normale), esiste una relazione specifica tra MAD e la deviazione standard (si veda il Capitolo {ref}cont-rv-distr-notebook). In una distribuzione normale, la MAD è proporzionale alla deviazione standard. La costante di proporzionalità dipende dalla forma esatta della distribuzione normale, ma in generale, la relazione è data da:\n\\[\n\\sigma \\approx k \\times \\text{MAD},\n\\]\ndove: - $ $ è la deviazione standard. - MAD è la Mediana della Deviazione Assoluta. - $ k $ è una costante che, per una distribuzione normale, è tipicamente presa come circa 1.4826.\nQuesta costante di 1.4826 è derivata dal fatto che, in una distribuzione normale, circa il 50% dei valori si trova entro 0.6745 deviazioni standard dalla media. Quindi, per convertire la MAD (basata sulla mediana) nella deviazione standard (basata sulla media), si usa il reciproco di 0.6745, che è approssimativamente 1.4826.\nLa formula completa per convertire la MAD in una stima della deviazione standard in una distribuzione normale è:\n\\[\n\\sigma \\approx 1.4826 \\times \\text{MAD}\n\\]\nQuesta relazione è utile per stimare la deviazione standard in modo più robusto, specialmente quando si sospetta la presenza di outlier o si ha a che fare con campioni piccoli. Di conseguenza, molti software restituiscono il valore MAD moltiplicato per questa costante per fornire un’indicazione più intuitiva della variabilità dei dati. Tuttavia, è importante notare che questa relazione si mantiene accurata solo per le distribuzioni che sono effettivamente normali. In presenza di distribuzioni fortemente asimmetriche o con elevati outlier, la deviazione standard e la MAD possono fornire indicazioni molto diverse sulla variabilità dei dati.\nPer verificare questo principio, calcoliamo la deviazione mediana assoluta dei valori math del campione di dati del progetto STAR.\n\n1.4826 * np.median(np.abs(df[\"math\"] - np.median(df[\"math\"])))\n\n41.5128\n\n\nIn questo caso, la MAD per i punteggi di matematica è simile alla deviazione standard.\n\nnp.std(df[\"math\"])\n\n38.82309689234648\n\n\nInfatti, la distribuzione dei punteggi math è approssimativamente gaussiana.\n\nplt.hist(df[\"math\"], bins=10, color=\"skyblue\", edgecolor = \"black\")\nplt.xlabel(\"math\")\nplt.ylabel(\"Frequenza\")\nplt.title(\"Distribuzione dei Punteggi di Matematica\")\nplt.show()\n\n\n\n\n\n\n\n\nVerifichiamo nuovamente il principio usando un campione di dati estratto da una popolazione normale. Usiamo, ad esempio, la distribuzione \\(\\mathcal{N}(100, 15)\\):\n\nx = np.random.normal(loc=100, scale=15, size=10000)\n1.4826 * np.median(np.abs(x - np.median(x)))\n\n15.179389799831695\n\n\n\n\n5.3.5 Quando usare la deviazione standard e MAD\nLa deviazione standard e la MAD sono entrambe misure di dispersione che forniscono informazioni su quanto i dati in un insieme si discostano dalla tendenza centrale. Tuttavia, ci sono alcune differenze tra le due misure e situazioni in cui può essere più appropriato utilizzare una rispetto all’altra.\n\nDeviazione standard: Questa misura è particolarmente utile per descrivere la dispersione dei dati in una distribuzione normale. La deviazione standard è una scelta appropriata se si vuole sapere quanto i dati sono distribuiti intorno alla media, o se si vuole confrontare la dispersione di due o più set di dati. Tuttavia, la deviazione standard è fortemente influenzata dalla presenza di dati anomali, e questo può rappresentare una limitazione in casi in cui sono presenti valori estremi nell’insieme di dati.\nDeviazione mediana assoluta (MAD): La MAD è meno sensibile ai valori anomali rispetto alla deviazione standard, il che la rende una scelta migliore quando ci sono valori anomali nell’insieme di dati. Inoltre, la MAD può essere una buona scelta quando si lavora con dati non normalmente distribuiti, poiché non assume una distribuzione specifica dei dati. La MAD è calcolata utilizzando la mediana e i valori assoluti delle deviazioni dei dati dalla mediana, il che la rende una misura robusta di dispersione.\n\nIn sintesi, se si sta lavorando con dati normalmente distribuiti, la deviazione standard è la misura di dispersione più appropriata. Se si lavora con dati non normalmente distribuiti o si hanno valori anomali nell’insieme di dati, la MAD può essere una scelta migliore. In ogni caso, la scelta tra le due misure dipende dal tipo di dati che si sta analizzando e dall’obiettivo dell’analisi.\n\n\n5.3.6 Indici di variabilità relativi\nA volte può essere necessario confrontare la variabilità di grandezze incommensurabili, ovvero di caratteri misurati con differenti unità di misura. In queste situazioni, le misure di variabilità descritte in precedenza diventano inadeguate poiché dipendono dall’unità di misura utilizzata. Per superare questo problema, si ricorre a specifici numeri adimensionali chiamati indici relativi di variabilità.\nIl più importante di questi indici è il coefficiente di variazione (\\(C_v\\)), definito come il rapporto tra la deviazione standard (\\(\\sigma\\)) e la media dei dati (\\(\\bar{x}\\)):\n\\[\nC_v = \\frac{\\sigma}{\\bar{x}}.\n\\tag{5.7}\\]\nIl coefficiente di variazione è un numero puro e permette di confrontare la variabilità di distribuzioni con unità di misura diverse.\nUn altro indice relativo di variabilità è la differenza interquartile rapportata a uno dei tre quartili (primo quartile, terzo quartile o mediana). Questo indice è definito come:\n\\[\n\\frac{x_{0.75} - x_{0.25}}{x_{0.25}}, \\qquad \\frac{x_{0.75} - x_{0.25}}{x_{0.75}}, \\qquad \\frac{x_{0.75} - x_{0.25}}{x_{0.50}}.\n\\]\nQuesti indici relativi di variabilità forniscono una misura adimensionale della dispersione dei dati, rendendo possibile il confronto tra grandezze con diverse unità di misura e facilitando l’analisi delle differenze di variabilità tra i dati.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Indici di posizione e di scala</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/04_loc_scale.html#la-fallacia-ergodica",
    "href": "chapters/chapter_2/04_loc_scale.html#la-fallacia-ergodica",
    "title": "5  Indici di posizione e di scala",
    "section": "5.4 La fallacia ergodica",
    "text": "5.4 La fallacia ergodica\nSebbene il concetto di “media” possa sembrare chiaro, ciò non implica che il suo utilizzo non presenti delle problematiche nell’ambito della pratica psicologica. Un aspetto su cui vale la pena soffermarsi è ciò che viene definito “fallacia ergodica”.\nIl concetto di “fallacia ergodica” (Speelman et al. 2024) si riferisce all’errore compiuto dai ricercatori quando assumono che le caratteristiche medie di un gruppo di individui possano essere applicate a ciascun individuo all’interno di quel gruppo, senza considerare le differenze individuali o le variazioni nel tempo. Questa fallacia emerge dalla pratica comune nella ricerca psicologica di raccogliere dati aggregati da gruppi di persone per stimare parametri della popolazione, al fine di confrontare comportamenti in condizioni diverse o esplorare associazioni tra diverse misurazioni della stessa persona.\nIl problema di questo approccio è che l’uso dei risultati basati sul gruppo per caratterizzare le caratteristiche degli individui o per estrapolare a persone simili a quelle del gruppo è ingiustificato, poiché le medie di gruppo possono fornire informazioni solo sui risultati collettivi, come la performance media del gruppo, e non consentono di fare affermazioni accurate sugli individui che compongono quel gruppo. La fallacia ergodica si basa sull’assunzione che per utilizzare legittimamente una statistica aggregata (ad esempio, la media) derivata da un gruppo per descrivere un individuo di quel gruppo, due condizioni devono essere soddisfatte: gli individui devono essere così simili da essere praticamente interscambiabili, e le caratteristiche degli individui devono essere temporalmente stabili.\nTuttavia, i fenomeni e i processi psicologici di interesse per i ricercatori sono per natura non uniformi tra gli individui e variabili nel tempo, sia all’interno degli individui che tra di loro. Di conseguenza, i risultati ottenuti dalla media di misure di comportamenti, cognizioni o stati emotivi di più individui non descrivono accuratamente nessuno di quegli individui in un dato momento, né possono tenere conto dei cambiamenti in quelle variabili per un individuo nel tempo.\nSpeelman et al. (2024) osservano che la stragrande maggioranza degli articoli che hanno analizzato include conclusioni nelle sezioni degli Abstract e/o delle Discussioni che implicano che i risultati trovati con dati aggregati di gruppo si applichino anche agli individui in quei gruppi e/o si applichino agli individui nella popolazione. Questa pratica riflette la fallacia ergodica, che consiste nell’assumere che i campioni siano sistemi ergodici quando non lo sono.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Indici di posizione e di scala</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/04_loc_scale.html#commenti-e-considerazioni-finali",
    "href": "chapters/chapter_2/04_loc_scale.html#commenti-e-considerazioni-finali",
    "title": "5  Indici di posizione e di scala",
    "section": "5.5 Commenti e considerazioni finali",
    "text": "5.5 Commenti e considerazioni finali\nLe statistiche descrittive ci permettono di ottenere indicatori sintetici che riassumono i dati di una popolazione o di un campione estratto da essa. Questi indicatori includono misure di tendenza centrale, come la media, la mediana e la moda, che ci forniscono informazioni sulla posizione centrale dei dati rispetto alla distribuzione. Inoltre, ci sono gli indici di dispersione, come la deviazione standard e la varianza, che ci indicano quanto i dati si disperdono attorno alla tendenza centrale. Questi indici ci aiutano a comprendere quanto i valori si discostano dalla media, e quindi ci forniscono un’idea della variabilità dei dati. In sintesi, le statistiche descrittive ci offrono un quadro chiaro e sintetico delle caratteristiche principali dei dati, consentendoci di comprendere meglio la loro distribuzione e variabilità.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Indici di posizione e di scala</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/04_loc_scale.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_2/04_loc_scale.html#informazioni-sullambiente-di-sviluppo",
    "title": "5  Indici di posizione e di scala",
    "section": "5.6 Informazioni sull’Ambiente di Sviluppo",
    "text": "5.6 Informazioni sull’Ambiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m\n\nLast updated: Fri Feb 16 2024\n\nPython implementation: CPython\nPython version       : 3.11.7\nIPython version      : 8.21.0\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.3.0\nMachine     : x86_64\nProcessor   : i386\nCPU cores   : 8\nArchitecture: 64bit\n\nmatplotlib: 3.8.2\npandas    : 2.2.0\narviz     : 0.17.0\nnumpy     : 1.26.4\nscipy     : 1.12.0\nseaborn   : 0.13.2\n\nWatermark: 2.4.3\n\n\n\n\n\n\n\nSpeelman, Craig P, Laura Parker, Benjamin J Rapley, and Marek McGann. 2024. “Most Psychological Researchers Assume Their Samples Are Ergodic: Evidence from a Year of Articles in Three Major Journals.” Collabra: Psychology 10 (1).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Indici di posizione e di scala</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/05_correlation.html",
    "href": "chapters/chapter_2/05_correlation.html",
    "title": "6  Le relazioni tra variabili",
    "section": "",
    "text": "6.1 Introduzione\nNel linguaggio comune, termini quali “dipendenza”, “associazione” e “correlazione” sono spesso utilizzati in modo intercambiabile. Tuttavia, da un punto di vista tecnico, “associazione” e “dipendenza” sono sinonimi e entrambi si distinguono dalla “correlazione”. L’associazione implica una relazione molto ampia: conoscere il valore di una variabile ci fornisce informazioni su un’altra variabile. Al contrario, la correlazione descrive una relazione più specifica e quantificabile, indicando se due variabili tendono a variare insieme in modo sistematico; ad esempio, in una tendenza crescente, se $ X &gt; _X $ allora è probabile che anche $ Y &gt; _Y $.\nÈ cruciale comprendere che non tutte le associazioni sono correlazioni e che, crucialmente, la correlazione non implica causalità. Questa distinzione è vitale per interpretare accuratamente i dati e per evitare conclusioni errate sulle relazioni tra variabili.\nIn questo capitolo, ci concentreremo su due misure statistiche fondamentali per valutare la relazione lineare tra due variabili: la covarianza e la correlazione. Questi strumenti sono essenziali per analizzare il grado e la direzione dell’associazione lineare tra le variabili, permettendoci di quantificare in che modo le variabili variano congiuntamente.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Le relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/05_correlation.html#i-dati-grezzi",
    "href": "chapters/chapter_2/05_correlation.html#i-dati-grezzi",
    "title": "6  Le relazioni tra variabili",
    "section": "6.2 I dati grezzi",
    "text": "6.2 I dati grezzi\nPer illustrare la correlazione e la covarianza, analizzeremo i dati raccolti da Zetsche, Buerkner, and Renneberg (2019) in uno studio che indaga le aspettative negative come meccanismo chiave nel mantenimento e nella reiterazione della depressione. Nello specifico, i ricercatori si sono proposti di determinare se gli individui depressi sviluppano aspettative accurate riguardo al loro umore futuro o se tali aspettative sono distortamente negative.\nUno dei loro studi ha coinvolto un campione di 30 soggetti con almeno un episodio depressivo maggiore, confrontati con un gruppo di controllo composto da 37 individui sani. La misurazione del livello di depressione è stata effettuata tramite il Beck Depression Inventory (BDI-II).\nIl BDI-II è uno strumento di autovalutazione utilizzato per valutare la gravità della depressione in adulti e adolescenti. Il test è stato sviluppato per identificare e misurare l’intensità dei sintomi depressivi sperimentati nelle ultime due settimane. I 21 item del test sono valutati su una scala a 4 punti, dove 0 rappresenta il grado più basso e 3 il grado più elevato di sintomatologia depressiva.\nNell’esercizio successivo, ci proponiamo di analizzare i punteggi di depressione BDI-II nel campione di dati fornito da Zetsche, Buerkner, and Renneberg (2019).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Le relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/05_correlation.html#definizione-delle-relazioni-tra-variabili",
    "href": "chapters/chapter_2/05_correlation.html#definizione-delle-relazioni-tra-variabili",
    "title": "6  Le relazioni tra variabili",
    "section": "6.3 Definizione delle relazioni tra variabili",
    "text": "6.3 Definizione delle relazioni tra variabili\nNel contesto delle indagini statistiche, spesso non ci limitiamo a esaminare la distribuzione di una singola variabile. Invece, il nostro interesse si concentra sulla relazione che emerge nei dati tra due o più variabili. Ma cosa significa esattamente quando diciamo che due variabili hanno una relazione?\nPer comprendere ciò, prendiamo ad esempio l’altezza e l’età tra un gruppo di bambini. In generale, è possibile notare che all’aumentare dell’età di un bambino, aumenta anche la sua altezza. Pertanto, conoscere l’età di un bambino, ad esempio tredici anni, e l’età di un altro, sei anni, ci fornisce un’indicazione su quale dei due bambini sia più alto.\nNel linguaggio statistico, definiamo questa relazione tra altezza e età come positiva, il che significa che all’aumentare dei valori di una delle variabili (in questo caso, l’età), ci aspettiamo di vedere valori più elevati anche nell’altra variabile (l’altezza). Tuttavia, esistono anche relazioni negative, in cui l’aumento di una variabile è associato a un diminuzione dell’altra (ad esempio, più età è correlata a meno pianto).\nNon si tratta solo di relazioni positive o negative; ci sono anche situazioni in cui le variabili non hanno alcuna relazione tra loro, definendo così una relazione nulla. Inoltre, le relazioni possono variare nel tempo, passando da positive a negative o da fortemente positive a appena positiva. In alcuni casi, una delle variabili può essere categorica, rendendo difficile parlare di “maggioranza” o “minoranza” ma piuttosto di “differente” (ad esempio, i bambini più grandi potrebbero semplicemente avere diverse preferenze rispetto ai bambini più piccoli, senza necessariamente essere “migliori” o “peggiori”).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Le relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/05_correlation.html#sec-scatter-plot",
    "href": "chapters/chapter_2/05_correlation.html#sec-scatter-plot",
    "title": "6  Le relazioni tra variabili",
    "section": "6.4 Grafico a dispersione",
    "text": "6.4 Grafico a dispersione\nIl metodo più diretto per visualizzare la relazione tra due variabili continue è tramite un grafico a dispersione, comunemente noto come “scatterplot”. Questo tipo di diagramma rappresenta le coppie di dati ottenute da due variabili, posizionandole sull’asse delle ascisse (orizzontale) e delle ordinate (verticale).\nPer rendere l’idea più chiara, consideriamo i dati dello studio condotto da Zetsche, Buerkner, and Renneberg (2019), in cui i ricercatori hanno utilizzato due scale psicometriche, il Beck Depression Inventory II (BDI-II) e la Center for Epidemiologic Studies Depression Scale (CES-D), per misurare il livello di depressione nei partecipanti. Il BDI-II è uno strumento di autovalutazione che valuta la presenza e l’intensità dei sintomi depressivi in pazienti adulti e adolescenti con diagnosi psichiatrica, mentre la CES-D è una scala di autovalutazione progettata per misurare i sintomi depressivi sperimentati nella settimana precedente nella popolazione generale, in particolare negli adolescenti e nei giovani adulti. Poiché entrambe le scale misurano lo stesso costrutto, ovvero la depressione, ci aspettiamo una relazione tra i punteggi ottenuti dal BDI-II e dalla CES-D. Un diagramma a dispersione ci consente di esaminare questa relazione in modo visuale e intuitivo.\n\n# Leggi i dati dal file CSV\ndf = pd.read_csv(\"../../data/data.mood.csv\", index_col=0)\n\n# Seleziona le colonne di interesse\ndf = df[[\"esm_id\", \"group\", \"bdi\", \"cesd_sum\"]]\n\n# Rimuovi le righe duplicate\ndf = df.drop_duplicates(keep=\"first\")\n\n# Rimuovi le righe con valori mancanti nella colonna \"bdi\"\ndf = df.dropna(subset=[\"bdi\"])\n\nPosizionando i valori del BDI-II sull’asse delle ascisse e quelli del CES-D sull’asse delle ordinate, ogni punto sul grafico rappresenta un individuo, di cui conosciamo il livello di depressione misurato dalle due scale. È evidente che i valori delle scale BDI-II e CES-D non possono coincidere per due motivi principali: (1) la presenza di errori di misurazione e (2) l’utilizzo di unità di misura arbitrarie per le due variabili. L’errore di misurazione è una componente inevitabile che influisce in parte su qualsiasi misurazione, ed è particolarmente rilevante in psicologia, dove la precisione degli strumenti di misurazione è generalmente inferiore rispetto ad altre discipline, come la fisica. Il secondo motivo per cui i valori delle scale BDI-II e CES-D non possono essere identici è che l’unità di misura della depressione è una questione arbitraria e non standardizzata. Tuttavia, nonostante le differenze dovute agli errori di misurazione e all’uso di unità di misura diverse, ci aspettiamo che, se le due scale misurano lo stesso costrutto (la depressione), i valori prodotti dalle due scale dovrebbero essere associati linearmente tra di loro. Per comprendere meglio il concetto di “associazione lineare”, è possibile esaminare i dati attraverso l’utilizzo di un diagramma a dispersione.\n\n# Crea uno scatterplot con colori diversi per i due gruppi\nplt.scatter(df[df[\"group\"] == \"mdd\"][\"bdi\"], df[df[\"group\"] == \"mdd\"][\"cesd_sum\"], label=\"Pazienti\", c=\"C0\")\nplt.scatter(df[df[\"group\"] == \"ctl\"][\"bdi\"], df[df[\"group\"] == \"ctl\"][\"cesd_sum\"], label=\"Controlli\", c=\"C2\")\n\n# Calcola i coefficienti della retta dei minimi quadrati\ncoeff_combined = np.polyfit(df[\"bdi\"], df[\"cesd_sum\"], 1)\n\n# Calcola la retta dei minimi quadrati\nline_combined = np.poly1d(coeff_combined)\n\n# Disegna la retta dei minimi quadrati\nx_values = np.linspace(df[\"bdi\"].min(), df[\"bdi\"].max(), 100)\nplt.plot(x_values, line_combined(x_values), linestyle='--', color='C3')\n\n# Etichette degli assi\nplt.xlabel(\"BDI-II\")\nplt.ylabel(\"CESD\")\n\n# Linee verticali ed orizzontali per le medie\nplt.axvline(np.mean(df[df[\"group\"] == \"mdd\"][\"bdi\"]), alpha=0.2, color=\"blue\")\nplt.axvline(np.mean(df[df[\"group\"] == \"ctl\"][\"bdi\"]), alpha=0.2, color=\"red\")\nplt.axhline(np.mean(df[df[\"group\"] == \"mdd\"][\"cesd_sum\"]), alpha=0.2, color=\"blue\")\nplt.axhline(np.mean(df[df[\"group\"] == \"ctl\"][\"cesd_sum\"]), alpha=0.2, color=\"red\")\n\n# Legenda\nplt.legend()\n\n# Mostra il grafico\nplt.show()\n\n\n\n\n\n\n\n\nOsservando il grafico a dispersione, è evidente che i dati mostrano una tendenza a distribuirsi in modo approssimativamente lineare. In termini statistici, ciò suggerisce una relazione di associazione lineare tra i punteggi CES-D e BDI-II.\nTuttavia, è importante notare che la relazione lineare tra le due variabili è lontana dall’essere perfetta. In una relazione lineare perfetta, tutti i punti nel grafico sarebbero allineati in modo preciso lungo una retta. Nella realtà, la dispersione dei punti dal comportamento lineare ideale è evidente.\nDi conseguenza, sorge la necessità di quantificare numericamente la forza e la direzione della relazione lineare tra le due variabili e di misurare quanto i punti si discostino da una relazione lineare ideale. Esistono vari indici statistici a disposizione per raggiungere questo obiettivo.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Le relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/05_correlation.html#covarianza",
    "href": "chapters/chapter_2/05_correlation.html#covarianza",
    "title": "6  Le relazioni tra variabili",
    "section": "6.5 Covarianza",
    "text": "6.5 Covarianza\nIniziamo a considerare il più importante di tali indici, chiamato covarianza. In realtà la definizione di questo indice non ci sorprenderà più di tanto in quanto, in una forma solo apparentemente diversa, l’abbiamo già incontrata in precedenza. Ci ricordiamo infatti che la varianza di una generica variabile \\(X\\) è definita come la media degli scarti quadratici di ciascuna osservazione dalla media:\n\\[\nS_{XX} = \\frac{1}{n} \\sum_{i=1}^n(X_i - \\bar{X}) (X_i - \\bar{X}).\n\\]\nLa varianza viene talvolta descritta come la “covarianza di una variabile con sé stessa”. Adesso facciamo un passo ulteriore. Invece di valutare la dispersione di una sola variabile, ci chiediamo come due variabili \\(X\\) e \\(Y\\) “variano insieme” (co-variano). È facile capire come una risposta a tale domanda possa essere fornita da una semplice trasformazione della formula precedente che diventa:\n\\[\nS_{XY} = \\frac{1}{n} \\sum_{i=1}^n(X_i - \\bar{X}) (Y_i - \\bar{Y}).\n\\tag{6.1}\\]\nL’Equation 6.1 ci fornisce la definizione della covarianza.\n\n6.5.1 Interpretazione\nPer capire il significato dell’Equation 6.1, supponiamo di dividere il grafico riportato nella ?sec-introduction in quattro quadranti definiti da una retta verticale passante per la media dei valori BDI-II e da una retta orizzontale passante per la media dei valori CES-D. Numeriamo i quadranti partendo da quello in basso a sinistra e muovendoci in senso antiorario.\nSe prevalgono punti nel I e III quadrante, allora la nuvola di punti avrà un andamento crescente (per cui a valori bassi di \\(X\\) tendono ad associarsi valori bassi di \\(Y\\) e a valori elevati di \\(X\\) tendono ad associarsi valori elevati di \\(Y\\)) e la covarianza avrà segno positivo. Mentre se prevalgono punti nel II e IV quadrante la nuvola di punti avrà un andamento decrescente (per cui a valori bassi di \\(X\\) tendono ad associarsi valori elevati di \\(Y\\) e a valori elevati di \\(X\\) tendono ad associarsi valori bassi di \\(Y\\)) e la covarianza avrà segno negativo. Dunque, il segno della covarianza ci informa sulla direzione della relazione lineare tra due variabili: l’associazione lineare si dice positiva se la covarianza è positiva, negativa se la covarianza è negativa.\nEsercizio. Implemento l’Equation 6.1 in Python.\n\ndef cov_value(x, y):\n\n    mean_x = sum(x) / float(len(x))\n    mean_y = sum(y) / float(len(y))\n\n    sub_x = [i - mean_x for i in x]\n    sub_y = [i - mean_y for i in y]\n\n    sum_value = sum([sub_y[i] * sub_x[i] for i in range(len(x))])\n    denom = float(len(x))\n\n    cov = sum_value / denom\n    return cov\n\nPer i dati mostrati nel diagramma, la covarianza tra BDI-II e CESD è 207.4\n\nx = df[\"bdi\"]\ny = df[\"cesd_sum\"]\n\ncov_value(x, y)\n\n207.42653810835637\n\n\nOppure, in maniera più semplice:\n\nnp.mean((x - np.mean(x)) * (y - np.mean(y)))\n\n207.42653810835628\n\n\nLo stesso risultato si ottiene con la funzione cov di NumPy.\n\nnp.cov(x, y, ddof=0)\n\narray([[236.23875115, 207.42653811],\n       [207.42653811, 222.83379247]])\n\n\nLa funzione np.cov(x, y, ddof=0) in Python, utilizzata tramite la libreria NumPy, calcola la covarianza tra due array, x e y. L’argomento ddof (Delta Degrees of Freedom) specifica il “correttore” da applicare al denominatore della formula di covarianza.\nQuando si imposta ddof=0, la formula utilizzata per il calcolo della covarianza divide la somma dei prodotti delle deviazioni dalla media per n, dove n è il numero totale degli elementi nel campione (ovvero, la dimensione del campione). Questo approccio assume che i dati forniti rappresentino l’intera popolazione da cui si vuole stimare la covarianza, producendo una stima non corretta (bias) se i dati sono effettivamente un campione di una popolazione più ampia. Il “bias” in questo contesto si riferisce al fatto che la stima tende sistematicamente a essere più piccola rispetto alla vera covarianza della popolazione da cui il campione è stato estratto.\nPer correggere questo errore sistematico e ottenere una stima non distorta (unbiased) della covarianza di una popolazione più ampia basandosi su un campione, si utilizza ddof=1. Questo significa che al denominatore della formula si sottrae 1 a n, dividendo quindi per n-1. Il correttore n-1 è noto come correttore di Bessel, e l’uso di ddof=1 rende la stima della covarianza non distorta nel contesto di un campione prelevato da una popolazione. La correzione è importante in statistica perché fornisce una stima più accurata delle proprietà della popolazione, soprattutto quando la dimensione del campione è piccola.\nIn sintesi: - Con ddof=0, si divide per n, assumendo che i dati rappresentino l’intera popolazione. Questo può introdurre un bias nella stima della covarianza se i dati sono in realtà un campione. - Con ddof=1, si divide per n-1, correggendo il bias e ottenendo una stima non distorta (unbiased) della covarianza se i dati rappresentano un campione di una popolazione più grande. Questo approccio è generalmente preferito per la stima delle proprietà della popolazione basata su campioni.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Le relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/05_correlation.html#correlazione",
    "href": "chapters/chapter_2/05_correlation.html#correlazione",
    "title": "6  Le relazioni tra variabili",
    "section": "6.6 Correlazione",
    "text": "6.6 Correlazione\nLa direzione della relazione tra le variabili è indicata dal segno della covarianza, ma il valore assoluto di questo indice non fornisce informazioni utili poiché dipende dall’unità di misura delle variabili. Ad esempio, considerando l’altezza e il peso delle persone, la covarianza sarà più grande se l’altezza è misurata in millimetri e il peso in grammi, rispetto al caso in cui l’altezza è in metri e il peso in chilogrammi. Pertanto, per descrivere la forza e la direzione della relazione lineare tra due variabili in modo adimensionale, si utilizza l’indice di correlazione.\nLa correlazione è ottenuta standardizzando la covarianza tramite la divisione delle deviazioni standard (\\(s_X\\), \\(s_Y\\)) delle due variabili:\n\\[\nr_{XY} = \\frac{S_{XY}}{S_X S_Y}.\n\\tag{6.2}\\]\nLa quantità che si ottiene dall’Equation 6.2 viene chiamata correlazione di Bravais-Pearson (dal nome degli autori che, indipendentemente l’uno dall’altro, l’hanno introdotta).\n\n6.6.1 Proprietà\nIl coefficiente di correlazione ha le seguenti proprietà:\n\nha lo stesso segno della covarianza, dato che si ottiene dividendo la covarianza per due numeri positivi;\nè un numero puro, cioè non dipende dall’unità di misura delle variabili;\nassume valori compresi tra -1 e +1.\n\n\n\n6.6.2 Interpretazione\nAll’indice di correlazione possiamo assegnare la seguente interpretazione:\n\n\\(r_{XY} = -1\\) \\(\\rightarrow\\) perfetta relazione negativa: tutti i punti si trovano esattamente su una retta con pendenza negativa (dal quadrante in alto a sinistra al quadrante in basso a destra);\n\\(r_{XY} = +1\\) \\(\\rightarrow\\) perfetta relazione positiva: tutti i punti si trovano esattamente su una retta con pendenza positiva (dal quadrante in basso a sinistra al quadrante in alto a destra);\n\\(-1 &lt; r_{XY} &lt; +1\\) \\(\\rightarrow\\) presenza di una relazione lineare di intensità diversa;\n\\(r_{XY} = 0\\) \\(\\rightarrow\\) assenza di relazione lineare tra \\(X\\) e \\(Y\\).\n\nEsercizio. Per i dati riportati nel diagramma della sezione {ref}sec-zetsche-scatter, la covarianza è 207.4. Il segno positivo della covarianza ci dice che tra le due variabili c’è un’associazione lineare positiva. Per capire quale sia l’intensità della relazione lineare calcoliamo la correlazione. Essendo le deviazioni standard del BDI-II e del CES-D rispettavamente uguali a 15.37 e 14.93, la correlazione diventa uguale a \\(\\frac{207.426}{15.38 \\cdot 14.93} = 0.904.\\) Tale valore è prossimo a 1.0, il che vuol dire che i punti del diagramma a dispersione non si discostano troppo da una retta con una pendenza positiva.\nTroviamo la correlazione con la funzione corrcoef():\n\nnp.corrcoef(x, y)\n\narray([[1.        , 0.90406202],\n       [0.90406202, 1.        ]])\n\n\nReplichiamo il risultato implementando l’eq. {eq}eq-cor-def:\n\ns_xy = np.mean((x - np.mean(x)) * (y - np.mean(y)))\ns_x = x.std(ddof=0)\ns_y = y.std(ddof=0)\nr_xy = s_xy / (s_x * s_y)\nprint(r_xy)\n\n0.9040620189474861\n\n\nUn altro modo ancora per trovare la correlazione tra i punteggi BDI-II e CESD è quello di standardizzare le due variabili per poi applicare la formula della covarianza:\n\nz_x = (x - np.mean(x)) / np.std(x, ddof=0)\nz_y = (y - np.mean(y)) / np.std(y, ddof=0)\nnp.mean(z_x * z_y)\n\n0.9040620189474862\n\n\nEsempio. Un uso interessante delle correlazioni viene fatto in un recente articolo di Guilbeault et al. (2024). Il concetto di “gender bias” si riferisce alla tendenza sistematica di favorire un sesso rispetto all’altro, spesso a scapito delle donne. Lo studio di Guilbeault et al. (2024) analizza come le immagini online influenzino la diffusione su vasta scala di questo preconcetto di genere.\nAttraverso un vasto insieme di immagini e testi raccolti online, gli autori dimostrano che sia le misurazioni basate sulle immagini che quelle basate sui testi catturano la frequenza con cui varie categorie sociali sono associate a rappresentazioni di genere, valutate su una scala da -1 (femminile) a 1 (maschile), con 0 che indica una neutralità di genere. Questo consente di quantificare il preconcetto di genere come una forma di bias statistico lungo tre dimensioni: la tendenza delle categorie sociali ad associarsi a un genere specifico nelle immagini e nei testi, la rappresentazione relativa delle donne rispetto agli uomini in tutte le categorie sociali nelle immagini e nei testi, e il confronto tra le associazioni di genere nei dati delle immagini e dei testi con la distribuzione empirica delle donne e degli uomini nella società. Il lavoro di Guilbeault et al. (2024) evidenzia che il preconcetto di genere è molto più evidente nelle immagini rispetto ai testi, come mostrato nella {numref}gender-bias-1-fig C.\nSi noti che, nel grafico della {numref}gender-bias-1-fig C, ogni punto può essere interpretato come una misura di correlazione. La misura utilizzata da Guilbeault et al. (2024) riflette il grado di associazione tra le categorie sociali e le rappresentazioni di genere presenti nelle immagini e nei testi analizzati. Quando la misura è vicina a +1, indica una forte associazione positiva tra una categoria sociale specifica e una rappresentazione di genere maschile, mentre un valore vicino a -1 indica una forte associazione negativa con una rappresentazione di genere femminile. Un valore di 0, invece, suggerisce che non vi è alcuna associazione tra la categoria sociale considerata e un genere specifico, indicando una sorta di neutralità di genere. In sostanza, questa misura di frequenza può essere interpretata come una correlazione che riflette la tendenza delle categorie sociali a essere rappresentate in un modo o nell’altro nelle immagini e nei testi analizzati, rispetto ai concetti di genere femminile e maschile.\n\n\n\nIl preconcetto di genere è più prevalente nelle immagini online (da Google Immagini) e nei testi online (da Google News). A. La correlazione tra le associazioni di genere nelle immagini da Google Immagini e nei testi da Google News per tutte le categorie sociali (n = 2.986), organizzate per decili. B. La forza dell’associazione di genere in queste immagini e testi online per tutte le categorie (n = 2.986), suddivisa in base al fatto che queste categorie siano inclinate verso il femminile o il maschile. C. Le associazioni di genere per un campione di occupazioni secondo queste immagini e testi online; questo campione è stato selezionato manualmente per evidenziare i tipi di categorie sociali e preconcetti di genere esaminati. (Figura tratta da Guilbeault et al. (2024)).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Le relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/05_correlation.html#correlazione-di-spearman",
    "href": "chapters/chapter_2/05_correlation.html#correlazione-di-spearman",
    "title": "6  Le relazioni tra variabili",
    "section": "6.7 Correlazione di Spearman",
    "text": "6.7 Correlazione di Spearman\nUn’alternativa per valutare la relazione lineare tra due variabili è il coefficiente di correlazione di Spearman, che si basa esclusivamente sull’ordine dei dati e non sugli specifici valori. Questo indice di associazione è particolarmente adatto quando gli psicologi sono in grado di misurare solo le relazioni di ordine tra diverse modalità di risposta dei soggetti, ma non l’intensità della risposta stessa. Tali variabili psicologiche che presentano questa caratteristica sono definite come “ordinali”.\n\n\n\n\n\n\nÈ importante ricordare che, nel caso di una variabile ordinale, non è possibile utilizzare le statistiche descrittive convenzionali come la media e la varianza per sintetizzare le osservazioni. Tuttavia, è possibile riassumere le osservazioni attraverso una distribuzione di frequenze delle diverse modalità di risposta. Come abbiamo appena visto, la direzione e l’intensità dell’associazione tra due variabili ordinali possono essere descritte utilizzando il coefficiente di correlazione di Spearman.\n\n\n\nPer fornire un esempio, consideriamo due variabili di scala ordinale e calcoliamo la correlazione di Spearman tra di esse.\n\nstats.spearmanr([1, 2, 3, 4, 5], [5, 6, 7, 8, 7])\n\nSignificanceResult(statistic=0.8207826816681233, pvalue=0.08858700531354381)",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Le relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/05_correlation.html#correlazione-nulla",
    "href": "chapters/chapter_2/05_correlation.html#correlazione-nulla",
    "title": "6  Le relazioni tra variabili",
    "section": "6.8 Correlazione nulla",
    "text": "6.8 Correlazione nulla\nUn aspetto finale da sottolineare riguardo alla correlazione è che essa descrive la direzione e l’intensità della relazione lineare tra due variabili. Tuttavia, la correlazione non cattura relazioni non lineari tra le variabili, anche se possono essere molto forti. È fondamentale comprendere che una correlazione pari a zero non implica l’assenza di una relazione tra le due variabili, ma indica solamente l’assenza di una relazione lineare tra di esse.\nLa figura seguente fornisce tredici esempi di correlazione nulla in presenza di una chiara relazione (non lineare) tra due variabili. In questi tredici insiemi di dati i coefficienti di correlazione di Pearson sono sempre uguali a 0. Ma questo non significa che non vi sia alcuna relazione tra le variabili.\n\ndatasaurus_data = pd.read_csv(\"../data/datasaurus.csv\")\ndatasaurus_data.groupby(\"dataset\").agg(\n    {\"x\": [\"count\", \"mean\", \"std\"], \"y\": [\"count\", \"mean\", \"std\"]}\n)\n\n\n\n\n\n\n\n\n\nx\ny\n\n\n\ncount\nmean\nstd\ncount\nmean\nstd\n\n\ndataset\n\n\n\n\n\n\n\n\n\n\naway\n142\n54.266100\n16.769825\n142\n47.834721\n26.939743\n\n\nbullseye\n142\n54.268730\n16.769239\n142\n47.830823\n26.935727\n\n\ncircle\n142\n54.267320\n16.760013\n142\n47.837717\n26.930036\n\n\ndino\n142\n54.263273\n16.765142\n142\n47.832253\n26.935403\n\n\ndots\n142\n54.260303\n16.767735\n142\n47.839829\n26.930192\n\n\nh_lines\n142\n54.261442\n16.765898\n142\n47.830252\n26.939876\n\n\nhigh_lines\n142\n54.268805\n16.766704\n142\n47.835450\n26.939998\n\n\nslant_down\n142\n54.267849\n16.766759\n142\n47.835896\n26.936105\n\n\nslant_up\n142\n54.265882\n16.768853\n142\n47.831496\n26.938608\n\n\nstar\n142\n54.267341\n16.768959\n142\n47.839545\n26.930275\n\n\nv_lines\n142\n54.269927\n16.769959\n142\n47.836988\n26.937684\n\n\nwide_lines\n142\n54.266916\n16.770000\n142\n47.831602\n26.937902\n\n\nx_shape\n142\n54.260150\n16.769958\n142\n47.839717\n26.930002\n\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(4, 4, figsize=(15, 15))\ndatasets = datasaurus_data[\"dataset\"].unique()\n\nfor i, dataset in enumerate(datasets):\n    row = i // 4\n    col = i % 4\n    ax = axs[row, col]\n    subset = datasaurus_data[datasaurus_data[\"dataset\"] == dataset]\n    ax.scatter(subset[\"x\"], subset[\"y\"], alpha=0.7)\n    ax.set_title(dataset)\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n\nplt.tight_layout()\nplt.show()\n\n/var/folders/s7/z86r4t9j6yx376cm120nln6w0000gn/T/ipykernel_14701/188333220.py:14: UserWarning: The figure layout has changed to tight\n  plt.tight_layout()",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Le relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/05_correlation.html#considerazioni-conclusive",
    "href": "chapters/chapter_2/05_correlation.html#considerazioni-conclusive",
    "title": "6  Le relazioni tra variabili",
    "section": "6.9 Considerazioni conclusive",
    "text": "6.9 Considerazioni conclusive\nNel concludere questo capitolo dedicato allo studio di correlazione e covarianza, è essenziale mettere in evidenza alcuni principi fondamentali. Il concetto di “associazione” equivale a quello di “dipendenza” e può manifestarsi sia attraverso cause dirette che indirette. Ciò significa che variazioni in una variabile possono influenzare o essere influenzate da un’altra, indipendentemente dalla natura diretta o mediata di questo legame.\nPer quanto riguarda la correlazione, questa descrive specifici tipi di associazione, come tendenze monotone. Ad esempio, la presenza di un incremento congiunto o di raggruppamenti in determinati intervalli tra due variabili è indicativa di correlazione. È però cruciale ricordare che correlazione non equivale a causalità; osservare una correlazione non implica necessariamente che una variabile causi l’altra.\nNei contesti in cui il numero di variabili è ampio rispetto alla grandezza del campione, possono emergere correlazioni elevate ma spurie. Questo accade perché un gran numero di variabili può accidentalmente generare correlazioni elevate, non riflettendo un legame sostanziale tra di esse.\nD’altra parte, con un numero elevato di osservazioni, anche le correlazioni più deboli possono sembrare robuste. In campioni di grandi dimensioni, anche le più piccole variazioni possono apparire accentuate rispetto alla variabilità generale del campione, benché queste correlazioni possano non avere una rilevanza pratica.\nIn conclusione, è fondamentale adottare un approccio critico e ben informato nell’interpretare i risultati delle analisi di correlazione e covarianza, tenendo sempre in considerazione le dimensioni del campione e il numero di variabili coinvolte. Tale prudenza aiuta a prevenire interpretazioni errate delle relazioni tra le variabili, specialmente riguardo alla causalità.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Le relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/05_correlation.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_2/05_correlation.html#informazioni-sullambiente-di-sviluppo",
    "title": "6  Le relazioni tra variabili",
    "section": "6.10 Informazioni sull’Ambiente di Sviluppo",
    "text": "6.10 Informazioni sull’Ambiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m\n\nLast updated: Sat Jun 08 2024\n\nPython implementation: CPython\nPython version       : 3.12.3\nIPython version      : 8.25.0\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.4.0\nMachine     : arm64\nProcessor   : arm\nCPU cores   : 8\nArchitecture: 64bit\n\nWatermark: 2.4.3\n\n\n\n\n\n\n\nZetsche, Ulrike, Paul-Christian Buerkner, and Babette Renneberg. 2019. “Future Expectations in Clinical Depression: Biased or Realistic?” Journal of Abnormal Psychology 128 (7): 678.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Le relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/06_causality.html",
    "href": "chapters/chapter_2/06_causality.html",
    "title": "7  Lo studio delle cause dei fenomeni",
    "section": "",
    "text": "7.1 Introduzione\nPer assicurare l’integrità e la validità scientifica dei modelli statistici, è essenziale abbinarli a un’analisi causale. La pura osservazione dei dati può rivelare correlazioni e pattern nei dati, ma senza un’indagine sulle cause che stanno dietro a queste correlazioni, le conclusioni tratte possono essere fuorvianti o incomplete.\nAnche nell’ambito di una discussione sull’analisi descrittiva dei dati (e, forse, soprattutto in un tale contesto), è importante sottolineare le limitazioni di un tale approccio. L’aspetto cruciale da tenere a mente è che le cause dei fenomeni non possono essere inferite solamente dall’analisi dei dati. Né dall’analisi descrittiva dei dati che stiamo discutendo adesso, né dall’analisi inferenziale dei dati che discuteremo in seguito. Pertanto, per giungere ad una comprensione dei fenomeni è necessario integrare il processo di modellazione statistica con una comprensione delle cause sottostanti del fenomeno oggetto d’esame. In altre parole, per una comprensione scientificamente valida, è necessario combinare la modellazione statistica con l’analisi causale.\nIn questo capitolo, ci concentreremo sull’introduzione dei concetti fondamentali dell’analisi causale, i quali costituiscono una base cruciale per l’interpretazione dei dati. Cominceremo con la distinzione tra correlazione e causalità. La correlazione indica una relazione tra due variabili, mettendo in evidenza la loro forza e direzione, mentre la causalità implica un legame di causa ed effetto tra le variabili. È di importanza cruciale comprendere che il fatto che due variabili siano correlate non implica automaticamente l’esistenza di un rapporto causale tra di esse. Il noto principio “correlazione non implica causalità” sottolinea questa distinzione critica. Numerosi esempi di correlazioni che non sono basate su una relazione causale diretta possono essere esaminati sul sito spurious correlations.\nOltre a questo concetto fondamentale, procederemo ad esplorare una serie di concetti introdotti da Judea Pearl nel suo testo “Causality” (Pearl 2009). Questi concetti sono essenziali per descrivere le relazioni tra variabili. Essi includono strutture di relazione come biforcazioni, catene, collider e strutture discendenti. Questa cornice concettuale ci permetterà di distinguere tra i diversi tipi di legami causali tra le variabili, ponendo così le basi per condurre un’analisi dei dati più completa e accurata.\nMediante esempi numerici, dimostreremo l’importanza dei fattori confondenti nell’analisi della correlazione e della covarianza. Questo passo ci condurrà oltre l’analisi delle relazioni bivariate di base, introducendoci alle dinamiche più complesse che regolano le relazioni tra le variabili.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lo studio delle cause dei fenomeni</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/06_causality.html#inferenza-causale",
    "href": "chapters/chapter_2/06_causality.html#inferenza-causale",
    "title": "7  Lo studio delle cause dei fenomeni",
    "section": "7.2 Inferenza Causale",
    "text": "7.2 Inferenza Causale\nL’inferenza causale si pone il problema di rappresentare il processo sottostante a un fenomeno e consente di prevedere gli effetti di un intervento. Oltre a anticipare le conseguenze di una causa, permette anche di esplorare scenari controfattuali, immaginando gli esiti alternativi che si sarebbero verificati con decisioni diverse. Questo tipo di ragionamento è cruciale in contesti puramente descrittivi così come nell’inferenza.\nMcElreath (2020) utilizza l’analogia dei Golem, potenti ma privi di saggezza e previsione, per descrivere un approccio limitato che è stato a lungo lo standard in psicologia. Questo approccio si concentra sul semplice test delle ipotesi nulle e non stabilisce una relazione chiara tra le problematiche causali della ricerca e i test statistici. Tale limitazione è stata identificata come una delle principali cause della crisi di replicabilità dei risultati nella ricerca psicologica (si veda il capitolo {ref}generalizability-crisis-notebook) e, di conseguenza, della crisi della psicologia stessa.\n\n\n\nEsempio di albero decisionale per la selezione di una procedura statistica appropriata. Iniziando dall’alto, l’utente risponde a una serie di domande riguardanti la misurazione e l’intento, arrivando infine al nome di una procedura. Sono possibili molti alberi decisionali simili. (Figura tratta da McElreath (2020)).\n\n\nUn problema evidenziato da McElreath (2020) è che processi causali completamente distinti possono generare la stessa distribuzione di risultati osservati. Pertanto, un approccio focalizzato esclusivamente sul test dell’ipotesi nulla non è in grado di distinguere tra questi diversi scenari. Questa limitazione è dimostrata numericamente nel capitolo {ref}causal-inference-notebook.\nIl test dell’ipotesi nulla, ovvero l’approccio frequentista, nonostante sia stato ampiamente utilizzato in psicologia per decenni, ha mostrato una bassa sensibilità nel rilevare le caratteristiche cruciali dei fenomeni studiati e un alto tasso di falsi positivi (Zwet et al. 2023).\nLa ricerca scientifica richiede una metodologia più sofisticata rispetto all’approccio che si limita a confutare ipotesi nulle. È essenziale sviluppare modelli causali che rispondano direttamente alle domande di ricerca. Inoltre, è fondamentale avere una strategia razionale per l’estrazione delle stime dei modelli e per la quantificazione dell’incertezza associata ad esse. In questo contesto, l’analisi bayesiana dei dati emerge come un approccio altamente efficace. Anche se in analisi semplici le differenze rispetto all’approccio frequentista potrebbero sembrare minime o addirittura introdurre alcune complicazioni, quando ci si trova ad affrontare analisi più realistiche e complesse, la differenza diventa sostanziale.\nPer condurre un’analisi scientifica dei dati che superi l’approccio “amatoriale” frequentista, McElreath (2020) suggerisce di seguire una serie di passaggi chiave:\n\nComprendere il concetto teorico del fenomeno oggetto dell’analisi.\nSviluppare modelli causali che descrivano accuratamente le relazioni tra le variabili coinvolte nel problema di ricerca, basandosi sulla teoria sottostante.\nFormulare modelli statistici appropriati che riflettano fedelmente il contesto scientifico e le relazioni causali identificate.\nEseguire simulazioni basate sui modelli causali per verificare se i modelli statistici sviluppati siano in grado di stimare correttamente ciò che è teoricamente atteso. Questa fase di verifica è cruciale per garantire la validità dei modelli.\nInfine, condurre l’analisi dei dati effettivi utilizzando i modelli statistici sviluppati, avendo la fiducia che riflettano accuratamente le teorie sottostanti e le relazioni causali.\n\nUn elemento chiave sottolineato da McElreath (2020) nel processo di analisi è l’utilizzo dei Grafi Aciclici Direzionati (DAG), che rappresentano uno strumento essenziale per realizzare il flusso di lavoro descritto. I DAG forniscono una rappresentazione grafica dei modelli causali, consentendo una visualizzazione chiara delle relazioni tra le variabili coinvolte. Questi grafici rendono trasparenti le assunzioni alla base dell’analisi, creando una connessione evidente tra le teorie sottostanti e l’analisi statistica. In contrasto, l’approccio frequentista è caratterizzato dall’assenza di ipotesi sulle relazioni sottostanti tra le variabili, rendendo difficile comprendere e interpretare le implicazioni scientifiche dei risultati ottenuti.\nPer un approfondimento di questi temi, consiglio fortemente la lettura del primo capitolo di Statistical Rethinking.\n\n7.2.1 I Disegni di Ricerca e la Causalità\nPer comprendere meglio l’uso dei DAG nell’inferenza causale, è necessario distinguere tra ricerche basate su disegni osservazionali e sperimentali {cite:p}rohrer2018thinking.\nNei disegni osservazionali i ricercatori osservano e registrano gli eventi senza intervenire direttamente. A differenza degli esperimenti controllati, qui non si manipolano le variabili studiate, ma si osservano le relazioni naturali che emergono. Questi studi sono preziosi quando gli esperimenti non sono fattibili per motivi etici o pratici. Tuttavia, sebbene siano efficaci nell’identificare correlazioni e tendenze, i disegni osservazionali spesso mancano della capacità di stabilire con certezza relazioni causali, a causa di variabili confondenti e bias di selezione. Esempi comuni di disegni osservazionali includono studi trasversali e longitudinali.\nI disegni sperimentali, d’altra parte, si basano sulla manipolazione controllata delle variabili e sulla randomizzazione. Quest’ultima, ovvero l’assegnazione casuale dei partecipanti ai gruppi, è fondamentale per ridurre i bias e garantire la validità delle conclusioni causali. Gli esperimenti randomizzati sono considerati il gold standard per indagare le relazioni causali, poiché la randomizzazione consente di bilanciare gli effetti delle variabili osservate e non osservate tra i diversi gruppi di trattamento.\nNonostante gli esperimenti offrano un elevato grado di certezza nella determinazione delle relazioni causali, i disegni osservazionali presentano vantaggi in termini di flessibilità e applicabilità in situazioni in cui gli esperimenti non possono essere condotti per ragioni etiche o pratiche. {cite:p}rohrer2018thinking nota che, mentre i disegni osservazionali spesso si basano su relazioni associative piuttosto che causali, gli esperimenti potrebbero non consentire una generalizzazione al di là delle condizioni artificiali del laboratorio.\nNel contesto della ricerca, il concetto di causalità si riferisce alla relazione tra una variabile (X) e un’altra (Y), in cui X può influenzare Y. Il linguaggio causale utilizza termini come “causa”, “influenza” e “determina”, a differenza di termini descrittivi come “associato” o “correlato”, che indicano semplicemente relazioni senza implicare un legame causale diretto. È importante ribadire che la causalità è concepita come un concetto probabilistico, il che significa che una variazione in X aumenta la probabilità di un certo risultato in Y, piuttosto che determinarlo in modo assoluto.\nNel campo della ricerca psicologica, spesso è necessario inferire relazioni causali da dati osservazionali poiché gli esperimenti randomizzati non sono sempre praticabili o eticamente accettabili. Tuttavia, questo approccio presenta diverse sfide che possono influenzare l’integrità e l’accuratezza delle inferenze tratte dagli studi. Per affrontare tali sfide e migliorare la solidità delle loro analisi, i ricercatori impiegano una serie di strategie metodologiche.\n\nUtilizzo di interventi surrogati: questa tecnica è preziosa in situazioni in cui la manipolazione diretta di una variabile di interesse è impraticabile o eticamente inaccettabile. Piuttosto che intervenire direttamente, i ricercatori si avvalgono di variabili surrogate (o proxy) che presumibilmente sono associate alla variabile di interesse principale. L’obiettivo è esplorare e dedurre gli effetti indiretti di una variabile su un’altra, stabilendo associazioni che, sebbene non dirette, possono fornire importanti informazioni sul fenomeno in esame. Tuttavia, questa strategia presenta delle sfide intrinseche, come la difficoltà nel garantire una relazione diretta tra la variabile surrogata e quella di interesse principale.\nLinguaggio cauto nell’interpretazione dei dati: i ricercatori adottano un linguaggio prudente quando interpretano dati osservazionali, evitando di fare affermazioni causali dirette quando i dati mostrano solo correlazioni.\nUtilizzo di metodi statistici avanzati per controllare variabili confondenti: Le variabili confondenti sono fattori esterni che possono influenzare la relazione tra le variabili studiate. L’utilizzo di tecniche statistiche avanzate, come la regressione multipla o l’analisi di matching, aiuta a controllare l’effetto di queste variabili confondenti, consentendo una stima più accurata della relazione tra le variabili di interesse. Tuttavia, questi metodi hanno limitazioni (si veda il capitolo {ref}causal-inference-notebook) e potrebbero non riuscire a controllare completamente tutte le variabili confondenti, specialmente in situazioni complesse.\n\n{cite:t}rohrer2018thinking sottolinea che, nonostante l’adozione di queste strategie, persistono sfide in termini di validità esterna (cioè la generalizzabilità dei risultati) e interpretazione dei dati. La validità esterna potrebbe essere limitata poiché le condizioni specifiche dello studio potrebbero non rappresentare situazioni più ampie o diverse, mentre l’interpretazione dei risultati può essere complicata dalla natura indiretta o surrogata delle evidenze raccolte.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lo studio delle cause dei fenomeni</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/06_causality.html#modelli-causali-grafici",
    "href": "chapters/chapter_2/06_causality.html#modelli-causali-grafici",
    "title": "7  Lo studio delle cause dei fenomeni",
    "section": "7.3 Modelli Causali Grafici",
    "text": "7.3 Modelli Causali Grafici\nL’analisi dei dati osservazionali per trarre inferenze causali è intrinsecamente complessa, ma con precauzioni adeguate e approcci metodologici appropriati, è possibile estrarre preziosi insight causali anche da questo tipo di dati. In questo contesto, Rohrer (2018) sottolinea l’importanza dei DAG come strumento fondamentale per la rappresentazione visiva delle ipotesi causali. Questi diagrammi non solo agevolano la comprensione delle relazioni causali tra le variabili, ma forniscono anche una guida per l’identificazione e il trattamento corretto delle variabili esterne, comunemente note come “variabili di confondimento”.\nUna delle funzioni più cruciali dei DAG è la loro capacità di individuare quali variabili di confondimento debbano essere considerate nell’analisi e quali possano essere trascurate senza compromettere l’integrità dell’inferenza causale. Questa caratteristica è essenziale poiché sia il controllo insufficiente sia quello eccessivo delle variabili di confondimento possono condurre a conclusioni erronee. In particolare, i DAG aiutano a determinare quando l’aggiustamento per determinate variabili di confondimento può migliorare l’accuratezza delle inferenze causali e quando, al contrario, tale aggiustamento potrebbe introdurre distorsioni (questo punto verrà approfondito nel capitolo {ref}causal-inference-notebook).\nGrazie ai DAG, i ricercatori possono esplicitare le loro ipotesi sulle relazioni causali tra le variabili, riducendo così il rischio di interpretare erroneamente i dati osservazionali. Questo approccio consente di sviluppare strategie di analisi più informate e metodologicamente solide, mirate a comprendere al meglio le dinamiche causali sottostanti.\n\n7.3.1 Importanza delle Assunzioni\nÈ cruciale comprendere che l’estrazione di conclusioni causali da dati correlazionali non può essere basata esclusivamente sui dati stessi. È invece necessario possedere delle conoscenze sulle dinamiche causali del fenomeno esaminato. Questo principio deriva dalla consapevolezza che una correlazione osservata può essere attribuita a una vasta gamma di fattori, inclusa l’influenza potenziale di variabili terze non considerate nell’analisi. Di conseguenza, qualsiasi tentativo di inferenza causale privo di una comprensione preliminare delle possibili relazioni causali rischia di essere fallace.\nNonostante queste sfide, è fondamentale non rinunciare davanti alla complessità intrinseca della ricerca osservazionale. Pur non potendo dimostrare inequivocabilmente la causalità, per via della loro incapacità di manipolare direttamente le variabili di interesse, gli studi osservazionali svolgono un ruolo irrinunciabile nel contesto scientifico. Questi studi sono fondamentali per la generazione di ipotesi e per guidare la ricerca futura verso domande scientifiche rilevanti. Allo stesso modo, gli studi sperimentali, benché considerati il gold standard per la determinazione delle relazioni causali, non sono esenti da limitazioni. Gli esperimenti, soprattutto quelli condotti in ambienti altamente controllati come i laboratori, presuppongono la generalizzabilità dei risultati al di là dei confini dello studio, un’assunzione che può non sempre trovare riscontro nella realtà.\nIn questo contesto, ciò che rende un’indagine rigorosa, sia essa osservazionale che sperimentale, è la capacità di riconoscere, articolare e comunicare chiaramente le premesse su cui si basa. Questo approccio non solo facilita una valutazione critica delle conclusioni causali da parte della comunità scientifica, ma promuove anche un dialogo costruttivo e produttivo all’interno della stessa, fondamentale per il progresso della conoscenza. La trasparenza nell’esposizione delle premesse rappresenta un prerequisito essenziale per un avanzamento scientifico informato, consapevole e, soprattutto, veritiero.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lo studio delle cause dei fenomeni</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/06_causality.html#introduzione-ai-dag",
    "href": "chapters/chapter_2/06_causality.html#introduzione-ai-dag",
    "title": "7  Lo studio delle cause dei fenomeni",
    "section": "7.4 Introduzione ai DAG",
    "text": "7.4 Introduzione ai DAG\nI Grafi Aciclici Diretti (DAG) offrono una rappresentazione grafica delle relazioni causali ipotizzate tra le variabili. In un DAG, le frecce vengono utilizzate per indicare le relazioni causali tra diverse variabili.\nSono chiamati così perché:\n\nLe variabili presenti nel diagramma (nodi) possono essere collegate tra loro solo attraverso frecce, anziché semplici linee di collegamento, da cui il termine “diretti”.\nPartendo da un nodo, non è possibile ritornare allo stesso nodo seguendo il percorso delle frecce, da cui il termine “aciclici”.\n\nLe variabili sono chiamate nodi del DAG. Un cammino tra due nodi è una sequenza di frecce che collegano i due nodi, indipendentemente dalla direzione delle frecce.\nAll’interno di un DAG, la freccia che va dal nodo \\(X\\) al nodo \\(Y\\) evidenzia una relazione causale, implicando che \\(X\\) abbia un’influenza su \\(Y\\). Tuttavia, questa relazione non è deterministica; piuttosto, le variazioni nel livello di \\(X\\) sono associate a cambiamenti in \\(Y\\) su base probabilistica. Pertanto, la presenza della freccia nel DAG causale indica che un aumento di \\(X\\) aumenta la probabilità che \\(Y\\) aumenti, ma non lo garantisce.\nSe tra due nodi c’è una sola freccia, il nodo dal quale parte la freccia è detto “genitore”, mentre quello di arrivo è detto “figlio”. Se, partendo da un nodo A, si arriva a un nodo B seguendo il verso di una successione di frecce, allora il nodo A è detto “antenato”, mentre B è detto “discendente”.\nI DAG permettono di identificare quali variabili agiscono come confondenti e quali no, basandosi sulla teoria sviluppata da Judea Pearl {cite:p}pearl2009causality. È importante rappresentare tutte le possibili relazioni nel DAG, poiché l’assenza di una freccia tra due variabili implica una certezza dell’assenza di relazione tra di esse.\nDue dei concetti fondamentali della teoria dei DAG sono la d-separazione e il back-door path.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lo studio delle cause dei fenomeni</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/06_causality.html#concetto-di-d-separazione",
    "href": "chapters/chapter_2/06_causality.html#concetto-di-d-separazione",
    "title": "7  Lo studio delle cause dei fenomeni",
    "section": "7.5 Concetto di d-separazione",
    "text": "7.5 Concetto di d-separazione\nLa d-separazione è un concetto fondamentale per comprendere come le variabili all’interno di un DAG possano influenzarsi reciprocamente. Essa definisce se un insieme di variabili può bloccare l’influenza di una variabile su un’altra. In altre parole, la d-separazione ci aiuta a capire se un percorso che collega due variabili è attivo o inattivo, considerando un insieme di nodi specifico.\n\n7.5.1 Definizione di d-separazione\nUn cammino tra due nodi è d-separato (o bloccato) da un insieme Λ di nodi se e solo se si verificano le seguenti condizioni:\n\nCatena (Sequenza di frecce): Se il cammino è costituito da una sequenza di frecce nella stessa direzione (ad esempio, \\(X \\rightarrow Z \\rightarrow Y\\)), il nodo intermedio (\\(Z\\)) deve appartenere all’insieme Λ affinché il cammino sia bloccato. Questo significa che la variabile \\(Z\\) deve essere inclusa nell’analisi per bloccare l’influenza di \\(X\\) su \\(Y\\).\nFork (Nodo con due frecce in uscita): Se nel cammino c’è un nodo da cui partono due frecce (ad esempio, \\(X \\leftarrow Z \\rightarrow Y\\)), il nodo di partenza delle frecce (\\(Z\\)) deve appartenere all’insieme Λ affinché il cammino sia bloccato. In questo caso, \\(Z\\) agisce come una variabile confondente, e includerla nell’analisi blocca il percorso.\nCollider (Nodo con due frecce in entrata): Se nel cammino c’è un nodo in cui convergono due frecce (ad esempio, \\(X \\rightarrow Z \\leftarrow Y\\)), né il nodo di convergenza (\\(Z\\)) né i suoi discendenti devono appartenere all’insieme Λ affinché il cammino sia bloccato. Un collider introduce un’associazione tra \\(X\\) e \\(Y\\) solo se \\(Z\\) o i suoi discendenti sono inclusi nell’analisi.\n\nLa d-separazione è uno strumento potente per identificare i percorsi di confondimento e determinare quali variabili devono essere considerate nel modello statistico per ottenere inferenze causali accurate.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lo studio delle cause dei fenomeni</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/06_causality.html#il-criterio-del-back-door",
    "href": "chapters/chapter_2/06_causality.html#il-criterio-del-back-door",
    "title": "7  Lo studio delle cause dei fenomeni",
    "section": "7.6 Il criterio del back-door",
    "text": "7.6 Il criterio del back-door\nIl criterio del back-door è uno strumento grafico utile per individuare le variabili confondenti che devono essere considerate per ottenere stime causali corrette. Questo criterio si basa sulla verifica dei percorsi non causali, detti “back-door paths”, che collegano l’esposizione all’outcome.\nIl criterio del back-door può essere applicato mediante i seguenti passaggi:\n\nEliminare le frecce dirette: Rimuovere tutte le frecce che vanno dall’esposizione all’outcome nel grafo.\nVerificare i percorsi non bloccati: Controllare se esistono percorsi non bloccati tra l’esposizione e l’outcome nel grafo modificato.\n\nSe tutti i percorsi sono bloccati, significa che non c’è confondimento, e quindi l’esposizione e l’outcome sono indipendenti. Se invece esistono percorsi non bloccati, è necessario individuare e aggiustare per le variabili che possono bloccarli.\n\n7.6.1 Esempi di applicazione del criterio del back-door\nConsideriamo un DAG che rappresenta la relazione tra autostima (A) e performance accademica (P), tenendo conto di variabili come il supporto familiare (S) e il livello di stress (L). Il DAG è strutturato come segue:\n\nA → P\nS → A\nS → P\nL ← S\n\nRimuovendo la freccia diretta A → P, rimangono i percorsi:\n\nA ← S → P\nA ← S → L → P\n\nPer bloccare questi percorsi, è necessario condizionare su S, che blocca entrambi i percorsi. Pertanto, per ottenere una stima non distorta della relazione tra autostima e performance accademica, è sufficiente aggiustare per il supporto familiare (S).\n\n7.6.1.1 Esempio 2: Relazione tra attività fisica e benessere psicologico\nConsideriamo un DAG con le seguenti relazioni:\n\nAttività fisica (F) → Benessere psicologico (B)\nGenere (G) → F\nGenere (G) → B\nEtà (E) → F\nEtà (E) → B\n\nIn questo caso, abbiamo i percorsi:\n\nF ← G → B\nF ← E → B\n\nRimuovendo la freccia diretta F → B, rimangono i percorsi non bloccati:\n\nF ← G → B\nF ← E → B\n\nPer bloccare questi percorsi, è necessario condizionare sia su G (genere) che su E (età). Pertanto, per ottenere una stima non distorta della relazione tra attività fisica e benessere psicologico, è necessario aggiustare per le variabili genere ed età.\n\n\n\n7.6.2 Procedura per determinare l’insieme sufficiente di variabili\nPer decidere se un insieme di variabili è sufficiente per l’aggiustamento, è possibile seguire questi passaggi:\n\nEliminare le frecce dell’esposizione: Rimuovere tutte le frecce che partono dalla variabile di esposizione.\nAggiungere archi di controllo: Aggiungere tutti gli archi necessari generati dal controllo sulle variabili dell’insieme considerato.\nVerificare i percorsi non bloccati: Se tutti i percorsi non bloccati tra l’esposizione e l’outcome contengono almeno una variabile dell’insieme considerato, allora l’insieme è sufficiente per il controllo.\n\nL’insieme può anche essere vuoto se non è necessario aggiustare per alcuna variabile.\nIn sintesi, il criterio del back-door consente di ridurre il numero di variabili da considerare per l’aggiustamento, risolvendo un importante problema pratico. Identificare l’insieme sufficiente minimale di variabili aiuta a mantenere l’analisi gestibile e precisa, migliorando la validità delle inferenze causali. Questo è particolarmente utile in psicologia, dove le variabili confondenti sono spesso numerose e complesse.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lo studio delle cause dei fenomeni</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/06_causality.html#strutture-causali-elementari",
    "href": "chapters/chapter_2/06_causality.html#strutture-causali-elementari",
    "title": "7  Lo studio delle cause dei fenomeni",
    "section": "7.7 Strutture Causali Elementari",
    "text": "7.7 Strutture Causali Elementari\nAll’interno dei DAG causali, si possono riconoscere quattro tipologie fondamentali di strutture causali: confondente, catena, collider e discendenti. La comprensione di queste strutture causali di base è fondamentale per analizzare i percorsi causali più complessi.\n\n\n\n```wswhuuru ../images/four_confounders.png\n\n\n\n\nheight: 250px\n\n\nname: four-confounders-fig\n\n\n\nI quattro confondenti di base. (Figura tratta da {cite:t}McElreath_rethinking).\n\n### Catena\n\nIl concetto di catena (*pipe*) in un DAG descrive una sequenza in cui una variabile $X$ influisce su una variabile $Y$ attraverso una variabile intermedia $Z$, formando una catena causale del tipo $X \\rightarrow Z \\rightarrow Y$. In questo scenario, $Z$ agisce come mediatore della relazione causale tra $X$ e $Y$. Controllando per $Z$, la relazione diretta osservata tra $X$ e $Y$ scompare perché $X$ influisce su $Y$ esclusivamente attraverso $Z$.\n\nIn sintesi,\n\n- $X$ e $Y$ sono associati: $X \\not\\perp Y$.\n- L'influenza di $X$ su $Y$ viene trasmessa tramite $Z$.\n- Stratificando i dati in base a $Z$, l'associazione tra $X$ e $Y$ scompare: $X \\perp Y \\mid Z$.\n\n**Esempio.** Consideriamo l'effetto dell'educazione sull'occupabilità attraverso l'acquisizione di competenze specifiche.\n\n**Variabili:**\n- $X$: Livello di Educazione (ad es., nessun diploma, diploma di scuola superiore, laurea)\n- $Z$: Competenze Specifiche Acquisite (ad es., competenze informatiche, competenze linguistiche, competenze professionali specifiche)\n- $Y$: Occupabilità (misurata come probabilità di ottenere un lavoro nel proprio campo di studio entro un certo periodo dopo il completamento degli studi)\n\n**Relazione Causale:**\nIl livello di educazione ($X$) influisce sull'occupabilità ($Y$) attraverso l'acquisizione di competenze specifiche ($Z$). In altre parole, non è tanto il titolo di studio in sé a determinare direttamente l'occupabilità, quanto le competenze specifiche acquisite durante il percorso educativo. Questo è un classico esempio di relazione di tipo \"pipe\".\n\n**Scenario senza Stratificazione:**\nSenza stratificare per $Z$, si potrebbe osservare una correlazione positiva tra $X$ e $Y$, suggerendo che un maggiore livello di educazione è associato a una maggiore occupabilità. Tuttavia, questa osservazione non chiarisce se il titolo di studio in sé sia il fattore determinante o se ciò dipenda dalle competenze acquisite.\n\n**Scenario con Stratificazione per $Z$:**\nQuando si stratifica per le competenze specifiche acquisite ($Z$), controllando quindi per il tipo e il livello di competenze ottenute, la relazione diretta tra il livello di educazione e l'occupabilità potrebbe scomparire o indebolirsi fortemente. Questo indica che, una volta considerate le competenze acquisite, il livello di educazione in sé non ha un impatto diretto sull'occupabilità. L'effetto osservato del livello di educazione sull'occupabilità è quindi mediato completamente dalle competenze specifiche acquisite.\n\nQuesto esempio mostra come, in una relazione a catena, la variabile mediatrice ($Z$) spiega completamente la relazione tra la variabile indipendente ($X$) e la variabile dipendente ($Y$).\n\n### Confondente\n\nIl concetto di confondente (*fork*) si riferisce a una struttura in cui una variabile comune $Z$ causa due variabili $X$ e $Y$, formando una relazione del tipo $X \\leftarrow Z \\rightarrow Y$. In questo scenario, $Z$ è una variabile confondente che può generare una correlazione apparente tra $X$ e $Y$, anche se non esiste una relazione causale diretta tra di loro. Analizzare adeguatamente il ruolo di $Z$ è fondamentale per comprendere le vere relazioni causali tra $X$ e $Y$.\n\n**Esempio.** Consideriamo l'influenza dell'età sul numero di scarpe e abilità matematiche.\n\n**Variabili:**\n- $Z$: Età (variabile confondente)\n- $X$: Numero di Scarpe\n- $Y$: Abilità Matematiche\n\n**Relazione Causale:**\nIn questo esempio, l'età ($Z$) è la variabile confondente che influisce sia sul numero di scarpe ($X$) che sulle abilità matematiche ($Y$). Con l'aumentare dell'età, generalmente aumenta il numero di scarpe a causa della crescita fisica e migliorano le abilità matematiche grazie all'apprendimento scolastico e allo sviluppo cognitivo.\n\n**Senza Considerare l'Età:**\nSe non consideriamo l'età ($Z$) nell'analisi, potremmo erroneamente concludere che esiste una relazione diretta tra il numero di scarpe ($X$) e le abilità matematiche ($Y$), dato che entrambi aumentano nel tempo. Tuttavia, questa correlazione non implica una relazione causale diretta tra il numero di scarpe e le abilità matematiche, ma è piuttosto il risultato dell'influenza dell'età.\n\n**Considerando l'Età:**\nUna volta che l'età viene inclusa nell'analisi come variabile confondente, diventa chiaro che qualsiasi correlazione osservata tra il numero di scarpe e le abilità matematiche è spiegata dalla loro relazione comune con l'età. Stratificando per età o utilizzando metodi statistici per controllare l'effetto dell'età, possiamo correttamente interpretare che non esiste una relazione causale diretta tra il numero di scarpe e le abilità matematiche.\n\nQuesto esempio illustra l'importanza di identificare e controllare le variabili confondenti nelle analisi causali. \n\n### Collider\n\nLa struttura causale nota come *collider* si verifica quando due variabili, $X$ e $Y$, convergono influenzando una terza variabile, $Z$, formando una configurazione del tipo $X \\rightarrow Z \\leftarrow Y$. In questo schema, $X$ e $Y$ sono indipendenti, il che significa che non esiste una relazione causale diretta o un'influenza reciproca evidente tra di loro. Sebbene entrambe influenzino $Z$, non si può dedurre l'esistenza di un legame causale diretto tra $X$ e $Y$ basandosi esclusivamente sulla loro comune influenza su $Z$.\n\n**Esempio.** Consideriamo quali variabili, il talento musicale, il supporto familiare e il successo in una carriera musicale.\n\n**Variabili:**\n- $X$: Talento Musicale\n- $Y$: Supporto Familiare\n- $Z$: Successo nella Carriera Musicale\n\n**Relazione Causale:**\nIn questo esempio, sia il talento musicale ($X$) che il supporto familiare ($Y$) influenzano il successo nella carriera musicale ($Z$). Tuttavia, il talento musicale e il supporto familiare sono indipendenti l'uno dall'altro. Non esiste una relazione causale diretta tra il talento musicale e il supporto familiare.\n\n**Senza Considerare $Z$:**\n$X$ e $Y$ sono indipendenti ($X \\perp Y$). Il talento musicale e il supporto familiare non hanno alcuna influenza reciproca diretta.\n\n**Considerando $Z$:**\nQuando ci si concentra esclusivamente sulle persone che hanno raggiunto il successo musicale ($Z$), si potrebbe erroneamente percepire un'associazione tra talento musicale e supporto familiare. Questo è conosciuto come *bias del collider*. Questa associazione artificiale emerge perché stiamo selezionando un sottogruppo basato su $Z$, che è influenzato sia da $X$ che da $Y$. Pertanto, quando si stratifica per $Z$, $X$ e $Y$ appaiono associati ($X \\not\\perp Y \\mid Z$).\n\nIn sintesi,\n\n- **Indipendenza:** $X$ e $Y$ sono indipendenti: $X \\perp Y$.\n- **Influenza:** Sia $X$ che $Y$ influenzano $Z$.\n- **Associazione Spuria:** Stratificando i dati in base a $Z$, emerge un'associazione spuria: $X \\not\\perp Y \\mid Z$.\n\nIl bias del collider si verifica quando la selezione di casi basata su $Z$ introduce un'associazione artificiale tra $X$ e $Y$, che non riflette le relazioni presenti nell'intera popolazione. Questo può portare a interpretazioni errate e conclusioni fuorvianti.\n\n### Il Discendente\n\nLa configurazione del *discendente* è caratterizzata dalla presenza di una variabile, $Z$, che riceve l'influenza da una variabile $X$ e trasmette tale effetto a una variabile $Y$. Inoltre, $Z$ esercita un'influenza diretta su un'altra variabile, $A$. Pertanto, $Z$ agisce sia come mediatore nella relazione causale tra $X$ e $Y$, sia come collegamento causale diretto con $A$. La struttura risultante può essere rappresentata come $X \\rightarrow Z \\rightarrow Y$, con una ramificazione aggiuntiva da $Z$ ad $A$ ($Z \\rightarrow A$).\n\n**Esempio.** Consideriamo l'effetto del supporto sociale sulla felicità attraverso l'autostima.\n\n**Variabili:**\n- $X$: Supporto Sociale\n- $Z$: Autostima\n- $Y$: Felicità\n- $A$: Livello di Stress\n\n**Relazione Causale:**\nIl supporto sociale ($X$) migliora l'autostima ($Z$), che a sua volta influisce sulla felicità ($Y$) e sul livello di stress ($A$). In questo caso, $Z$ agisce come mediatore tra $X$ e $Y$ e ha un effetto diretto su $A$.\n\n**Senza Considerare $A$**: Concentrandosi sul percorso $X \\rightarrow Z \\rightarrow Y$, si può isolare l'effetto del supporto sociale sulla felicità attraverso l'autostima, mantenendo chiara la relazione causale.\n\n**Considerando $A$**: Quando si include $A$ nell'analisi, si introduce complessità poiché $A$ può aprire nuovi percorsi causali o introdurre bias, complicando l'interpretazione dell'effetto diretto di $X$ su $Y$. Se $A$ ha un discendente $D$ che funge da collider, influenzato sia da $A$ che da una causa comune non osservata con $Y$ ($U$), condizionare su $D$ può introdurre un bias aprendo un percorso non causale ($A \\rightarrow D \\leftarrow U \\rightarrow Y$). Questo esemplifica come l'inclusione di discendenti e colliders possa alterare l'interpretazione degli effetti causali.\n\nIn sintesi,\n\n- $X$ e $Y$ sono associati causalmente attraverso $Z$: $X \\not\\perp Y$.\n- $A$ fornisce informazioni su $Z$.\n- Stratificando i dati in base ad $A$, l'associazione tra $X$ e $Y$ può indebolirsi o scomparire: $X \\perp Y \\mid A$.\n\nComprendere il ruolo dei discendenti e dei collider è cruciale per evitare interpretazioni errate dei dati e per fare inferenze causali accurate. \n\n## Considerazioni Conclusive\n\nNegli studi basati su dati osservazionali, districarsi tra le complesse maglie delle inferenze causali richiede un'acuta analisi e una particolare sensibilità verso vari aspetti critici del processo di ricerca. Ogni fase, dall'identificazione dei fattori confondenti all'analisi accurata dei percorsi causali, passando per la gestione dei mediatori e dei collider, fino alla valutazione della robustezza interna ed esterna degli studi, presenta sfide e opportunità.\n\nIl primo passo verso un'interpretazione corretta dei dati osservazionali consiste nel saper riconoscere e gestire le variabili confondenti. Queste variabili, agendo su entrambe le variabili indipendenti e dipendenti, possono generare correlazioni ingannevoli che mascherano la vera natura delle relazioni in esame.\n\nL'uso dei DAG (Grafi Aciclici Diretti) emerge come uno strumento molto utile in questo contesto, offrendo una rappresentazione visiva dei percorsi causali e fornendo indicazioni preziose su come evitare trappole analitiche quali il sovracontrollo o il controllo inappropriato di variabili post-trattamento.\n\nQuando si tratta di mediatori e collider, il terreno si fa ancora più insidioso. I mediatori, essendo ponti tra cause ed effetti, necessitano di una gestione oculata per non perdere di vista l'effetto causale che si intende esplorare. Allo stesso tempo, è fondamentale resistere alla tentazione di controllare indiscriminatamente i collider, poiché ciò può aprire la porta a correlazioni spurie che distorcono la realtà dei fenomeni studiati.\n\nLa questione del bilanciamento tra validità interna ed esterna ricorda che, sebbene gli esperimenti randomizzati possano offrire garanzie solide di validità interna, la loro capacità di generalizzare i risultati al mondo esterno – la validità esterna – non è sempre garantita. Solo un approccio che valorizza la diversità metodologica, abbracciando tanto gli studi sperimentali quanto quelli osservazionali, può rispondere in modo completo alle domande di ricerca.\n\nIn conclusione, per studiare con successo le dinamiche nascoste che regolano le relazioni tra le variabili, è essenziale non solo un'attenta valutazione dei dati disponibili ma anche una profonda riflessione sulle strutture causali in gioco. Integrare diversi approcci di ricerca arricchisce la nostra comprensione e ci avvicina a conclusioni causali solide e generalizzabili.\n\nUn sommario ironico di questi concetti è fornito nella vignetta di [xkcd](https://www.explainxkcd.com/wiki/index.php/2560:_Confounding_Variables).\n\n## Informazioni sull'Ambiente di Sviluppo\n\n::: {#d99129b7-4eef-4219-bc89-77b7165e7b49 .cell execution_count=14}\n``` {.python .cell-code}\n%load_ext watermark\n%watermark -n -u -v -iv -w -m\n\nLast updated: Sat Feb 03 2024\n\nPython implementation: CPython\nPython version       : 3.11.7\nIPython version      : 8.19.0\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.3.0\nMachine     : x86_64\nProcessor   : i386\nCPU cores   : 8\nArchitecture: 64bit\n\nnumpy     : 1.26.2\nseaborn   : 0.13.0\nscipy     : 1.11.4\nmatplotlib: 3.8.2\narviz     : 0.17.0\ngraphviz  : 0.20.1\npandas    : 2.1.4\n\nWatermark: 2.4.3\n\n\n:::\n\n\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd Edition. Boca Raton, Florida: CRC Press.\n\n\nPearl, Judea. 2009. Causality. Cambridge University Press.\n\n\nRohrer, Julia M. 2018. “Thinking Clearly about Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42.\n\n\nZwet, Erik van, Andrew Gelman, Sander Greenland, Guido Imbens, Simon Schwab, and Steven N Goodman. 2023. “A New Look at p Values for Randomized Clinical Trials.” NEJM Evidence 3 (1): EVIDoa2300003.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lo studio delle cause dei fenomeni</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html",
    "title": "8  Linguaggio Stan",
    "section": "",
    "text": "8.1 Introduzione\nNel presente capitolo, presenteremo un linguaggio di programmazione probabilistica denominato Stan. Stan consente di estrarre campioni da distribuzioni di probabilità mediante la costruzione di una catena di Markov, la cui distribuzione di equilibrio (o stazionaria) coincide con la distribuzione desiderata. Il nome del linguaggio deriva da uno dei pionieri del metodo Monte Carlo, Stanislaw Ulam. Un’introduzione dettagliata al linguaggio Stan è fornita in (appendix-cmdstanpy?). In questo capitolo, utilizzeremo Stan per fare inferenza su una proporzione.\nIl linguaggio di programmazione probabilistica Stan è compatibile con diverse piattaforme e offre varie interfacce (R, Python, Julia). In questo corso, useremo CmdStanPy, un’interfaccia per Stan pensata per gli utenti di Python. CmdStanPy è un pacchetto puramente in Python3 che è un wrapper di CmdStan, l’interfaccia a riga di comando per Stan scritta in C++. Pertanto, oltre a Python3, CmdStanPy richiede un toolchain C++ per compilare ed eseguire i modelli Stan.\nLa procedura per installare CmdStanPy e i componenti sottostanti di CmdStan dal repository conda-forge è descritta nel capitolo Appendix A.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#inferenza-bayesiana-e-metodi-mcmc",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#inferenza-bayesiana-e-metodi-mcmc",
    "title": "8  Linguaggio Stan",
    "section": "8.2 Inferenza Bayesiana e Metodi MCMC",
    "text": "8.2 Inferenza Bayesiana e Metodi MCMC\nL’inferenza bayesiana, impiegata per la stima dei parametri, la previsione e la valutazione della probabilità di eventi, si basa sulle aspettative a posteriori. Queste aspettative si configurano come integrali multidimensionali nello spazio dei parametri. Stan, un software all’avanguardia per l’analisi statistica, si avvale del metodo Monte Carlo per risolvere questi integrali complessi. I metodi Monte Carlo sfruttano il campionamento casuale per affrontare integrali ad alta dimensionalità.\nTuttavia, per la maggior parte dei problemi bayesiani, non è possibile utilizzare i metodi Monte Carlo standard, poiché non è fattibile generare campioni indipendenti dalla densità a posteriori di interesse, fatta eccezione per modelli estremamente semplici con prior coniugati. Di conseguenza, è necessario ricorrere ai metodi Monte Carlo a Catena di Markov (MCMC), che producono campioni correlati tra loro. Stan implementa il Monte Carlo Hamiltoniano (HMC), il metodo MCMC più efficiente e scalabile per le densità target. Altri metodi, come il Metropolis-Hastings e il campionamento di Gibbs, risultano più semplici ma meno efficienti dell’HMC.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#stan-e-la-programmazione-probabilistica",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#stan-e-la-programmazione-probabilistica",
    "title": "8  Linguaggio Stan",
    "section": "8.3 Stan e la Programmazione Probabilistica",
    "text": "8.3 Stan e la Programmazione Probabilistica\nStan si configura come un linguaggio di programmazione probabilistica (PPL) concepito per definire modelli statistici complessi e effettuare inferenze su di essi. Un PPL consente di esprimere modelli probabilistici in modo conciso e di utilizzare algoritmi avanzati per l’inferenza. Ciò risulta particolarmente utile nell’inferenza bayesiana, dove si aggiornano le distribuzioni a priori con dati osservati per ottenere distribuzioni a posteriori.\n\n8.3.1 Struttura di un Programma Stan\nUn programma Stan richiede la specificazione di variabili e parametri, definendo le distribuzioni a priori dei parametri del modello statistico e la funzione di verosimiglianza. In sostanza, un programma Stan descrive l’interazione tra dati e parametri e le distribuzioni probabilistiche che li governano. Questo consente di effettuare inferenze sulle distribuzioni a posteriori dei parametri del modello, dedotte dai dati osservati e dalle distribuzioni a priori.\n\n\n8.3.2 Esecuzione di un Programma Stan\nUn programma Stan utilizza metodi di inferenza avanzati:\n\nCampionamento MCMC: Stan impiega metodi come il Monte Carlo a Catena di Markov per generare campioni dalle distribuzioni a posteriori.\nInferenza Variazionale: Un metodo approssimativo che fornisce stime delle distribuzioni a posteriori.\nApprossimazione di Laplace: Un ulteriore metodo approssimativo per l’inferenza.\n\nStan è accessibile attraverso vari linguaggi di programmazione e strumenti di analisi open-source, tra cui Python, R e Julia, ed è compatibile con gli strumenti di analisi bayesiana integrati in questi linguaggi. È inoltre disponibile in ambienti come Mathematica, Stata e MATLAB, sebbene queste interfacce siano meno complete.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#simulazione-in-avanti-e-problema-inverso",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#simulazione-in-avanti-e-problema-inverso",
    "title": "8  Linguaggio Stan",
    "section": "8.4 Simulazione in Avanti e Problema Inverso",
    "text": "8.4 Simulazione in Avanti e Problema Inverso\nStan genera dati attraverso procedure pseudo-casuali, applicabili sia per simulazioni in avanti che per risolvere il problema inverso.\n\n8.4.1 Simulazione in Avanti\nLa simulazione in avanti consiste nella generazione di dati simulati a partire da un insieme di parametri noti di un modello probabilistico. In altri termini, date determinate assunzioni sui parametri di un modello, si utilizza la simulazione in avanti per prevedere i possibili risultati.\nAd esempio, consideriamo uno studio clinico con \\(N\\) soggetti e una probabilità \\(\\theta\\) di esito positivo per ciascun soggetto. Conoscendo il valore di \\(\\theta\\) e il numero di soggetti \\(N\\), possiamo impiegare una distribuzione binomiale per simulare il numero di pazienti che avranno un esito positivo. Questo processo ci consente di generare dati che riflettono le nostre assunzioni sui parametri del modello.\nIn notazione statistica, questo si esprime come:\n\\[\nY \\sim \\text{Binomiale}(N, \\theta)\n\\]\ndove \\(Y\\) rappresenta il numero di esiti positivi su \\(N\\) pazienti, con probabilità \\(\\theta\\) di esito positivo per ciascun paziente.\n\n8.4.1.1 Esempio di Simulazione in Avanti\nSupponiamo di avere \\(N = 100\\) soggetti in uno studio clinico e un tasso di successo \\(\\theta = 0.3\\). Possiamo simulare un risultato \\(Y\\) generando casualmente il numero di soggetti con esito positivo. Utilizzando una distribuzione binomiale, possiamo calcolare la probabilità di ottenere esattamente \\(y\\) esiti positivi su \\(N\\) tentativi:\n\\[\np(Y = y \\mid N, \\theta) = \\binom{N}{y} \\cdot \\theta^y \\cdot (1 - \\theta)^{N - y}\n\\]\nQuesta espressione ci permette di calcolare la probabilità di ottenere un certo numero di successi, dato il numero di soggetti e la probabilità di successo.\n\n\n\n8.4.2 Il Problema Inverso\nIl problema inverso consiste nella stima dei parametri del modello, come la probabilità di successo \\(\\theta\\), dato un insieme di dati osservati. Supponiamo di avere i seguenti dati: \\(N = 100\\) soggetti e \\(y = 32\\) esiti positivi. L’obiettivo è stimare \\(\\theta\\), la probabilità di successo.\nNell’approccio bayesiano, si inizia specificando una distribuzione a priori per \\(\\theta\\). Supponiamo di utilizzare una distribuzione Beta(\\(\\alpha\\), \\(\\beta\\)) come prior per \\(\\theta\\), dove \\(\\alpha\\) e \\(\\beta\\) sono parametri scelti in base alle conoscenze precedenti. La distribuzione a posteriori di \\(\\theta\\) data l’osservazione \\(y\\) è ancora una distribuzione Beta, ma con parametri aggiornati:\n\\[\n\\theta \\mid y \\sim \\text{Beta}(\\alpha + y, \\beta + N - y)\n\\]\nAd esempio, scegliendo una distribuzione a priori non informativa con \\(\\alpha = 1\\) e \\(\\beta = 1\\), la distribuzione a posteriori diventa:\n\\[\n\\theta \\mid y \\sim \\text{Beta}(1 + 32, 1 + 100 - 32) = \\text{Beta}(33, 69)\n\\]\nQuesta distribuzione a posteriori fornisce una stima aggiornata della probabilità di successo \\(\\theta\\) considerando i dati osservati. Utilizzando Stan, è possibile ottenere campioni da questa distribuzione a posteriori per calcolare statistiche riassuntive e effettuare previsioni.\nIn sintesi, la simulazione in avanti e il problema inverso rappresentano due approcci complementari: la simulazione in avanti genera dati simulati da parametri noti, mentre il problema inverso stima i parametri del modello dai dati osservati.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#un-primo-programma-in-stan",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#un-primo-programma-in-stan",
    "title": "8  Linguaggio Stan",
    "section": "8.5 Un Primo Programma in Stan",
    "text": "8.5 Un Primo Programma in Stan\n\n8.5.1 Generazione di Dati Casuali\nSupponiamo di voler generare dei valori casuali $ Y $ da una distribuzione binomiale con parametri $ N $ e $ $. Ad esempio, possiamo impostare $ = 0.3 $, per rappresentare una probabilità del 30% di un esito positivo (in statistica, il termine ‘successo’ indica un esito positivo), e possiamo impostare $ N = 100 $. Il seguente programma Stan può essere utilizzato per generare valori di $ Y $ compresi tra 0 e 100.\ndata {\n  int&lt;lower=0&gt; N;\n  real&lt;lower=0, upper=1&gt; theta;\n}\n\ngenerated quantities {\n  int&lt;lower=0, upper=N&gt; y;\n  y = binomial_rng(N, theta);\n}\n\n\n8.5.2 Organizzazione di un Programma Stan\nLa prima cosa da notare è che un programma Stan è organizzato in blocchi. Qui abbiamo due blocchi: un blocco dei dati contenente le dichiarazioni delle variabili che devono essere fornite come dati e un blocco delle quantità generate, che non solo dichiara variabili ma assegna loro un valore. In questo programma Stan, la variabile y viene assegnata come risultato di una singola estrazione da una distribuzione \\(\\textrm{binomiale}(N, \\theta)\\), che Stan fornisce attraverso la funzione binomial_rng.\n\n\n8.5.3 Tipi di Variabili in Stan\nLa seconda cosa da notare è che tutte le variabili in un programma Stan sono dichiarate con tipi specifici. Stan utilizza la tipizzazione statica, il che significa che, a differenza di Python o R, il tipo di una variabile è dichiarato nel programma prima del suo utilizzo, piuttosto che essere determinato al momento dell’esecuzione in base al valore assegnato. Una volta dichiarato, il tipo di una variabile non cambia mai. Stan utilizza anche la tipizzazione forte, il che significa che, a differenza di C o C++, non è possibile aggirare le restrizioni di tipo per accedere direttamente alla memoria.\nIl programma dichiara tre variabili: N e y di tipo int (interi) e theta di tipo real (numeri reali). Nei computer, gli interi hanno limiti precisi e i numeri reali possono avere errori di calcolo. Stan utilizza numeri reali con precisione doppia (64 bit) secondo lo standard IEEE 754, tranne in alcune operazioni ottimizzate che possono perdere un po’ di precisione.\n\n\n8.5.4 Vincoli sui Tipi\nUn tipo di variabile può avere dei vincoli. Poiché N è un conteggio, deve essere maggiore o uguale a zero, cosa che indichiamo con il vincolo lower=0. Allo stesso modo, la variabile y, che rappresenta il numero di esiti positivi su N, deve essere compresa tra 0 e N (inclusi); questo è indicato con il vincolo lower=0, upper=N. Infine, la variabile theta è un numero reale e deve essere compresa tra 0 e 1, cosa che indichiamo con il vincolo lower=0, upper=1. Anche se tecnicamente i limiti per i numeri reali sono aperti, in pratica possiamo ottenere valori di 0 o 1 a causa di errori di arrotondamento nei calcoli.\n\n\n8.5.5 Esecuzione del Programma Stan\nLa funzione cmdstan_model() crea un nuovo oggetto CmdStanModel a partire da un file contenente un programma Stan. In background, CmdStan traduce un programma Stan in C++ e creare un eseguibile compilato.\n\nmodel = CmdStanModel(stan_file='../../stan/binomial-rng.stan')\n\nDurante l’esecuzione, il programma Stan compilato richiede i valori di N e theta. Ad ogni iterazione, il programma campiona un valore di y utilizzando il suo generatore di numeri pseudocasuali integrato. I valori di N e theta devono essere forniti in un dizionario Python.\n\nN = 100\ntheta = 0.3\ndata = {'N': N, 'theta': theta}\n\nInfine campioniamo dal modello utilizzando il metodo sample di CmdStanModel.\n\ntrace = model.sample(\n    data=data, seed=123, \n    chains=1,\n    iter_sampling=10, \n    iter_warmup=1,\n    show_progress=False, \n    show_console=False\n)\n\n\n\n8.5.6 Costruzione del Modello\nIl costruttore di CmdStanModel viene utilizzato per creare un modello a partire da un programma Stan presente nel file specificato. Si consiglia vivamente di utilizzare un file separato per i programmi Stan, in modo da facilitarne la condivisione, permettere l’uso sia di virgolette che di apostrofi e rendere più agevole individuare le linee di riferimento nei messaggi di errore. In pratica, il processo inizia con la traduzione del programma Stan in una classe C++ utilizzando un traduttore specifico. Successivamente, il programma C++ viene compilato, operazione che richiede circa venti secondi.\n\n\n8.5.7 Interfaccia Python\nNell’interfaccia Python, il metodo sample() accetta i seguenti argomenti:\n\ndata: i dati letti nel blocco dati del programma Stan,\nseed: generatore di numeri pseudocasuali per la riproducibilità,\nchains: il numero di simulazioni da eseguire (parallel_chains indica quante eseguire in parallelo),\niter_sampling: numero di estrazioni (cioè, dimensione del campione) da restituire,\niter_warmup: numero di iterazioni di riscaldamento per tarare i parametri dell’algoritmo di campionamento (non necessari qui, quindi impostato a 0),\nshow_progress: se True, stampa aggiornamenti di progresso,\nshow_console: apre un monitor di progresso GUI.\n\nIl risultato della chiamata a sample() sull’istanza del modello viene assegnato alla variabile trace e contiene le 10 estrazioni richieste con l’argomento iter_sampling = 10.\nQuando si chiama model.sample(...), CmdStan esegue Stan come programma C++ autonomo in un processo in background. Questo programma inizia copiando i dati forniti nell’argomento data di Python in un file, quindi legge quel file di dati per costruire un oggetto C++ che rappresenta il modello statistico. Poiché il nostro programma Stan ha solo un blocco di quantità generate, l’unico compito rimanente della classe C++ è generare il numero richiesto di estrazioni. Per ciascuna delle estrazioni specificate da iter_sampling, Stan utilizza un generatore di numeri pseudocasuali per ottenere un valore dalla distribuzione binomiale specificata.\nLa generazione di numeri casuali è determinata dal valore seed specificato nella chiamata.\n\n\n8.5.8 Estrazione dei Risultati\nUna volta completato il campionamento, possiamo estrarre il campione di 10 valori per la variabile scalare y sotto forma di array e quindi stampare i loro valori insieme ai valori delle variabili di input.\n\ny = trace.stan_variable('y')\nprint(\"N =\", N, \";  theta =\", theta, \";  y(0:10) =\", *y.astype(int))\n\nN = 100 ;  theta = 0.3 ;  y(0:10) = 28 34 31 29 26 25 31 28 30 36",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#integrazione-monte-carlo",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#integrazione-monte-carlo",
    "title": "8  Linguaggio Stan",
    "section": "8.6 Integrazione Monte Carlo",
    "text": "8.6 Integrazione Monte Carlo\nIl calcolo bayesiano si basa sulla media delle incertezze nella stima dei parametri. In generale, ciò implica il calcolo di aspettative, che sono medie ponderate con pesi dati dalle densità di probabilità. In questa sezione, introdurremo i metodi Monte Carlo per calcolare un semplice integrale che corrisponde all’aspettativa di una variabile indicatrice discreta. Utilizzeremo l’esempio classico del lancio di freccette su un bersaglio per stimare la costante matematica \\(\\pi\\).\n\n8.6.1 Esempio: Stima di \\(\\pi\\)\nImmaginiamo un quadrato di lato 2 centrato sull’origine. Genereremo punti casuali uniformemente distribuiti all’interno di questo quadrato. Per ogni punto \\((x, y)\\), verificheremo se cade all’interno del cerchio di raggio unitario inscritto nel quadrato, cioè se la distanza dall’origine è minore di 1:\n\\[\n\\sqrt{x^2 + y^2} &lt; 1,\n\\]\nche si semplifica a:\n\\[\nx^2 + y^2 &lt; 1.\n\\]\nLa proporzione di tali punti rappresenta la proporzione dell’area del quadrato occupata dal cerchio. Poiché il quadrato ha un’area di 4, l’area del cerchio è pari a 4 volte la proporzione dei punti che cadono all’interno del cerchio.\nQuindi, se generiamo un numero sufficiente di punti casuali e contiamo quanti di essi cadono all’interno del cerchio, possiamo stimare \\(\\pi\\) come:\n\\[\n\\pi \\approx 4 \\times \\frac{\\text{numero di punti dentro il cerchio}}{\\text{numero totale di punti}}.\n\\]\n\n\n8.6.2 Codice Stan\nEsaminiamo il corrispondente codice Stan.\ngenerated quantities {\n  real&lt;lower=-1, upper=1&gt; x = uniform_rng(-1, 1);\n  real&lt;lower=-1, upper=1&gt; y = uniform_rng(-1, 1);\n  int&lt;lower=0, upper=1&gt; inside = x^2 + y^2 &lt; 1;\n  real&lt;lower=0, upper=4&gt; pi = 4 * inside;\n}\n\nVariabili x e y:\n\nVengono generate casualmente e uniformemente nell’intervallo \\((-1, 1)\\). Questo significa che stiamo campionando punti all’interno di un quadrato di lato 2 centrato sull’origine.\n\nVariabile inside:\n\nÈ un indicatore che verifica se il punto \\((x, y)\\) cade all’interno del cerchio unitario. La condizione \\(x^2 + y^2 &lt; 1\\) è vera se il punto \\((x, y)\\) è all’interno del cerchio di raggio 1 centrato sull’origine, e falsa altrimenti.\nSe la condizione è vera, inside è impostato a 1, altrimenti a 0.\n\nVariabile pi:\n\npi viene calcolata come 4 volte il valore di inside.\n\n\nIl programma Stan genera punti casuali, verifica se cadono all’interno del cerchio e usa la proporzione di punti che cadono all’interno del cerchio per stimare \\(\\pi\\). Moltiplicando il valore indicatore per 4, otteniamo una stima di \\(\\pi\\) basata su ciascun punto generato. La stima finale di \\(\\pi\\) sarà la media di queste stime su molti punti campionati.\n\n\n8.6.3 Media Campionaria dell’Indicatore\nDopo aver generato un numero sufficiente di punti casuali e aver verificato quanti di essi cadono all’interno del cerchio, calcoliamo la media campionaria dell’indicatore inside. Questo indicatore è uguale a 1 se il punto è dentro il cerchio e a 0 se è fuori. La media di questi valori ci dà la proporzione dei punti che cadono dentro il cerchio.\nQuesta proporzione è una stima della probabilità che un punto casuale sia all’interno del cerchio. Moltiplicando questa proporzione per 4, otteniamo una stima di \\(\\pi\\).\nMatematicamente, possiamo scrivere questo processo come segue:\n\\[\n\\mathbb{E}[4 \\cdot \\textrm{I}(\\sqrt{X^2 + Y^2} \\leq 1)] = \\int_{-1}^1 \\int_{-1}^1 4 \\cdot \\textrm{I}(x^2 + y^2 &lt; 1) \\, \\textrm{d}x \\, \\textrm{d}y = \\pi,\n\\]\ndove \\(\\textrm{I}()\\) è l’indicatore che ritorna 1 se il suo argomento è vero e 0 altrimenti.\nIn altre parole, stiamo calcolando l’aspettativa di 4 volte l’indicatore che un punto casuale \\((x, y)\\) cade dentro il cerchio unitario. Questo valore atteso è uguale a \\(\\pi\\), il che ci permette di stimare \\(\\pi\\) usando i metodi Monte Carlo.\n\n\n8.6.4 Compilazione e Campionamento\nCompiliamo e poi campioniamo dal modello, prendendo un campione di dimensione \\(M = 10,000\\) estrazioni.\n\nM = 10_000\nmodel = CmdStanModel(stan_file='../../stan/monte-carlo-pi.stan')\n\nsample = model.sample(\n    chains=1, iter_warmup=1, iter_sampling=M,\n    show_progress=False, show_console=False,\n    seed=123\n)\n\nx_draws = sample.stan_variable('x')\ny_draws = sample.stan_variable('y')\ninside_draws = sample.stan_variable('inside')\npi_draws = sample.stan_variable('pi')\n\ndf = pd.DataFrame({'N': 1000, 'x': x_draws, 'y': y_draws, 'inside': inside_draws})\n\nplt.figure(figsize=(5, 5))\nplt.scatter(df['x'], df['y'], c=df['inside'], cmap='coolwarm', s=1)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Monte Carlo Simulation of Pi Estimation')\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()\n\n\n\n\n\n\n\n\nSuccessivamente, calcoliamo la media campionaria dell’indicatore dentro-il-cerchio, che produce una stima della probabilità che un punto sia dentro il cerchio:\n\nPr_is_inside = np.mean(inside_draws)\npi_hat = np.mean(pi_draws)\nprint(f\"Pr[Y is inside circle] = {Pr_is_inside:.3f};\")\nprint(f\"estimate for pi = {pi_hat:.3f}\")\n\nPr[Y is inside circle] = 0.786;\nestimate for pi = 3.144\n\n\nIl valore esatto di \\(\\pi\\) fino a tre cifre decimali è \\(3.142\\). Con il nostro metodo, ci avviciniamo a questo valore, ma non lo raggiungiamo esattamente, il che è tipico dei metodi Monte Carlo. Aumentando il numero di estrazioni, l’errore diminuisce. Teoricamente, con un numero sufficiente di estrazioni, possiamo ottenere qualsiasi precisione desiderata; tuttavia, in pratica, dobbiamo accontentarci di pochi decimali di accuratezza nelle nostre stime Monte Carlo. Questo di solito non è un problema, poiché l’incertezza statistica tende a dominare rispetto all’imprecisione numerica nella maggior parte delle applicazioni.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#metodi-monte-carlo-a-catena-di-markov",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#metodi-monte-carlo-a-catena-di-markov",
    "title": "8  Linguaggio Stan",
    "section": "8.7 Metodi Monte Carlo a Catena di Markov",
    "text": "8.7 Metodi Monte Carlo a Catena di Markov\nNelle sezioni precedenti, abbiamo visto come generare un campione eseguendo una serie di estrazioni indipendenti e calcolare la media dei risultati per ottenere stime delle aspettative.\nNei moderni modelli bayesiani, raramente è possibile generare estrazioni indipendenti dalle distribuzioni di interesse. Questo rende i semplici metodi Monte Carlo non applicabili. Solo in rari casi, con modelli molto semplici, è possibile ottenere estrazioni indipendenti, ma questi casi sono limitati (Diaconis e Ylvisaker 1979). Prima della rivoluzione dei metodi Monte Carlo a catena di Markov (MCMC) negli anni ’90, l’inferenza bayesiana era per lo più limitata a questi modelli semplici.\nL’introduzione dei metodi MCMC ha rivoluzionato l’inferenza bayesiana. Questi metodi permettono di generare campioni da distribuzioni complesse dove non è possibile ottenere estrazioni indipendenti. Tra questi, il metodo Hamiltonian Monte Carlo (HMC) è particolarmente efficiente e scalabile, grazie all’uso della differenziazione automatica. HMC è implementato in Stan e ha ampliato notevolmente la gamma di modelli che possono essere analizzati in tempi ragionevoli.\n\n8.7.1 Catene di Markov\nNei metodi Monte Carlo a catena di Markov, ogni estrazione dipende dall’estrazione precedente. Una sequenza di variabili casuali in cui ciascuna dipende solo dalla variabile precedente è chiamata catena di Markov. In altre parole, una catena di Markov è una sequenza di variabili casuali dove ogni variabile è condizionatamente indipendente dalle precedenti, dato il valore della variabile immediatamente precedente.\n\n\n8.7.2 Esempio di Catena di Markov\nConsideriamo un esempio semplice con tre catene di Markov, tutte con una distribuzione stazionaria di \\(\\theta = 0.5\\). Questo significa che, col tempo, la distribuzione delle estrazioni converge a una distribuzione Bernoulli con parametro \\(\\theta\\) pari a 0.5. La media a lungo termine delle estrazioni si avvicinerà a 0.5, poiché questo è il valore atteso di una variabile Bernoulli con parametro \\(\\theta\\).\nEcco un programma Stan che implementa una catena di Markov:\ndata {\n  int&lt;lower=0&gt; M;\n  real&lt;lower=0, upper=1&gt; rho;  // probabilità di rimanere nello stesso stato\n}\ngenerated quantities {\n  array[M] int&lt;lower=0, upper=1&gt; y;  // Catena di Markov\n  y[1] = bernoulli_rng(0.5);\n  for (m in 2:M) {\n    y[m] = bernoulli_rng(y[m - 1] ? rho : 1 - rho);\n  } \n}\nIn questo programma, y[m] viene assegnato utilizzando un’operazione ternaria. Se y[m - 1] è 1, y[m] è assegnato a bernoulli_rng(rho), altrimenti è assegnato a bernoulli_rng(1 - rho). Questo crea una catena di Markov dove la probabilità di rimanere nello stesso stato è rho.\nIn conclusione, i metodi Monte Carlo a catena di Markov, come l’Hamiltonian Monte Carlo, hanno ampliato notevolmente le capacità dell’inferenza bayesiana, permettendo di affrontare modelli complessi che non possono essere analizzati con semplici estrazioni indipendenti.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#il-problema-inverso-delle-nascite-di-laplace",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#il-problema-inverso-delle-nascite-di-laplace",
    "title": "8  Linguaggio Stan",
    "section": "8.8 Il Problema Inverso delle Nascite di Laplace",
    "text": "8.8 Il Problema Inverso delle Nascite di Laplace\nQuando si dispone di un modello che genera dati a partire dai parametri, il problema inverso consiste nell’inferire i valori dei parametri dai dati osservati. Questo tipo di problema richiede di ragionare a ritroso dalle osservazioni, attraverso il modello di misurazione e il modello diretto, per stimare parametri come, ad esempio, la probabilità di successo. La risoluzione dei problemi inversi è uno degli ambiti in cui le statistiche bayesiane eccellono.\n\n8.8.1 Storia e Applicazione\nUn decennio dopo la pubblicazione della regola di Bayes, Laplace utilizzò la funzione beta di Eulero per derivare formalmente la distribuzione a posteriori. In questa sezione, analizzeremo il problema di Laplace utilizzando Stan.\nLaplace raccolse dati sul sesso dei bambini nati vivi a Parigi tra il 1745 e il 1770:\n\n\n\nSesso\nNascite vive\n\n\n\n\nFemmina\n105.287\n\n\nMaschio\n110.312\n\n\n\nLaplace si chiese se, sulla base di questi dati, la probabilità di nascita dei maschi fosse superiore a quella delle femmine.\n\n\n8.8.2 Modello di Laplace\nLaplace adottò la seguente distribuzione campionaria per modellare il numero di maschi nati su un totale di \\(N\\) nascite:\n\\[\ny \\sim \\text{binomiale}(N, \\theta),\n\\]\ndove \\(N\\) è il numero totale di nascite, \\(\\theta\\) è la probabilità di nascita di un maschio e \\(y\\) è il numero di nascite maschili.\n\n\n8.8.3 Distribuzione a Priori\nLaplace utilizzò la seguente distribuzione a priori per \\(\\theta\\):\n\\[\n\\theta \\sim \\text{beta}(1, 1),\n\\]\ndove la distribuzione \\(\\text{beta}(1, 1)\\) è uniforme sull’intervallo \\(\\theta \\in (0, 1)\\) poiché la densità è proporzionale a una costante:\n\\[\n\\text{beta}(\\theta \\mid 1, 1) \\propto \\theta^{1 - 1} \\cdot (1 - \\theta)^{1 - 1} = 1.\n\\]\n\n\n8.8.4 Distribuzione a Posteriori\nIl modello di Laplace è abbastanza semplice da permettere una soluzione analitica della distribuzione a posteriori:\n\\[\n\\begin{aligned}\n    p(\\theta \\mid y, N) &\\propto p(y \\mid N, \\theta) \\cdot p(\\theta) \\\\\n    &= \\text{binomiale}(y \\mid N, \\theta) \\cdot \\text{beta}(\\theta \\mid 1, 1) \\\\\n    &\\propto \\theta^y \\cdot (1 - \\theta)^{N - y} \\cdot \\theta^{1 - 1} \\cdot (1 - \\theta)^{1 - 1} \\\\\n    &= \\theta^{y} \\cdot (1 - \\theta)^{N - y} \\\\\n    &\\propto \\text{beta}(\\theta \\mid y + 1, N - y + 1).\n\\end{aligned}\n\\]\nQuindi, possiamo concludere che:\n\\[\np(\\theta \\mid y, N) = \\text{beta}(\\theta \\mid y + 1, N - y + 1).\n\\]\n\n\n8.8.5 Implementazione in Stan\nA differenza del primo modello Stan che abbiamo visto, che generava solo dati, il seguente programma Stan richiede che vengano forniti dati, specificamente il numero di nascite maschili (\\(y\\)) e il numero totale di nascite (\\(N\\)). Il modello ci permetterà di stimare la probabilità di nascita di un maschio (\\(\\theta\\)) e la probabilità che nascano più maschi che femmine (\\(\\theta &gt; 0.5\\)).\nEcco come possiamo specificare il modello Stan:\n\nstan_file = os.path.join(\n    project_directory, 'stan', 'sex-ratio.stan')\n\nwith open(stan_file, 'r') as f:\n    print(f.read())\n\ndata {\n  int&lt;lower = 0&gt; N;\n  int&lt;lower = 0, upper = N&gt; y;\n  int&lt;lower = 0&gt; alpha_prior;\n  int&lt;lower = 0&gt; beta_prior;\n}\nparameters {\n  real&lt;lower=0, upper=1&gt; theta;\n}\nmodel {\n  theta ~ beta(alpha_prior, beta_prior);\n  y ~ binomial(N, theta);\n}\ngenerated quantities {\n  int&lt;lower=0, upper=1&gt; boys_gt_girls = theta &gt; 0.5;\n}\n\n\n\nIn questo programma Stan, vediamo che sia il numero totale di nascite (\\(N\\)) sia il numero di nascite maschili (\\(y\\)) sono forniti come dati. Poi ci sono due blocchi aggiuntivi: un blocco dei parametri, usato per dichiarare valori sconosciuti (qui, solo il tasso di nascite maschili \\(\\theta\\)), e un blocco del modello, dove specifichiamo la distribuzione a priori e la verosimiglianza. La distribuzione a posteriori viene calcolata da Stan combinando queste due componenti. Inoltre, c’è un blocco delle quantità generate dove viene calcolata una variabile booleana che indica se la probabilità di nascita dei maschi \\(\\theta\\) è maggiore di 0.5.\nIl modello di Laplace e la sua implementazione in Stan ci permettono di affrontare il problema inverso delle nascite, inferendo la probabilità di nascita di un maschio dai dati osservati. Utilizzando le tecniche bayesiane, possiamo stimare non solo la probabilità di nascita di un maschio, ma anche la probabilità che nascano più maschi che femmine.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#campionare-dalla-distribuzione-a-posteriori",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#campionare-dalla-distribuzione-a-posteriori",
    "title": "8  Linguaggio Stan",
    "section": "8.9 Campionare dalla Distribuzione a Posteriori",
    "text": "8.9 Campionare dalla Distribuzione a Posteriori\nQuando eseguiamo un programma Stan, esso genera una serie di campioni casuali che approssimano la distribuzione a posteriori. Con il proseguire delle estrazioni, questi campioni tendono a diventare sempre più simili a veri campioni della distribuzione a posteriori, fino a diventare numericamente indistinguibili da essa.\nStan utilizza un algoritmo Markov Chain Monte Carlo (MCMC), che può introdurre autocorrelazione nei campioni della distribuzione a posteriori. In altre parole, i campioni non sono indipendenti tra loro, ma ogni campione è correlato (o anti-correlato) con il campione precedente.\nL’autocorrelazione non introduce bias nelle stime Monte Carlo, ma i campioni positivamente autocorrelati, che si osservano nei modelli più complessi, aumentano la varianza delle stime rispetto ai campioni indipendenti. Questo incremento della varianza aumenta l’errore quadratico medio atteso, che è una combinazione di errore dovuto al bias (qui nullo) e alla varianza. Al contrario, nei modelli molto semplici, l’autocorrelazione negativa riduce la varianza rispetto ai campioni indipendenti, riducendo quindi l’errore quadratico medio atteso.\nPer affrontare problemi ad alta dimensionalità, Duane et al. (1987) hanno introdotto l’algoritmo Hamiltonian Monte Carlo (HMC) che migliora l’efficienza del campionamento. Per Stan, Hoffman e Gelman (2014) hanno sviluppato una versione adattiva dell’HMC chiamata No-U-Turn Sampler (NUTS), successivamente migliorata da Betancourt (2017a). NUTS può essere estremamente efficiente, generando campioni anti-correlati che possono portare a stime Monte Carlo più precise rispetto ai campioni indipendenti.\n\n8.9.1 Compilazione del Codice Stan\nPer utilizzare Stan, dobbiamo compilare il codice del modello. Questo crea un file eseguibile che, nel nostro caso, abbiamo chiamato model.\n\nmodel = CmdStanModel(stan_file=stan_file)\n\nI dati devono essere contenuti in un dizionario.\n\nboys = 110312\ngirls = 105287\n\ndata = {\n    'N': boys + girls, \n    'y': boys,\n    \"alpha_prior\" : 1,\n    \"beta_prior\" : 1\n    }\n\nprint(data)\n\n{'N': 215599, 'y': 110312, 'alpha_prior': 1, 'beta_prior': 1}\n\n\nPossiamo ora eseguire il campionamento MCMC con la seguente chiamata.\n\nsample = model.sample(\n    data=data,\n    iter_warmup = 1000,\n    iter_sampling = 10_000,\n    seed = 123,\n    show_progress = False, \n    show_console = False\n)\n\nIl metodo $sample() viene applicato al file eseguibile del modello Stan che abbiamo compilato e nominato model.\nAvendo assunto una distribuzione a priori per il parametro \\(\\theta\\), l’algoritmo procede in maniera ciclica, aggiornando la distribuzione a priori di \\(\\theta\\) condizionandola ai valori già generati. Dopo un certo numero di iterazioni, l’algoritmo raggiunge la convergenza, e i valori estratti possono essere considerati campioni dalla distribuzione a posteriori di \\(\\theta\\).\nAll’inizio del campionamento, la distribuzione dei campioni può essere significativamente diversa dalla distribuzione stazionaria. Questo periodo iniziale è chiamato “burn-in”. Durante il burn-in, i campioni possono non rappresentare accuratamente la distribuzione a posteriori e sono tipicamente scartati. Man mano che il numero di iterazioni aumenta, la distribuzione dei campioni si avvicina sempre più alla distribuzione target.\nDopo aver eseguito il modello in Stan, otteniamo una serie di campioni \\(\\theta^{(m)}\\) dalla distribuzione a posteriori \\(p(\\theta \\mid N, y)\\). Ogni campione rappresenta un possibile valore di \\(\\theta\\) compatibile con i dati osservati \\(y\\). Procediamo quindi a estrarre i campioni a posteriori per le variabili theta e boys_gt_girls.\n\ntheta_draws = sample.stan_variable('theta')\nboys_gt_girls_draws = sample.stan_variable('boys_gt_girls')\n\nTracciando un istogramma di questi campioni, possiamo visualizzare dove i valori di \\(\\theta\\) sono più probabili e comprendere meglio la forma della distribuzione a posteriori. L’istogramma ci fornisce diverse informazioni:\n\nValore più probabile di \\(\\theta\\): Questo è il valore intorno al quale i campioni sono più concentrati, noto come la moda della distribuzione.\nDistribuzione dei possibili valori di \\(\\theta\\): Questo ci dà un’idea dell’incertezza nella stima di \\(\\theta\\).\n\nSe l’istogramma è stretto e concentrato attorno a un valore specifico, significa che c’è poca incertezza nella stima di \\(\\theta\\). In altre parole, possiamo essere abbastanza sicuri che il valore vero di \\(\\theta\\) sia vicino a questo valore.\nSe l’istogramma è largo e distribuito, significa che c’è maggiore incertezza nella stima di \\(\\theta\\). Questo indica che i dati osservati non forniscono una stima precisa e che il valore di \\(\\theta\\) potrebbe variare notevolmente.\n\nplt.hist(theta_draws, bins=30, alpha=0.5, color='b', edgecolor='black', density=True)\n\n# Aggiunta di titolo e etichette agli assi\nplt.title('Istogramma della distribizione a posteriori di theta')\nplt.xlabel('Valori')\nplt.ylabel('Frequenza')\n\nplt.show()",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#stime-puntuali-bayesiane",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#stime-puntuali-bayesiane",
    "title": "8  Linguaggio Stan",
    "section": "8.10 Stime Puntuali Bayesiane",
    "text": "8.10 Stime Puntuali Bayesiane\nIn termini bayesiani, una stima puntuale per un parametro \\(\\Theta\\) condizionato sui dati osservati \\(Y = y\\) è un singolo valore \\(\\hat{\\theta} \\in \\mathbb{R}^D\\) che riassume la distribuzione a posteriori \\(p(\\theta \\mid y)\\). La notazione \\(\\hat{\\theta}\\) è convenzionale nella statistica per indicare una stima di un parametro \\(\\theta\\). In questa sezione definiamo tre stimatori e discutiamo come i due stimatori bayesiani minimizzino una funzione di perdita tra il valore vero e la stima. Torneremo alla funzione di perdita e alle proprietà degli stimatori dopo averli definiti.\n\n8.10.1 Stimatore della Media Posteriori\nLa stima puntuale bayesiana più comune per un parametro è la media posteriori,\n\\[\n\\begin{align}\n\\widehat{\\theta}\n&= \\mathbb{E}[\\Theta \\mid Y = y] \\\\\n&= \\int_{\\Theta} \\theta \\cdot p(\\theta \\mid y) \\, \\textrm{d}\\theta \\\\\n&= \\lim_{M \\rightarrow \\infty} \\, \\frac{1}{M} \\sum_{m=1}^M \\theta^{(m)} \\\\\n&\\approx \\frac{1}{M} \\sum_{m=1}^M \\theta^{(m)},\n\\end{align}\n\\]\ndove nelle ultime due righe, ogni estrazione è distribuita approssimativamente secondo la distribuzione a posteriori,\n\\[\n\\theta^{(m)} \\sim p(\\theta \\mid y).\n\\]\nAbbiamo introdotto la notazione di aspettativa condizionale nella prima riga di questa definizione. Le aspettative sono semplicemente medie ponderate, con i pesi dati da una densità di probabilità. L’inferenza bayesiana coinvolge aspettative sulla distribuzione a posteriori, la cui notazione concisa è quella dell’aspettativa condizionale,\n\\[\n\\mathbb{E}\\!\n\\left[ f(\\Theta) \\mid Y = y \\right]\n= \\int_{\\mathbb{R^N}} f(\\theta) \\cdot p_{\\Theta \\mid Y}(\\theta \\mid y) \\, \\textrm{d}\\theta,\n\\]\ndove \\(\\Theta\\) e \\(Y\\) sono variabili casuali, mentre \\(\\theta\\) e \\(y\\) sono variabili vincolate ordinarie.\nPer il modello di Laplace, la stima per il tasso di nascite maschili \\(\\theta\\) condizionata sui dati di nascita \\(y\\) è calcolata come la media campionaria delle estrazioni per theta.\n\ntheta_hat = np.mean(theta_draws)\nprint(f\"estimated theta = {theta_hat:.3f}\")\n\nestimated theta = 0.512\n\n\n\n\n8.10.2 Stimatore della Mediana Posteriori, Quantili e Intervalli\nUn’alternativa popolare alla stima puntuale bayesiana è la mediana posteriori, \\(\\theta^+\\). La mediana è il valore tale che, per ogni dimensione \\(d \\in 1{:}D\\),\n\\[\n\\Pr[\\Theta_d \\leq \\theta^+_d] = \\frac{1}{2}.\n\\]\nIn altre parole, la mediana è il valore che divide la distribuzione a posteriori in due parti uguali: il 50% dei campioni è al di sotto della mediana e il 50% è al di sopra. La mediana posteriori può essere calcolata prendendo la mediana dei campioni dalla distribuzione a posteriori.\nEcco come calcolare la mediana posteriori utilizzando Python:\n\ntheta_plus = np.median(theta_draws)\nprint(f\"estimated (median) theta = {theta_plus:.3f}\")\n\nestimated (median) theta = 0.512\n\n\nPoiché la distribuzione a posteriori per i dati di Laplace è quasi simmetrica, la media posteriori e la mediana posteriori sono molto simili.\n\n\n8.10.3 Quantili e Intervalli di Credibilità\nOltre alla mediana, possiamo anche calcolare i quantili e gli intervalli di credibilità per fornire ulteriori informazioni sulla distribuzione a posteriori. I quantili sono valori che dividono la distribuzione in intervalli con una probabilità specificata. Gli intervalli di credibilità indicano l’intervallo entro il quale cade una certa percentuale della distribuzione a posteriori.\n\n8.10.3.1 Quantili\nAd esempio, se vogliamo calcolare il quantile al 95% della distribuzione a posteriori, possiamo semplicemente prendere il valore che si trova al 95° percentile nella sequenza ordinata dei campioni. Di seguito sono riportati i quantili al 5% e al 95% della distribuzione a posteriori di Laplace, calcolati utilizzando i quantili empirici.\n\nquantile_05 = np.quantile(theta_draws, 0.05)\nquantile_95 = np.quantile(theta_draws, 0.95)\nprint(f\"\"\"0.05 quantile = {quantile_05:.3f};\n0.95 quantile = {quantile_95:.3f}\"\"\")\n\n0.05 quantile = 0.510;\n0.95 quantile = 0.513\n\n\n\n\n8.10.3.2 Intervalli Posteriori\nInsieme, il quantile al 5% e al 95% ci forniscono i limiti del nostro intervallo di probabilità centrale al 90%. Questo intervallo è definito come l’intervallo che contiene il 90% della massa di probabilità a posteriori, con il 5% della massa rimanente al di sotto dell’intervallo e il 5% al di sopra.\n\n\n\n8.10.4 Errore di Stima e Bias\nL’errore di una stima è la differenza tra la stima stessa e il valore vero del parametro,\n\\[\n\\textrm{err} = \\hat{\\theta} - \\theta.\n\\]\nLa nostra stima \\(\\hat{\\theta}\\) è implicitamente una funzione dei dati \\(y\\), quindi anche l’errore dipende dai dati. Possiamo rendere esplicita questa dipendenza scrivendo\n\\[\n\\text{err}(y) = \\hat{\\theta}(y) - \\theta.\n\\]\nIl bias di uno stimatore è definito come l’errore atteso, cioè la media dell’errore rispetto alla distribuzione dei dati per la variabile casuale \\(Y\\),\n\\[\n\\begin{align}\n\\text{bias}\n&= \\mathbb{E}[\\text{err}(Y)] \\\\\n&= \\mathbb{E}[\\hat{\\theta}(Y) - \\theta] \\\\\n&= \\int_Y (\\hat{\\theta}(y) - \\theta) \\, \\text{d}y.\n\\end{align}\n\\]\nIn altre parole, il bias misura quanto, in media, la stima \\(\\hat{\\theta}\\) si discosta dal valore vero \\(\\theta\\) considerando tutte le possibili realizzazioni dei dati \\(Y\\). Un bias nullo indica che lo stimatore è corretto in media, cioè non tende a sovrastimare o sottostimare il valore vero del parametro.\n\n\n8.10.5 Stimatore della Moda Posteriori\nUno stimatore popolare, sebbene non strettamente bayesiano, è la moda a posteriori, che rappresenta il valore del parametro \\(\\theta\\) per cui la densità a posteriori è massima. Formalmente, è definita come:\n\\[\n\\theta^* = \\text{arg max}_\\theta \\ p(\\theta \\mid y).\n\\]\nLa stima \\(\\theta^*\\) è spesso chiamata stima MAP (Maximum A Posteriori). La moda a posteriori non è considerata un vero stimatore bayesiano perché non tiene conto dell’incertezza nella stessa misura in cui lo fanno altri metodi bayesiani. In altre parole, non minimizza una funzione di perdita basata sui valori veri dei parametri, ma cerca semplicemente il valore più probabile dato i dati osservati.\n\n\n8.10.6 Caratteristiche della Moda Posteriori\n\nNon considera l’incertezza: La stima MAP si focalizza solo sul valore più probabile della distribuzione a posteriori, senza tenere conto della variabilità dei dati.\nMassimo della densità a posteriori: La moda a posteriori rappresenta il punto in cui la densità a posteriori raggiunge il suo massimo.\nPossibili limitazioni: La stima MAP potrebbe non esistere in alcuni casi, come nei modelli in cui la densità cresce senza limiti. Questo può accadere, ad esempio, nei modelli bayesiani gerarchici o in distribuzioni semplici come la distribuzione esponenziale con parametro 1 (\\(\\textrm{esponenziale}(1)\\)).\n\n\n\n8.10.7 Funzioni di Perdita e Proprietà degli Stimatori\nLa media a posteriori è uno stimatore bayesiano popolare per due ragioni principali. Primo, è uno stimatore non distorto, il che significa che ha un bias nullo. Secondo, ha l’errore quadratico medio atteso minimo tra tutti gli stimatori non distorti. L’errore quadratico di una stima è definito come:\n\\[\n\\text{err}^2(y) = \\left(\\hat{\\theta}(y) - \\theta\\right)^2.\n\\]\nQuesta è una funzione di perdita, che misura la differenza tra una stima \\(\\hat{\\theta}\\) e il valore vero \\(\\theta\\). Tuttavia, la media a posteriori potrebbe non esistere se la distribuzione a posteriori ha code molto ampie, come accade nella distribuzione di Cauchy standard.\n\n\n8.10.8 Proprietà della Mediana Posteriori\nLa mediana a posteriori \\(\\theta^+\\) ha tre proprietà interessanti:\n\nSempre ben definita: La mediana a posteriori è sempre ben definita, anche per densità con poli o code molto ampie.\nMinimizzazione dell’errore assoluto atteso: La mediana minimizza l’errore assoluto atteso, il che la rende robusta.\nRobustezza ai valori anomali: La mediana è meno sensibile ai valori anomali rispetto alla media, perché minimizza l’errore assoluto anziché l’errore quadrato.\n\n\n\n8.10.9 Concentrazione sulle Medie a Posteriori\nIn questa introduzione a Stan, ci concentreremo principalmente sulle medie a posteriori. La media a posteriori non solo fornisce una stima non distorta, ma minimizza anche l’errore quadratico medio atteso, rendendola uno strumento potente per l’inferenza bayesiana. Tuttavia, è importante essere consapevoli delle sue limitazioni, specialmente in presenza di distribuzioni a posteriori con code molto ampie.\n\n\n8.10.10 Errore (Markov Chain) Monte Carlo e Dimensione del Campione Effettivo\nQuando utilizziamo un campionatore di catene di Markov per stimare parametri, otteniamo una sequenza di campioni casuali. Questa sequenza è essa stessa una variabile casuale, perché è composta da molte variabili casuali. A causa di questa natura casuale, ogni esecuzione del campionatore può produrre risultati leggermente diversi, introducendo quello che è noto come errore Monte Carlo.\nL’errore Monte Carlo è l’errore introdotto dal fatto che utilizziamo solo un numero finito di campioni ($ M $) per stimare i parametri. Questo tipo di errore si verifica perché, con un numero limitato di campioni, non possiamo catturare perfettamente l’intera distribuzione a posteriori.\n\n8.10.10.1 Errore Standard di Monte Carlo (MCMC)\nStan riporta l’errore standard di Monte Carlo (MCMC) insieme alle stime della media. L’errore standard MCMC per un parametro scalare $ _d $ è definito come:\n\\[\n\\text{mcmc-se} = \\frac{\\textrm{sd}[\\Theta_d \\mid Y = y]}{\\sqrt{N^{\\text{eff}}}},\n\\]\ndove: - \\(\\text{sd}[\\Theta_d \\mid Y = y]\\) è la deviazione standard del parametro $ _d $ nella distribuzione a posteriori. - \\(N^{\\text{eff}}\\) è la dimensione del campione effettivo, che riflette il numero di campioni indipendenti equivalenti ottenuti dal campionatore.\n\n\n8.10.10.2 Dimensione del Campione Effettivo\nNel classico teorema del limite centrale, la dimensione del campione (numero di estrazioni indipendenti) appare al posto di \\(N^{\\text{eff}}\\). Tuttavia, nel contesto delle catene di Markov, i campioni successivi sono correlati tra loro. La dimensione del campione effettivo (\\(N^{\\text{eff}}\\)) tiene conto di questa correlazione e rappresenta il numero di estrazioni indipendenti che porterebbero allo stesso errore delle nostre estrazioni correlate.\nLa dimensione del campione effettivo per un campione di dimensione $ M $ è definita come:\n\\[\nN^{\\text{eff}} = \\frac{M}{\\text{IAT}},\n\\]\ndove \\(\\text{IAT}\\) è il tempo di autocorrelazione integrata. Sebbene non sia definito formalmente qui, può essere considerato come l’intervallo tra estrazioni effettivamente indipendenti nella nostra catena di Markov. Se l’autocorrelazione è bassa, \\(\\text{IAT}\\) sarà vicino a 1; se l’autocorrelazione è alta, \\(\\text{IAT}\\) sarà molto più alto.\nIn sintesi, \\(N^{\\text{eff}}\\) rappresenta il numero di estrazioni indipendenti che porterebbero allo stesso errore delle estrazioni correlate della nostra catena di Markov.\nIn conclusione, l’errore standard di Monte Carlo (MCMC) fornisce una misura di quanto varierebbero le nostre stime se ripetessimo il processo di campionamento più volte. È un indicatore dell’affidabilità delle nostre stime, tenendo conto della casualità introdotta dall’utilizzo di un numero finito di campioni. Conoscere questo errore ci aiuta a valutare la precisione delle nostre stime e a comprendere meglio l’incertezza associata ai risultati ottenuti tramite il campionamento di catene di Markov.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#stima-delle-probabilità-di-evento",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#stima-delle-probabilità-di-evento",
    "title": "8  Linguaggio Stan",
    "section": "8.11 Stima delle Probabilità di Evento",
    "text": "8.11 Stima delle Probabilità di Evento\nLaplace non cercava semplicemente un valore specifico per \\(\\theta\\). Voleva sapere qual era la probabilità che \\(\\theta\\) fosse maggiore di \\(\\frac{1}{2}\\) dopo aver osservato \\(y\\) nascite maschili su un totale di \\(N\\) nascite. In termini di teoria della probabilità, voleva stimare la probabilità di un evento.\nUn sottoinsieme di parametri è noto come evento. Possiamo convertire le condizioni sui parametri in eventi. Ad esempio, la condizione \\(\\theta &gt; \\frac{1}{2}\\) può essere espressa come l’evento:\n\\[ A = \\left\\{ \\theta \\in \\Theta : \\theta &gt; \\frac{1}{2} \\right\\}. \\]\nData una misura di probabilità, la probabilità dell’evento \\(A\\), ossia che il tasso di nascite maschili sia superiore a quello delle nascite femminili, sarà ben definita. Poiché possiamo convertire le condizioni in eventi, possiamo trattarle come tali. Questo ci permette di scrivere \\(\\Pr\\!\\left[\\Theta &gt; \\frac{1}{2} \\, \\big| \\, N, y\\right]\\) per indicare la probabilità dell’evento \\(\\Theta &gt; \\frac{1}{2}\\).\n\n8.11.1 Probabilità di Evento tramite Indicatori\nLa funzione indicatrice \\(\\textrm{I}\\) assegna il valore 1 alle proposizioni vere e 0 a quelle false. Ad esempio, \\(\\textrm{I}(\\theta &gt; \\frac{1}{2}) = 1\\) se la proposizione \\(\\theta &gt; \\frac{1}{2}\\) è vera, cioè quando \\(\\theta\\) è maggiore di un mezzo.\nLe probabilità di evento sono definite come aspettative condizionali posteriori delle funzioni indicatrici per eventi:\n\\[\n\\begin{align}\n\\Pr[\\Theta &gt; 0.5 \\mid N, y]\n&= \\mathbb{E}\\!\\left[\\textrm{I}[\\Theta &gt; 0.5] \\mid N, y\\right] \\\\\n&= \\int_{\\Theta} \\textrm{I}(\\theta &gt; 0.5) \\cdot p(\\theta \\mid N, y) \\, \\textrm{d}\\theta \\\\\n&\\approx \\frac{1}{M} \\sum_{m=1}^M \\textrm{I}(\\theta^{(m)} &gt; 0.5),\n\\end{align}\n\\]\ndove \\(\\theta^{(m)}\\) rappresenta i campioni dalla distribuzione a posteriori \\(p(\\theta \\mid N, y)\\) per \\(m = 1, 2, \\ldots, M\\).\n\n\n8.11.2 Eventi come Indicatori in Stan\nIn Stan, possiamo codificare direttamente il valore della funzione indicatrice e assegnarlo a una variabile nel blocco delle quantità generate.\ngenerated quantities {\n  int&lt;lower=0, upper=1&gt; boys_gt_girls = theta &gt; 0.5;\n}\nLe espressioni condizionali come theta &gt; 0.5 assumono il valore 1 se sono vere e 0 se sono false. In notazione matematica, scriveremmo \\(\\textrm{I}(\\theta &gt; 0.5)\\), che assume valore 1 se \\(\\theta &gt; 0.5\\) e 0 altrimenti. In Stan, come in C++, trattiamo &gt; come un operatore binario che restituisce 0 o 1, quindi scriviamo semplicemente theta &gt; 0.5.\n\n\n8.11.3 La Risposta alla Domanda di Laplace\nLa media a posteriori della variabile boys_gt_girls è quindi la nostra stima per \\(\\Pr[\\theta &gt; 0.5 \\mid N, y]\\). È essenzialmente 1. Stampando a 15 cifre decimali, vediamo\n\nPr_boy_gt_girl = np.mean(boys_gt_girls_draws)\nprint(f\"estimated Pr[boy more likely] = {Pr_boy_gt_girl:.15f}\")\n\nestimated Pr[boy more likely] = 1.000000000000000\n\n\nCome possiamo vedere di seguito, tutti i nostri campioni per \\(\\theta\\) sono maggiori di \\(\\frac{1}{2}\\), ovvero boys_gt_girls_draws è sempre uguale a 1:\n\nnp.unique(boys_gt_girls_draws)\n\narray([1.])\n\n\nIl valore 1 restituito come stima solleva l’importante problema della precisione numerica. Laplace calcolò il risultato analiticamente, che è\n\\[\n\\Pr\\!\\left[\\Theta &gt; \\frac{1}{2} \\ \\bigg| \\ N, y\\right] \\approx 1 - 10^{-27}.\n\\]\nQuindi avremmo bisogno di un numero astronomico di campioni a posteriori prima di generare un valore di \\(\\theta\\) inferiore a \\(\\frac{1}{2}\\). Come detto, la risposta di 1.0 è molto vicina alla risposta vera e ben entro il nostro errore Monte Carlo atteso.\n\n\n8.11.4 Statistiche di riepilogo MCMC da Stan\nCon Stan, possiamo ottenere un riepilogo completo della variabile \\(\\theta\\) nella distribuzione a posteriori. Per fare ciò, basta chiamare la funzione .summary() sul campione. Questo riepilogo include tutte le statistiche rilevanti.\n\nsample.summary(sig_figs = 3)\n\n\n\n\n\n\n\n\n\nMean\nMCSE\nStdDev\n5%\n50%\n95%\nN_Eff\nN_Eff/s\nR_hat\n\n\n\n\nlp__\n-149000.000\n0.004770\n6.720000e-01\n-149000.00\n-149000.000\n-149000.000\n19800.0\n60700.0\n1.0\n\n\ntheta\n0.512\n0.000009\n1.090000e-03\n0.51\n0.512\n0.513\n13700.0\n41900.0\n1.0\n\n\nboys_gt_girls\n1.000\nNaN\n9.380000e-14\n1.00\n1.000\n1.000\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\nL’istruzione print(sample.diagnose()) in Stan viene utilizzata per eseguire una diagnosi completa del campionamento MCMC. Questa funzione fornisce una serie di statistiche diagnostiche che aiutano a valutare la qualità e la convergenza del campionamento.\nQuesti sono alcuni degli aspetti che possono essere diagnosticati:\n\nConvergenza: La diagnosi verifica se le catene di Markov sono convergenti, ad esempio controllando il valore di \\(\\hat{R}\\). Un valore di \\(\\hat{R}\\) vicino a 1 indica che le catene sono ben mescolate e convergenti.\nAutocorrelazione: Fornisce informazioni sull’autocorrelazione delle catene, che può influire sull’efficienza del campionamento. Bassa autocorrelazione è desiderabile per ottenere campioni indipendenti.\nEfficienza del campionamento: Viene calcolata la dimensione del campione effettivo (\\(N_{\\text{eff}}\\)), che indica quanti campioni indipendenti equivarrebbero ai campioni correlati ottenuti.\nVarianza e Deviazione Standard: Viene riportata la varianza e la deviazione standard dei campioni, aiutando a comprendere la distribuzione a posteriori del parametro.\n\n\nprint(sample.diagnose())\n\nProcessing csv files: /var/folders/s7/z86r4t9j6yx376cm120nln6w0000gn/T/tmphjrqbjja/sex-ratiov0k3fisb/sex-ratio-20240722114626_1.csv, /var/folders/s7/z86r4t9j6yx376cm120nln6w0000gn/T/tmphjrqbjja/sex-ratiov0k3fisb/sex-ratio-20240722114626_2.csv, /var/folders/s7/z86r4t9j6yx376cm120nln6w0000gn/T/tmphjrqbjja/sex-ratiov0k3fisb/sex-ratio-20240722114626_3.csv, /var/folders/s7/z86r4t9j6yx376cm120nln6w0000gn/T/tmphjrqbjja/sex-ratiov0k3fisb/sex-ratio-20240722114626_4.csv\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nEffective sample size satisfactory.\n\nSplit R-hat values satisfactory all parameters.\n\nProcessing complete, no problems detected.\n\n\n\nUn grafico con le tracce si ottiene nel modo seguente:\n\n_ = az.plot_trace(sample, var_names=(\"theta\"), combined=False)",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#riscaldamento",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#riscaldamento",
    "title": "8  Linguaggio Stan",
    "section": "9.1 Riscaldamento",
    "text": "9.1 Riscaldamento\nDurante le fasi iniziali di riscaldamento, Stan cerca di trovare la regione di alta probabilità da cui campionare, adattare una buona dimensione del passo e stimare la varianza a posteriori. La varianza stimata viene utilizzata per migliorare l’efficienza del campionatore, un processo chiamato “precondizionamento”. Precondizionare significa ridimensionare i parametri per rendere il campionamento più efficiente.\nStan può anche stimare una matrice di covarianza completa, che rappresenta le relazioni tra tutti i parametri. Utilizzando questa matrice, Stan può effettuare rotazioni e ridimensionamenti dei parametri per campionare in modo più efficace. In questo contesto, “rotazione e scalatura” si riferiscono alla trasformazione dei parametri in una nuova base (rotazione) e alla regolazione delle loro scale (scalatura) per facilitare il campionamento, rendendolo più rapido e affidabile. Per ulteriori dettagli su questi processi, si può fare riferimento a Neal (2011).\nIl riscaldamento converge quando la dimensione del passo e le stime della covarianza a posteriori diventano stabili. Con più catene, è possibile verificare che tutte convergano verso una dimensione del passo e una stima della covarianza simili. A meno che non ci siano problemi, generalmente non misuriamo la convergenza dell’adattamento, ma piuttosto se otteniamo campioni a posteriori ragionevoli dopo il riscaldamento.\nDurante la fase di riscaldamento, Stan non produce una catena di Markov coerente perché utilizza la memoria per adattarsi alle condizioni del modello. Questo adattamento serve a trovare i migliori parametri di campionamento. Tuttavia, una volta terminato il riscaldamento e iniziata la fase di campionamento, Stan inizia a produrre una vera e propria catena di Markov.\nLe nostre analisi a posteriori si baseranno esclusivamente sui campioni generati durante questa fase di campionamento, non sui campioni raccolti durante il riscaldamento. È comunque possibile salvare ed esaminare i campioni del riscaldamento per comprendere meglio come il processo di adattamento è avvenuto e se ci sono stati problemi.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#riduzione-potenziale-della-scala-e-widehatr",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#riduzione-potenziale-della-scala-e-widehatr",
    "title": "8  Linguaggio Stan",
    "section": "9.2 Riduzione potenziale della scala e \\(\\widehat{R}\\)",
    "text": "9.2 Riduzione potenziale della scala e \\(\\widehat{R}\\)\nStan utilizza la statistica di riduzione potenziale della scala \\(\\widehat{R}\\) (pronunciata “R hat”). Dato un insieme di catene di Markov, Stan divide ciascuna di esse a metà per assicurarsi che la prima metà e la seconda metà della catena concordino, quindi calcola le varianze all’interno di ciascuna catena e tra tutte le catene e le confronta. La statistica \\(\\widehat{R}\\) converge a 1 quando le catene di Markov convergono alla stessa distribuzione.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#quante-catene-per-quanto-tempo",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#quante-catene-per-quanto-tempo",
    "title": "8  Linguaggio Stan",
    "section": "9.3 Quante catene per quanto tempo?",
    "text": "9.3 Quante catene per quanto tempo?\nUna semplice regola empirica consiste nell’eseguire quattro catene finché \\(\\widehat{R} \\leq 1.01\\) e la dimensione campionaria effettiva (ESS) è superiore a 100. La raccomandazione di avere una dimensione campionaria effettiva di “soli” 100 è dovuta al fatto che questo valore implica un errore standard pari a \\(\\frac{1}{10}\\) della deviazione standard. Poiché la deviazione standard a posteriori rappresenta l’incertezza residua, calcolare le medie con una precisione maggiore è raramente utile.\nIl modo più semplice per ottenere \\(\\widehat{R} \\leq 1.01\\) e \\(N_{\\text{eff}} &gt; 100\\) è iniziare con 100 iterazioni di riscaldamento e 100 iterazioni di campionamento. Se i valori di \\(\\widehat{R}\\) sono troppo alti o se la dimensione campionaria effettiva è troppo bassa, raddoppiare il numero di iterazioni di riscaldamento e di campionamento e riprovare. Eseguire più iterazioni di riscaldamento è importante perché il campionamento non sarà efficiente se il riscaldamento non è convergente. Utilizzare lo stesso numero di iterazioni di riscaldamento e di campionamento può comportare un costo massimo doppio rispetto alle impostazioni ottimali, che non sono note in anticipo.\nAnche se si utilizzano più di quattro catene, è necessario assicurarsi che la dimensione campionaria effettiva sia almeno 25 per catena. Non è tanto per l’inferenza, quanto per garantire la fiducia nello stimatore della dimensione campionaria effettiva, che non è affidabile se è molto inferiore. Un modo per verificare l’adeguatezza dello stimatore ESS è raddoppiare il numero di campioni e assicurarsi che anche l’ESS raddoppi. Se ciò non accade, significa che la prima stima dell’ESS non è affidabile.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#esecuzione-delle-catene-contemporaneamente",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#esecuzione-delle-catene-contemporaneamente",
    "title": "8  Linguaggio Stan",
    "section": "9.4 Esecuzione delle catene contemporaneamente",
    "text": "9.4 Esecuzione delle catene contemporaneamente\nÈ possibile impostare il numero di catene da eseguire utilizzando l’argomento chains del metodo sample(). Inoltre, è possibile controllare quante catene possono essere eseguite contemporaneamente con l’argomento parallel_cores (che per default è impostato su 1, ovvero esecuzione sequenziale).\nSe il numero massimo di catene parallele è impostato troppo basso, le risorse della CPU potrebbero non essere sfruttate appieno. Al contrario, se è impostato troppo alto, la CPU o la memoria potrebbero diventare il collo di bottiglia, rallentando le prestazioni complessive rispetto all’esecuzione con un numero inferiore di catene parallele.\nIn progetti personali sul nostro hardware, l’obiettivo è solitamente ottenere la massima dimensione campionaria effettiva nel minor tempo possibile. Tuttavia, a volte è necessario lasciare abbastanza potenza di elaborazione per continuare a lavorare su altre attività come documenti, email, ecc.\n\n9.4.1 Matrici, Vettori o Array in Stan\nStan offre vari tipi di dati per gestire operazioni di algebra lineare e per definire strutture di dati più generali come gli array. Capire le differenze tra questi tipi è fondamentale per sapere cosa possiamo fare con essi e per ottimizzare la velocità di esecuzione del nostro modello.\n\nTipi di base per l’algebra lineare:\n\nvector: un vettore colonna di dimensione N.\nrow_vector: un vettore riga di dimensione N.\nmatrix: una matrice di dimensioni N1 × N2.\n\nArray:\n\nGli array possono essere creati con qualsiasi tipo di elemento e possono avere più dimensioni. Ad esempio:\n\narray[N] real a; definisce un array unidimensionale di numeri reali.\narray[N1, N2] real m; definisce un array bidimensionale di numeri reali.\n\n\nIntercambiabilità e limitazioni:\n\nAnche se possiamo usare sia vector che array per contenitori unidimensionali, l’algebra matriciale (come la moltiplicazione) è definita solo per vettori e matrici, non per array.\nAlcune funzioni, come normal_lpdf, accettano sia vettori che array.\n\nEsempi pratici:\n\nQuando definiamo una media (mu) come somma di un parametro (alpha) e il prodotto di un vettore di carichi (c_load) con un coefficiente (beta), dobbiamo usare i vettori:\nvector[N] mu = alpha + c_load * beta;\nPer utilizzare un generatore di numeri casuali (_rng) in modo vettoriale, dobbiamo usare un array:\narray[N] real p_size_pred = normal_rng(alpha + c_load * beta, sigma);\n\n\nIn sintesi, la scelta tra vector, row_vector, matrix e array dipende dalle operazioni che si desidera eseguire e dalle specifiche esigenze del modello. Scegliere il tipo di dato appropriato permette di sfruttare appieno le funzionalità di Stan e ottimizzare le prestazioni del modello.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#modello-di-esecuzione-di-stan",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#modello-di-esecuzione-di-stan",
    "title": "8  Linguaggio Stan",
    "section": "9.5 Modello di esecuzione di Stan",
    "text": "9.5 Modello di esecuzione di Stan\nI programmi Stan sono composti da diversi blocchi. Ecco una panoramica di ciascun blocco, di quando viene eseguito e di cosa fa. Nessuno di questi blocchi è obbligatorio, ma se presenti, devono seguire quest’ordine.\n\n\n\n\n\n\n\n\nBlocco\nQuando viene eseguito\nCosa fa\n\n\n\n\nfunctions\nsecondo necessità\nDefinizione delle funzioni create dall’utente\n\n\ndata\nuna volta\nLettura dei dati per costruire il modello\n\n\ntransformed data\nuna volta\nDefinizione dei dati trasformati\n\n\nparameters\nuna volta / densità logaritmica\nDefinizione dei parametri con i relativi vincoli\n\n\ntransformed parameters\nuna volta / densità logaritmica\nDefinizione dei parametri trasformati\n\n\nmodel\nuna volta / densità logaritmica\nValutazione della densità logaritmica del modello\n\n\ngenerated quantities\nuna volta / per estrazione\nDefinizione delle quantità generate\n\n\n\n\n9.5.1 Dati e dati trasformati\nIl blocco data contiene solo le dichiarazioni delle variabili. Queste variabili vengono lette una volta durante il caricamento dei dati.\nIl blocco transformed data contiene sia dichiarazioni che definizioni delle variabili. Questo blocco serve per calcolare nuove variabili a partire dai dati originali, come predittori standardizzati o costanti per i priori. Può anche includere la generazione pseudocasuale di numeri. Viene eseguito una volta, dopo la lettura dei dati, per definire le nuove variabili trasformate.\nIn ogni blocco, tutte le variabili devono avere il loro tipo e dimensione dichiarati (che possono dipendere dai dati). Le variabili locali all’interno dei blocchi, invece, sono dichiarate senza specificare la dimensione.\nI vincoli sulle variabili nel blocco data vengono controllati mentre i dati vengono letti, mentre quelli nel blocco transformed data vengono verificati alla fine dell’esecuzione del blocco. Se ci sono violazioni dei vincoli nei dati o nei dati trasformati, si genera un’eccezione che interrompe l’esecuzione del programma.\nLe variabili definite nel blocco transformed data possono essere assegnate una volta, ma non possono essere riassegnate dopo l’esecuzione del blocco.\n\n\n9.5.2 Parametri e Parametri Trasformati\nIl blocco parameters serve a dichiarare le variabili su cui è basato il modello. In pratica, si tratta di elencare i parametri che il modello utilizzerà, specificandone le dimensioni. Quando il blocco viene eseguito, vengono forniti i valori concreti di questi parametri.\nI vincoli sui parametri sono utilizzati per trasformare le variabili vincolate in variabili non vincolate. Ad esempio, se una variabile ha un vincolo lower=0 (cioè deve essere maggiore o uguale a zero), questa variabile viene trasformata usando il logaritmo per renderla non vincolata. È essenziale dichiarare tutti i vincoli necessari sui parametri affinché il modello funzioni correttamente su tutto lo spazio dei parametri.\nIl blocco transformed parameters permette di definire nuove variabili che sono funzioni dei parametri originali e dei dati. Gli utenti possono creare le loro trasformazioni dei parametri in questo blocco. I vincoli su queste nuove variabili vengono verificati alla fine dell’esecuzione del blocco. Se questi vincoli non sono rispettati, viene generata un’eccezione che di solito porta al rifiuto della proposta corrente.\nLe variabili dichiarate nel blocco parameters sono simili agli argomenti di una funzione: la funzione di densità logaritmica del programma Stan prende questi parametri come input. Quindi, i valori dei parametri vengono sempre forniti dall’esterno del programma Stan.\nDopo l’esecuzione del blocco transformed parameters, le variabili dichiarate in esso non possono essere modificate ulteriormente.\nLa differenza principale tra le variabili dichiarate come locali nel blocco model e quelle nel blocco transformed parameters è che le variabili trasformate vengono stampate e sono disponibili anche nel blocco generated quantities.\n\n\n9.5.3 Modello\nLo scopo del blocco model è definire la funzione che calcola la densità logaritmica del modello. Una volta caricati i dati, il compito principale di un programma Stan è fornire questa funzione di densità logaritmica non normalizzata sui parametri non vincolati. Algoritmi esterni, come ottimizzatori, campionatori o metodi di inferenza variazionale, forniranno i valori dei parametri non vincolati per la valutazione.\nIl valore della densità logaritmica non normalizzata calcolato dal modello viene conservato in una variabile chiamata target. Le densità posteriori (che ci interessano) sono calcolate moltiplicando i fattori delle funzioni di densità o massa di probabilità. In termini logaritmici, questo equivale ad aggiungere i termini delle funzioni di densità o massa non normalizzate alla target.\nL’accumulatore target parte da zero e viene incrementato durante l’esecuzione del programma Stan. Come accennato prima, la prima cosa che questa funzione di densità logaritmica non normalizzata fa è trasformare i parametri vincolati in non vincolati e aggiungere un aggiustamento logaritmico per il cambio di variabili alla target. Questo processo è automatico e fornisce i valori dei parametri trasformati al codice che verrà eseguito successivamente nel blocco model.\nLa densità logaritmica accumulata in target può essere incrementata direttamente, come mostrato nell’esempio seguente:\ntarget += -0.5 * x^2;\nAnche se non è possibile usare direttamente target come variabile, il suo valore attuale può essere recuperato tramite la funzione target(), utile per il debugging.\nLe istruzioni di campionamento sono una scorciatoia per incrementare target. Ad esempio, l’istruzione\nx ~ normal(0, 1);\nè equivalente a\ntarget += normal_lupdf(x | 0, 1);\nQui, _lupdf indica che si tratta di una funzione di densità di probabilità logaritmica non normalizzata.\nLa barra verticale | è utilizzata per separare le variabili osservate dai parametri. La notazione lpdf denota una funzione di densità di probabilità logaritmica, mentre lpmf indica una funzione di massa di probabilità logaritmica. Le varianti lupdf e lupmf sono le loro controparti non normalizzate, che possono omettere le costanti di normalizzazione che non dipendono dai parametri. A meno che non siano necessarie, ad esempio in un componente di un modello di mescolanza, è più efficiente usare le forme lupdf e lupmf incrementando direttamente target o tramite istruzioni di campionamento.\n\n\n9.5.4 Quantità generate\nIl blocco generated quantities viene eseguito una volta per ogni campione generato, anziché ogni volta che viene calcolata la densità logaritmica. Con algoritmi come il campionamento Monte Carlo Hamiltoniano, ogni campione può richiedere diverse valutazioni della densità logaritmica.\nUn vantaggio delle quantità generate è che vengono calcolate utilizzando numeri in virgola mobile a doppia precisione, il che le rende molto efficienti. Questo blocco può anche utilizzare numeri pseudocasuali. I vincoli sui dati generati vengono verificati alla fine del blocco, ma eventuali errori non causano il rigetto del campione, solo possibili avvertimenti o valori non definiti (NaN).\nLe quantità generate non influenzano il calcolo della densità logaritmica, ma sono comunque una parte importante del modello statistico. Sono utilizzate principalmente per fare previsioni su nuovi dati, basandosi sui parametri stimati dal modello. Questo processo è noto come inferenza predittiva posteriore. In altre parole, ci permette di fare previsioni su nuovi dati utilizzando i valori dei parametri generati dal modello.\nEsempi di utilizzo delle quantità generate includono la previsione di nuovi valori o il calcolo di statistiche derivate dai parametri stimati. Le quantità generate offrono un modo per esplorare ulteriormente il comportamento del modello e fare inferenze utili dai dati simulati.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#informazioni-sullambiente-di-sviluppo",
    "title": "8  Linguaggio Stan",
    "section": "9.6 Informazioni sull’Ambiente di Sviluppo",
    "text": "9.6 Informazioni sull’Ambiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m -p cmdstanpy\n\nLast updated: Mon Jul 22 2024\n\nPython implementation: CPython\nPython version       : 3.12.4\nIPython version      : 8.26.0\n\ncmdstanpy: 1.2.4\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.5.0\nMachine     : arm64\nProcessor   : arm\nCPU cores   : 8\nArchitecture: 64bit\n\ncmdstanpy : 1.2.4\nnumpy     : 1.26.4\npandas    : 2.2.2\nmatplotlib: 3.9.1\narviz     : 0.18.0\nlogging   : 0.5.1.2\n\nWatermark: 2.4.3",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a04_virtual_env.html",
    "href": "chapters/chapter_7/a04_virtual_env.html",
    "title": "Online Appendix A — Ambienti virtuali",
    "section": "",
    "text": "A.1 Concetto di Ambiente Virtuale\nUn ambiente virtuale è uno spazio di lavoro isolato sul vostro computer, dove potete installare e utilizzare librerie Python senza interferire con il sistema principale. Questo isolamento consente di gestire le versioni delle librerie in modo efficiente, mantenendo il sistema organizzato e sicuro.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Ambienti virtuali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a04_virtual_env.html#vantaggi-delluso-degli-ambienti-virtuali",
    "href": "chapters/chapter_7/a04_virtual_env.html#vantaggi-delluso-degli-ambienti-virtuali",
    "title": "Online Appendix A — Ambienti virtuali",
    "section": "A.2 Vantaggi dell’Uso degli Ambienti Virtuali",
    "text": "A.2 Vantaggi dell’Uso degli Ambienti Virtuali\n\nIsolamento: Permette di selezionare e mantenere versioni specifiche di Python e delle librerie, garantendo la compatibilità e la stabilità del progetto.\nOrdine e Sicurezza: Mantiene separato l’ambiente virtuale dal sistema principale, evitando conflitti e assicurando che le modifiche non influenzino altri programmi.\nRiproducibilità del Codice: Consente di condividere il codice in modo che funzioni correttamente su altri computer, garantendo coerenza e riproducibilità del lavoro.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Ambienti virtuali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a04_virtual_env.html#procedura-per-la-creazione-di-un-ambiente-virtuale-con-conda",
    "href": "chapters/chapter_7/a04_virtual_env.html#procedura-per-la-creazione-di-un-ambiente-virtuale-con-conda",
    "title": "Online Appendix A — Ambienti virtuali",
    "section": "A.3 Procedura per la Creazione di un Ambiente Virtuale con Conda",
    "text": "A.3 Procedura per la Creazione di un Ambiente Virtuale con Conda\nPer creare e gestire ambienti virtuali, è possibile utilizzare conda, uno strumento incluso in Anaconda. Seguire i seguenti passaggi:\n\nAssicurarsi di avere Anaconda correttamente installato sul sistema.\nUtilizzare il terminale su macOS/Linux o PowerShell su Windows.\nEvitare di installare pacchetti direttamente nell’ambiente base di Conda.\nCreare un nuovo ambiente virtuale usando il comando conda create.\nAttivare l’ambiente virtuale appena creato utilizzando conda activate.\nInstallare i pacchetti necessari all’interno dell’ambiente virtuale.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Ambienti virtuali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a04_virtual_env.html#gestione-dellambiente-virtuale",
    "href": "chapters/chapter_7/a04_virtual_env.html#gestione-dellambiente-virtuale",
    "title": "Online Appendix A — Ambienti virtuali",
    "section": "A.4 Gestione dell’Ambiente Virtuale",
    "text": "A.4 Gestione dell’Ambiente Virtuale\nPer una gestione più efficiente degli ambienti virtuali, è consigliabile utilizzare la linea di comando anziché l’interfaccia grafica di Anaconda. Questo offre maggiore controllo e flessibilità nel processo di creazione e gestione degli ambienti.\nSeguendo correttamente questi passaggi, è possibile sfruttare appieno i vantaggi degli ambienti virtuali, garantendo un ambiente di sviluppo Python pulito e ben organizzato.\nÈ fondamentale **evitare l'installazione diretta di pacchetti nell'ambiente `base` di Conda**. Assicuratevi sempre di seguire attentamente i seguenti passaggi:\n\n1. Disattivate l'ambiente `base`.\n2. Create un nuovo ambiente virtuale.\n3. Attivate il nuovo ambiente appena creato.\n\nSolo dopo aver completato questi passaggi, è sicuro procedere con l'installazione dei pacchetti necessari. È possibile verificare l'ambiente attivo osservando il nome visualizzato all'inizio del prompt nel terminale.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Ambienti virtuali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a04_virtual_env.html#gestione-degli-ambienti-virtuali-passaggi-essenziali",
    "href": "chapters/chapter_7/a04_virtual_env.html#gestione-degli-ambienti-virtuali-passaggi-essenziali",
    "title": "Online Appendix A — Ambienti virtuali",
    "section": "A.5 Gestione degli Ambienti Virtuali: Passaggi Essenziali",
    "text": "A.5 Gestione degli Ambienti Virtuali: Passaggi Essenziali\n\nDisattivare l’Ambiente Virtuale Corrente: Se siete in un ambiente virtuale e desiderate uscirne, utilizzate il comando:\nconda deactivate\nSe non siete in un ambiente virtuale, potete procedere al passaggio successivo.\nCreare un Nuovo Ambiente Virtuale: Per utilizzare il campionatore CmdStan e il linguaggio di programmazione probabilistica Stan, adottate l’ambiente virtuale cmdstan_env. Se si utilizza conda, è possibile installare CmdStanPy e i componenti sottostanti di CmdStan dal repository conda-forge tramite la seguente procedura:\nconda create -n cmdstan_env -c conda-forge cmdstanpy\nConda richiederà la conferma digitando y. Questo passaggio crea l’ambiente e installa cmdstanpy insieme alle dipendenze necessarie.\nAttivare il Nuovo Ambiente: Per utilizzare l’ambiente appena creato, attivatelo tramite:\nconda activate cmdstan_env\nInstallare le Librerie Richieste: All’interno dell’ambiente, installate altre librerie necessarie. Ecco come installare le librerie che utilizzeremo:\nconda install -c conda-forge jax numpyro bambi arviz seaborn jupyter-book ipywidgets watermark pingouin networkx -y \nNota: Gli utenti Windows potrebbero dover utilizzare nutpie come alternativa a jax.\nComandi Utili per Gestire Ambienti e Librerie:\n\nElencare gli ambienti virtuali disponibili e verificare quello attivo:\nconda env list\nRimuovere un ambiente virtuale specifico, ad esempio my_env:\nconda env remove -n my_env\nRimuovere una libreria da un ambiente specifico, ad esempio package_name:\nconda remove -n nome_ambiente package_name\n\n\nSeguendo attentamente questi passaggi e utilizzando i comandi di gestione, sarete in grado di creare e gestire efficacemente gli ambienti virtuali con Conda, garantendo una gestione pulita e ordinata delle dipendenze dei vostri progetti.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Ambienti virtuali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a04_virtual_env.html#conda-forge",
    "href": "chapters/chapter_7/a04_virtual_env.html#conda-forge",
    "title": "Online Appendix A — Ambienti virtuali",
    "section": "A.6 Conda Forge",
    "text": "A.6 Conda Forge\nÈ consigliato aggiungere Conda Forge come canale aggiuntivo da cui Conda può cercare e installare i pacchetti. Conda Forge è una collezione di pacchetti gestita dalla community.\n\nA.6.1 Aggiungere Conda Forge\nPer aggiungere Conda Forge come canale, eseguire i seguenti comandi:\nconda config --add channels conda-forge\nconda config --set channel_priority strict\n\nconda config --add channels conda-forge: Aggiunge Conda Forge come canale aggiuntivo per la ricerca e l’installazione dei pacchetti.\nconda config --set channel_priority strict: Imposta la priorità dei canali su “strict”, dando priorità ai pacchetti trovati nei canali elencati per primi nel file di configurazione .condarc.\n\n\n\nA.6.2 Vantaggi dell’Uso di Conda Forge\n\nAmpia Disponibilità di Pacchetti: Conda Forge offre un numero maggiore di pacchetti rispetto al canale predefinito di Conda, aumentando le possibilità di trovare il pacchetto necessario senza ricorrere ad altre soluzioni.\nAggiornamenti Frequenti: I pacchetti su Conda Forge vengono aggiornati più frequentemente, rendendo più probabile trovare le versioni più recenti.\nCoerenza e Compatibilità: Utilizzando la priorità “strict” e Conda Forge, si aumenta la coerenza e la compatibilità tra i pacchetti, riducendo il rischio di conflitti tra dipendenze.\n\nAggiungere Conda Forge come canale e impostare la priorità dei canali su “strict” sono pratiche consigliate per migliorare la gestione dei pacchetti con Conda. Questo approccio aiuta a mantenere l’ambiente stabile, aggiornato e compatibile con le ultime versioni dei pacchetti disponibili.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Ambienti virtuali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a04_virtual_env.html#utilizzo-di-graphviz-opzionale",
    "href": "chapters/chapter_7/a04_virtual_env.html#utilizzo-di-graphviz-opzionale",
    "title": "Online Appendix A — Ambienti virtuali",
    "section": "A.7 Utilizzo di Graphviz (Opzionale)",
    "text": "A.7 Utilizzo di Graphviz (Opzionale)\nPer utilizzare il pacchetto Python graphviz, è necessario installare prima Graphviz sul vostro computer. Le istruzioni specifiche per l’installazione variano a seconda del sistema operativo e sono disponibili sul sito ufficiale di Graphviz.\n\nA.7.1 Installazione di Graphviz\n\nInstallazione di Graphviz: Seguire le istruzioni sul sito di Graphviz per installare il software sul vostro sistema operativo (Windows, macOS, Linux).\nVerifica dell’Installazione: Dopo l’installazione, assicuratevi che Graphviz sia correttamente installato eseguendo il seguente comando nel terminale:\ndot -V\nQuesto comando dovrebbe restituire la versione di Graphviz installata.\n\n\n\nA.7.2 Installazione del Pacchetto Python graphviz\nDopo aver installato Graphviz, potete procedere con l’installazione del pacchetto Python graphviz nel vostro ambiente virtuale. Seguite questi passaggi:\n\nAttivare l’Ambiente Virtuale: Attivate l’ambiente virtuale in cui avete installato pymc (ad esempio, pymc_env) nella vostra console:\nconda activate pymc_env\nInstallare il Pacchetto Python graphviz: Eseguite il seguente comando per installare il pacchetto graphviz tramite Conda Forge:\nconda install -c conda-forge graphviz\n\n\n\nA.7.3 Vantaggi dell’Utilizzo di Graphviz\nL’installazione di Graphviz e del relativo pacchetto Python consente di creare e visualizzare grafici e diagrammi all’interno del vostro ambiente Python. Questo può essere particolarmente utile per visualizzare strutture di dati complesse, flussi di lavoro o grafi probabilistici.\nSeguendo questi passaggi, sarete in grado di utilizzare le funzionalità di Graphviz nel vostro ambiente Python, migliorando le capacità di visualizzazione e analisi dei vostri progetti.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Ambienti virtuali</span>"
    ]
  },
  {
    "objectID": "99-references.html",
    "href": "99-references.html",
    "title": "References",
    "section": "",
    "text": "McElreath, Richard. 2020. Statistical Rethinking: A\nBayesian Course with Examples in R and\nStan. 2nd Edition. Boca Raton, Florida: CRC Press.\n\n\nPearl, Judea. 2009. Causality. Cambridge University Press.\n\n\nRohrer, Julia M. 2018. “Thinking Clearly about Correlations and\nCausation: Graphical Causal Models for Observational Data.”\nAdvances in Methods and Practices in Psychological Science 1\n(1): 27–42.\n\n\nSpeelman, Craig P, Laura Parker, Benjamin J Rapley, and Marek McGann.\n2024. “Most Psychological Researchers Assume Their Samples Are\nErgodic: Evidence from a Year of Articles in Three Major\nJournals.” Collabra: Psychology 10 (1).\n\n\nStevens, Stanley Smith. 1946. “On the Theory of Scales of\nMeasurement.” Science 103 (2684): 677–80.\n\n\nZetsche, Ulrike, Paul-Christian Buerkner, and Babette Renneberg. 2019.\n“Future Expectations in Clinical Depression: Biased or\nRealistic?” Journal of Abnormal Psychology 128 (7): 678.\n\n\nZwet, Erik van, Andrew Gelman, Sander Greenland, Guido Imbens, Simon\nSchwab, and Steven N Goodman. 2023. “A New Look at p Values for\nRandomized Clinical Trials.” NEJM Evidence 3 (1):\nEVIDoa2300003.",
    "crumbs": [
      "Appendices",
      "References"
    ]
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Analisi dei dati per psicologi",
    "section": "Syllabus",
    "text": "Syllabus\nIl Syllabus può essere scaricato utilizzando questo link.",
    "crumbs": [
      "Benvenuti"
    ]
  }
]