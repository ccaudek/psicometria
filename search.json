[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analisi dei dati per psicologi",
    "section": "",
    "text": "Benvenuti\nQuesto sito web √® dedicato al materiale didattico dell‚Äôinsegnamento di Psicometria (A.A. 2024/2025), rivolto agli studenti del primo anno del Corso di Laurea in Scienze e Tecniche Psicologiche dell‚ÄôUniversit√† degli Studi di Firenze.\nIl corso √® strutturato per fornire agli studenti una formazione teorica e pratica approfondita nell‚Äôinferenza statistica, enfatizzando particolarmente le applicazioni pratiche attraverso la programmazione. Attraverso esercitazioni guidate, gli studenti impareranno a manipolare e analizzare dati psicologici utilizzando Python, acquisendo cos√¨ le competenze necessarie per prendere decisioni informate e realizzare interpretazioni precise nei loro progetti di modellazione.\nIl programma copre un ampio spettro di tecniche, partendo dall‚Äôanalisi descrittiva per arrivare fino ai modelli gerarchici avanzati. Si pone un forte accento sull‚Äôinferenza causale, approcciata da una prospettiva bayesiana, includendo l‚Äôuso di Grafi Aciclici Diretti (DAG) per esplorare in modo approfondito le relazioni causali. L‚Äôintento √® di andare oltre i limiti della modellazione lineare tradizionale, mostrando come integrare efficacemente i modelli psicologici avanzati nell‚Äôanalisi statistica.",
    "crumbs": [
      "Benvenuti"
    ]
  },
  {
    "objectID": "index.html#informazioni-sullinsegnamento",
    "href": "index.html#informazioni-sullinsegnamento",
    "title": "Analisi dei dati per psicologi",
    "section": "Informazioni sull‚Äôinsegnamento",
    "text": "Informazioni sull‚Äôinsegnamento\nCodice: B000286 - PSICOMETRIA  Modulo: B000286 - PSICOMETRIA (Cognomi L-Z)  Corso di laurea: Scienze e Tecniche Psicologiche  Anno Accademico: 2024-2025  Materiali didattici: √à sufficiente disporre di un laptop/computer funzionante. Tutti i materiali didattici e il software necessario sono forniti gratuitamente a tutti gli studenti, senza richiedere alcun acquisto. Calendario: Il corso si terr√† dal 3 marzo al 31 maggio 2025. Orario delle lezioni: Le lezioni si svolgeranno il luned√¨ e il marted√¨ dalle 8:30 alle 10:30 e il gioved√¨ dalle 11:30 alle 13:30. Luogo: Le lezioni si terranno presso il Plesso didattico La Torretta. Modalit√† di svolgimento della didattica: Le lezioni ed esercitazioni saranno svolte in modalit√† frontale.\n\n\n\n\n\n\nIl presente sito web costituisce l‚Äôunica fonte ufficiale da consultare per ottenere informazioni sul programma dell‚Äôinsegnamento B000286 - PSICOMETRIA (Cognomi L-Z) A.A. 2024-2025 e sulle modalit√† d‚Äôesame.",
    "crumbs": [
      "Benvenuti"
    ]
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Analisi dei dati per psicologi",
    "section": "Syllabus",
    "text": "Syllabus\nIl Syllabus pu√≤ essere scaricato utilizzando questo link.",
    "crumbs": [
      "Benvenuti"
    ]
  },
  {
    "objectID": "chapters/chapter_1/00_prelims.html",
    "href": "chapters/chapter_1/00_prelims.html",
    "title": "1¬† Preliminari",
    "section": "",
    "text": "1.1 Iniziare ad Usare Python üêç\nIn questo corso, utilizzeremo Python all‚Äôinterno di un ambiente Jupyter Notebook. L‚Äôappendice Appendix A fornisce le istruzioni per installare Jupyter Notebook sul vostro computer.\nIn alternativa, √® possibile scrivere uno script Python in un file con estensione .py, il quale pu√≤ essere eseguito tramite il comando python nome_file.py dalla linea di comando.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/00_prelims.html#jupyter-notebook",
    "href": "chapters/chapter_1/00_prelims.html#jupyter-notebook",
    "title": "1¬† Preliminari",
    "section": "1.2 Jupyter Notebook",
    "text": "1.2 Jupyter Notebook\nI Jupyter Notebook offrono un ambiente interattivo in cui √® possibile eseguire il codice suddiviso in celle. Sebbene sia possibile eseguire il codice nelle celle in qualsiasi ordine, √® considerata una pratica consigliata eseguirle in sequenza al fine di prevenire errori e garantire una corretta esecuzione del codice.\nI Jupyter Notebook supportano due tipi di celle:\n\nCelle di Testo: Queste celle consentono di scrivere testo formattato utilizzando la sintassi Markdown. Questo permette agli autori di inserire del testo descrittivo, comprese immagini, formule in formato \\(\\LaTeX\\), tabelle e altro ancora. Le celle di testo facilitano la documentazione del processo di analisi dei dati in modo chiaro e comprensibile.\nCelle di Codice: Le celle di codice consentono di scrivere e eseguire codice Python. Il codice pu√≤ essere eseguito facendo clic sul triangolo situato a sinistra di ogni cella. Diverse celle possono contenere istruzioni diverse e possono essere eseguite in sequenza. √à importante notare che una funzione definita in una cella precedente pu√≤ essere utilizzata solo se la cella precedente √® stata eseguita.\n\nQui sotto abbiamo una cella di codice.\n\n# Make plot\n%matplotlib inline\nimport math\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntheta = np.arange(0, 4 * math.pi, 0.1)\neight = plt.figure()\naxes = eight.add_axes([0, 0, 1, 1])\naxes.plot(0.5 * np.sin(theta), np.cos(theta / 2))\n\n\n\n\n\n\n\n\nQuando lavori con il notebook, puoi essere all‚Äôinterno di una cella, digitando i suoi contenuti, oppure al di fuori delle celle, muovendoti nel notebook.\nQuando sei all‚Äôinterno di una cella, premi Esc per uscirne. Quando ti muovi al di fuori delle celle, premi Invio per entrare.\n\nFuori da una cella:\n\nUsa i tasti freccia per muoverti.\nPremi Shift+Invio per eseguire il codice nel blocco.\n\nAll‚Äôinterno di una cella:\n\nPremi Tab per suggerire completamenti delle variabili.\n\n\nIl nome \"Jupyter\" deriva dalle tre principali lingue di programmazione supportate: Julia, Python e R. Tuttavia, √® possibile utilizzare i Jupyter Notebook con molte altre lingue di programmazione.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/00_prelims.html#esecuzione-locale-o-su-colab",
    "href": "chapters/chapter_1/00_prelims.html#esecuzione-locale-o-su-colab",
    "title": "1¬† Preliminari",
    "section": "1.3 Esecuzione Locale o su Colab",
    "text": "1.3 Esecuzione Locale o su Colab\nI Jupyter Notebook possono essere eseguiti sia in locale, sul vostro computer, che su un server remoto, come Google Colab. Questa flessibilit√† permette agli utenti di accedere ai propri notebook da qualsiasi dispositivo connesso a Internet e di condividere agevolmente il proprio lavoro con altri.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/00_prelims.html#kernel-nei-jupyter-notebook",
    "href": "chapters/chapter_1/00_prelims.html#kernel-nei-jupyter-notebook",
    "title": "1¬† Preliminari",
    "section": "1.4 Kernel nei Jupyter Notebook",
    "text": "1.4 Kernel nei Jupyter Notebook\nI Jupyter Notebook sono strumenti che agevolano la programmazione in Python grazie a un componente essenziale: il kernel. Quest‚Äôultimo funge da motore di esecuzione per il codice Python presente nelle celle dei notebook. Ogni volta che eseguite una cella, il suo contenuto viene processato dal kernel. La caratteristica pi√π significativa del kernel √® la sua capacit√† di preservare lo stato delle variabili e delle funzioni tra le diverse celle. In pratica, ci√≤ significa che potete definire variabili o funzioni in una cella e poi riutilizzarle in celle successive. Questa interattivit√† facilita l‚Äôesecuzione iterativa del codice e offre un modo dinamico per esplorare i dati.\nDurante l‚Äôinstallazione di Jupyter Notebook, di norma ricevete anche IPython, un kernel ottimizzato per Python.\nSi noti che il kernel Python deve essere installato all‚Äôinterno di un ambiente di sviluppo dedicato noto come ‚Äúambiente virtuale‚Äù (per ulteriori dettagli, si veda {ref}sec-virtual-environment). Questo ambiente svolge un ruolo fondamentale nel separare e isolare le librerie e le dipendenze necessarie per il kernel. Ci√≤ aiuta a evitare conflitti tra diverse configurazioni e garantisce il corretto funzionamento del codice nel contesto desiderato. L‚Äôutilizzo di ambienti specifici risulta particolarmente vantaggioso nei progetti che richiedono versioni particolari di librerie.\nIn questo insegnamento, faremo ampio uso della funzionalit√† conda, inclusa nell‚Äôinstallazione di Anaconda, per la gestione di questi ambienti. conda mette a disposizione funzioni utili per la creazione, la gestione e l‚Äôattivazione di ambienti separati, ciascuno con le sue configurazioni e dipendenze uniche. Questa capacit√† semplifica notevolmente la transizione tra diversi ambienti, garantendo che ogni progetto o kernel disponga delle risorse necessarie per operare in modo efficiente e senza interferenze.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/00_prelims.html#visual-studio-code",
    "href": "chapters/chapter_1/00_prelims.html#visual-studio-code",
    "title": "1¬† Preliminari",
    "section": "1.5 Visual Studio Code",
    "text": "1.5 Visual Studio Code\nIl modo pi√π semplice di usare un Jupyter Notebook √® all‚Äôinterno di Visual Studio Code. Dopo aver installato Visual Studio Code, √® necessario installare l‚Äôestensione Python per sfruttare le funzionalit√† specifiche per Python, inclusa la capacit√† di lavorare con Jupyter Notebook.\n\nIn Visual Studio Code, si clicca sull‚Äôicona delle estensioni (quadrati che si intersecano) nella barra laterale a sinistra.\nSi cerca ‚ÄúPython‚Äù nella barra di ricerca e si seleziona l‚Äôestensione ufficiale offerta da Microsoft.\nSi clicca su ‚ÄúInstall‚Äù.\n\nUna volta completate le installazioni, siete pronti per creare il vostro primo Jupyter Notebook in VS Code.\n\nSi apre Visual Studio Code.\nSi clicca su File &gt; New File.\nSi preme Ctrl+Shift+P per aprire la Palette dei Comandi.\nSi digita ‚ÄúJupyter‚Äù e si seleziona Jupyter: Create New Blank Notebook.\nSi aprir√† un nuovo notebook dove si pu√≤ iniziare a scrivere il codice Python nelle celle.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/01_python_1.html",
    "href": "chapters/chapter_1/01_python_1.html",
    "title": "2¬† Python (1)",
    "section": "",
    "text": "2.1 Scrivere Codice con il Supporto dei LLM\nI Large Language Models (LLM), come GPT-4, stanno rivoluzionando non solo molte applicazioni legate alla creazione di testi in linguaggio naturale, ma stanno anche aprendo nuove e interessanti potenzialit√† nel campo della programmazione e dell‚Äôanalisi dei dati. La programmazione, con le sue regole esplicite e la sintassi strutturata, si adatta particolarmente bene alle capacit√† di riconoscimento dei pattern degli LLM. A differenza dei linguaggi umani, ricchi di significati ambigui ed espressioni idiomatiche, i linguaggi di programmazione sono rigorosi e precisi.\nNel contesto dell‚Äôanalisi dei dati, √® naturale unire le capacit√† degli LLM con la potenza computazionale dei linguaggi statistici come R o di programmazione come Python. Sono gi√† stati pubblicati libri su come integrare le capacit√† degli LLM con l‚Äôanalisi dei dati Matter (2025), e sta emergendo una nuova branca dell‚Äôingegneria dedicata allo sviluppo dei prompt pi√π efficaci per l‚Äôuso con gli LLM. Pertanto, il problema non √® se utilizzare gli LLM a supporto della programmazione, ma come farlo nel modo pi√π efficace.\nUn principio fondamentale √® che, maggiore √® la conoscenza delle regole sintattiche di un linguaggio di programmazione, maggiore sar√† l‚Äôefficacia dell‚Äôuso degli LLM per scopi di analisi dei dati. Quindi, anche se i problemi di programmazione richiesti in questo corso di analisi dei dati sono relativamente semplici rispetto alle capacit√† degli LLM, gli utenti che utilizzeranno gli LLM a supporto delle proprie attivit√† di programmazione trarranno certamente beneficio dalla conoscenza delle regole sintattiche del linguaggio di programmazione utilizzato, che nel nostro caso sar√† principalmente Python (esamineremo anche degli esempi in R).\nGli LLM si basano sul concetto di ‚Äúpredizione del prossimo token‚Äù. Questo significa che sono addestrati per prevedere la parola o il carattere successivo in una sequenza, basandosi su quelli precedenti. Questo principio √® fondamentale per la capacit√† degli LLM di generare codice e per il loro utilizzo nel migliorare i flussi di lavoro di analisi dei dati in Python o R. Oltre a miliardi di parole provenienti da testi comuni (siti web, libri, articoli di riviste), gli LLM di OpenAI, come GPT-4, sono stati addestrati su un vasto corpus di codice open-source e discussioni sul codice presenti su piattaforme come Stack Overflow. Questo consente loro di generare codice sintatticamente e semanticamente corretto in molte situazioni.\nGli LLM possono assistere in diversi compiti di programmazione in Python e R, tra cui l‚Äôidentificazione degli errori negli script, la scrittura di codice a partire da descrizioni in linguaggio naturale, l‚Äôottimizzazione del codice, la generazione di documentazione e la creazione di casi di test.\nAcquisire conoscenze sull‚Äôuso pratico dell‚ÄôAI/LLM nelle attivit√† di programmazione e analisi √® pi√π di una tendenza: √® una necessit√† nel mercato del lavoro attuale. I professionisti che possono utilizzare efficacemente l‚ÄôAI/LLM per migliorare le loro competenze di programmazione e integrare questi strumenti in Python e R avranno un vantaggio competitivo. Man mano che l‚ÄôAI e gli LLM diventano sempre pi√π diffusi nell‚Äôindustria e nel mondo accademico, la domanda per queste competenze continuer√† a crescere.\nL‚Äôobiettivo di questo capitolo √® fornire una panoramica della sintassi di Python e delle principali funzioni di pacchetti come Pandas, Numpy e Matplotlib, utili per la data science. Python √® un linguaggio di programmazione versatile e di facile lettura, adatto a numerosi usi. Anche se il suo nome √® un omaggio al gruppo comico Monty Python, apprendere Python richiede tempo, pratica e impegno.\nQuesta guida si concentra sull‚Äôinsegnamento dei principi di base della programmazione, piuttosto che sui dettagli tecnici. Con questa conoscenza, gli studenti saranno pi√π capaci di risolvere problemi specifici e di cercare autonomamente la sintassi appropriata per il problema da risolvere.\nNel mondo attuale, in cui l‚Äôintelligenza artificiale riveste un ruolo sempre pi√π centrale, √® fondamentale sviluppare la capacit√† di pensare in modo algoritmico. Questa competenza va oltre la mera programmazione e offre un approccio strutturato per risolvere problemi complessi. Sebbene gli LLM siano in grado di risolvere molti dei problemi di programmazione che affronteremo in questo corso, una comprensione approfondita dei principi della programmazione rimane essenziale per interpretare, modificare o migliorare le soluzioni proposte da tali sistemi.\nIn conclusione, anche nell‚Äôera dell‚ÄôIA, una solida comprensione dei fondamenti della programmazione √® indispensabile.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python (1)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/01_python_1.html#espressioni-e-operatori",
    "href": "chapters/chapter_1/01_python_1.html#espressioni-e-operatori",
    "title": "2¬† Python (1)",
    "section": "2.2 Espressioni e Operatori",
    "text": "2.2 Espressioni e Operatori\nI programmi sono insiemi di espressioni che elaborano dati per fornire istruzioni specifiche al computer. Ad esempio, in Python, l‚Äôoperazione di moltiplicazione si esegue utilizzando l‚Äôasterisco (*) tra due numeri. Quando si incontra un‚Äôespressione come 3 * 4, il computer la valuta e produce il risultato, che pu√≤ essere visualizzato in una cella successiva di un notebook Jupyter.\nLe regole sintattiche in un linguaggio di programmazione come Python sono stringenti. Ad esempio, non √® consentito inserire due simboli asterisco in sequenza senza un operando intermedio. Qualora un‚Äôespressione violi queste norme sintattiche, il sistema ritorner√† un ‚ÄúSyntaxError‚Äù, un errore che indica la non conformit√† alle regole del linguaggio. Per esempio\n3 * * 4\nrestituisce:\n Cell In[3], line 1\n    3 * * 4\n        ^\nSyntaxError: invalid syntax\nAnche piccole modifiche in un‚Äôespressione possono cambiarne completamente il significato. Nell‚Äôesempio successivo, lo spazio tra i due asterischi * √® stato rimosso. Tuttavia, poich√© gli asterischi compaiono tra due espressioni numeriche, l‚Äôespressione √® corretta e indica l‚Äôelevamento a potenza del primo numero al secondo: 3 elevato alla quarta potenza (\\(3 \\times 3 \\times 3 \\times 3\\)). In programmazione, simboli come * e ** sono noti come ‚Äúoperatori‚Äù, mentre i valori su cui agiscono sono denominati ‚Äúoperandi‚Äù.\n\n3 ** 4\n\n81\n\n\nLa tabella seguente elenca i principali operatori binari utilizzati in Python, chiamati cos√¨ perch√© agiscono su due operandi.\n\n\n\nOperazione\nOperatore\n\n\n\n\naddizione\n+\n\n\nsottrazione\n-\n\n\nmoltiplicazione\n*\n\n\ndivisione (reale)\n/\n\n\ndivisione (intera; rimuove il resto)\n//\n\n\nresto (modulo)\n%\n\n\nelevamento a potenza\n**\n\n\n\nLe due operazioni che potrebbero essere meno familiari sono % (trova il resto di una divisione) e // (esegui una divisione scartando il resto).\nPer esempio, la divisione intera (scartando il resto) di 11/2 produce 5.\n\n11 // 2\n\n5\n\n\nIl resto di 11/2 √® 1.\n\n11 % 2\n\n1\n\n\nUsando gli operatori che abbiamo elencato in precedenza possiamo usare Python come un calcolatore.\n\nprint(\"4 + 2 √®\", 4 + 2)\nprint(\"4 - 2 √®\", 4 - 2)\nprint(\"4 * 2 √®\", 4 * 2)\nprint(\"4 / 2 √®\", 4 / 2)\nprint(\"4 ** 2 √®\", 4**2)\nprint(\"9 % 4 √®\", 9 % 4)\nprint(\"9 // 4 √®\", 9 // 4)\n\n4 + 2 √® 6\n4 - 2 √® 2\n4 * 2 √® 8\n4 / 2 √® 2.0\n4 ** 2 √® 16\n9 % 4 √® 1\n9 // 4 √® 2\n\n\nL‚Äôapplicazione degli operatori aritmetici in Python dipende dalle seguenti regole di precedenza degli operatori, che sono analoghe a quelle usate in algebra.\n\nLe espressioni tra parentesi vengono valutate per prime.\nSuccessivamente si valutano gli elevamenti a potenza.\nIn seguito, si valutano moltiplicazioni, divisioni e moduli.\nPer ultime vengono valutate somme e sottrazioni.\n\n\n1 + 2 * 3 * 4 * 5 / 6 ** 3 + 7 + 8 - 9 + 10\n\n17.555555555555557\n\n\n\n1 + 2 * (3 * 4 * 5 / 6) ** 3 + 7 + 8 - 9 + 10\n\n2017.0",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python (1)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/01_python_1.html#variabili",
    "href": "chapters/chapter_1/01_python_1.html#variabili",
    "title": "2¬† Python (1)",
    "section": "2.3 Variabili",
    "text": "2.3 Variabili\nQuando generiamo un risultato, la risposta viene visualizzata ma non viene memorizzata da nessuna parte, come abbiamo visto negli esempi precedenti. Se vogliamo recuperare quel risultato, dobbiamo memorizzarlo. Lo mettiamo in un oggetto, e diamo un nome a quell‚Äôoggetto. Questa √® una variabile.\nPer creare una variabile facciamo uso di un‚Äôistruzione di assegnazione. In un‚Äôistruzione di assegnazione, si specifica un nome seguito dal simbolo di uguale (=) e dall‚Äôespressione che si desidera assegnare a tale nome. L‚Äôoperazione di assegnazione consiste nell‚Äôassociare il valore dell‚Äôespressione a destra del simbolo di uguale al nome a sinistra. Da quel momento in poi, ogni volta che il nome viene utilizzato in un‚Äôespressione, il valore associato durante l‚Äôassegnazione viene utilizzato al suo posto.\n\na = 10\nb = 20\na + b\n\n30\n\n\n\na = 1/4\nb = 2 * a\nb\n\n0.5\n\n\n\nmy_var = 100\nconst = 3\n\nmy_var * const\n\n300\n\n\nIn Python, ogni ‚Äúoggetto‚Äù √® un‚Äôarea di memoria nel computer. Una ‚Äúvariabile‚Äù funge da etichetta che fa riferimento a quest‚Äôarea. Se un oggetto non ha pi√π etichette (ovvero, non ci sono pi√π variabili che lo referenziano), i dati contenuti nell‚Äôoggetto diventano inaccessibili. Il Garbage Collector del linguaggio si occuper√† di rilevare questi oggetti non referenziati e liberare la memoria, permettendo che venga riutilizzata per nuovi dati.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python (1)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/01_python_1.html#nomi-delle-variabili",
    "href": "chapters/chapter_1/01_python_1.html#nomi-delle-variabili",
    "title": "2¬† Python (1)",
    "section": "2.4 Nomi delle Variabili",
    "text": "2.4 Nomi delle Variabili\nI nomi delle variabili in Python possono contenere caratteri alfanumerici da a-z, A-Z, 0-9 e alcuni caratteri speciali come _. I nomi delle variabili normali devono iniziare con una lettera. I nomi delle variabili non possono contenere uno spazio; invece, √® comune utilizzare il carattere _ per sostituire ogni spazio. Sta al programmatore scegliere nomi facili da interpretare.\nPer convenzione, i nomi delle variabili iniziano con una lettera minuscola, mentre i nomi delle classi iniziano con una lettera maiuscola.\nInoltre, ci sono una serie di parole chiave (keyword) in Python che non possono essere utilizzate come nomi di variabili. Queste parole chiave sono:\n\nimport keyword\nprint(*keyword.kwlist, sep=\"\\n\")\n\nFalse\nNone\nTrue\nand\nas\nassert\nasync\nawait\nbreak\nclass\ncontinue\ndef\ndel\nelif\nelse\nexcept\nfinally\nfor\nfrom\nglobal\nif\nimport\nin\nis\nlambda\nnonlocal\nnot\nor\npass\nraise\nreturn\ntry\nwhile\nwith\nyield\n\n\nSi presti attenzione alla parola chiave ‚Äúlambda‚Äù, che potrebbe facilmente essere un nome di variabile naturale in un programma scientifico. Tuttavia, essendo una parola chiave, non pu√≤ essere utilizzata come nome di variabile.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python (1)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/01_python_1.html#tipologie-di-dati-in-python",
    "href": "chapters/chapter_1/01_python_1.html#tipologie-di-dati-in-python",
    "title": "2¬† Python (1)",
    "section": "2.5 Tipologie di Dati in Python",
    "text": "2.5 Tipologie di Dati in Python\nIn Python, le variabili possono appartenere a diverse tipologie di dati, ciascuna con caratteristiche e utilizzi specifici.\n\n2.5.1 Stringhe (String)\nLe stringhe sono sequenze di caratteri, utilizzate per rappresentare testo. In Python, le stringhe possono essere create utilizzando apici singoli (' '), doppi (\" \") o tripli (''' ''' oppure \"\"\" \"\"\") per delimitare il testo. Esempi di stringhe sono:\n\"Hello, world!\"\n'Beyonce-Lemonade.txt'\n\"lemonade\"\nSi noti il risultato ottenuto quando si applica l‚Äôoperatore + a due stringhe.\n\n\"data\" + \"science\"\n\n'datascience'\n\n\n\n\"data\" + \" \" + \"science\"\n\n'data science'\n\n\nSia le virgolette singole che doppie possono essere utilizzate per creare le stringhe: ‚Äúciao‚Äù e ‚Äòciao‚Äô sono espressioni equivalenti. Tuttavia, le virgolette doppie sono spesso preferite poich√© consentono di includere virgolette singole all‚Äôinterno delle stringhe.\n\n\"Che cos'√® una parola?\"\n\n\"Che cos'√® una parola?\"\n\n\nL‚Äôespressione precedente avrebbe prodotto un SyntaxError se fosse stata racchiusa da virgolette singole.\n\n2.5.1.1 Parsing strings\nIn Python, una stringa √® concepita come una sequenza ordinata di caratteri. Grazie all‚Äôoperatore di indicizzazione, rappresentato dalle parentesi quadre [], √® possibile accedere a singoli elementi della stringa.\n√à importante ricordare che l‚Äôindicizzazione in Python parte da zero.\nL‚Äôindice del primo carattere √® [0], quello del secondo √® [1], del terzo [2], e cos√¨ via. Questa funzionalit√† consente di manipolare o consultare specifici segmenti della stringa, piuttosto che gestirla come un blocco unico.\nConsideriamo questo verso di Eugenio Montale:\n\nmy_string = \"Tendono alla chiarit√† le cose oscure\"\nprint(my_string)\n\nTendono alla chiarit√† le cose oscure\n\n\n\nmy_string[0]\n\n'T'\n\n\n\nmy_string[3]\n\n'd'\n\n\n\nlen(my_string)\n\n36\n\n\nLa stringa ‚Äúmy_string‚Äù conta 36 caratteri. Pertanto, gli indici validi per questa stringa vanno da 0 a 35. Per accedere all‚Äôultimo carattere, √® necessario utilizzare l‚Äôindice 35, che corrisponde a 36 meno 1.\n\nmy_string[35]\n\n'e'\n\n\nUn modo efficiente per ottenere l‚Äôultimo carattere √® ricorrere alla funzione len, sottraendo 1 al risultato:\n\nmy_string[len(my_string) - 1]\n\n'e'\n\n\nSi noti che len() √® una funzione. Una funzione √® un blocco di codice che esegue un‚Äôoperazione specifica. I programmatori chiamano anche gli input delle funzioni ‚Äúparametri‚Äù o ‚Äúargomenti‚Äù.\nLa funzione len prende un input e restituisce un output. L‚Äôoutput √® la lunghezza di ci√≤ che √® stato passato come input.\n\n\n2.5.1.2 Slicing strings\nOltre a estrarre caratteri individuali da una stringa, Python offre la possibilit√† di selezionare segmenti di testo attraverso la tecnica dello ‚Äúslicing‚Äù. Questo meccanismo √® simile all‚Äôindicizzazione, ma utilizza due indici separati da un carattere a due punti (:). Il primo indice indica la posizione di partenza dello ‚Äúslicing‚Äù nella stringa, mentre il secondo indice segnala il punto in cui terminare l‚Äôestrazione del segmento.\n\nmy_string[2:4]\n\n'nd'\n\n\nSe si omette il primo indice, Python utilizzer√† l‚Äôinizio della stringa; se si omette il secondo, utilizzer√† la fine della stringa.\n\nmy_string[:4]\n\n'Tend'\n\n\n\nmy_string[4:]\n\n'ono alla chiarit√† le cose oscure'\n\n\n\n\n2.5.1.3 Metodi\nA partire da una stringa esistente, si possono generare nuove stringhe mediante l‚Äôutilizzo di metodi specifici per le stringhe. Questi metodi sono essenzialmente funzioni che agiscono direttamente sull‚Äôoggetto stringa. Per invocare un metodo, basta posizionare un punto subito dopo la stringa e seguire con il nome del metodo desiderato. Ad esempio, il metodo successivo converte tutti i caratteri della stringa in maiuscole.\n\nmy_string.upper()\n\n'TENDONO ALLA CHIARIT√Ä LE COSE OSCURE'\n\n\nIl metodo my_string.title() √® utilizzato per convertire la prima lettera di ogni parola nella stringa my_string in maiuscolo, mentre rende tutte le altre lettere minuscole. In pratica, trasforma la stringa in una forma ‚Äúa titolo‚Äù, in cui ogni parola inizia con una lettera maiuscola.\n\nmy_string.title()\n\n'Tendono Alla Chiarit√† Le Cose Oscure'\n\n\n\n\n\n2.5.2 Numeri Interi (Integer)\nI numeri interi rappresentano numeri senza una componente decimale. In Python, possono essere creati assegnando un valore senza parte decimale a una variabile. Esempio di un numero intero √®:\n\nage = 20\n\n\n\n2.5.3 Numeri in Virgola Mobile (Float)\nI numeri in virgola mobile, o ‚Äúfloat‚Äù, rappresentano numeri che hanno una componente decimale. Sono creati assegnando un valore con una parte decimale a una variabile. Esempio di un numero float √®:\n\ntemperature = 36.4\n\nI numeri in virgola mobile, o ‚Äúfloat‚Äù, hanno una precisione limitata a circa 15-16 cifre decimali; oltre questo limite, la precisione viene persa. Nonostante questa limitazione, sono sufficienti per la maggior parte delle applicazioni.\nInoltre, √® possibile utilizzare la notazione scientifica per rappresentare numeri molto grandi o molto piccoli. In questa notazione, m * 10^n viene comunemente abbreviato come mEn, dove ‚ÄúE‚Äù rappresenta l‚Äôesponente dieci. Ad esempio, 1E9 equivale a un miliardo (\\(1 \\times 10^9\\)) e 1E-9 rappresenta un miliardesimo (\\(1 \\times 10^{-9}\\)).\n\n\n2.5.4 Valori Booleani (Boolean)\nI valori booleani possono assumere solo due stati: vero (True) o falso (False). Sono utilizzati per rappresentare le condizioni logiche e sono ottenuti attraverso espressioni di confronto. Esempio di un valore booleano √®:\n\nis_raining = False\n\nNel contesto delle operazioni aritmetiche, True √® equivalente al numero intero 1, mentre False corrisponde a 0. Questo permette di includere valori booleani in calcoli matematici. Per esempio:\n\nTrue + True + False\n\n2\n\n\nUn valore booleano viene ritornato quando si valuta un confronto. Per esempio:\n\n3 &gt; 1 + 1\n\nIl valore True indica che il confronto √® valido; Python ha confermato questo semplice fatto sulla relazione tra 3 e 1+1.\nSi noti la regola di precedenza: gli operatori &gt;, &lt;, &gt;=, &lt;=, ==, != hanno la precedenza pi√π bassa (vengono valutati per ultimi), il che significa che nell‚Äôespressione precedente viene prima valutato (1 + 1) e poi (3 &gt; 2).\n\n\n2.5.5 Operatori di confronto\nUn operatore di confronto √® un operatore che esegue un qualche tipo di confronto e restituisce un valore booleano (True oppure False). Per esempio, l‚Äôoperatore == confronta le espressioni su entrambi i lati e restituisce True se hanno gli stessi valori e False altrimenti. L‚Äôopposto di == √® !=, che si pu√≤ leggere come ‚Äònon uguale al valore di‚Äô. Gli operatori di confronto sono elencati qui sotto:\n\n\n\nConfronto\nOperatore\n\n\n\n\nMinore\n&lt;\n\n\nMaggiore\n&gt;\n\n\nMinore o uguale\n&lt;=\n\n\nMaggiore o uguale\n&gt;=\n\n\nUguale\n==\n\n\nNon uguale\n!=\n\n\n\nAd esempio:\n\na = 4\nb = 2\n\nprint(\"a &gt; b\", \"is\", a &gt; b)\nprint(\"a &lt; b\", \"is\", a &lt; b)\nprint(\"a == b\", \"is\", a == b)\nprint(\"a &gt;= b\", \"is\", a &gt;= b)\nprint(\"a &lt;= b\", \"is\", a &lt;= b)\n\nNella cella seguente si presti attenzione all‚Äôuso di = e di ==:\n\nboolean_condition = 10 == 20\nprint(boolean_condition)\n\nL‚Äôoperatore = √® un‚Äôistruzione di assegnazione. Ovvero, crea un nuovo oggetto. L‚Äôoperatore == valuta invece una condizione logica e ritorna un valore booleano.\nUn‚Äôespressione pu√≤ contenere pi√π confronti e tutti devono essere veri affinch√© l‚Äôintera espressione sia vera. Ad esempio:\n\n1 &lt; 1 + 1 &lt; 3\n\n\n\n2.5.6 Tipizzazione Dinamica in Python\nPython √® un linguaggio con tipizzazione dinamica, il che significa che il tipo di una variabile √® determinato dal valore che le viene assegnato durante l‚Äôesecuzione del programma e non necessita di essere dichiarato esplicitamente.\nPer identificare il tipo di una variabile o del risultato di un‚Äôespressione, Python mette a disposizione la funzione type(). Questa funzione, quando chiamata con una variabile o un‚Äôespressione come argomento, restituisce il tipo di dati corrispondente.\nNell‚Äôesempio seguente il programma stamper√† &lt;class 'str'&gt;, indicando che x √® una variabile di tipo ‚Äústringa‚Äù.\n\nx = \"hello\"\nprint(type(x))\n\n&lt;class 'str'&gt;\n\n\nApplichiamo la funzione type() alle altre variabili che abbiamo definito in precedenza.\n\nage = 20\nprint(type(age))\n\n&lt;class 'int'&gt;\n\n\n\ntemperature = 36.4\nprint(type(temperature))\n\n&lt;class 'float'&gt;\n\n\n\nis_raining = False\nprint(type(is_raining))\n\n&lt;class 'bool'&gt;\n\n\n\n\n2.5.7 Operatori Booleani\nGli operatori booleani (o operatori logici) confrontano espressioni (non valori) e ritornano un valore booleano. Python ha tre operatori logici:\n\nand ‚Äì Ritorna True solo se entrambi le espressioni sono vere, altrimenti ritorna False\nor ‚Äì Ritorna True se almeno una delle due espressioni √® vera, altrimenti ritorna False.\nnot ‚Äì Ritorna True se l‚Äôespressione √® falsa, altrimenti ritorna False.\n\nAd esempio:\n\na = 2\nb = 3\n\n(a + b &gt; a) and (a + b &gt; b)\n\nTrue\n\n\nNella cella sopra le parentesi tonde sono opzionali ma facilitano la lettura.\nL‚Äôoperatore and restituisce True solo se entrambe le condizioni booleane sono vere. Ad esempio, True and False restituir√† False perch√© una delle condizioni √® falsa:\n\nTrue and False\n\nFalse\n\n\nL‚Äôoperatore or restituisce True se almeno una delle due condizioni booleane √® vera. Ad esempio, True or False restituir√† True perch√© almeno una delle condizioni √® vera.\n\nTrue or False\n\nTrue\n\n\nL‚Äôoperatore not viene utilizzato per invertire il valore di verit√† di una condizione booleana. Ad esempio, not True restituir√† False e not False restituir√† True.\n\nnot True\n\nFalse\n\n\nAlcuni esempi sono i seguenti (si noti l‚Äôuso della funzione len()):\n\nprint(3 &gt; 2)  # True, because 3 is greater than 2\nprint(3 &gt;= 2)  # True, because 3 is greater than 2\nprint(3 &lt; 2)  # False,  because 3 is greater than 2\nprint(2 &lt; 3)  # True, because 2 is less than 3\nprint(2 &lt;= 3)  # True, because 2 is less than 3\nprint(3 == 2)  # False, because 3 is not equal to 2\nprint(3 != 2)  # True, because 3 is not equal to 2\nprint(len(\"mango\") == len(\"avocado\"))  # False\nprint(len(\"mango\") != len(\"avocado\"))  # True\nprint(len(\"mango\") &lt; len(\"avocado\"))  # True\nprint(len(\"milk\") != len(\"meat\"))  # False\nprint(len(\"milk\") == len(\"meat\"))  # True\nprint(len(\"tomato\") == len(\"potato\"))  # True\nprint(len(\"python\") &gt; len(\"dragon\"))  # False\n\nTrue\nTrue\nFalse\nTrue\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\nAltri esempi di come questi operatori possono essere utilizzati sono i seguenti:\n\nprint(3 &gt; 2 and 4 &gt; 3)  # True - because both statements are true\nprint(3 &gt; 2 and 4 &lt; 3)  # False - because the second statement is false\nprint(3 &lt; 2 and 4 &lt; 3)  # False - because both statements are false\nprint(\"True and True: \", True and True)\nprint(3 &gt; 2 or 4 &gt; 3)  # True - because both statements are true\nprint(3 &gt; 2 or 4 &lt; 3)  # True - because one of the statements is true\nprint(3 &lt; 2 or 4 &lt; 3)  # False - because both statements are false\nprint(\"True or False:\", True or False)\nprint(not 3 &gt; 2)  # False - because 3 &gt; 2 is true, then not True gives False\nprint(not True)  # False - Negation, the not operator turns true to false\nprint(not False)  # True\nprint(not not True)  # True\nprint(not not False)  # False\n\nTrue\nFalse\nFalse\nTrue and True:  True\nTrue\nTrue\nFalse\nTrue or False: True\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\nAbbiamo tralasciato alcuni operatori in Python. Due di quelli che abbiamo omesso sono gli operatori di appartenenza, in e not in. Gli altri operatori che abbiamo tralasciato sono gli operatori bitwise e gli operatori sugli insiemi, che verranno trattati in seguito.\n\n\n2.5.8 Valori Numerici di True e False\n√à fondamentale comprendere i valori numerici associati alle parole chiave True e False. Queste due parole chiave hanno i valori numerici di 1 e 0, rispettivamente.\n\nTrue == 1\n\nTrue\n\n\n\nFalse == 0\n\nTrue\n\n\n\nTrue + False\n\n1\n\n\n\ntype(True + False)\n\nint",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python (1)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/01_python_1.html#sequenze",
    "href": "chapters/chapter_1/01_python_1.html#sequenze",
    "title": "2¬† Python (1)",
    "section": "2.6 Sequenze",
    "text": "2.6 Sequenze\nOltre ai numeri e ai valori booleani, Python supporta anche un insieme di ‚Äúcontenitori‚Äù, ovvero i seguenti tipi strutturati:\n\nle liste,\nle tuple,\ngli insiemi,\ni dizionari.\n\n\n2.6.1 Le tuple\nUna tupla √® una collezione di diversi tipi di dati che √® ordinata e immutabile (non modificabile). Le tuple sono scritte tra parentesi tonde, (). Una volta creata una tupla, non √® possibile modificarne i contenuti.\n\ncolors = (\"Rosso\", \"Nero\", \"Bianco\")\ncolors\n\n('Rosso', 'Nero', 'Bianco')\n\n\n\ntype(colors)\n\ntuple\n\n\nLe stringhe sono tuple di caratteri. Pertanto non sono modificabili.\n\n\n2.6.2 Le liste\nGli oggetti di tipo lista sono simili alle tuple, ma con alcune differenze. La lista √® un oggetto mutabile, il che significa che possiamo aggiungere o rimuovere elementi dalla lista anche dopo la sua creazione. Una lista viene creata separando i suoi elementi tramite virgola e racchiudendo il tutto tra parentesi quadre.\nSi noti che una lista √® una struttura dati eterogenea contentente una sequenza di elementi che possono essere di tipo diverso.\n\nmy_list = [\"Pippo\", 3, -2.953, [1, 2, 3]]\nmy_list\n\n['Pippo', 3, -2.953, [1, 2, 3]]\n\n\n\ntype(my_list)\n\nlist\n\n\nLa lista my_list √® composta da diversi elementi: una stringa (‚ÄúPippo‚Äù), un numero intero (3), un numero decimale (-2.953) e un‚Äôaltra lista ([1, 2, 3]).\nGli elementi nella lista sono ordinati in base all‚Äôindice, il quale rappresenta la loro posizione all‚Äôinterno della lista. Gli indici delle liste partono da 0 e aumentano di uno. Per accedere a un elemento della lista tramite il suo indice, si utilizza la notazione delle parentesi quadre: nome_lista[indice]. Ad esempio:\n\nmy_list[1]\n\n3\n\n\n\nmy_list[0]\n\n'Pippo'\n\n\nPython prevede alcune funzioni che elaborano liste, come per esempio len che restituisce il numero di elementi contenuti in una lista:\n\nlen(my_list)\n\n4\n\n\nBench√© questa lista contenga come elemento un‚Äôaltra lista, tale lista nidificata conta comunque come un singolo elemento. La lunghezza di di my_list √® quattro.\nUna lista vuota si crea nel modo seguente:\n\nempty_list = []\nlen(empty_list)\n\n0\n\n\nEcco alcuni esempi.\n\nfruits = [\"banana\", \"orange\", \"mango\", \"lemon\"]  # list of fruits\nvegetables = [\"Tomato\", \"Potato\", \"Cabbage\", \"Onion\", \"Carrot\"]  # list of vegetables\n\nprint(\"Fruits:\", fruits)\nprint(\"Number of fruits:\", len(fruits))\nprint(\"Vegetables:\", vegetables)\nprint(\"Number of vegetables:\", len(vegetables))\n\nFruits: ['banana', 'orange', 'mango', 'lemon']\nNumber of fruits: 4\nVegetables: ['Tomato', 'Potato', 'Cabbage', 'Onion', 'Carrot']\nNumber of vegetables: 5\n\n\nSupponiamo di voler ordinare in ordine alfabetico i nomi presenti nella lista. Per fare ci√≤, √® necessario utilizzare il metodo sort sulla lista utilizzando la notazione con il punto (dot notation):\n\nnames = [\"Carlo\", \"Giovanni\", \"Giacomo\"]\nnames.sort()\n\nTale metodo per√≤ non restituisce alcun valore, in quanto l‚Äôordinamento √® eseguito in place: dopo l‚Äôinvocazione, gli elementi della lista saranno stati riposizionati nell‚Äôordine richiesto. Visualizziamo la listra trasformata:\n\nnames\n\n['Carlo', 'Giacomo', 'Giovanni']\n\n\nL‚Äôinvocazione di metodi (e di funzioni) prevede anche la possibilit√† di specificare degli argomenti opzionali. Per esempio:\n\nnames.sort(reverse=True)\nnames\n\n['Giovanni', 'Giacomo', 'Carlo']\n\n\nIl metodo remove() pu√≤ essere usato per rimuovere elementi da una lista.\n\nprint(fruits)\nfruits.remove(\"banana\")\nprint(fruits)\n\n['banana', 'orange', 'mango', 'lemon']\n['orange', 'mango', 'lemon']\n\n\nIl metodo insert() pu√≤ essere usato per aggiungere elementi ad una lista.\n\nprint(fruits)\nfruits.insert(2, \"watermelon\")\nprint(fruits)\n\n['orange', 'mango', 'lemon']\n['orange', 'mango', 'watermelon', 'lemon']\n\n\n√à possibile copiare una lista in una nuova variabile:\n\nprint(fruits)\nnew_fruits = fruits.copy()\n\n['orange', 'mango', 'watermelon', 'lemon']\n\n\n\nprint(new_fruits)\n\n['orange', 'mango', 'watermelon', 'lemon']\n\n\n\n\n2.6.3 Operazioni su liste\nL‚Äôoperatore + concatena liste:\n\na = [1, 2, 3]\nb = [4, 5, 6]\nc = a + b\nprint(c)\n\n[1, 2, 3, 4, 5, 6]\n\n\nIn maniera simile, l‚Äôoperatore * ripete una lista un certo numero di volte:\n\n[0] * 4\n\n[0, 0, 0, 0]\n\n\n\n[1, 2, 3] * 3\n\n[1, 2, 3, 1, 2, 3, 1, 2, 3]\n\n\nL‚Äôaspetto importante da considerare √® che, essendo una sequenza di elementi eterogenei, √® difficile eseguire operazioni algebriche sulle liste in Python puro. Ad esempio, consideriamo la seguente lista:\n\nx = [1, 2, 3]\nx\n\n[1, 2, 3]\n\n\nSe desideriamo calcolare una semplice operazione, come la media di x, √® necessario seguire una procedura abbastanza articolata. Ad esempio:\n\ntotal = 0\ncounter = 0\n\nfor num in x:\n    counter += 1\n    total += num\n\navg = total / counter\n\nprint(avg)\n\n2.0\n\n\nIndubbiamente, sarebbe preferibile ottenere questo risultato con un approccio pi√π semplice. In seguito, vedremo che se utilizziamo una sequenza di elementi omogenei, il problema pu√≤ essere risolto in modo molto pi√π agevole. Ad esempio,\n\nimport numpy as np\n\nx = np.array([1, 2, 3])\nnp.mean(x)\n\n2.0\n\n\nPossiamo contare il numero degli elementi specificati che sono contenuti in una lista usando count().\n\nages = [22, 19, 24, 25, 26, 24, 25, 24]\nprint(ages.count(24))         \n\n3\n\n\nPossiamo trovare l‚Äôindice di un elemento in una lista con index().\n\nages.index(24)  # index of the first occurrence\n\n2\n\n\n\n\n2.6.4 Operatore slice\nL‚Äôoperatore di slice (:) applicato alle liste in Python consente di estrarre una porzione specifica di elementi dalla lista. L‚Äôoperatore di slice ha la seguente sintassi: lista[inizio:fine:passo].\n\ninizio rappresenta l‚Äôindice di partenza dell‚Äôintervallo (inclusivo).\nfine rappresenta l‚Äôindice di fine dell‚Äôintervallo (esclusivo).\npasso rappresenta il passo o l‚Äôincremento tra gli indici degli elementi selezionati (facoltativo).\n\nEcco alcuni esempi per illustrare l‚Äôutilizzo dell‚Äôoperatore di slice:\n\nlista = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n# Estrarre una porzione della lista\nporzione = lista[2:6]  # [3, 4, 5, 6]\n\n# Estrarre una porzione con un passo specifico\nporzione_passo = lista[1:9:2]  # [2, 4, 6, 8]\n\n# Estrarre una porzione dalla fine della lista\nporzione_fine = lista[6:]  # [7, 8, 9, 10]\n\n# Estrarre una porzione dall'inizio della lista\nporzione_inizio = lista[:5]  # [1, 2, 3, 4, 5]\n\n\n\n2.6.5 Gli insiemi\nGli insiemi sono collezioni finite di elementi distinti e non memorizzati in un ordine specifico. Un insieme non pu√≤ contenere pi√π di un‚Äôistanza dello stesso elemento. Per creare un insieme si utilizzano le parentesi graffe {}. Ad esempio:\n\nmy_set = {\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"}\nmy_set\n\n{'A', 'B', 'C', 'D', 'E', 'F'}\n\n\n\ntype(my_set)\n\nset\n\n\nGli oggetti di tipo ‚Äúset‚Äù sono utili per eseguire operazioni matematiche sugli insiemi.\nPer verificare se un elemento esiste in un insieme usiamo l‚Äôoperatore in.\n\nprint(\"Does set my_set contain D? \", \"D\" in my_set)\n\nDoes set my_set contain D?  True\n\n\nL‚Äôunione di due insieme si ottiene con union().\n\nfruits = {\"banana\", \"orange\", \"mango\", \"lemon\"}\nvegetables = {\"tomato\", \"potato\", \"cabbage\", \"onion\", \"carrot\"}\nprint(fruits.union(vegetables))\n\n{'carrot', 'onion', 'lemon', 'mango', 'tomato', 'potato', 'banana', 'cabbage', 'orange'}\n\n\nL‚Äôintersezione di due insieme si trova con intersection().\n\npython = {\"p\", \"y\", \"t\", \"h\", \"o\", \"n\"}\ndragon = {\"d\", \"r\", \"a\", \"g\", \"o\", \"n\"}\npython.intersection(dragon)\n\n{'n', 'o'}\n\n\nUn insieme pu√≤ essere un sottoinsieme o un sovrainsieme di altri insiemi.\nPer verificare se un insieme √® un sottoinsieme di un altro, si utilizza il metodo issubset(). Per verificare se un insieme √® un sovrainsieme di un altro, si utilizza il metodo issuperset().\n\nst1 = {\"item1\", \"item2\", \"item3\", \"item4\"}\nst2 = {\"item2\", \"item3\"}\nst2.issubset(st1)\n\nTrue\n\n\n\nst1.issuperset(st2) \n\nTrue\n\n\nLa differenza tra due insiemi si ottiene con difference().\n\nwhole_numbers = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\neven_numbers = {0, 2, 4, 6, 8, 10}\nwhole_numbers.difference(even_numbers)\n\n{1, 3, 5, 7, 9}\n\n\nPossiamo verificare se due insiemi sono disgiunti, ovvero non hanno elementi in comune, utilizzando il metodo isdisjoint().\n\nst1 = {\"item1\", \"item2\", \"item3\", \"item4\"}\nst2 = {\"item2\", \"item3\"}\nst2.isdisjoint(st1)\n\nFalse\n\n\n\n\n2.6.6 I dizionari\nGli oggetti di tipo ‚Äúdizionario‚Äù vengono utilizzati per creare coppie chiave-valore, dove ogni chiave √® unica. Un dizionario viene creato specificando ogni coppia come chiave : valore, separando le diverse coppie con una virgola e racchiudendo il tutto tra parentesi graffe. Ad esempio:\n\nmusic = {\n    \"blues\": \"Betty Smith\",\n    \"classical\": \"Gustav Mahler\",\n    \"pop\": \"David Bowie\",\n    \"jazz\": \"John Coltrane\",\n}\n\nL‚Äôaccesso agli elementi di un dizionario viene fatto specificando all‚Äôinterno di parentesi quadre la chiave per ottenere o modificare il valore corrispondente:\n\nmusic[\"pop\"]\n\n'David Bowie'\n\n\nPer trovare il numero di coppie key: value nel dizionario usiamo len().\n\nprint(len(music))\n\n4\n\n\n\nmusic[\"new music\"] = \"Missy Mazzoli\"\nprint(music)\n\n{'blues': 'Betty Smith', 'classical': 'Gustav Mahler', 'pop': 'David Bowie', 'jazz': 'John Coltrane', 'new music': 'Missy Mazzoli'}\n\n\n\n\n2.6.7 Contenitori vuoti\nA volte √® utile creare dei contenitori vuoti. I comandi per creare liste vuote, tuple vuote, dizionari vuoti e insiemi vuoti sono rispettivamente lst = [], tup=(), dic={} e st = set().",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python (1)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/01_python_1.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_1/01_python_1.html#informazioni-sullambiente-di-sviluppo",
    "title": "2¬† Python (1)",
    "section": "2.7 Informazioni sull‚ÄôAmbiente di Sviluppo",
    "text": "2.7 Informazioni sull‚ÄôAmbiente di Sviluppo\nAlla fine di ogni capitolo e, in effetti, alla fine (o all‚Äôinizio) di qualsiasi notebook che creiamo, √® utile includere informazioni sull‚Äôambiente di calcolo, compresi i numeri di versione di tutti i pacchetti che utilizziamo. Il pacchetto watermark pu√≤ essere usato per questo scopo. Il pacchetto watermark contiene comandi speciali ed √® un‚Äôestensione di IPython. In generale, per utilizzare tali comandi speciali, li precediamo con il segno % o %% in una cella. Utilizziamo la funzione speciale built-in %load_ext per caricare watermark, e quindi utilizziamo %watermark per invocarlo.\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m \n\nLast updated: Wed Jul 24 2024\n\nPython implementation: CPython\nPython version       : 3.12.4\nIPython version      : 8.26.0\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.5.0\nMachine     : arm64\nProcessor   : arm\nCPU cores   : 8\nArchitecture: 64bit\n\nWatermark: 2.4.3\n\n\n\nEcco una spiegazione dettagliata delle opzioni che sono state utilizzate nell‚Äôistruzione precedente.\n\n-n o --datename: Aggiunge la data e l‚Äôora correnti al watermark. Questo pu√≤ essere utile per mantenere una cronologia delle modifiche o delle esecuzioni del notebook.\n-u o --updated: Mostra l‚Äôultima volta in cui il notebook √® stato salvato. √à utile per tenere traccia delle modifiche recenti apportate al notebook.\n-v o --python: Mostra la versione di Python utilizzata nel kernel del notebook. Questo √® importante per garantire la compatibilit√† del codice e replicare gli ambienti di lavoro.\n-iv o --iversions: Visualizza le versioni delle librerie importate nel notebook. √à fondamentale per la replicabilit√† degli esperimenti e degli analisi, dato che diverse versioni delle librerie possono comportare risultati diversi.\n-w o --watermark: Aggiunge il watermark stesso, che √® semplicemente il logo ‚Äúwatermark‚Äù. √à pi√π una questione estetica che funzionale.\n-m o --machine: Fornisce informazioni sulla macchina su cui viene eseguito il Jupyter Notebook, come il tipo di sistema operativo e l‚Äôarchitettura della macchina (ad esempio, x86_64). Questo pu√≤ essere utile per documentare l‚Äôambiente hardware in cui vengono eseguiti gli esperimenti.\n\nQueste opzioni forniscono un modo semplice e immediato per documentare e tracciare importanti metadati nei notebook Jupyter.\n\n\n\n\nMatter, Ulrich. 2025. Data Analysis with AI and R. 1st Edition. New York, NY: Manning Publications.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Python (1)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/02_python_2.html",
    "href": "chapters/chapter_1/02_python_2.html",
    "title": "3¬† Python (2)",
    "section": "",
    "text": "3.1 Funzioni\nLo scopo delle funzioni √® raggruppare il codice in un formato organizzato, leggibile e riutilizzabile, contribuendo cos√¨ a ridurre la ridondanza del codice.\nUna regola generale per le funzioni √® che dovrebbero essere di piccole dimensioni e svolgere un‚Äôunica operazione.\nNella programmazione, una funzione accetta un input, esegue operazioni su di esso e pu√≤ restituire un output. Python mette a disposizione un‚Äôampia gamma di funzioni integrate, e si pu√≤ anche importare funzioni da pacchetti aggiuntivi o definirne di nuove.\nPer definire una nuova funzione in Python, si utilizza la parola chiave def, seguita dal nome della funzione e dai nomi simbolici dei suoi argomenti, separati da virgole e racchiusi tra parentesi. La definizione continua con i due punti (:) e il corpo della funzione, le cui istruzioni devono essere indentate. Il valore restituito dalla funzione viene specificato tramite la parola chiave return, generalmente nella riga finale del corpo della funzione.\ndef add_numbers(a, b):\n    \"\"\"\n    returns the sum of the two numeric arguments\n    \"\"\"\n    the_sum = a + b\n    return the_sum\nUna volta definita una funzione, √® possibile eseguirla chiamandola e passando gli argomenti appropriati. Ad esempio, possiamo chiamare la funzione add_numbers per sommare due numeri, come ad esempio 20 e 10:\nadd_numbers(20, 10)\n\n30\nConsideriamo la funzione roll_die():\nimport random\n\ndef roll_die():\n    \"\"\"\n    returns a random int between 1 and 6\n    \"\"\"\n    return random.choice([1, 2, 3, 4, 5, 6])\nIl corpo della funzione √® composto da una singola riga di codice che utilizza la funzione choice() della libreria random, a cui viene passata una lista. Questo significa che una funzione pu√≤ utilizzare altre funzioni che sono gi√† state definite. In questo caso, la funzione si limita a specificare l‚Äôargomento da passare a choice(). La funzione choice() restituir√† un numero casuale tra quelli specificati in input. Pertanto, la funzione roll_die() simula il lancio di un dado:\nroll_die()\n\n6\nroll_die()\n\n5\nSi noti inoltre la docstring, cio√® una stringa (in genere racchiusa tra ‚Äú‚Äú‚Äú‚Ä¶‚Äù‚Äú‚Äú) che si trova come prima istruzione all‚Äôinterno di una funzione. La docstring contiene informazioni sullo scopo e sulle modalit√† d‚Äôuso della funzione.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python (2)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/02_python_2.html#funzioni",
    "href": "chapters/chapter_1/02_python_2.html#funzioni",
    "title": "3¬† Python (2)",
    "section": "",
    "text": "3.1.1 Introspection\nUsando un punto interrogativo (?) prima o dopo una variabile √® possibile visualizzare alcune informazioni generale su quell‚Äôoggetto. Nel caso di una funzione viene stampata la doc string.\n\nroll_die?\n\nSignature: roll_die()\nDocstring: returns a random int between 1 and 6\nFile:      /var/folders/cl/wwjrsxdd5tz7y9jr82nd5hrw0000gn/T/ipykernel_13125/63164766.py\nType:      function\n\n\n\n\n3.1.2 Metodi\nLe funzioni definite all‚Äôinterno di una classe, chiamate ‚Äúmetodi‚Äù, rappresentano operazioni specifiche che possono essere eseguite sugli oggetti di quella classe. Una classe √® una struttura concettuale che rappresenta un concetto o un oggetto nel contesto del problema che stiamo affrontando. Ad esempio, nel capitolo sull‚Äôintroduzione a Pandas, lavorando con dati organizzati in una tabella, utilizziamo un oggetto chiamato DataFrame, appartenente alla classe ‚Äúpandas.DataFrame‚Äù. Un DataFrame √® una struttura tabellare che contiene dati disposti in righe e colonne.\nI metodi specifici della classe DataFrame offrono funzionalit√† per manipolare e analizzare i dati in questa struttura. Per esempio, il metodo ‚Äúhist()‚Äù genera istogrammi dei valori presenti in una colonna specifica del DataFrame. Per invocare un metodo su un oggetto DataFrame, come ‚Äúdf‚Äù, utilizziamo la sintassi ‚Äúnome_oggetto.nome_metodo()‚Äù e possiamo passare eventuali parametri richiesti tra parentesi.\nD‚Äôaltra parte, gli attributi rappresentano le caratteristiche o le propriet√† degli oggetti di una classe. Gli attributi possono essere richiamati utilizzando la sintassi ‚Äúnome_oggetto.nome_attributo‚Äù e restituiscono un valore specifico associato a quell‚Äôoggetto. Per esempio, l‚Äôattributo ‚Äú.shape‚Äù applicato a un DataFrame come ‚Äúdf.shape‚Äù restituisce il numero di righe e colonne presenti nel DataFrame.\nIn sintesi, una classe definisce un tipo di oggetto che ha attributi che ne descrivono le caratteristiche e metodi che rappresentano le azioni eseguibili su di esso. Gli attributi forniscono informazioni specifiche sull‚Äôoggetto, mentre i metodi consentono di effettuare operazioni e manipolazioni sui dati contenuti nell‚Äôoggetto stesso.\n\n\n3.1.3 La funzione lambda\nPython offre una sintassi alternativa che consente di definire funzioni ‚Äúinline‚Äù, cio√® in una singola linea di codice. Queste funzioni, chiamate funzioni anonime, non richiedono una definizione esplicita poich√© vengono utilizzate solo nel punto in cui sono dichiarate. Per creare una funzione anonima, utilizziamo la parola chiave lambda, seguita da un elenco di argomenti separati da virgole, due punti ‚Äú:‚Äù e l‚Äôespressione che definisce il comportamento della funzione basandosi sugli argomenti forniti.\nlambda argomento1, argomento2, ... : espressione\nQuesta sintassi permette di creare funzioni semplici ed espressive in modo conciso.\nNell‚Äôesempio seguente, la funzione somma 1 al valore passato come input:\n\n(lambda x : x + 1)(2)\n\n3\n\n\nQuando eseguiamo (lambda x : x + 1)(2), avviene quanto segue:\n\nL‚Äôinterprete Python definisce la funzione lambda lambda x : x + 1.\nLa funzione lambda viene immediatamente chiamata con l‚Äôargomento 2.\nAll‚Äôinterno della funzione lambda, x viene sostituito da 2, quindi l‚Äôespressione x + 1 diventa 2 + 1.\nLa funzione lambda restituisce 3.\n\nQuindi, il risultato dell‚Äôespressione (lambda x : x + 1)(2) √® 3.\nIn sintesi, la funzione lambda (lambda x : x + 1) definisce una funzione che aggiunge 1 al suo argomento. Quando la chiamiamo con l‚Äôargomento 2, otteniamo 3 come risultato.\nIn questo secondo esempio sommiamo i due numeri in entrata:\n\n(lambda x, y: x + y)(2, 3)\n\n5\n\n\nLa sintassi seguente √® valida in quanto, per l‚Äôinterprete, il carattere _ corrisponde all‚Äôultima funzione che √® stata valutata:\n\nlambda x, y: x + y\n\n&lt;function __main__.&lt;lambda&gt;(x, y)&gt;\n\n\n\n_(20, 10)\n\n30\n\n\nSi noti che abbiamo valutato la funzione lambda x, y: x + y in una cella precedente a quella che contiene _(20, 10); inserendo le due espressioni in una singola cella si ottiene un SyntaxError.\n\n\n3.1.4 Le funzioni map() e filter()\nPer gli esercizi che svolgeremo in seguito, risultano utili le funzioni map() e filter().\nLa funzione map() prende come input una funzione e una lista, e restituisce il risultato dell‚Äôapplicazione della funzione a ciascun elemento della lista (√® anche possibile usare qualsiasi oggetto iterabile al posto della lista). La lista stessa rimane invariata. Ad esempio, la seguente linea di codice eleva al quadrato ciascuno degli elementi della lista a e salva il risultato nella lista b:\n\na = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nb = list(map(lambda x: x * x, a))\nb\n\n[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n\n\nUn‚Äôaltra funzione molto utile per manipolare gli oggetti iterabili √® la funzione filter(). Questa funzione filtra un oggetto iterabile selezionando solo gli elementi che rispondono ad un determinato predicato. (Il predicato √® una funzione che restituisce un booleano). Per esempio\n\nc = list(filter(lambda x: x &gt; 50, b))\nc\n\n[64, 81, 100]\n\n\nSia map() che filter() restituiscono risultati che non sono ancora stati calcolati.\n\nfilter(lambda x: x &gt; 50, b)\n\n&lt;filter at 0x171da9720&gt;\n\n\nPossiamo visualizzare il risultato convertendolo in una lista:\n\nlist(filter(lambda x: x &gt; 50, b))\n\n[64, 81, 100]\n\n\n\n\n3.1.5 La funzione zip()\nLa funzione zip() crea una lista di tuple dagli elementi di due contenitori. Come nel caso delle operazioni precedenti, gli elementi vengono calcolati solo quando viene richiesto. Per esempio:\n\na = list(range(4))\na\n\n[0, 1, 2, 3]\n\n\n\nb = list(range(4, 8))\nb\n\n[4, 5, 6, 7]\n\n\n\nb = zip(a, b)\nb\n\n&lt;zip at 0x172034f80&gt;\n\n\n\nlist(b)\n\n[(0, 4), (1, 5), (2, 6), (3, 7)]",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python (2)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/02_python_2.html#il-flusso-di-esecuzione",
    "href": "chapters/chapter_1/02_python_2.html#il-flusso-di-esecuzione",
    "title": "3¬† Python (2)",
    "section": "3.2 Il flusso di esecuzione",
    "text": "3.2 Il flusso di esecuzione\nIn Python il codice viene eseguito sequenzialmente, partendo dalla prima riga fino a quando non c‚Äô√® pi√π nulla da eseguire. L‚Äôordine di esecuzione delle varie istruzioni √® detto flusso di esecuzione.\nPer esempio la cella seguente prima memorizza la lista names, poi la lista born e infine la lista dead.\n\nnames = [\"Sigmund Freud\", \"Jean Piaget\", \"Burrhus Frederic Skinner\", \"Albert Bandura\"]\nborn = [1856, 1896, 1904, 1925]\ndead = [1939, 1980, 1990, None]\n\nHo usato il valore speciale None in quanto non risulta disponibile l‚Äôanno. In queste situazioni si parla di valori mancanti (missing values) che, di norma, vengono indicati con la sigla NA (not available).\nLa cella seguente include le istruzioni condizionali che specificano se e quando devono essere eseguiti determinati blocchi di codice. La pi√π semplice istruzione di controllo √® l‚Äôistruzione if. Per esempio:\n\nname = \"Maria\"\ngrade = 29\n\nif name == \"Maria\" and grade &gt; 28:\n    print(\"Maria, hai ottenuto un ottimo voto all'esame!\")\n\nif name == \"Giovanna\" or grade &gt; 28:\n    print(\n        \"Tu potresti essere Giovanna oppure potresti avere ottenuto un ottimo voto all'esame.\"\n    )\n\nif name != \"Giovanna\" and grade &gt; 28:\n    print(\"Tu non sei Giovanna ma hai ottenuto un ottimo voto all'esame.\")\n\nMaria, hai ottenuto un ottimo voto all'esame!\nTu potresti essere Giovanna oppure potresti avere ottenuto un ottimo voto all'esame.\nTu non sei Giovanna ma hai ottenuto un ottimo voto all'esame.\n\n\nTutte e tre le condizioni precedenti ritornano True, quindi vengono stampati tutti e tre i messaggi.\nSi noti che == e != confrontano valori, mentre is e not confrontano oggetti. Per esempio,\n\nname_list = [\"Maria\", \"Giovanna\"]\nname_list_two = [\"Marco\", \"Francesco\"]\n\n# Compare values\nprint(name_list == name_list_two)\n\n# Compare objects\nprint(name_list is name_list_two)\n\nFalse\nFalse\n\n\nUna delle parole chiave condizionali pi√π utili √® in. Un esempio √® il seguente:\n\nname_list = [\"Maria\", \"Giovanna\", \"Marco\", \"Francesco\"]\n\nprint(\"Giovanna\" in name_list)\nprint(\"Luca\" in name_list)\n\nTrue\nFalse\n\n\nLa condizione opposta √® not in.\n\nprint(\"Luca\" not in name_list)\n\nTrue\n\n\nFacciamo un altro esempio.\n\nage = 26\nif age &gt;= 18:\n    print(\"Sei maggiorenne\")\n\nSei maggiorenne\n\n\nPython dispone di un‚Äôespressione ternaria che introduce la potenza dell‚Äôistruzione ‚Äòelse‚Äô in una sintassi concisa:\n\nage = 26\nb = \"Sei maggiorenne\" if age &gt;=18 else \"Sei minorenne\"\nprint(b)\n\nSei maggiorenne\n\n\n\nage = 16\nb = \"Sei maggiorenne\" if age &gt;=18 else \"Sei minorenne\"\nprint(b)\n\nSei minorenne\n\n\nUna struttura di selezione leggermente pi√π complessa √® ‚Äúif-else‚Äù. La sintassi di questa struttura √® la seguente:\nif &lt;condizione&gt;:\n    &lt;istruzione_se_condizione_vera&gt;\nelse:\n    &lt;istruzione_se_condizione_falsa&gt;\nLa semantica di ‚Äúif-else‚Äù √® quella che ci si aspetta: la condizione tra la parola chiave if e il carattere di due punti viene valutata: se risulta vera viene eseguita l‚Äôistruzione alla linea seguente, altrimenti viene eseguita l‚Äôistruzione dopo la parola chiave else. Anche in questo caso l‚Äôindentazione permette di identificare quali istruzioni devono essere eseguite nei due rami della selezione. Per esempio:\n\nage = 16\nif age &gt;= 18:\n    print(\"Sei maggiorenne\")\nelse:\n    print(\"Sei minorenne\")\n\nSei minorenne\n\n\nIn presenza di pi√π di due possibilit√† mutuamente esclusive ed esaustive possiamo usare l‚Äôistruzione elif. Per esempio:\n\ncfu = 36\nthesis_defense = False\n\nif cfu &gt;= 180 and thesis_defense == True:\n    print(\"Puoi andare a festeggiare!\")\nelif cfu &gt;= 180 and thesis_defense == False:\n    print(\"Devi ancora superare la prova finale!\")\nelse:\n    print(\"Ripassa tra qualche anno!\")\n\nRipassa tra qualche anno!\n\n\n\n3.2.1 Commenti\nIn Python √® possibile usare il carattere # per aggiungere commenti al codice. Ogni riga di commento deve essere preceduta da un #. I commenti non devono spiegare il metodo (cosa fa il codice: quello si vede), ma bens√¨ lo scopo: quello che noi intendiamo ottenere. I primi destinatari dei commenti siamo noi stessi tra un po‚Äô di tempo, ovvero quando ci saremo dimenticati cosa avevamo in mente quando abbiamo scritto il codice.\n\n# This is a comment and will not be executed.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python (2)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/02_python_2.html#cicli",
    "href": "chapters/chapter_1/02_python_2.html#cicli",
    "title": "3¬† Python (2)",
    "section": "3.3 Cicli",
    "text": "3.3 Cicli\nUn ciclo √® un modo per eseguire una porzione di codice pi√π di una volta. I cicli sono fondamentali nei linguaggi di programmazione. Come molti altri linguaggi di programmazione, Python ha due tipi di cicli per gestire tutte le proprie necessit√† di iterazione: il ciclo ‚Äúwhile‚Äù e il ciclo ‚Äúfor‚Äù.\n\n3.3.1 Il ciclo while\nil ciclo while permette l‚Äôesecuzione di un blocco di codice finch√© una determinata condizione √® True. Per esempio:\n\ncounter = 0\n\nwhile counter &lt;= 10:\n    print(counter)\n    counter += 1\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nIl codice counter += 1 √® equivalente a counter = counter + 1 e, ogni qualvolta viene eseguito il ciclo, riassegna alla variabile counter il valore che aveva in precedenza + 1.\nL‚Äôistruzione while controlla se alla variabile counter √® associato un valore minore o uguale a 10. Nel primo passo del ciclo la condizione √® soddisfatta, avendo noi definito counter = 0, pertanto il programma entra nel loop, stampa il valore della variabile counter e incrementa counter di un‚Äôunit√†.\nQuesto comportamento si ripete finch√© la condizione counter &lt;= 10 risulta True. Quando il contatore counter assume il valore 11 il ciclo while si interrompe e il blocco di codice del ciclo non viene pi√π eseguito.\n\n\n3.3.2 Il ciclo for\nIl ciclo for √® un costrutto di controllo di flusso che viene utilizzato per iterare su una sequenza di valori, come ad esempio una lista, una tupla, una stringa o un dizionario.\nLa sintassi generale di un ciclo for in Python √® la seguente:\nfor element in sequence:\n    # codice da eseguire\nDove element √® una variabile temporanea che assume il valore di ciascun elemento della sequenza ad ogni iterazione del ciclo, e sequence √® la sequenza di valori su cui iterare.\nDurante l‚Äôesecuzione del ciclo, il blocco di codice indentato sotto la linea for viene eseguito una volta per ogni elemento della sequenza. Ad ogni iterazione, la variabile elemento assume il valore dell‚Äôelemento corrente della sequenza e il codice all‚Äôinterno del blocco viene eseguito con questo valore.\nIl ciclo for √® spesso utilizzato per eseguire operazioni su ciascun elemento di una sequenza, come ad esempio la somma degli elementi di una lista o la stampa di ciascun carattere di una stringa. Per esempio\n\nnumbers = [0, 1, 2, 3, 4, 5]\nfor number in numbers: # number is temporary name to refer to the list's items, valid only inside this loop\n    print(number)\n\n0\n1\n2\n3\n4\n5\n\n\n\nlanguage = \"Python\"\nfor letter in language:\n    print(letter)\n\nP\ny\nt\nh\no\nn\n\n\nLa funzione range() √® spesso usata nei cicli for e permette di impostare un intervallo di esecuzione tanto ampio quanto il numero che le passiamo come parametro meno uno.\nLa funzione range() prende tre parametri: start (default 0), stop e step (default 1), ovvero un punto di inizio dell‚Äôintervallo, un punto di fine e un passo di avanzamento. L‚Äôindicizzazione Python parte da 0; quindi range(0, 11, 1) una lista di 11 elementi, da 0 a 10 inclusi.\n\nprint(list(range(0, 11, 1)))\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\nAd esempio, impostiamo un punto di inizio a 3, il punto di fine a 11 e un passo di 2:\n\nprint(list(range(3, 12, 2)))\n\n[3, 5, 7, 9, 11]\n\n\nIn un ciclo for, l‚Äôintervallo di range() corrisponde al numero di iterazioni che verranno eseguite, ovvero al numero di volte che il ciclo verr√† processato. Nel caso seguente, l‚Äôindice del ciclo (qui chiamato number) assume il valore 0 la prima volta che il ciclo viene eseguito e il valore 10 nell‚Äôultima esecuzione del ciclo.\n\nfor number in range(11):\n    print(number)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\nfor number in range(3, 12, 2):\n    print(number)\n\n3\n5\n7\n9\n11\n\n\n\n3.3.2.1 Cicli for annidati\nSono possibili i cicli for annidati, vale a dire un ciclo posto all‚Äôinterno del corpo di un altro (chiamato ciclo esterno). Al suo primo passo, il ciclo esterno mette in esecuzione quello interno che esegue il proprio blocco di codice fino alla conclusione. Quindi, al secondo passo, il ciclo esterno rimette in esecuzione quello interno. Questo si ripete finch√© il ciclo esterno non termina. Per esempio:\n\nfor i in range(4):\n    for j in range(4):\n        print((i, j))\n\n(0, 0)\n(0, 1)\n(0, 2)\n(0, 3)\n(1, 0)\n(1, 1)\n(1, 2)\n(1, 3)\n(2, 0)\n(2, 1)\n(2, 2)\n(2, 3)\n(3, 0)\n(3, 1)\n(3, 2)\n(3, 3)\n\n\n\n\n3.3.2.2 Modificare gli elementi di una lista\nIl ciclo for √® il modo pi√π comune per scorrere gli elementi di una lista, come abbiamo visto in precedenza.\n\nfor name in name_list:\n    print(name)\n\nMaria\nGiovanna\nMarco\nFrancesco\n\n\nQuesto approccio pu√≤ essere usato se abbiamo solo bisogno di leggere gli elementi della lista. Nel ciclo seguente, ad esempio, leggiamo gli elementi d una lista per incrementare una variabile cos√¨ da calcolare una somma.\n\nnumbers = [2, -4, 1, 6, 3]\n\ntotal = 0\nfor num in numbers:\n    total += num\n\nprint(total)\n\n8\n\n\nMa se vogliamo cambiare gli elementi di una lista l‚Äôapproccio precedente non funziona e dobbiamo usare gli indici. Nell‚Äôesempio seguente, questo risultato viene ottenuto utilizzando le funzioni range e len:\n\nnumbers = [2, -4, 1, 6, 3]\n\nfor i in range(len(numbers)):\n    numbers[i] = numbers[i] * 2\n\nprint(numbers)\n\n[4, -8, 2, 12, 6]\n\n\nNel codice seguente, la funzione len() ritorna 5.\n\nnumbers = [2, -4, 1, 6, 3]\nlen(numbers)\n\n5\n\n\nQuindi, range(5) produce la seguente sequenza iterabile:\n\nlist(range(5))\n\n[0, 1, 2, 3, 4]\n\n\nQuesti sono gli indici che verranno usati nelle iterazioni del ciclo for.\n\nLa prima volta che il ciclo viene eseguito, l‚Äôindice i vale 0 e numbers[i] si riferisce al primo elemento della lista;\nla seconda volta che il ciclo viene eseguito, i vale 1 e numbers[i] si riferisce al secondo elemento della lista;\ne cos√¨ via.\n\nL‚Äôistruzione di assegnazione nel corpo del ciclo for usa i per leggere il valore i-esimo della lista originale (a destra dell‚Äôuguale) e per assegnargli un nuovo valore (a sinistra dell‚Äôuguale).\n\n\n\n3.3.3 List comprehension\nUna list comprehension √® un modo conciso di creare una lista. √à un modo compatto per creare una nuova lista. Accade speso di dover creare una lista dove ciascun elemento √® il risultato di un‚Äôoperazione condotta sugli elementi di un‚Äôaltra lista o di un iterabile; oppure, di dover estrarre gli elementi che soddisfano una certa condizione. Per esempio, supponiamo di volere sommare una costante ad una lista di numeri. Usando un ciclo for possiamo procedere nel modo seguente (si noti l‚Äôuso della funzione append):\n\nnew_list = []\nk = 10\nfor x in range(10):\n    new_list.append(x + k)\n\nnew_list\n\n[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n\n\nOppure, in maniera pi√π semplice, possiamo usare una list comprehension:\n\nnew_list = [x + k for x in range(10)]\nnew_list\n\n[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n\n\nUna list comprehension √® racchiusa tra parentesi quadre; contiene un‚Äôespressione, seguita da una clausola for, seguita da zero o pi√π clausole for o if. La sintassi √® la seguente:\n[ &lt;expression&gt; for item in iterable &lt;if optional_condition&gt; ]\nIl risultato √® una nuova lista costruita valutando l‚Äôespressione nel contesto delle clausole for e if che la seguono. Una list comprehension combina dunque un ciclo for e (se necessario) una o pi√π condizioni logiche in una singola riga di codice. Esaminiamo una variante dell‚Äôesempio precedente.\n\nlist1 = [1, 2, 3, 4, 5, 6]\nprint(\"list1:\", list1)\n\nlist1: [1, 2, 3, 4, 5, 6]\n\n\n\nlist2 = [item + 1 for item in list1]\nprint(\"list2:\", list2)\n\nlist2: [2, 3, 4, 5, 6, 7]\n\n\nSi noti che la parola item avrebbe potuto essere quasi qualsiasi stringa (in precedenza abbiamo usato x). La possiamo immaginare con la seguente definizione: ...per ogni elemento in .... Nel seguente esempio, sommiamo 1 agli elementi di list1 solo se sono pari:\n\nlist3 = [item + 1 for item in list1 if item % 2 == 0] \nprint('list3:', list3)\n\nlist3: [3, 5, 7]\n\n\nFacciamo un altro esempio usando range():\n\nnum_list = range(50, 60)\n[1 + num for num in num_list]\n\n[51, 52, 53, 54, 55, 56, 57, 58, 59, 60]\n\n\nQui selezioniamo solo i numeri pari (oltre allo zero):\n\n[i for i in range(11) if i % 2 == 0]\n\n[0, 2, 4, 6, 8, 10]\n\n\nSpecificando una condizione, possiamo cambiare il segno solo dei numeri dispari nella lista:\n\n[-i if i % 2 else i for i in range(11)]\n\n[0, -1, 2, -3, 4, -5, 6, -7, 8, -9, 10]\n\n\nPossiamo anche eseguire pi√π iterazioni simultaneamente:\n\n[(i, j) for i in range(3) for j in range(4)]\n\n[(0, 0),\n (0, 1),\n (0, 2),\n (0, 3),\n (1, 0),\n (1, 1),\n (1, 2),\n (1, 3),\n (2, 0),\n (2, 1),\n (2, 2),\n (2, 3)]\n\n\nIn questo esempio vengono selezionati solo i nomi inclusi nella lista female_names:\n\nfirst_names = [\"Maria\", \"Marco\", \"Francesco\", \"Giovanna\"]\nfemale_names = [\"Alice\", \"Maria\", \"Giovanna\", \"Lisa\"]\nfemale_list = [name for name in first_names if name in female_names]\nprint(female_list)\n\n['Maria', 'Giovanna']\n\n\nNel seguente esempio vengono estratte le prime tre lettere di ciascuno dei nomi che compongono una lista:\n\nletters = [name[0:3] for name in first_names] \nletters\n\n['Mar', 'Mar', 'Fra', 'Gio']\n\n\nPer estrarre l‚Äôultimo carattere di una stringa usiamo [-1]:\n\nmy_string = \"barbabl√π\"\nmy_string[-1]\n\n'√π'\n\n\nPossiamo dunque usare seguente list comprehension estrae gli ultimi tre caratteri di ciascun elemento della lista first_names.\n\nletters = [name[-3:] for name in first_names] \nletters\n\n['ria', 'rco', 'sco', 'nna']\n\n\n√à possibile impiegare un‚Äôespressione ternaria all‚Äôinterno di una list comprehension per sfruttare la versatilit√† dell‚Äôistruzione ‚Äòelse‚Äô in modo sintatticamente efficace. Ad esempio, possiamo sostituire tutti i numeri dispari di una lista (un un NumPy array) con il valore 99.\n\nnum = np.array([4, 7, 2, 6, 3, 9])  # pu√≤ anche essere una lista Python\n[e if e % 2 == 0 else 99 for e in num]\n\n[4, 99, 2, 6, 99, 99]",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python (2)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/02_python_2.html#librerie-e-moduli",
    "href": "chapters/chapter_1/02_python_2.html#librerie-e-moduli",
    "title": "3¬† Python (2)",
    "section": "3.4 Librerie e moduli",
    "text": "3.4 Librerie e moduli\n\n3.4.1 Importare moduli\nI moduli (anche conosciuti come librerie in altri linguaggi) sono dei file usati per raggruppare funzioni e altri oggetti. Python include una lista estensiva di moduli standard (anche conosciuti come Standard Library), ma √® anche possibile scaricarne o definirne di nuovi. Prima di potere utilizzare le funzioni non presenti nella Standard Library all‚Äôinterno dei nostri programmi dobbiamo importare dei moduli aggiuntivi, e per fare ci√≤ usiamo il comando import.\nL‚Äôimportazione pu√≤ riguardare un intero modulo oppure solo uno (o pi√π) dei suoi elementi. Consideriamo per esempio la funzione mean. Essa √® disponibile nel modulo numpy. L‚Äôistruzione import numpy importa tutto il modulo numpy. Dopo che un modulo √® stato importato, √® possibile accedere a un suo generico elemento usando il nome del modulo, seguito da un punto e dal nome dell‚Äôelemento in questione. Ad esempio, numpy.mean().\nIndicare il nome di un modulo per poter accedere ai suoi elementi ha spesso l‚Äôeffetto di allungare il codice, diminuendone al contempo la leggibilit√†. √à per questo motivo che √® possibile importare un modulo specificando un nome alternativo, pi√π corto. √à quello che succede quando scriviamo l‚Äôistruzione import numpy as np. In questo caso, l‚Äôistruzione precedente diventa np.mean().\nI moduli pi√π complessi sono organizzati in strutture gerarchiche chiamate package. La seguente cella importa il modulo pyplot che √® contenuto nel package matplotlib (matplotlib √® la libreria di riferimento in Python per la creazione di grafici).\n\nimport matplotlib.pyplot as plt\n\nQui di seguito sono descritte tutte le possibilit√†:\n\n# import everything from library\nimport random\n# call function by\nrandom.random()\n\n0.16777284588756924\n\n\n\n#import everything, but change name\nimport random as rnd\n# call function by\nrnd.random()\n\n0.05690270000491682\n\n\n\n# select what to import from library\nfrom random import random\n#call function by\nrandom()\n\n0.037974565142151695\n\n\n\n# import everything from library\nfrom random import *\n# call function by\nrandom()\n\n0.20988431417194764\n\n\nNella cella seguente importiamo seaborn con il nome sns e usiamo le sue funzionalit√† per impostare uno stile e una palette di colori per la visualizzazione dei grafici.\n\nimport seaborn as sns\nsns.set_theme()\nsns.set_palette(\"colorblind\")\n\nNell‚Äôesempio seguente calcoliamo la somma degli elementi della lista numerica primes usando funzione sum() contenuta nella libreria NumPy che abbiamo importato con il nome di np:\n\nimport numpy as np\n\nprimes = [1, 2, 3, 5, 7, 11, 13]\nnp.sum(primes)\n\n42\n\n\nCalcolo la media di primes:\n\nnp.mean(primes)\n\n6.0\n\n\nScriviamo una nuova funzione per la media, \\(\\bar{x} = n^{-1}\\sum_{i=1}^n x_i\\):\n\ndef my_mean(x):\n    res = np.sum(x) / len(x)\n    return res\n\n\nmy_mean(primes)\n\n6.0\n\n\nSi noti che, nel corpo di una funzione, √® possibile usare altre funzioni: qui, np.sum() e len().\n√à sempre possibile usare la funzione di help su una funzione:\n\nhelp(sum)\n\nHelp on built-in function sum in module builtins:\n\nsum(iterable, /, start=0)\n    Return the sum of a 'start' value (default: 0) plus an iterable of numbers\n    \n    When the iterable is empty, return the start value.\n    This function is intended specifically for use with numeric values and may\n    reject non-numeric types.\n\n\n\nIn Visual Studio Code √® sufficiente posizionare il cursore sul nome della funzione.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python (2)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/02_python_2.html#formattazione-del-codice",
    "href": "chapters/chapter_1/02_python_2.html#formattazione-del-codice",
    "title": "3¬† Python (2)",
    "section": "3.5 Formattazione del codice",
    "text": "3.5 Formattazione del codice\n\nduibult ../images/code_quality_2x.png :align: center",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python (2)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/02_python_2.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_1/02_python_2.html#informazioni-sullambiente-di-sviluppo",
    "title": "3¬† Python (2)",
    "section": "3.6 Informazioni sull‚ÄôAmbiente di Sviluppo",
    "text": "3.6 Informazioni sull‚ÄôAmbiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m\n\nLast updated: Mon Jan 29 2024\n\nPython implementation: CPython\nPython version       : 3.11.7\nIPython version      : 8.19.0\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.3.0\nMachine     : x86_64\nProcessor   : i386\nCPU cores   : 8\nArchitecture: 64bit\n\npandas    : 2.1.4\nnumpy     : 1.26.2\nseaborn   : 0.13.0\narviz     : 0.17.0\nmatplotlib: 3.8.2\n\nWatermark: 2.4.3",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python (2)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/03_numpy.html",
    "href": "chapters/chapter_1/03_numpy.html",
    "title": "4¬† NumPy",
    "section": "",
    "text": "4.1 Preparazione del Notebook\nimport numpy as np",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/03_numpy.html#utilizzo-degli-array-nel-modulo-numpy",
    "href": "chapters/chapter_1/03_numpy.html#utilizzo-degli-array-nel-modulo-numpy",
    "title": "4¬† NumPy",
    "section": "4.2 Utilizzo degli Array nel Modulo NumPy",
    "text": "4.2 Utilizzo degli Array nel Modulo NumPy\nIn Python standard, abbiamo a disposizione tipi di dati numerici (come numeri interi e decimali) e strutture come liste, dizionari e insiemi. NumPy, d‚Äôaltro canto, introduce un nuovo tipo di struttura dati: l‚Äôarray N-dimensionale, noto come ndarray. Questi array hanno alcune caratteristiche distintive:\n\nDimensioni: Gli ndarray possono variare nel numero di dimensioni, definite come ‚Äúassi‚Äù. Ad esempio, un array pu√≤ essere unidimensionale (simile a un vettore lineare), bidimensionale (come una matrice o una tabella), tridimensionale (simile a un cubo), e cos√¨ via.\nTipo di Dato: A differenza delle liste in Python standard che possono contenere diversi tipi di dati, ogni elemento all‚Äôinterno di un ndarray deve essere dello stesso tipo, come numeri interi, decimali, booleani o stringhe.\nForma: La ‚Äúforma‚Äù di un ndarray si riferisce alle sue dimensioni, ovvero quante righe, colonne o altri livelli di profondit√† ha. Per esempio, la forma (3, 4) indica un array con 3 righe e 4 colonne.\nIndicizzazione: Gli ndarray possono essere indicizzati in modo simile agli array standard di Python, ma offrono anche opzioni pi√π avanzate per l‚Äôindicizzazione.\n\nGli ndarray sono potenti per manipolare e analizzare i dati, grazie alle loro funzioni e metodi che includono operazioni matematiche e statistiche, trasformazioni e altre manipolazioni dei dati.\nTerminologia Importante: - Size: Indica il numero totale di elementi in un array. - Rank: Si riferisce al numero di dimensioni, o assi, di un array. - Shape: Denota le dimensioni specifiche dell‚Äôarray, ovvero una sequenza di numeri che rappresentano il conteggio degli elementi in ogni dimensione.\nCome Creare un ndarray: Il modo pi√π diretto per creare un ndarray √® attraverso la conversione di una lista Python. Ad esempio, √® possibile creare un array unidimensionale (1-D) a partire da una lista standard di Python.\n\nx = np.array([1, 2, 3, 4, 5, 6])\n\nL‚Äôistruzione precedente crea un array in NumPy, assegnandolo alla variabile x. Questo array √® un vettore unidimensionale contenente sei elementi, che sono i numeri interi specificati all‚Äôinterno delle parentesi quadre.\n\nprint(x)\n\n[1 2 3 4 5 6]\n\n\nIndicizzazione\nSe vogliamo estrarre un singolo elemento del vettore lo indicizziamo con la sua posizione (si ricordi che l‚Äôindice inizia da 0):\n\nx[0]\n\n1\n\n\n\nx[2]\n\n3\n\n\nUn array 2-D si crea nel modo seguente:\n\ny = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\nprint(y)\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n\nEstraiamo un singolo elemento dall‚Äôarray:\n\ny[0, 2]\n\n3\n\n\nEstraiamo la seconda riga dall‚Äôarray:\n\ny[1]\n\narray([5, 6, 7, 8])\n\n\nEstraiamo la seconda colonna dall‚Äôarray:\n\ny[:, 1] \n\narray([ 2,  6, 10])\n\n\nLa sintassi con i due punti √® chiamata ‚Äúslicing‚Äù dell‚Äôarray.\n\n# Display the first row of the array\nprint(\"Displaying the first row:\")\nprint(y[0, :])\n\nDisplaying the first row:\n[1 2 3 4]\n\n\n\n# Show the last two elements in the first row\nprint(\"Showing the last two elements in the first row:\")\nprint(y[0, -2:])\n\nShowing the last two elements in the first row:\n[3 4]\n\n\n\n# Retrieve every second element in the first row\nprint(\"Retrieving every second element in the first row:\")\nprint(y[0, ::2])\n\nRetrieving every second element in the first row:\n[1 3]\n\n\n\n# Extract a submatrix from the original array\nprint(\"Extracting a submatrix:\")\nprint(y[:2, 1:3])\n\nExtracting a submatrix:\n[[2 3]\n [6 7]]\n\n\n\n4.2.1 Funzioni per ndarray\nNumpy offre varie funzioni per creare ndarray. Per esempio, √® possibile creare un array 1-D con la funzione .arange(start, stop, incr, dtype=..) che fornisce l‚Äôintervallo di numeri compreso fra start, stop, al passo incr:\n\nz = np.arange(2, 9, 2)\nprint(z)\n\n[2 4 6 8]\n\n\nSi usa spesso .arange per creare sequenze a incrementi unitari:\n\nw = np.arange(11)\nprint(w)\n\n[ 0  1  2  3  4  5  6  7  8  9 10]\n\n\nUn‚Äôaltra funzione molto utile √® .linspace:\n\nx = np.linspace(0, 10, num=20)\nprint(x)\n\n[ 0.          0.52631579  1.05263158  1.57894737  2.10526316  2.63157895\n  3.15789474  3.68421053  4.21052632  4.73684211  5.26315789  5.78947368\n  6.31578947  6.84210526  7.36842105  7.89473684  8.42105263  8.94736842\n  9.47368421 10.        ]\n\n\nFissati gli estremi (qui 0, 10) e il numero di elementi desiderati, .linspace determina in maniera automatica l‚Äôincremento.\nUna propriet√† molto utile dei ndarray √® la possibilit√† di filtrare gli elementi di un array che rispondono come True ad un criterio. Per esempio:\n\nprint(x[x &gt; 7])\n\n[ 7.36842105  7.89473684  8.42105263  8.94736842  9.47368421 10.        ]\n\n\nperch√© solo gli ultimi sei elementi di x rispondono True al criterio \\(x &gt; 7\\).\nLe dimensioni (‚Äúassi‚Äù) di un ndarray vengono ritornate dal metodo .dim. Per esempio:\n\nprint(y)\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n\n\ny.ndim\n\n2\n\n\n\nprint(y.max(axis=1))\n\n[ 4  8 12]\n\n\n\nprint(y.max(axis=0))\n\n[ 9 10 11 12]\n\n\nIl numero di elementi per ciascun asse viene ritornato dal metodo .shape:\n\ny.shape\n\n(3, 4)",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/03_numpy.html#manipolazione-di-array-con-numpy",
    "href": "chapters/chapter_1/03_numpy.html#manipolazione-di-array-con-numpy",
    "title": "4¬† NumPy",
    "section": "4.3 Manipolazione di Array con NumPy",
    "text": "4.3 Manipolazione di Array con NumPy\nNumPy rende pi√π agevole lavorare con grandi quantit√† di dati. Un concetto fondamentale in NumPy sono gli array monodimensionali, spesso utilizzati per rappresentare vettori, ovvero sequenze di numeri che possono rappresentare, ad esempio, le misurazioni di una variabile specifica. Grazie a NumPy, possiamo eseguire operazioni aritmetiche su questi vettori in modo semplice, applicando la stessa operazione a tutti gli elementi dell‚Äôarray contemporaneamente.\n\n4.3.1 Cosa Significa Vettorizzare un‚ÄôOperazione\nLa vettorizzazione √® una delle funzionalit√† pi√π efficaci di NumPy. Quando diciamo che un‚Äôoperazione √® vettorizzata, significa che questa operazione viene applicata in un colpo solo a tutti gli elementi dell‚Äôarray, invece di dover agire su ciascun elemento individualmente. Questo approccio rende la manipolazione di grandi insiemi di dati non solo pi√π veloce ma anche pi√π intuitiva, poich√© consente di trattare l‚Äôintero insieme di dati come un‚Äôunica entit√† anzich√© come una serie di punti dati individuali.\nSupponiamo di avere raccolto i dati di 4 individui\n\nm = np.array([1.62, 1.75, 1.55, 1.74])\nkg = np.array([55.4, 73.6, 57.1, 59.5])\n\nprint(m)\nprint(kg)\n\n[1.62 1.75 1.55 1.74]\n[55.4 73.6 57.1 59.5]\n\n\ndove m √® l‚Äôarray che contiene i dati relativi all‚Äôaltezza in metri dei quattro individui e kg √® l‚Äôarray che contiene i dati relativi al peso in kg. I dati sono organizzati in modo tale che il primo elemento di entrambi i vettori si riferisce alle misure del primo individuo, il secondo elemento dei due vettori si riferisce alle misure del secondo individuo, ecc.\nSupponiamo di volere calcolare l‚Äôindice BMI:\n\\[\nBMI = \\frac{kg}{m^2}.\n\\]\nPer il primo individuo del campione, l‚Äôindice di massa corporea √®\n\n55.4 / 1.62**2\n\n21.109586953208346\n\n\nSi noti che non abbiamo bisogno di scrivere 55.4 / (1.62**2) in quanto, in Python, l‚Äôelevazione a potenza viene eseguita prima della somma e della divisione (come in tutti i linguaggi). Usando i dati immagazzinati nei due vettori, lo stesso risultato si ottiene nel modo seguente:\n\nkg[0] / m[0]**2\n\n21.109586953208346\n\n\nSe ora non specifichiamo l‚Äôindice (per esempio, [0]), le operazioni aritmetiche indicate verranno eseguite per ciascuna coppia di elementi corrispondenti nei due vettori:\n\nbmi = kg / m**2\n\nOtteniamo cos√¨, con una sola istruzione, l‚Äôindice BMI dei quattro individui:\n\nbmi.round(1)\n\narray([21.1, 24. , 23.8, 19.7])\n\n\nQuesto esempio illustra come le operazioni aritmetiche standard vengano eseguite elemento per elemento negli array, grazie al processo di vettorizzazione.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/03_numpy.html#broadcasting",
    "href": "chapters/chapter_1/03_numpy.html#broadcasting",
    "title": "4¬† NumPy",
    "section": "4.4 Broadcasting",
    "text": "4.4 Broadcasting\nIl broadcasting √® una caratteristica distintiva di NumPy che facilita l‚Äôesecuzione di operazioni tra array di dimensioni diverse o tra un array e uno scalare, anche se le loro dimensioni non sono direttamente compatibili. Grazie al broadcasting, NumPy √® in grado di ‚Äúespandere‚Äù automaticamente le dimensioni di uno degli operandi per rendere possibile l‚Äôoperazione.\nQuesto significa che possiamo, per esempio, eseguire un‚Äôoperazione tra un array e un numero singolo (un vettore e uno scalare) o tra due array di dimensioni differenti, senza la necessit√† di modificare manualmente le dimensioni di questi array. Il broadcasting si occupa di adattare le dimensioni in modo coerente per consentire l‚Äôoperazione desiderata. Ci√≤ rende il codice pi√π snello e leggibile, eliminando la necessit√† di espandere gli array manualmente.\nIn breve, il broadcasting in NumPy √® un potente strumento che semplifica l‚Äôesecuzione di operazioni su array di dimensioni diverse o tra array e scalari, automatizzando l‚Äôallineamento delle dimensioni.\n\n4.4.1 Esempio di Broadcasting\nImmaginiamo di avere un array A con dimensioni 3x3 e un numero scalare B. Senza broadcasting, dovremmo espandere B in un array 3x3 riempiendo ogni cella con il valore di B per eseguire un‚Äôoperazione come l‚Äôaddizione su ciascun elemento di A. Grazie al broadcasting, possiamo semplicemente scrivere A + B, e NumPy si occuper√† automaticamente di ‚Äúespandere‚Äù B durante l‚Äôoperazione, applicando il valore scalare a ogni elemento di A.\n\n# Creiamo un array 3x3\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n# Definiamo uno scalare\nB = 5\n\n# Applichiamo il broadcasting per aggiungere lo scalare a ogni elemento dell'array\nC = A + B\n\nprint(C)\n\n[[ 6  7  8]\n [ 9 10 11]\n [12 13 14]]\n\n\nIn questo esempio, C conterr√† l‚Äôarray originale A con ogni elemento incrementato di 5, dimostrando come il broadcasting semplifichi operazioni che altrimenti richiederebbero passaggi aggiuntivi.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/03_numpy.html#altre-operazioni-sugli-array",
    "href": "chapters/chapter_1/03_numpy.html#altre-operazioni-sugli-array",
    "title": "4¬† NumPy",
    "section": "4.5 Altre operazioni sugli array",
    "text": "4.5 Altre operazioni sugli array\nC‚Äô√® un numero enorme di funzioni predefinite in NumPy che calcolano automaticamente diverse quantit√† sugli ndarray. Ad esempio:\n\nmean(): calcola la media di un vettore o matrice;\nsum(): calcola la somma di un vettore o matrice;\nstd(): calcola la deviazione standard;\nmin(): trova il minimo nel vettore o matrice;\nmax(): trova il massimo;\nndim: dimensione del vettore o matrice;\nshape: restituisce una tupla con la ‚Äúforma‚Äù del vettore o matrice;\nsize: restituisce la dimensione totale del vettore (=ndim) o della matrice;\ndtype: scrive il tipo numpy del dato;\nzeros(num): scrive un vettore di num elementi inizializzati a zero;\narange(start,stop,step): genera un intervallo di valori (interi o reali, a seconda dei valori di start, ecc.) intervallati di step. Nota che i dati vengono generati nell‚Äôintervallo aperto [start,stop)!\nlinstep(start,stop,num): genera un intervallo di num valori interi o reali a partire da start fino a stop (incluso!);\nastype(tipo): converte l‚Äôndarray nel tipo specificato\n\nPer esempio:\n\nx = np.array([1, 2, 3])\nprint(x)\n\n[1 2 3]\n\n\n\n[x.min(), x.max(), x.sum(), x.mean(), x.std()]\n\n[1, 3, 6, 2.0, 0.816496580927726]",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/03_numpy.html#lavorare-con-formule-matematiche",
    "href": "chapters/chapter_1/03_numpy.html#lavorare-con-formule-matematiche",
    "title": "4¬† NumPy",
    "section": "4.6 Lavorare con formule matematiche",
    "text": "4.6 Lavorare con formule matematiche\nL‚Äôimplementazione delle formule matematiche sugli array √® un processo molto semplice con Numpy. Possiamo prendere ad esempio la formula della deviazione standard che discuteremo nel capitolo {ref}loc-scale-notebook:\n\\[\ns = \\sqrt{\\sum_{i=1}^n\\frac{(x_i - \\bar{x})^2}{n}}\n\\]\nL‚Äôimplementazione su un array NumPy √® la seguente:\n\nprint(x)\n\n[1 2 3]\n\n\n\nnp.sqrt(np.sum((x - np.mean(x)) ** 2) / np.size(x))\n\n0.816496580927726\n\n\nQuesta implementazione funziona nello stesso modo sia che x contenga 3 elementi (come nel caso presente) sia che x contenga migliaia di elementi. √à importante notare l‚Äôutilizzo delle parentesi tonde per specificare l‚Äôordine di esecuzione delle operazioni. In particolare, nel codice fornito, si inizia calcolando la media degli elementi del vettore x per mezzo della funzione np.mean(x). Questa operazione produce uno scalare, ovvero un singolo valore numerico che rappresenta la media degli elementi del vettore. L‚Äôutilizzo delle parentesi tonde √® fondamentale per garantire l‚Äôordine corretto delle operazioni. In questo caso, la funzione np.mean() viene applicata al vettore x prima di qualsiasi altra operazione matematica. Senza le parentesi tonde, le operazioni verrebbero eseguite in un ordine diverso e il risultato potrebbe essere errato.\n\nnp.mean(x)\n\n2.0\n\n\nSuccessivamente, eseguiamo la sottrazione dei singoli elementi del vettore x per la media del vettore stesso, ovvero \\(x_i - \\bar{x}\\), utilizzando il meccanismo del broadcasting.\n\nx - np.mean(x)\n\narray([-1.,  0.,  1.])\n\n\nEleviamo poi al quadrato gli elementi del vettore che abbiamo ottenuto:\n\n(x - np.mean(x)) ** 2\n\narray([1., 0., 1.])\n\n\nSommiamo gli elementi del vettore:\n\nnp.sum((x - np.mean(x)) ** 2)\n\n2.0\n\n\nDividiamo il numero ottenuto per \\(n\\). Questa √® la varianza di \\(x\\):\n\nres = np.sum((x - np.mean(x)) ** 2) / np.size(x)\nres\n\n0.6666666666666666\n\n\nInfine, per ottenere la deviazione standard, prendiamo la radice quadrata:\n\nnp.sqrt(res)\n\n0.816496580927726\n\n\nIl risultato ottenuto coincide con quello che si trova applicando la funzione np.std():\n\nnp.std(x)\n\n0.816496580927726",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/03_numpy.html#slicing",
    "href": "chapters/chapter_1/03_numpy.html#slicing",
    "title": "4¬† NumPy",
    "section": "4.7 Slicing",
    "text": "4.7 Slicing\nPer concludere, spendiamo ancora alcune parole sull‚Äôindicizzazione degli ndarray.\nSlicing in Numpy √® un meccanismo che consente di selezionare una porzione di un array multidimensionale, ovvero una sotto-matrice o un sotto-vettore. Per selezionare una porzione di un array, si utilizza la sintassi [start:stop:step], dove start indica l‚Äôindice di partenza della porzione, stop indica l‚Äôindice di fine e step indica il passo da utilizzare per la selezione. Se uno o pi√π di questi valori vengono omessi, vengono utilizzati dei valori di default.\nAd esempio, se abbiamo un array arr di dimensione (3, 4) e vogliamo selezionare la seconda colonna, possiamo usare la sintassi arr[:, 1]. In questo caso, il simbolo : indica che vogliamo selezionare tutte le righe, mentre il numero 1 indica che vogliamo selezionare la seconda colonna.\nInoltre, possiamo utilizzare il meccanismo di slicing anche per selezionare porzioni di array multidimensionali. Ad esempio, se abbiamo un array arr di dimensione (3, 4, 5) e vogliamo selezionare la prima riga di ciascuna matrice 4x5, possiamo usare la sintassi arr[:, 0, :].\nPer esempio, creiamo l‚Äôarray x di rango 2 con shape (3, 4):\n\nx = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\nprint(x)\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n\nUtilizziamo il meccanismo di slicing per estrarre la sottomatrice composta dalle prime 2 righe e dalle colonne 1 e 2. y √® l‚Äôarray risultante di dimensione (2, 2):\n\ny = x[:2, 1:3]\nprint(y)\n\n[[2 3]\n [6 7]]\n\n\n√à importante sapere che uno slice di un array in Numpy √® una vista degli stessi dati, il che significa che modificarlo implica la modifica dell‚Äôarray originale. In pratica, quando si modifica uno slice di un array, si sta modificando direttamente l‚Äôarray originale e tutte le altre visualizzazioni dell‚Äôarray vedranno la stessa modifica. Questo avviene perch√© Numpy √® progettato per gestire enormi quantit√† di dati, pertanto cerca di evitare il pi√π possibile di effettuare copie dei dati.\nQuesto comportamento deve essere preso in considerazione durante la modifica degli array in Numpy, al fine di evitare modifiche accidentali o indesiderate. In alcuni casi, √® possibile utilizzare il metodo copy() per creare una copia indipendente di un array e lavorare sulla copia senza modificare l‚Äôoriginale. Vediamo un esempio.\n\nprint(x[0, 1])   \n\n2\n\n\n\ny[0, 0] = 77     \n\n\nprint(x)\n\n[[ 1 77  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n\n\nx = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n\nz = x.copy()\nprint(z)\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n\n\nz[0, 1] = 33\nprint(z)\n\n[[ 1 33  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n\n\nprint(x)\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/03_numpy.html#copia-e-copia-profonda-in-python",
    "href": "chapters/chapter_1/03_numpy.html#copia-e-copia-profonda-in-python",
    "title": "4¬† NumPy",
    "section": "4.8 Copia e ‚ÄúCopia Profonda‚Äù in Python",
    "text": "4.8 Copia e ‚ÄúCopia Profonda‚Äù in Python\nIn Python, per ottimizzare le prestazioni, le assegnazioni di solito non copiano gli oggetti sottostanti. Questo √® particolarmente importante, ad esempio, quando gli oggetti vengono passati tra funzioni, per evitare una quantit√† eccessiva di copie in memoria quando non sono necessarie (questo approccio √® noto tecnicamente come ‚Äúpassaggio per riferimento‚Äù).\nConsideriamo il seguente esempio con un array A:\n\nA = np.array([[1, 2], [3, 4]])\n\nSe creiamo un nuovo riferimento B a A:\n\nB = A\n\nOra B si riferisce allo stesso insieme di dati di A. Se modifichiamo B, anche A viene modificato di conseguenza:\n\nB[0,0] = 10\n\nDopo questa modifica, sia B che A saranno:\n\nprint(A)\n\n[[10  2]\n [ 3  4]]\n\n\nSe desideriamo evitare questo comportamento, in modo tale che B diventi un oggetto completamente indipendente da A, dobbiamo effettuare una cosiddetta ‚Äúcopia profonda‚Äù utilizzando la funzione copy:\n\nB = np.copy(A)\n\nOra, se modificassimo B, A non subirebbe alcuna modifica. Ad esempio:\n\nB[0,0] = -5\n\nA questo punto, B sar√†:\n\nprint(B)\n\n[[-5  2]\n [ 3  4]]\n\n\nMa A rimarr√† invariato:\n\nprint(A)\n\n[[10  2]\n [ 3  4]]\n\n\nQuesto esempio mostra chiaramente la differenza tra una semplice assegnazione, che crea un riferimento all‚Äôoggetto originale, e una ‚Äúcopia profonda‚Äù, che crea un nuovo oggetto indipendente.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/03_numpy.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_1/03_numpy.html#informazioni-sullambiente-di-sviluppo",
    "title": "4¬† NumPy",
    "section": "4.9 Informazioni sull‚ÄôAmbiente di Sviluppo",
    "text": "4.9 Informazioni sull‚ÄôAmbiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m\n\nLast updated: Mon Jan 29 2024\n\nPython implementation: CPython\nPython version       : 3.11.7\nIPython version      : 8.19.0\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.3.0\nMachine     : x86_64\nProcessor   : i386\nCPU cores   : 8\nArchitecture: 64bit\n\nnumpy: 1.26.2\n\nWatermark: 2.4.3",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/04_pandas.html",
    "href": "chapters/chapter_1/04_pandas.html",
    "title": "5¬† Pandas (1)",
    "section": "",
    "text": "5.1 Preparazione del NoteBook\nimport pandas as pd\nimport numpy as np\n\n# Di default, Pandas mostrer√† 60 righe e 20 colonne. \n# Modifichiamo qui le impostazioni di visualizzazione predefinite di Pandas per mostrare pi√π righe.\npd.options.display.max_rows = 100",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Pandas (1)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/04_pandas.html#series",
    "href": "chapters/chapter_1/04_pandas.html#series",
    "title": "5¬† Pandas (1)",
    "section": "5.2 Series",
    "text": "5.2 Series\nIn Pandas, una Series √® un array unidimensionale composto da una sequenza di valori omogenei, simile ad un ndarray, accompagnato da un array di etichette chiamato ‚Äúindex‚Äù. A differenza degli indici degli array Numpy, che sono sempre interi e partono da zero, gli oggetti Series supportano etichette personalizzate che possono essere, ad esempio, delle stringhe. Inoltre, gli oggetti Series possono contenere dati mancanti che vengono ignorati da molte delle operazioni della classe.\nIl modo pi√π semplice di creare un oggetto Series √® di convertire una lista. Per esempio:\n\ngrades = pd.Series([27, 30, 24, 18, 22, 20, 29])\n\n√à possibile ottenere la rappresentazione dell‚Äôarray dell‚Äôoggetto e dell‚Äôindice dell‚Äôoggetto Series tramite i suoi attributi array e index, rispettivamente.\n\ngrades.array\n\n&lt;NumpyExtensionArray&gt;\n[27, 30, 24, 18, 22, 20, 29]\nLength: 7, dtype: int64\n\n\n\ngrades.index\n\nRangeIndex(start=0, stop=7, step=1)\n\n\nOppure, possiamo semplicemente stampare i contenuti dell‚Äôoggetto Series direttamente:\n\nprint(grades)\n\n0    27\n1    30\n2    24\n3    18\n4    22\n5    20\n6    29\ndtype: int64\n\n\nPer accedere agli elementi di un oggetto Series si usano le parentesi quadre contenenti un indice:\n\ngrades[0]\n\n27\n\n\n\ngrades[0:3]\n\n0    27\n1    30\n2    24\ndtype: int64\n\n\n√à possibile filtrare gli elementi di un oggetto Series con un array booleano:\n\ngrades &gt; 24\n\n0     True\n1     True\n2    False\n3    False\n4    False\n5    False\n6     True\ndtype: bool\n\n\n\ngrades[grades &gt; 24]\n\n0    27\n1    30\n6    29\ndtype: int64\n\n\n√à possibile manipolare gli elementi di un oggetto Series con le normali operazioni aritmetiche mediante la vettorializzazione:\n\ngrades / 10\n\n0    2.7\n1    3.0\n2    2.4\n3    1.8\n4    2.2\n5    2.0\n6    2.9\ndtype: float64\n\n\n\nnp.sqrt(grades)\n\n0    5.196152\n1    5.477226\n2    4.898979\n3    4.242641\n4    4.690416\n5    4.472136\n6    5.385165\ndtype: float64\n\n\nGli oggetti Series hanno diversi metodi per svolgere varie operazioni, per esempio per ricavare alcune statistiche descrittive:\n\n[grades.count(), grades.mean(), grades.min(), grades.max(), grades.std(), grades.sum()]\n\n[7, 24.285714285714285, 18, 30, 4.572172558506722, 170]\n\n\nMolto utile √® il metodo .describe():\n\ngrades.describe()\n\ncount     7.000000\nmean     24.285714\nstd       4.572173\nmin      18.000000\n25%      21.000000\n50%      24.000000\n75%      28.000000\nmax      30.000000\ndtype: float64",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Pandas (1)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/04_pandas.html#dataframe",
    "href": "chapters/chapter_1/04_pandas.html#dataframe",
    "title": "5¬† Pandas (1)",
    "section": "5.3 DataFrame",
    "text": "5.3 DataFrame\nUn pandas.DataFrame √® composto da righe e colonne. Ogni colonna di un dataframe √® un oggetto pandas.Series: quindi, un dataframe √® una collezione di serie. A differenza di un array NumPy, un dataframe pu√≤ combinare pi√π tipi di dati, come numeri e testo, ma i dati in ogni colonna sono dello stesso tipo.\nEsistono molti modi per costruire un DataFrame. Un primo metodo √® quello di utilizzare un dizionario che include una o pi√π liste o array Numpy di uguale lunghezza. Per esempio:\n\ndata = {\n    \"name\": [\n        \"Maria\",\n        \"Anna\",\n        \"Francesco\",\n        \"Cristina\",\n        \"Gianni\",\n        \"Gabriella\",\n        \"Stefano\",\n    ],\n    \"sex\": [\"f\", \"f\", \"m\", \"f\", \"m\", \"f\", \"m\"],\n    \"group\": [\"a\", \"b\", \"a\", \"b\", \"b\", \"c\", \"a\"],\n    \"x\": [1, 2, 3, 4, 5, 6, 7],\n    \"y\": [8, 9, 10, 11, 12, 13, 14],\n    \"z\": [15, 16, 17, 18, 19, 20, 21],\n}\nframe = pd.DataFrame(data)\nframe\n\n\n\n\n\n\n\n\n\nname\nsex\ngroup\nx\ny\nz\n\n\n\n\n0\nMaria\nf\na\n1\n8\n15\n\n\n1\nAnna\nf\nb\n2\n9\n16\n\n\n2\nFrancesco\nm\na\n3\n10\n17\n\n\n3\nCristina\nf\nb\n4\n11\n18\n\n\n4\nGianni\nm\nb\n5\n12\n19\n\n\n5\nGabriella\nf\nc\n6\n13\n20\n\n\n6\nStefano\nm\na\n7\n14\n21\n\n\n\n\n\n\n\n\nOppure possiamo procedere nel modo seguente:\n\ndf = pd.DataFrame()\n\ndf[\"x\"] = [1, 2, 3, 4, 5, 6, 7]\ndf[\"y\"] = [8, 9, 10, 11, 12, 13, 14]\ndf[\"z\"] = [14.4, 15.1, 16.7, 17.3, 18.9, 19.3, 20.2]\ndf[\"group\"] = [\"a\", \"b\", \"a\", \"b\", \"b\", \"c\", \"a\"]\ndf[\"sex\"] = [\"f\", \"f\", \"m\", \"f\", \"m\", \"f\", \"m\"]\ndf[\"name\"] = [\n    \"Maria\",\n    \"Anna\",\n    \"Francesco\",\n    \"Cristina\",\n    \"Gianni\",\n    \"Gabriella\",\n    \"Stefano\",\n]\n\nprint(df)\n\n   x   y     z group sex       name\n0  1   8  14.4     a   f      Maria\n1  2   9  15.1     b   f       Anna\n2  3  10  16.7     a   m  Francesco\n3  4  11  17.3     b   f   Cristina\n4  5  12  18.9     b   m     Gianni\n5  6  13  19.3     c   f  Gabriella\n6  7  14  20.2     a   m    Stefano\n\n\nMolto spesso un DataFrame viene creato dal caricamento di dati da file.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Pandas (1)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/04_pandas.html#lettura-di-dati-da-file",
    "href": "chapters/chapter_1/04_pandas.html#lettura-di-dati-da-file",
    "title": "5¬† Pandas (1)",
    "section": "5.4 Lettura di dati da file",
    "text": "5.4 Lettura di dati da file\nDi solito la quantit√† di dati da analizzare √® tale che non √® pensabile di poterli immettere manualmente in una o pi√π liste. Normalmente i dati sono memorizzati su un file ed √® necessario importarli. La lettura (importazione) dei file √® il primo fondamentale passo nel processo pi√π generale di analisi dei dati.\nIn un primo esempio, importiamo i dati da un repository remoto.\n\nurl = \"https://raw.githubusercontent.com/pandas-dev/pandas/master/doc/data/titanic.csv\"\ntitanic = pd.read_csv(url, index_col=\"Name\")\n\n√à possibile usare il metodo .head() per visualizzare le prime cinque righe.\n\ntitanic.head()\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\nName\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBraund, Mr. Owen Harris\n1\n0\n3\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\n2\n1\n1\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\nHeikkinen, Miss. Laina\n3\n1\n3\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n4\n1\n1\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\nAllen, Mr. William Henry\n5\n0\n3\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\nLe statistiche descrittive per ciascuna colonna si ottengono con il metodo describe.\n\ntitanic.describe()\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n891.000000\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n446.000000\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n\n\nstd\n257.353842\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n\n\nmin\n1.000000\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n223.500000\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n\n\n50%\n446.000000\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n668.500000\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n\n\nmax\n891.000000\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\n\nIn questo modo possiamo ottenere informazioni sui nomi dei passeggeri, la sopravvivenza (0 o 1), l‚Äôet√†, il prezzo del biglietto, ecc. Con le statistiche riassuntive vediamo che l‚Äôet√† media √® di 29,7 anni, il prezzo massimo del biglietto √® di 512 USD, il 38% dei passeggeri √® sopravvissuto, ecc.\nPer fare un secondo esempio, importo i dati dal file penguins.csv situato nella directory ‚Äúdata‚Äù del mio computer. I dati relativi ai pinguini di Palmer sono resi disponibili da Kristen Gorman e dalla Palmer station, Antarctica LTER. La seguente cella legge il contenuto del file penguins.csv e lo inserisce nell‚Äôoggetto df utilizzando la funzione read_csv() di Pandas.\n\ndf = pd.read_csv(\"../data/penguins.csv\")\n\nPer il DataFrame df il significato delle colonne √® il seguente:\n\nspecies: a factor denoting penguin type (Ad√©lie, Chinstrap and Gentoo)\nisland: a factor denoting island in Palmer Archipelago, Antarctica (Biscoe, Dream or Torgersen)\nbill_length_mm: a number denoting bill length (millimeters)\nbill_depth_mm: a number denoting bill depth (millimeters)\nflipper_length_mm: an integer denoting flipper length (millimeters)\nbody_mass_g: an integer denoting body mass (grams)\nsex: a factor denoting sexuality (female, male)\nyear: the year of the study\n\nUsiamo il metodo .head() per visualizzare le prime cinque righe.\n\ndf.head()\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\n\nA volte potrebbero esserci dati estranei alla fine del file, quindi √® importante anche controllare le ultime righe:\n\ndf.tail()\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n339\nChinstrap\nDream\n55.8\n19.8\n207.0\n4000.0\nmale\n2009\n\n\n340\nChinstrap\nDream\n43.5\n18.1\n202.0\n3400.0\nfemale\n2009\n\n\n341\nChinstrap\nDream\n49.6\n18.2\n193.0\n3775.0\nmale\n2009\n\n\n342\nChinstrap\nDream\n50.8\n19.0\n210.0\n4100.0\nmale\n2009\n\n\n343\nChinstrap\nDream\n50.2\n18.7\n198.0\n3775.0\nfemale\n2009\n\n\n\n\n\n\n\n\nUn breve tutorial in formato video √® disponibile tramite il seguente [collegamento](https://drive.google.com/file/d/12y7jZ0McvZBXThg6yjFgWx2ljQKrhoYR/view?usp=share_link), il quale illustra come effettuare la lettura dei dati da un file esterno in Visual Studio Code. \nL‚Äôattributo .dtypes restituisce il tipo dei dati:\n\ndf.dtypes\n\nspecies               object\nisland                object\nbill_length_mm       float64\nbill_depth_mm        float64\nflipper_length_mm    float64\nbody_mass_g          float64\nsex                   object\nyear                   int64\ndtype: object\n\n\nGli attributi pi√π comunemente usati sono elencati di seguito:\n\n\n\n\n\n\n\nAttributo\nRitorna\n\n\n\n\ndtypes\nIl tipo di dati in ogni colonna\n\n\nshape\nUna tupla con le dimensioni del DataFrame object (numero di righe, numero di colonne)\n\n\nindex\nL‚Äôoggetto Index lungo le righe del DataFrame\n\n\ncolumns\nIl nome delle colonne\n\n\nvalues\nI dati contenuti nel DataFrame\n\n\nempty\nCheck if the DataFrame object is empty\n\n\n\nPer esempio, l‚Äôistruzione della cella seguente restituisce l‚Äôelenco con i nomi delle colonne del DataFrame df:\n\ndf.columns\n\nIndex(['species', 'island', 'bill_length_mm', 'bill_depth_mm',\n       'flipper_length_mm', 'body_mass_g', 'sex', 'year'],\n      dtype='object')\n\n\nLe dimensioni del Data Frame si ottengono con l‚Äôattributo .shape, che ritorna il numero di righe e di colonne. Nel caso presente, ci sono 344 righe e 8 colonne.\n\ndf.shape\n\n(344, 8)\n\n\nCome abbiamo gi√† visto in precedenza, un sommario dei dati si ottiene con il metodo .describe():\n\ndf.describe()\n\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nyear\n\n\n\n\ncount\n342.000000\n342.000000\n342.000000\n342.000000\n344.000000\n\n\nmean\n43.921930\n17.151170\n200.915205\n4201.754386\n2008.029070\n\n\nstd\n5.459584\n1.974793\n14.061714\n801.954536\n0.818356\n\n\nmin\n32.100000\n13.100000\n172.000000\n2700.000000\n2007.000000\n\n\n25%\n39.225000\n15.600000\n190.000000\n3550.000000\n2007.000000\n\n\n50%\n44.450000\n17.300000\n197.000000\n4050.000000\n2008.000000\n\n\n75%\n48.500000\n18.700000\n213.000000\n4750.000000\n2009.000000\n\n\nmax\n59.600000\n21.500000\n231.000000\n6300.000000\n2009.000000\n\n\n\n\n\n\n\n\nUna descrizione del DataFrame si ottiene con il metodo .info().\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \n 7   year               344 non-null    int64  \ndtypes: float64(4), int64(1), object(3)\nmemory usage: 21.6+ KB\n\n\nSi noti che, alle volte, abbiamo utilizzato la sintassi `df.word` e talvolta la sintassi `df.word()`. Tecnicamente, la classe Pandas Dataframe ha sia attributi che metodi. Gli attributi sono `.word`, mentre i metodi sono `.word()` o `.word(arg1, arg2, ecc.)`. Per sapere se qualcosa √® un metodo o un attributo √® necessario leggere la documentazione.\nAbbiamo visto in precedenza come possiamo leggere i dati in un dataframe utilizzando la funzione read_csv(). Pandas comprende anche molti altri formati, ad esempio utilizzando le funzioni read_excel(), read_hdf(), read_json(), ecc. (e i corrispondenti metodi per scrivere su file: to_csv(), to_excel(), to_hdf(), to_json(), ecc.).",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Pandas (1)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/04_pandas.html#gestione-dei-dati-mancanti",
    "href": "chapters/chapter_1/04_pandas.html#gestione-dei-dati-mancanti",
    "title": "5¬† Pandas (1)",
    "section": "5.5 Gestione dei dati mancanti",
    "text": "5.5 Gestione dei dati mancanti\nNell‚Äôoutput di .info() troviamo la colonna ‚ÄúNon-Null Count‚Äù, ovvero il numero di dati non mancanti per ciascuna colonna del DataFrame. Da questo si nota che le colonne del DataFrame df contengono alcuni dati mancanti. La gestione dei dati mancanti √® un argomento complesso. Per ora ci limitiamo ad escludere tutte le righe che, in qualche colonna, contengono dei dati mancanti.\nOttengo il numero di dati per ciascuna colonna del DataFrame:\n\ndf.isnull().sum()\n\nspecies               0\nisland                0\nbill_length_mm        2\nbill_depth_mm         2\nflipper_length_mm     2\nbody_mass_g           2\nsex                  11\nyear                  0\ndtype: int64\n\n\nRimuovo i dati mancanti con il metodo .dropna(). L‚Äôargomento inplace=True specifica il DataFrame viene trasformato in maniera permanente.\n\ndf.dropna(inplace=True)\n\nVerifico che i dati mancanti siano stati rimossi.\n\ndf.shape\n\n(333, 8)\n\n\nIn alternativa, possiamo rimuovere solo le righe del DataFrame per le quali ci sono dei dati mancanti rispetto a specifiche colonne. Per esempio\n\ndf = pd.read_csv(\"../data/penguins.csv\")\ndf0 = df.copy()\n\nmissing_data = df0.isnull()[[\"bill_length_mm\", \"body_mass_g\"]].any(axis=1)\n# Drop rows with any missing data\ndf0_cleaned = df0.loc[~missing_data]\ndf0_cleaned.shape\n\n(342, 8)",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Pandas (1)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/04_pandas.html#rinominare-le-colonne",
    "href": "chapters/chapter_1/04_pandas.html#rinominare-le-colonne",
    "title": "5¬† Pandas (1)",
    "section": "5.6 Rinominare le colonne",
    "text": "5.6 Rinominare le colonne\n√à possibile rinominare tutte le colonne passando al metodo .rename() un dizionario che specifica quali colonne devono essere mappate a cosa. Nella cella seguente facciamo prima una copia del DataFrame con il metodo copy() e poi rinominiamo sex che diventa gender e year che diventa year_of_the_study:\n\ndf1 = df.copy()\n\n# rename(columns={\"OLD_NAME\": \"NEW_NAME\"})\ndf1.rename(columns={\n    \"sex\": \"gender\", \n    \"year\": \"year_of_the_study\"\n    }, \n           inplace=True)\ndf1.head()\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\ngender\nyear_of_the_study\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\n\nSi noti che in Python valgono le seguenti regole.\n\n- Il nome di una variabile deve iniziare con una lettera o con il trattino basso (*underscore*) `_`.\n- Il nome di una variabile non pu√≤ iniziare con un numero.\n- Un nome di variabile pu√≤ contenere solo caratteri alfanumerici e il trattino basso (A-z, 0-9 e _).\n- I nomi delle variabili fanno distinzione tra maiuscole e minuscole (`age`, `Age` e `AGE` sono tre variabili diverse).\n\nGli spazi non sono consentiti nel nome delle variabili: come separatore usate il trattino basso.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Pandas (1)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/04_pandas.html#estrarre-i-dati-dal-dataframe",
    "href": "chapters/chapter_1/04_pandas.html#estrarre-i-dati-dal-dataframe",
    "title": "5¬† Pandas (1)",
    "section": "5.7 Estrarre i dati dal DataFrame",
    "text": "5.7 Estrarre i dati dal DataFrame\nUna parte cruciale del lavoro con i DataFrame √® l‚Äôestrazione di sottoinsiemi di dati: vogliamo trovare le righe che soddisfano un determinato insieme di criteri, vogliamo isolare le colonne/righe di interesse, ecc. Per rispondere alle domande di interesse dell‚Äôanalisi dei dati, molto spesso √® necessario selezionare un sottoinsieme del DataFrame.\n\n5.7.1 Colonne\n√à possibile estrarre una colonna da un DataFrame usando una notazione simile a quella che si usa per il dizionario (DataFrame['word']) o utilizzando la notazione DataFrame.word. Per esempio:\n\ndf[\"bill_length_mm\"]\n\n0      39.1\n1      39.5\n2      40.3\n4      36.7\n5      39.3\n       ... \n339    55.8\n340    43.5\n341    49.6\n342    50.8\n343    50.2\nName: bill_length_mm, Length: 333, dtype: float64\n\n\n\ndf.bill_length_mm\n\n0      39.1\n1      39.5\n2      40.3\n4      36.7\n5      39.3\n       ... \n339    55.8\n340    43.5\n341    49.6\n342    50.8\n343    50.2\nName: bill_length_mm, Length: 333, dtype: float64\n\n\nSe tra parentesi quadre indichiamo una lista di colonne, come nel caso di df[['bill_length_mm','species']], otteniamo un nuovo DataFrame costituito unicamente dalle colonne selezionate:\n\ndf[[\"bill_length_mm\", \"species\"]]\n\n\n\n\n\n\n\n\n\nbill_length_mm\nspecies\n\n\n\n\n0\n39.1\nAdelie\n\n\n1\n39.5\nAdelie\n\n\n2\n40.3\nAdelie\n\n\n4\n36.7\nAdelie\n\n\n5\n39.3\nAdelie\n\n\n...\n...\n...\n\n\n339\n55.8\nChinstrap\n\n\n340\n43.5\nChinstrap\n\n\n341\n49.6\nChinstrap\n\n\n342\n50.8\nChinstrap\n\n\n343\n50.2\nChinstrap\n\n\n\n\n333 rows √ó 2 columns\n\n\n\n\n\n\n5.7.2 Righe\nIn un pandas.DataFrame, anche le righe hanno un nome. I nomi delle righe sono chiamati index:\n\ndf.index\n\nInt64Index([  0,   1,   2,   4,   5,   6,   7,  12,  13,  14,\n            ...\n            334, 335, 336, 337, 338, 339, 340, 341, 342, 343],\n           dtype='int64', length=333)\n\n\nCi sono vari metodi per estrarre sottoinsimi di righe da un DataFrame. √à possibile fare riferimento ad un intervallo di righe mediante un indice di slice. Per esempio, possiamo ottenere le prime 3 righe del DataFrame df nel modo seguente:\n\ndf[0:3]\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n\n\n\n\n\n\nSi noti che in Python una sequenza √® determinata dal valore iniziale e quello finale ma si interrompe ad n-1. Pertanto, per selezionare una singola riga (per esempio, la prima) dobbiamo procedere nel modo seguente:\n\ndf[0:1]\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n\n\n\n\n\n\n\n\n5.7.3 Indicizzazione, selezione e filtraggio\nPoich√© l‚Äôoggetto DataFrame √® bidimensionale, √® possibile selezionare un sottoinsieme di righe e colonne utilizzando le etichette degli assi (loc) o gli indici delle righe (iloc).\nPer esempio, usando l‚Äôattributo iloc posso selezionare la prima riga del DataFrame:\n\ndf.iloc[0]\n\nspecies                 Adelie\nisland               Torgersen\nbill_length_mm            39.1\nbill_depth_mm             18.7\nflipper_length_mm        181.0\nbody_mass_g             3750.0\nsex                       male\nyear                      2007\nName: 0, dtype: object\n\n\nLa cella seguene seleziona le prime tre righe del DataFrame:\n\ndf.iloc[0:3]\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n\n\n\n\n\n\nL‚Äôattributo loc consente di selezionare simultaneamente righe e colonne per ‚Äúnome‚Äù. Il ‚Äúnome‚Äù delle righe √® l‚Äôindice di riga. Per esempio, visualizzo il quinto valore della colonna body_mass_g:\n\ndf.loc[4, \"body_mass_g\"]\n\n3450.0\n\n\noppure, il quinto valore delle colonne bill_length_mm, bill_depth_mm, flipper_length_mm:\n\ndf.loc[4, [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"]]\n\nbill_length_mm        36.7\nbill_depth_mm         19.3\nflipper_length_mm    193.0\nName: 4, dtype: object\n\n\nVisualizzo ora le prime tre righe sulle tre colonne precedenti. Si noti l‚Äôuso di : per definire un intervallo di valori sull‚Äôindice di riga.\n\ndf.loc[0:2, [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"]]\n\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\n\n\n\n\n0\n39.1\n18.7\n181.0\n\n\n1\n39.5\n17.4\n186.0\n\n\n2\n40.3\n18.0\n195.0\n\n\n\n\n\n\n\n\nUna piccola variante della sintassi precedente si rivela molto utile. Qui, il segno di due punti (:) signfica ‚Äútutte le righe‚Äù:\n\nkeep_cols = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"]\nprint(df.loc[:, keep_cols])\n\n     bill_length_mm  bill_depth_mm  flipper_length_mm\n0              39.1           18.7              181.0\n1              39.5           17.4              186.0\n2              40.3           18.0              195.0\n4              36.7           19.3              193.0\n5              39.3           20.6              190.0\n..              ...            ...                ...\n339            55.8           19.8              207.0\n340            43.5           18.1              202.0\n341            49.6           18.2              193.0\n342            50.8           19.0              210.0\n343            50.2           18.7              198.0\n\n[333 rows x 3 columns]\n\n\n\n\n5.7.4 Filtrare righe in maniera condizionale\nIn precedenza abbiamo utilizzato la selezione delle righe in un DataFrame in base alla loro posizione. Tuttavia, √® pi√π comune selezionare le righe del DataFrame utilizzando una condizione logica, cio√® tramite l‚Äôindicizzazione booleana.\nIniziamo con un esempio relativo ad una condizione specificata sui valori di una sola colonna. Quando applichiamo un operatore logico come &gt;, &lt;, ==, != ai valori di una colonna del DataFrame, il risultato √® una sequenza di valori booleani (True, False), uno per ogni riga nel DataFrame, i quali indicano se, per quella riga, la condizione √® vera o falsa. Ad esempio:\n\ndf[\"island\"] == \"Torgersen\"\n\n0       True\n1       True\n2       True\n4       True\n5       True\n       ...  \n339    False\n340    False\n341    False\n342    False\n343    False\nName: island, Length: 333, dtype: bool\n\n\nUtilizzando i valori booleani che sono stati ottenuti in questo modo √® possibile filtrare le righe del DataFrame, ovvero, ottenere un nuovo DataFrame nel quale la condizione logica specificata √® vera su tutte le righe. Per esempio, nella cella seguente selezioniamo solo le osservazioni relative all‚Äôisola Torgersen, ovvero tutte le righe del DataFrame nelle quali la colonna island assume il valore Torgersen.\n\nonly_torgersen = df[df[\"island\"] == \"Torgersen\"]\nonly_torgersen.head()\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n\n\n\n\n\n\nIn maniera equivalente, possiamo scrivere:\n\nonly_torgersen = df.loc[df[\"island\"] == \"Torgersen\"]\nonly_torgersen.head()\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n\n\n\n\n\n\n√à possibile combinare pi√π condizioni logiche usando gli operatori & (e), | (oppure). Si presti attenzione all‚Äôuso delle parentesi.\n\ndf.loc[(df[\"island\"] == \"Torgersen\") & (df[\"sex\"] == \"female\")].head()\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n12\nAdelie\nTorgersen\n41.1\n17.6\n182.0\n3200.0\nfemale\n2007\n\n\n\n\n\n\n\n\n\n\n5.7.5 Metodo .query\n√à anche possibile filtrare le righe del DataFrame usando il metodo query(). Ci sono diversi modi per generare sottoinsiemi con Pandas. I metodi loc e iloc consentono di recuperare sottoinsiemi in base alle etichette di riga e colonna o all‚Äôindice intero delle righe e delle colonne. E Pandas ha una notazione a parentesi quadre che consente di utilizzare condizioni logiche per recuperare righe di dati specifiche. Ma la sintassi di questi metodi non √® la pi√π trasparente. Inoltre, tali metodi sono difficili da usare insieme ad altri metodi di manipolazione dei dati in modo organico.\nIl metodo .query di Pandas cerca di risolve questi problemi. Il metodo .query consente di ‚Äúinterrogare‚Äù un DataFrame e recuperare sottoinsiemi basati su condizioni logiche. La sintassi √® un po‚Äô pi√π snella rispetto alla notazione a parentesi quadre di Pandas. Inoltre, il metodo .query pu√≤ essere utilizzato con altri metodi di Pandas in modo snello e semplice, rendendo la manipolazione dei dati maggiormente fluida e diretta.\nLa sintassi √® la seguente:\nyour_data_frame.query(expression, inplace = False)\nL‚Äôespressione utilizzata nella query √® una sorta di espressione logica che descrive quali righe restituire in output. Se l‚Äôespressione √® vera per una particolare riga, la riga verr√† inclusa nell‚Äôoutput. Se l‚Äôespressione √® falsa per una particolare riga, quella riga verr√† esclusa dall‚Äôoutput.\nIl parametro inplace consente di specificare se si desidera modificare direttamente il DataFrame con cui si sta lavorando.\nPer esempio:\n\neval_string = \"island == 'Torgersen' & sex == 'female' & year != 2009\"\ndf.query(eval_string)[[\"bill_depth_mm\", \"flipper_length_mm\"]]\n\n\n\n\n\n\n\n\n\nbill_depth_mm\nflipper_length_mm\n\n\n\n\n1\n17.4\n186.0\n\n\n2\n18.0\n195.0\n\n\n4\n19.3\n193.0\n\n\n6\n17.8\n181.0\n\n\n12\n17.6\n182.0\n\n\n15\n17.8\n185.0\n\n\n16\n19.0\n195.0\n\n\n18\n18.4\n184.0\n\n\n68\n16.6\n190.0\n\n\n70\n19.0\n190.0\n\n\n72\n17.2\n196.0\n\n\n74\n17.5\n190.0\n\n\n76\n16.8\n191.0\n\n\n78\n16.1\n187.0\n\n\n80\n17.2\n189.0\n\n\n82\n18.8\n187.0\n\n\n\n\n\n\n\n\nUn altro esempio usa la keyword in per selezionare solo le righe relative alle due isole specificate.\n\neval_string = \"island in ['Torgersen', 'Dream']\"\ndf.query(eval_string)[[\"bill_depth_mm\", \"flipper_length_mm\"]]\n\n\n\n\n\n\n\n\n\nbill_depth_mm\nflipper_length_mm\n\n\n\n\n0\n18.7\n181.0\n\n\n1\n17.4\n186.0\n\n\n2\n18.0\n195.0\n\n\n4\n19.3\n193.0\n\n\n5\n20.6\n190.0\n\n\n...\n...\n...\n\n\n339\n19.8\n207.0\n\n\n340\n18.1\n202.0\n\n\n341\n18.2\n193.0\n\n\n342\n19.0\n210.0\n\n\n343\n18.7\n198.0\n\n\n\n\n170 rows √ó 2 columns\n\n\n\n\nIl metodo query() pu√≤ anche essere utilizzato per selezionare le righe di un DataFrame in base alle relazioni tra le colonne. Ad esempio,\n\ndf.query(\"bill_length_mm &gt; 3*bill_depth_mm\")[[\"bill_depth_mm\", \"flipper_length_mm\"]]\n\n\n\n\n\n\n\n\n\nbill_depth_mm\nflipper_length_mm\n\n\n\n\n152\n13.2\n211.0\n\n\n153\n16.3\n230.0\n\n\n154\n14.1\n210.0\n\n\n155\n15.2\n218.0\n\n\n156\n14.5\n215.0\n\n\n...\n...\n...\n\n\n272\n14.3\n215.0\n\n\n273\n15.7\n222.0\n\n\n274\n14.8\n212.0\n\n\n275\n16.1\n213.0\n\n\n293\n17.8\n181.0\n\n\n\n\n106 rows √ó 2 columns\n\n\n\n\n√à anche possibile fare riferimento a variabili non contenute nel DataFrame usando il carattere @.\n\noutside_var = 21\ndf.query(\"bill_depth_mm &gt; @outside_var\")[[\"bill_depth_mm\", \"flipper_length_mm\"]]\n\n\n\n\n\n\n\n\n\nbill_depth_mm\nflipper_length_mm\n\n\n\n\n13\n21.2\n191.0\n\n\n14\n21.1\n198.0\n\n\n19\n21.5\n194.0\n\n\n35\n21.1\n196.0\n\n\n49\n21.2\n191.0\n\n\n61\n21.1\n195.0",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Pandas (1)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/04_pandas.html#selezione-casuale-di-un-sottoinsieme-di-righe",
    "href": "chapters/chapter_1/04_pandas.html#selezione-casuale-di-un-sottoinsieme-di-righe",
    "title": "5¬† Pandas (1)",
    "section": "5.8 Selezione casuale di un sottoinsieme di righe",
    "text": "5.8 Selezione casuale di un sottoinsieme di righe\nIl metodo sample() viene usato per ottenere un sottoinsieme casuale di righe del DataFrame. L‚Äôargomento replace=False indica l‚Äôestrazione senza rimessa (default); se specifichiamo replace=True otteniamo un‚Äôestrazione con rimessa. L‚Äôargomento n specifica il numero di righe che vogliamo ottenere. Ad esempio\n\ndf_sample = df.sample(4)\ndf_sample\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n204\nGentoo\nBiscoe\n45.1\n14.4\n210.0\n4400.0\nfemale\n2008\n\n\n69\nAdelie\nTorgersen\n41.8\n19.4\n198.0\n4450.0\nmale\n2008\n\n\n296\nChinstrap\nDream\n42.4\n17.3\n181.0\n3600.0\nfemale\n2007\n\n\n208\nGentoo\nBiscoe\n43.8\n13.9\n208.0\n4300.0\nfemale\n2008\n\n\n\n\n\n\n\n\n\ndf_sample = df[[\"bill_length_mm\", \"bill_depth_mm\"]].sample(4)\ndf_sample\n\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\n\n\n\n\n175\n46.3\n15.8\n\n\n133\n37.5\n18.5\n\n\n182\n47.3\n15.3\n\n\n251\n51.1\n16.5",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Pandas (1)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/04_pandas.html#selezione-di-colonne",
    "href": "chapters/chapter_1/04_pandas.html#selezione-di-colonne",
    "title": "5¬† Pandas (1)",
    "section": "5.9 Selezione di colonne",
    "text": "5.9 Selezione di colonne\nIl metodo drop() prende in input una lista con i nomi di colonne che vogliamo escludere dal DataFrame e pu√≤ essere usato per creare un nuovo DataFrame o per sovrascrivere quello di partenza. √à possibile usare le espressioni regolari (regex) per semplificare la ricerca dei nomi delle colonne.\nIn *regex* il simbolo `$` significa \"la stringa finisce con\"; il simbolo `^` significa \"la stringa inizia con\". L'espressione `regex` pu√≤ contenere (senza spazi) il simbolo `|` che significa \"oppure\". \nNel codice della cella seguente, alla funzione .columns.str.contains() viene passata l‚Äôespressione regolare mm$|year che significa: tutte le stringhe (in questo caso, nomi di colonne) che finiscono con mm oppure la stringa (nome di colonna) year.\n\nmask = df.columns.str.contains(\"mm$|year\", regex=True)\ncolumns_to_drop = df.columns[mask]\ncolumns_to_drop\n\nIndex(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'year'], dtype='object')\n\n\n\ndf_new = df.drop(columns=columns_to_drop)\ndf_new.head()\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n3750.0\nmale\n\n\n1\nAdelie\nTorgersen\n3800.0\nfemale\n\n\n2\nAdelie\nTorgersen\n3250.0\nfemale\n\n\n4\nAdelie\nTorgersen\n3450.0\nfemale\n\n\n5\nAdelie\nTorgersen\n3650.0\nmale\n\n\n\n\n\n\n\n\nIn un altro esempio, creaiamo l‚Äôelenco delle colonne che iniziano con la lettera ‚Äúb‚Äù, insieme a year e sex.\n\nmask = df.columns.str.contains(\"^b|year|sex\", regex=True)\ncolumns_to_drop = df.columns[mask]\ncolumns_to_drop\n\nIndex(['bill_length_mm', 'bill_depth_mm', 'body_mass_g', 'sex', 'year'], dtype='object')\n\n\nOppure l‚Äôelenco delle colonne che contengono il patten ‚Äúlength‚Äù.\n\nmask = df.columns.str.contains(\"length\")\ncolumns_to_drop = df.columns[mask]\ncolumns_to_drop\n\nIndex(['bill_length_mm', 'flipper_length_mm'], dtype='object')",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Pandas (1)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/04_pandas.html#creare-nuove-colonne",
    "href": "chapters/chapter_1/04_pandas.html#creare-nuove-colonne",
    "title": "5¬† Pandas (1)",
    "section": "5.10 Creare nuove colonne",
    "text": "5.10 Creare nuove colonne\nPer ciascuna riga, calcoliamo\n\nbill_length_mm - bill_depth_mm\nbill_length_mm / (body_mass_g / 1000)\n\nPer ottenere questo risultato possiamo usare una lambda function.\n\ndf = df.assign(\n    bill_difference=lambda x: x.bill_length_mm - x.bill_depth_mm,\n    bill_ratio=lambda x: x.bill_length_mm / (x.body_mass_g / 1000),\n)\ndf.head()\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\nbill_difference\nbill_ratio\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n20.4\n10.426667\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n22.1\n10.394737\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n22.3\n12.400000\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n17.4\n10.637681\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n18.7\n10.767123\n\n\n\n\n\n\n\n\nIn maniera pi√π semplice possiamo procedere nel modo seguente:\n\ndf[\"bill_ratio2\"] = df[\"bill_length_mm\"] / (df[\"body_mass_g\"] / 1000)\ndf.head()\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\nbill_difference\nbill_ratio\nbill_ratio2\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n20.4\n10.426667\n10.426667\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n22.1\n10.394737\n10.394737\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n22.3\n12.400000\n12.400000\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n17.4\n10.637681\n10.637681\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n18.7\n10.767123\n10.767123\n\n\n\n\n\n\n\n\nUn‚Äôutile funzionalit√† √® quella che consente di aggiungere una colonna ad un DataFrame (o di mofificare una colonna gi√† esistente) sulla base di una condizione True/False. Questo risultato pu√≤ essere raggiunto usando np.where(), con la seguente sintassi:\nnp.where(condition, value if condition is true, value if condition is false)\nSupponiamo di avere un DataFrame df con due colonne, A e B, e vogliamo creare una nuova colonna C che contenga il valore di A quando questo √® maggiore di 0, e il valore di B altrimenti. Possiamo utilizzare la funzione where() per ottenere ci√≤ come segue:\n\n# Creiamo un DataFrame di esempio\ndf = pd.DataFrame({\"A\": [-1, 2, 3, -4], \"B\": [5, 6, 0, 8]})\n\n# Creiamo una nuova colonna 'C' usando la funzione where()\ndf[\"C\"] = df[\"A\"].where(df[\"A\"] &gt; 0, df[\"B\"])\n\nprint(df)\n\n   A  B  C\n0 -1  5  5\n1  2  6  2\n2  3  0  3\n3 -4  8  8",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Pandas (1)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/04_pandas.html#formato-long-e-wide",
    "href": "chapters/chapter_1/04_pandas.html#formato-long-e-wide",
    "title": "5¬† Pandas (1)",
    "section": "5.11 Formato long e wide",
    "text": "5.11 Formato long e wide\nNella data analysis, i termini ‚Äúformato long‚Äù e ‚Äúformato wide‚Äù sono usati per descrivere la struttura di un set di dati. l formato wide (in inglese ‚Äúwide format‚Äù) rappresenta una struttura di dati in cui ogni riga rappresenta una singola osservazione e ogni variabile √® rappresentata da pi√π colonne. Un esempio √® il seguente, nel quale per ciascun partecipante, identificato da Name e ID abbiamo i punteggi di un ipotetico test per 6 anni consecutivi.\n\nscores = {\n    \"Name\": [\"Maria\", \"Carlo\", \"Giovanna\", \"Irene\"],\n    \"ID\": [1, 2, 3, 4],\n    \"2017\": [85, 87, 89, 91],\n    \"2018\": [96, 98, 100, 102],\n    \"2019\": [100, 102, 106, 106],\n    \"2020\": [89, 95, 98, 100],\n    \"2021\": [94, 96, 98, 100],\n    \"2022\": [100, 104, 104, 107],\n}\n\nwide_data = pd.DataFrame(scores)\nwide_data\n\n\n\n\n\n\n\n\n\nName\nID\n2017\n2018\n2019\n2020\n2021\n2022\n\n\n\n\n0\nMaria\n1\n85\n96\n100\n89\n94\n100\n\n\n1\nCarlo\n2\n87\n98\n102\n95\n96\n104\n\n\n2\nGiovanna\n3\n89\n100\n106\n98\n98\n104\n\n\n3\nIrene\n4\n91\n102\n106\n100\n100\n107\n\n\n\n\n\n\n\n\nIl formato long (in inglese ‚Äúlong format‚Äù) rappresenta una struttura di dati in cui ogni riga rappresenta una singola osservazione e ogni colonna rappresenta una singola variabile. Questo formato √® quello che viene richiesto per molte analisi statistiche. In Pandas √® possibile usare la funzione melt per trasformare i dati dal formato wide al formato long. Un esempio √® riportato qui sotto. Sono state mantenute le due colonne che identificano ciascun partecipante, ma i dati del test, che prima erano distribuiti su sei colonne, ora sono presenti in una singola colonna. Al DataFrame, inoltre, √® stata aggiunta una colonna che riporta l‚Äôanno.\n\nlong_data = wide_data.melt(id_vars=[\"Name\", \"ID\"], var_name=\"Year\", value_name=\"Score\")\nlong_data\n\n\n\n\n\n\n\n\n\nName\nID\nYear\nScore\n\n\n\n\n0\nMaria\n1\n2017\n85\n\n\n1\nCarlo\n2\n2017\n87\n\n\n2\nGiovanna\n3\n2017\n89\n\n\n3\nIrene\n4\n2017\n91\n\n\n4\nMaria\n1\n2018\n96\n\n\n5\nCarlo\n2\n2018\n98\n\n\n6\nGiovanna\n3\n2018\n100\n\n\n7\nIrene\n4\n2018\n102\n\n\n8\nMaria\n1\n2019\n100\n\n\n9\nCarlo\n2\n2019\n102\n\n\n10\nGiovanna\n3\n2019\n106\n\n\n11\nIrene\n4\n2019\n106\n\n\n12\nMaria\n1\n2020\n89\n\n\n13\nCarlo\n2\n2020\n95\n\n\n14\nGiovanna\n3\n2020\n98\n\n\n15\nIrene\n4\n2020\n100\n\n\n16\nMaria\n1\n2021\n94\n\n\n17\nCarlo\n2\n2021\n96\n\n\n18\nGiovanna\n3\n2021\n98\n\n\n19\nIrene\n4\n2021\n100\n\n\n20\nMaria\n1\n2022\n100\n\n\n21\nCarlo\n2\n2022\n104\n\n\n22\nGiovanna\n3\n2022\n104\n\n\n23\nIrene\n4\n2022\n107\n\n\n\n\n\n\n\n\nPer migliorare la leggibilit√† dei dati, √® possibile riordinare le righe del set di dati utilizzando la funzione sort_values. In questo modo, le informazioni saranno presentate in un ordine specifico, che pu√≤ rendere pi√π facile la lettura dei dati.\n\nlong_data.sort_values(by=[\"ID\", \"Year\"])\n\n\n\n\n\n\n\n\n\nName\nID\nYear\nScore\n\n\n\n\n0\nMaria\n1\n2017\n85\n\n\n4\nMaria\n1\n2018\n96\n\n\n8\nMaria\n1\n2019\n100\n\n\n12\nMaria\n1\n2020\n89\n\n\n16\nMaria\n1\n2021\n94\n\n\n20\nMaria\n1\n2022\n100\n\n\n1\nCarlo\n2\n2017\n87\n\n\n5\nCarlo\n2\n2018\n98\n\n\n9\nCarlo\n2\n2019\n102\n\n\n13\nCarlo\n2\n2020\n95\n\n\n17\nCarlo\n2\n2021\n96\n\n\n21\nCarlo\n2\n2022\n104\n\n\n2\nGiovanna\n3\n2017\n89\n\n\n6\nGiovanna\n3\n2018\n100\n\n\n10\nGiovanna\n3\n2019\n106\n\n\n14\nGiovanna\n3\n2020\n98\n\n\n18\nGiovanna\n3\n2021\n98\n\n\n22\nGiovanna\n3\n2022\n104\n\n\n3\nIrene\n4\n2017\n91\n\n\n7\nIrene\n4\n2018\n102\n\n\n11\nIrene\n4\n2019\n106\n\n\n15\nIrene\n4\n2020\n100\n\n\n19\nIrene\n4\n2021\n100\n\n\n23\nIrene\n4\n2022\n107",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Pandas (1)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/04_pandas.html#copia-di-un-data-frame",
    "href": "chapters/chapter_1/04_pandas.html#copia-di-un-data-frame",
    "title": "5¬† Pandas (1)",
    "section": "5.12 Copia di un data frame",
    "text": "5.12 Copia di un data frame\nQuando in Python definiamo un nuovo data frame basandoci su un data frame esistente con l‚Äôistruzione new_df = old_df, √® importante essere consapevoli del fatto che non stiamo creando un nuovo data frame indipendente. In realt√†, new_df diventa solamente un riferimento all‚Äôoggetto originale old_df nell‚Äôambiente corrente. Questo significa che qualsiasi modifica apportata a new_df si rifletter√† automaticamente anche in old_df. In pratica, abbiamo un unico oggetto data frame accessibile attraverso due nomi diversi.\nPer creare effettivamente una copia indipendente di old_df, in modo che le modifiche apportate a questa copia non influiscano sull‚Äôoriginale, dobbiamo utilizzare il metodo .copy(). Questo metodo crea un nuovo oggetto data frame che √® una copia del data frame originale. L‚Äôistruzione corretta per fare ci√≤ in Python √® la seguente:\nnew_df = old_df.copy()\nUtilizzando old_df.copy(), otteniamo due data frame completamente indipendenti. Modifiche apportate a new_df non avranno alcun impatto su old_df, permettendoci di lavorare con i dati in modo sicuro e senza rischi di sovrascrittura o alterazione involontaria dei dati originali. Questa pratica √® fondamentale per mantenere l‚Äôintegrit√† dei dati e per gestire correttamente le variabili all‚Äôinterno di un programma Python.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Pandas (1)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/04_pandas.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_1/04_pandas.html#informazioni-sullambiente-di-sviluppo",
    "title": "5¬† Pandas (1)",
    "section": "5.13 Informazioni sull‚ÄôAmbiente di Sviluppo",
    "text": "5.13 Informazioni sull‚ÄôAmbiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m\n\nLast updated: Mon Jan 29 2024\n\nPython implementation: CPython\nPython version       : 3.11.7\nIPython version      : 8.19.0\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.3.0\nMachine     : x86_64\nProcessor   : i386\nCPU cores   : 8\nArchitecture: 64bit\n\nnumpy : 1.26.2\npandas: 2.1.4\n\nWatermark: 2.4.3",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Pandas (1)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/05_pandas_aggregate.html",
    "href": "chapters/chapter_1/05_pandas_aggregate.html",
    "title": "6¬† Pandas (2)",
    "section": "",
    "text": "6.1 Preparazione del Notebook\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport arviz as az\nimport seaborn as sns\n%config InlineBackend.figure_format = 'retina'\nRANDOM_SEED = 42\nrng = np.random.default_rng(RANDOM_SEED)\naz.style.use(\"arviz-darkgrid\")\nsns.set_theme(palette=\"colorblind\")",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Pandas (2)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/05_pandas_aggregate.html#calcolo-delle-statistiche-descrittive",
    "href": "chapters/chapter_1/05_pandas_aggregate.html#calcolo-delle-statistiche-descrittive",
    "title": "6¬† Pandas (2)",
    "section": "6.2 Calcolo delle statistiche descrittive",
    "text": "6.2 Calcolo delle statistiche descrittive\nAgli oggetti Pandas possono essere applicati vari metodi matematici e statistici. La maggior parte di questi rientra nella categoria della riduzione di dati o delle statistiche descrittive. Rispetto ai metodi degli array NumPy, i metodi Pandas consentono la gestione dei dati mancanti. Alcuni dei metodi disponibili per gli oggetti Pandas sono elencati di seguito.\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\ncount\nNumber of non-NA values\n\n\ndescribe\nCompute set of summary statistics\n\n\nmin, max\nCompute minimum and maximum values\n\n\nargmin, argmax\nCompute index locations (integers) at which minimum or maximum value is obtained, respectively; not available on DataFrame objects\n\n\nidxmin, idxmax\nCompute index labels at which minimum or maximum value is obtained, respectively\n\n\nquantile\nCompute sample quantile ranging from 0 to 1 (default: 0.5)\n\n\nsum\nSum of values\n\n\nmean\nMean of values\n\n\nmedian\nArithmetic median (50% quantile) of values\n\n\nmad\nMean absolute deviation from mean value\n\n\nprod\nProduct of all values\n\n\nvar\nSample variance of values\n\n\nstd\nSample standard deviation of values\n\n\nskew\nSample skewness (third moment) of values\n\n\nkurt\nSample kurtosis (fourth moment) of values\n\n\ncumsum\nCumulative sum of values\n\n\ncummin, cummax\nCumulative minimum or maximum of values, respectively\n\n\ncumprod\nCumulative product of values\n\n\ndiff\nCompute first arithmetic difference (useful for time series)\n\n\npct_change\nCompute percent changes\n\n\n\nTali metodi possono essere applicati a tutto il DataFrame, oppure soltanto ad una o pi√π colonne.\nPer fare un esempio, esamineremo nuovamente i dati penguins.csv. Come in precedenza, dopo avere caricato i dati, rimuoviamo i dati mancanti.\n\ndf = pd.read_csv(\"../data/penguins.csv\")\ndf.dropna(inplace=True)\n\nUsiamo il metodo describe() su tutto il DataFrame:\n\ndf.describe(include=\"all\")\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\ncount\n333\n333\n333.000000\n333.000000\n333.000000\n333.000000\n333\n333.000000\n\n\nunique\n3\n3\nNaN\nNaN\nNaN\nNaN\n2\nNaN\n\n\ntop\nAdelie\nBiscoe\nNaN\nNaN\nNaN\nNaN\nmale\nNaN\n\n\nfreq\n146\n163\nNaN\nNaN\nNaN\nNaN\n168\nNaN\n\n\nmean\nNaN\nNaN\n43.992793\n17.164865\n200.966967\n4207.057057\nNaN\n2008.042042\n\n\nstd\nNaN\nNaN\n5.468668\n1.969235\n14.015765\n805.215802\nNaN\n0.812944\n\n\nmin\nNaN\nNaN\n32.100000\n13.100000\n172.000000\n2700.000000\nNaN\n2007.000000\n\n\n25%\nNaN\nNaN\n39.500000\n15.600000\n190.000000\n3550.000000\nNaN\n2007.000000\n\n\n50%\nNaN\nNaN\n44.500000\n17.300000\n197.000000\n4050.000000\nNaN\n2008.000000\n\n\n75%\nNaN\nNaN\n48.600000\n18.700000\n213.000000\n4775.000000\nNaN\n2009.000000\n\n\nmax\nNaN\nNaN\n59.600000\n21.500000\n231.000000\n6300.000000\nNaN\n2009.000000\n\n\n\n\n\n\n\n\nSe desideriamo solo le informazioni relative alle variabili qualitative, usiamo l‚Äôargomento include='object'.\n\ndf.describe(include=\"object\")\n\n\n\n\n\n\n\n\n\nspecies\nisland\nsex\n\n\n\n\ncount\n333\n333\n333\n\n\nunique\n3\n3\n2\n\n\ntop\nAdelie\nBiscoe\nmale\n\n\nfreq\n146\n163\n168\n\n\n\n\n\n\n\n\nI valori NaN indicano dati mancanti. Ad esempio, la colonna species contiene stringhe, quindi non esiste alcun valore per mean; allo stesso modo, bill_length_mm √® una variabile numerica, quindi non vengono calcolate le statistiche riassuntive per le variabili categoriali (unique, top, freq).\nEsaminimiamo le colonne singolarmente. Ad esempio, troviamo la media della colonna bill_depth_mm.\n\ndf[\"bill_depth_mm\"].mean()\n\n17.164864864864867\n\n\nPer la deviazione standard usiamo il metodo std(). Si noti l‚Äôargomento opzionale ddof:\n\ndf[\"bill_length_mm\"].std(ddof=1)\n\n5.46866834264756\n\n\nLa cella seguente fornisce l‚Äôindice della riga nella quale la colonna bill_length_mm assume il suo valore massimo:\n\ndf[\"bill_length_mm\"].idxmax()\n\n185\n\n\nLa colonna species nel DataFrame df √® una variabile a livello nominale. Elenchiamo le modalit√† di tale variabile.\n\ndf[\"species\"].unique()\n\narray(['Adelie', 'Gentoo', 'Chinstrap'], dtype=object)\n\n\nIl metodo value_counts ritorna la distribuzione di frequenza assoluta:\n\ndf[\"species\"].value_counts()\n\nspecies\nAdelie       146\nGentoo       119\nChinstrap     68\nName: count, dtype: int64\n\n\nPer le frequenze relative si imposta l‚Äôargomento normalize=True:\n\nprint(df[\"species\"].value_counts(normalize=True))\n\nspecies\nAdelie       0.438438\nGentoo       0.357357\nChinstrap    0.204204\nName: proportion, dtype: float64\n\n\nConsideriamo la lunghezza del becco dei pinguini suddivisa per ciascuna specie. Con l‚Äôistruzione seguente, possiamo generare gli istogrammi corrispondenti che rappresentano la distribuzione della lunghezza del becco in ciascun gruppo.\n\n_ = df.hist(\n    column=\"bill_length_mm\",\n    by=[\"species\"],\n    bins=20,\n    figsize=(12, 4),\n    layout=(1, 3),\n    rwidth=0.9,\n)\n\n\n\n\n\n\n\n\n\ndf\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n339\nChinstrap\nDream\n55.8\n19.8\n207.0\n4000.0\nmale\n2009\n\n\n340\nChinstrap\nDream\n43.5\n18.1\n202.0\n3400.0\nfemale\n2009\n\n\n341\nChinstrap\nDream\n49.6\n18.2\n193.0\n3775.0\nmale\n2009\n\n\n342\nChinstrap\nDream\n50.8\n19.0\n210.0\n4100.0\nmale\n2009\n\n\n343\nChinstrap\nDream\n50.2\n18.7\n198.0\n3775.0\nfemale\n2009\n\n\n\n\n333 rows √ó 8 columns\n\n\n\n\n\n6.2.1 Aggregazione dei dati\nIl riepilogo di pi√π valori in un unico indice va sotto il nome di ‚Äúaggregazione‚Äù dei dati. Il metodo aggregate() pu√≤ essere applicato ai DataFrame e restituisce un nuovo DataFrame pi√π breve contenente solo i valori aggregati. Il primo argomento di aggregate() specifica quale funzione o quali funzioni devono essere utilizzate per aggregare i dati. Molte comuni funzioni di aggregazione sono disponibili nel modulo statistics. Ad esempio:\n\nmedian(): la mediana;\nmean(): la media;\nstdev(): la deviazione standard;\n\nSe vogliamo applicare pi√π funzioni di aggregazione, allora possiamo raccogliere prima le funzioni in una lista e poi passare la lista ad aggregate().\n\n# List of summary statistics functions\nsummary_stats = [np.min, np.median, np.mean, np.std, np.max]\n\n# Calculate summary statistics for numeric columns using aggregate\nresult = df[\n    [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n].aggregate(summary_stats)\n\nprint(result)\n\n        bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\nmin          32.100000      13.100000         172.000000  2700.000000\nmedian       44.500000      17.300000         197.000000  4050.000000\nmean         43.992793      17.164865         200.966967  4207.057057\nstd           5.468668       1.969235          14.015765   805.215802\nmax          59.600000      21.500000         231.000000  6300.000000\n\n\nSi noti che Pandas ha applicato le funzioni di riepilogo a ogni colonna, ma, per alcune colonne, le statistiche riassuntive non si possono calcolare, ovvero tutte le colonne che contengono stringhe anzich√© numeri. Di conseguenza, vediamo che alcuni dei risultati per tali colonne sono contrassegnati con ‚ÄúNaN‚Äù. Questa √® un‚Äôabbreviazione di ‚ÄúNot a Number‚Äù, talvolta utilizzata nell‚Äôanalisi dei dati per rappresentare valori mancanti o non definiti.\nMolto spesso vogliamo calcolare le statistiche descrittive separatamente per ciascun gruppo di osservazioni ‚Äì per esempio, nel caso presente, potremmo volere distinguere le statistiche descrittive in base alla specie dei pinguini. Questo risultato si ottiene con il metodo .groupby().\nIl nome ‚Äúgroup by‚Äù deriva da un comando nel linguaggio del database SQL, ma forse √® pi√π semplice pensarlo nei termini coniati da Hadley Wickham: split, apply, combine. Un esempio canonico di questa operazione di split-apply-combine, in cui ‚Äúapply‚Äù √® un‚Äôaggregazione di sommatoria, √® illustrato nella figura seguente:\nvjovpxb ../images/split_apply_combine.png :height: 400px :name: split_apply_combine\nLa figura rende chiaro ci√≤ che si ottiene con groupby:\n\nla fase ‚Äúsplit‚Äù prevede la suddivisione e il raggruppamento di un DataFrame in base al valore della chiave specificata;\nla fase ‚Äúapply‚Äù implica il calcolo di alcune funzioni, solitamente un‚Äôaggregazione, una trasformazione o un filtro, all‚Äôinterno dei singoli gruppi;\nla fase ‚Äúcombine‚Äù unisce i risultati di queste operazioni in una matrice di output.\n\nPer esempio, ragruppiamo le osservazioni body_mass_g in funzione delle modalit√† della variabile species.\n\ngrouped = df[\"body_mass_g\"].groupby(df[\"species\"])\n\n&lt;pandas.core.groupby.generic.SeriesGroupBy object at 0x16276b490&gt;\n\n\nCalcoliamo ora la media della variabile body_mass_g separatamente per ciascun gruppo di osservazioni.\n\ngrouped.mean()\n\nspecies\nAdelie       3706.164384\nChinstrap    3733.088235\nGentoo       5092.436975\nName: body_mass_g, dtype: float64\n\n\n√à possibile applicare criteri di classificazione multipli. Per fare un altro esempio, contiamo il numero di pinguini presenti sulle tre isole, distinguendoli per specie e genere.\n\ndf.groupby([\"island\", \"species\", \"sex\"]).size()\n\nisland     species    sex   \nBiscoe     Adelie     female    22\n                      male      22\n           Gentoo     female    58\n                      male      61\nDream      Adelie     female    27\n                      male      28\n           Chinstrap  female    34\n                      male      34\nTorgersen  Adelie     female    24\n                      male      23\ndtype: int64\n\n\nCon il metodo aggregate() possiamo applicare diverse funzioni di aggregazione alle osservazioni ragruppate. Ad esempio\n\nsummary_stats = [np.mean, np.std]\n# Group by \"species\" and calculate summary statistics for numeric columns\nresult = df.groupby(\"species\").agg(\n    {col: summary_stats for col in df.columns if pd.api.types.is_numeric_dtype(df[col])}\n)\n\nprint(result)\n\n          bill_length_mm           bill_depth_mm           flipper_length_mm  \\\n                    mean       std          mean       std              mean   \nspecies                                                                        \nAdelie         38.823973  2.662597     18.347260  1.219338        190.102740   \nChinstrap      48.833824  3.339256     18.420588  1.135395        195.823529   \nGentoo         47.568067  3.106116     14.996639  0.985998        217.235294   \n\n                     body_mass_g                     year            \n                std         mean         std         mean       std  \nspecies                                                              \nAdelie     6.521825  3706.164384  458.620135  2008.054795  0.811816  \nChinstrap  7.131894  3733.088235  384.335081  2007.970588  0.863360  \nGentoo     6.585431  5092.436975  501.476154  2008.067227  0.789025  \n\n\nNella cella seguente troviamo la media di body_mass_g e flipper_length_mm separatamente per ciascuna isola e ciascuna specie:\n\ndf.groupby([\"island\", \"species\"])[[\"body_mass_g\", \"flipper_length_mm\"]].mean()\n\n\n\n\n\n\n\n\n\n\nbody_mass_g\nflipper_length_mm\n\n\nisland\nspecies\n\n\n\n\n\n\nBiscoe\nAdelie\n3709.659091\n188.795455\n\n\nGentoo\n5092.436975\n217.235294\n\n\nDream\nAdelie\n3701.363636\n189.927273\n\n\nChinstrap\n3733.088235\n195.823529\n\n\nTorgersen\nAdelie\n3708.510638\n191.531915\n\n\n\n\n\n\n\n\nFacciamo la stessa cosa per la deviazione standard.\n\ndf.groupby([\"island\", \"species\"])[[\"body_mass_g\", \"flipper_length_mm\"]].std(ddof=1)\n\n\n\n\n\n\n\n\n\n\nbody_mass_g\nflipper_length_mm\n\n\nisland\nspecies\n\n\n\n\n\n\nBiscoe\nAdelie\n487.733722\n6.729247\n\n\nGentoo\n501.476154\n6.585431\n\n\nDream\nAdelie\n448.774519\n6.480325\n\n\nChinstrap\n384.335081\n7.131894\n\n\nTorgersen\nAdelie\n451.846351\n6.220062\n\n\n\n\n\n\n\n\nPrestiamo attenzione alla seguente sintassi:\n\nsummary_stats = (\n    df.loc[:, [\"island\", \"species\", \"body_mass_g\", \"flipper_length_mm\"]]\n    .groupby([\"island\", \"species\"])\n    .aggregate([\"mean\", \"std\", \"count\"])\n)\nsummary_stats\n\n\n\n\n\n\n\n\n\n\nbody_mass_g\nflipper_length_mm\n\n\n\n\nmean\nstd\ncount\nmean\nstd\ncount\n\n\nisland\nspecies\n\n\n\n\n\n\n\n\n\n\nBiscoe\nAdelie\n3709.659091\n487.733722\n44\n188.795455\n6.729247\n44\n\n\nGentoo\n5092.436975\n501.476154\n119\n217.235294\n6.585431\n119\n\n\nDream\nAdelie\n3701.363636\n448.774519\n55\n189.927273\n6.480325\n55\n\n\nChinstrap\n3733.088235\n384.335081\n68\n195.823529\n7.131894\n68\n\n\nTorgersen\nAdelie\n3708.510638\n451.846351\n47\n191.531915\n6.220062\n47\n\n\n\n\n\n\n\n\nNell‚Äôistruzione precedente selezioniamo tutte le righe (:) di tre colonne di interesse: df.loc[:, [\"island\", \"species\", \"body_mass_g\", \"flipper_length_mm\"]]. L‚Äôistruzione .groupby([\"island\", \"species\"]) ragruppa le osservazioni (righe) secondo le modalit√† delle variabili island e species. Infine .aggregate([\"mean\", \"std\", \"count\"]) applica i metodi statistici specificati a ciascun gruppo di osservazioni. Con questa sintassi la sequenza delle operazioni da eseguire diventa molto intuitiva.\n√à possibile approfondire questo argomento consultanto il capitolo 10 del testo Python for Data Analysis di {cite:t}mckinney2022python.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Pandas (2)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/05_pandas_aggregate.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_1/05_pandas_aggregate.html#informazioni-sullambiente-di-sviluppo",
    "title": "6¬† Pandas (2)",
    "section": "6.3 Informazioni sull‚ÄôAmbiente di Sviluppo",
    "text": "6.3 Informazioni sull‚ÄôAmbiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m\n\nLast updated: Mon Jan 29 2024\n\nPython implementation: CPython\nPython version       : 3.11.7\nIPython version      : 8.19.0\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.3.0\nMachine     : x86_64\nProcessor   : i386\nCPU cores   : 8\nArchitecture: 64bit\n\nscipy     : 1.11.4\nseaborn   : 0.13.0\nnumpy     : 1.26.2\npandas    : 2.1.4\narviz     : 0.17.0\nmatplotlib: 3.8.2\n\nWatermark: 2.4.3",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Pandas (2)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/06_pandas_functions.html",
    "href": "chapters/chapter_1/06_pandas_functions.html",
    "title": "7¬† Pandas (3)",
    "section": "",
    "text": "7.1 pd.read_csv, pd.read_excel\nLa prima funzione da menzionare √® read_csv o read_excel. Le funzioni vengono utilizzate per leggere un file CSV o un file Excel in formato DataFrame di Pandas. Qui stiamo utilizzando la funzione read_csv per leggere il dataset penguins. In precedenza abbiamo anche visto come la funzione dropna viene utilizzata per rimuovere tutte le righe del DataFrame che includono dati mancanti.\ndf = pd.read_csv(\"../data/penguins.csv\")\ndf.dropna(inplace=True)\ndf.tail()\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n339\nChinstrap\nDream\n55.8\n19.8\n207.0\n4000.0\nmale\n2009\n\n\n340\nChinstrap\nDream\n43.5\n18.1\n202.0\n3400.0\nfemale\n2009\n\n\n341\nChinstrap\nDream\n49.6\n18.2\n193.0\n3775.0\nmale\n2009\n\n\n342\nChinstrap\nDream\n50.8\n19.0\n210.0\n4100.0\nmale\n2009\n\n\n343\nChinstrap\nDream\n50.2\n18.7\n198.0\n3775.0\nfemale\n2009",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Pandas (3)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/06_pandas_functions.html#columns",
    "href": "chapters/chapter_1/06_pandas_functions.html#columns",
    "title": "7¬† Pandas (3)",
    "section": "7.2 .columns",
    "text": "7.2 .columns\nQuando si dispone di un grande dataset, pu√≤ essere difficile visualizzare tutte le colonne. Utilizzando la funzione columns, √® possibile stampare tutte le colonne del dataset.\n\ndf.columns\n\nIndex(['species', 'island', 'bill_length_mm', 'bill_depth_mm',\n       'flipper_length_mm', 'body_mass_g', 'sex', 'year'],\n      dtype='object')",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Pandas (3)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/06_pandas_functions.html#drop",
    "href": "chapters/chapter_1/06_pandas_functions.html#drop",
    "title": "7¬† Pandas (3)",
    "section": "7.3 .drop()",
    "text": "7.3 .drop()\n√à possibile eliminare alcune colonne non necessarie utilizzando drop.\n\ndf = df.drop(columns=[\"year\"])\ndf.columns\n\nIndex(['species', 'island', 'bill_length_mm', 'bill_depth_mm',\n       'flipper_length_mm', 'body_mass_g', 'sex'],\n      dtype='object')",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Pandas (3)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/06_pandas_functions.html#len",
    "href": "chapters/chapter_1/06_pandas_functions.html#len",
    "title": "7¬† Pandas (3)",
    "section": "7.4 len()",
    "text": "7.4 len()\nFornisce il numero di righe di un DataFrame.\n\nlen(df)\n\n333",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Pandas (3)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/06_pandas_functions.html#query",
    "href": "chapters/chapter_1/06_pandas_functions.html#query",
    "title": "7¬† Pandas (3)",
    "section": "7.5 .query()",
    "text": "7.5 .query()\n√à possibile filtrare un DataFrame utilizzando un‚Äôespressione booleana.\n\ndf1 = df.query(\"species == 'Chinstrap' & island == 'Dream'\")\nlen(df1)\n\n68",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Pandas (3)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/06_pandas_functions.html#iloc",
    "href": "chapters/chapter_1/06_pandas_functions.html#iloc",
    "title": "7¬† Pandas (3)",
    "section": "7.6 .iloc[]",
    "text": "7.6 .iloc[]\nQuesta funzione accetta come parametri gli indici delle righe e delle colonne, fornendo una selezione del DataFrame in base a questi. In questo caso, stiamo selezionando le prime 3 righe di dati e le colonne con indice 2, 3 e 5.\n\ndf2 = df.iloc[:3, [2, 3, 5]]\ndf2\n\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nbody_mass_g\n\n\n\n\n0\n39.1\n18.7\n3750.0\n\n\n1\n39.5\n17.4\n3800.0\n\n\n2\n40.3\n18.0\n3250.0",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Pandas (3)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/06_pandas_functions.html#loc",
    "href": "chapters/chapter_1/06_pandas_functions.html#loc",
    "title": "7¬† Pandas (3)",
    "section": "7.7 .loc[]",
    "text": "7.7 .loc[]\nQuesta funzione compie un‚Äôoperazione molto simile a quella della funzione .iloc. Tuttavia, in questo caso, abbiamo la possibilit√† di specificare gli indici delle righe che desideriamo, insieme ai nomi delle colonne che vogliamo includere nella nostra selezione.\n\ndf3 = df.loc[[2, 4, 6], [\"island\", \"flipper_length_mm\", \"sex\"]]\ndf3\n\n\n\n\n\n\n\n\n\nisland\nflipper_length_mm\nsex\n\n\n\n\n2\nTorgersen\n195.0\nfemale\n\n\n4\nTorgersen\n193.0\nfemale\n\n\n6\nTorgersen\n181.0\nfemale",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Pandas (3)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/06_pandas_functions.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_1/06_pandas_functions.html#informazioni-sullambiente-di-sviluppo",
    "title": "7¬† Pandas (3)",
    "section": "7.8 Informazioni sull‚ÄôAmbiente di Sviluppo",
    "text": "7.8 Informazioni sull‚ÄôAmbiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m\n\nLast updated: Mon Jan 29 2024\n\nPython implementation: CPython\nPython version       : 3.11.7\nIPython version      : 8.19.0\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.3.0\nMachine     : x86_64\nProcessor   : i386\nCPU cores   : 8\nArchitecture: 64bit\n\nseaborn   : 0.13.0\nnumpy     : 1.26.2\narviz     : 0.17.0\npandas    : 2.1.4\nmatplotlib: 3.8.2\nscipy     : 1.11.4\n\nWatermark: 2.4.3",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Pandas (3)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/07_matplotlib.html",
    "href": "chapters/chapter_1/07_matplotlib.html",
    "title": "8¬† Matplotlib",
    "section": "",
    "text": "8.1 Preparazione del Notebook\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport arviz as az\n# Questa istruzione consente di visualizzare i grafici generati dai comandi di \n# plot direttamente all'interno del notebook.\n%config InlineBackend.figure_format = 'retina'\n\n# Questo valore viene usato come seed per il random number generator.\nRANDOM_SEED = 42\n\n# In questa riga, stai utilizzando il generatore di numeri casuali di NumPy per \n# creare una nuova istanza denominata rng. La funzione np.random.default_rng() viene \n# utilizzata per inizializzare un generatore di numeri casuali con un seme specifico, \n# che in questo caso √® RANDOM_SEED.\nrng = np.random.default_rng(RANDOM_SEED)\n\n# Queste due righe di codice sono spesso utilizzate per personalizzare l'aspetto dei \n# grafici in Python utilizzando le librerie ArviZ e Seaborn.\n# Questa riga di codice utilizza il metodo use() della libreria ArviZ per impostare uno \n# stile specifico per i tuoi grafici. In particolare, sta impostando lo stile chiamato \n# \"arviz-darkgrid\". Gli stili in ArviZ determinano come saranno visualizzati i grafici, inclusi colori, linee di griglia e altri dettagli estetici.\naz.style.use(\"arviz-darkgrid\")\n\n# Questa riga di codice utilizza la libreria Seaborn per impostare il tema dei grafici. \n# In questo caso, il tema viene impostato utilizzando set_theme() con il parametro palette \n# impostato su \"colorblind\". Questo significa che i colori utilizzati nei grafici saranno \n# scelti in modo da essere adatti alle persone con deficit visivi dei colori, rendendo i \n# grafici pi√π accessibili.\nsns.set_theme(palette=\"colorblind\")",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Matplotlib</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/07_matplotlib.html#linterfaccia-pyplot-per-creare-grafici",
    "href": "chapters/chapter_1/07_matplotlib.html#linterfaccia-pyplot-per-creare-grafici",
    "title": "8¬† Matplotlib",
    "section": "8.2 L‚ÄôInterfaccia pyplot per Creare Grafici",
    "text": "8.2 L‚ÄôInterfaccia pyplot per Creare Grafici\nMatplotlib √® una libreria in Python famosa per la creazione di grafici, e la sua interfaccia pyplot √® particolarmente apprezzata per la sua semplicit√†. Vediamo in dettaglio come funzionano le sue funzioni principali. Per comprendere meglio come funzionano le sue funzioni principali, possiamo fare un parallelo con il disegno su un supporto fisico.\n\nPrepariamo la Tela: Iniziamo con plt.figure(), che √® analogo a ottenere una tela bianca pronta per essere dipinta. √à il punto di partenza, una superficie vuota su cui creeremo il nostro grafico.\nDefiniamo le Aree di Disegno: Successivamente, utilizzando plt.subplot() o plt.axes(), creiamo delle aree specifiche o ‚Äúassi‚Äù sulla nostra tela. Questi assi corrispondono a diverse sezioni in cui posizioneremo vari elementi del nostro grafico, come se suddividessimo la tela fisica in diverse parti.\nAggiungiamo Elementi al Grafico: Una volta definiti gli assi, entriamo nel processo di creazione. Usandando funzioni come plt.plot() per tracciare linee o plt.scatter() per punti, aggiungiamo elementi grafici alla nostra area di disegno. √à simile a disegnare direttamente sulla tela fisica.\nRendiamo il Grafico Comprensibile: Per garantire che il grafico sia chiaro e informativo, aggiungiamo etichette e titoli con plt.xlabel(), plt.ylabel() e plt.title().",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Matplotlib</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/07_matplotlib.html#principi-fondamentali-di-pyplot",
    "href": "chapters/chapter_1/07_matplotlib.html#principi-fondamentali-di-pyplot",
    "title": "8¬† Matplotlib",
    "section": "8.3 Principi Fondamentali di Pyplot",
    "text": "8.3 Principi Fondamentali di Pyplot\nEsploriamo le funzionalit√† essenziali di pyplot di Matplotlib:\n\nCreazione di Grafici Lineari: Utilizzando plt.plot(x, y), √® possibile generare grafici lineari. Questa funzione necessita delle coordinate x e y per disegnare il grafico, semplificando cos√¨ la rappresentazione visiva dei dati.\nDenominazione degli Assi: √à fondamentale assegnare un‚Äôetichetta appropriata agli assi per migliorare la comprensione del grafico. Si possono denominare gli assi tramite plt.xlabel('Nome') per l‚Äôasse X e plt.ylabel('Nome') per l‚Äôasse Y, facilitando l‚Äôinterpretazione dei dati visualizzati.\nInserimento del Titolo: Un titolo descrittivo clarifica lo scopo o il contesto del grafico. Aggiungere un titolo √® semplice con plt.title('Titolo'), che aiuta a comunicare il messaggio principale del grafico in modo efficace.\nInserimento di Legende: Per grafici che includono pi√π serie di dati o elementi distinti, l‚Äôaggiunta di una legenda √® cruciale per la distinzione tra questi. La funzione plt.legend() permette di integrare una legenda, migliorando la leggibilit√† del grafico.\nEsposizione del Grafico: Una volta completata la composizione del grafico, il passo finale √® la sua visualizzazione. Attraverso plt.show(), √® possibile mostrare il grafico elaborato, offrendo una visione complessiva dei dati analizzati.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Matplotlib</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/07_matplotlib.html#esempio-1-grafico-lineare-semplice",
    "href": "chapters/chapter_1/07_matplotlib.html#esempio-1-grafico-lineare-semplice",
    "title": "8¬† Matplotlib",
    "section": "8.4 Esempio 1: Grafico lineare semplice",
    "text": "8.4 Esempio 1: Grafico lineare semplice\n\nx = [1, 2, 3, 4]\ny = [10, 20, 30, 40]\n\nplt.plot(x, y)\nplt.xlabel(\"Asse X\")\nplt.ylabel(\"Asse Y\")\nplt.title(\"Grafico Lineare Semplice\");",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Matplotlib</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/07_matplotlib.html#esempio-2-grafico-con-legenda-e-stile",
    "href": "chapters/chapter_1/07_matplotlib.html#esempio-2-grafico-con-legenda-e-stile",
    "title": "8¬† Matplotlib",
    "section": "8.5 Esempio 2: Grafico con legenda e stile",
    "text": "8.5 Esempio 2: Grafico con legenda e stile\n\nx = [1, 2, 3, 4]\ny1 = [10, 20, 30, 40]\ny2 = [5, 15, 25, 35]\n\nplt.plot(x, y1, label=\"Linea 1\", color=\"C0\", linestyle=\"--\")\nplt.plot(x, y2, label=\"Linea 2\", color=\"C3\", linestyle=\"-\")\nplt.xlabel(\"Asse X\")\nplt.ylabel(\"Asse Y\")\nplt.title(\"Grafico con Legenda\")\nplt.legend();",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Matplotlib</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/07_matplotlib.html#esempio-3-istogramma",
    "href": "chapters/chapter_1/07_matplotlib.html#esempio-3-istogramma",
    "title": "8¬† Matplotlib",
    "section": "8.6 Esempio 3: Istogramma",
    "text": "8.6 Esempio 3: Istogramma\n\ndata = rng.normal(100, 15, 1000)\n\nplt.hist(data, bins=20)\nplt.xlabel(\"Valori\")\nplt.ylabel(\"Frequenza\")\nplt.title(\"Istogramma\");",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Matplotlib</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/07_matplotlib.html#esempio-4-pannelli-multipli",
    "href": "chapters/chapter_1/07_matplotlib.html#esempio-4-pannelli-multipli",
    "title": "8¬† Matplotlib",
    "section": "8.7 Esempio 4: pannelli multipli",
    "text": "8.7 Esempio 4: pannelli multipli\nFacciamo un altro esempio usando i dati penguins.csv.\n\ndf = pd.read_csv(\"../data/penguins.csv\")\ndf.dropna(inplace=True)\n\n\nplt.figure(figsize=(9, 8))\n\nplt.subplot(2, 2, 1)\nplt.hist(df[\"bill_depth_mm\"], 10, density=True, color=\"C3\")\nplt.title(\"Bill depth (mm)\");\n\nplt.subplot(2, 2, 2)\nsns.kdeplot(df[\"bill_length_mm\"], fill=True)\nplt.title(\"KDE of Bill length (mm)\");\n\nplt.subplot(2, 2, 3)\nplt.scatter(x=df[\"bill_length_mm\"], y=df[\"bill_depth_mm\"], alpha=0.4)\nplt.title(\"Bill depth as a function of bill length\");\n\nplt.subplot(2, 2, 4)\nplt.boxplot(df[\"bill_length_mm\"])\nplt.title(\"Boxplot of Bill Length (mm)\")\n\nplt.tight_layout()\n\n/var/folders/cl/wwjrsxdd5tz7y9jr82nd5hrw0000gn/T/ipykernel_55046/1324325854.py:19: UserWarning: The figure layout has changed to tight\n  plt.tight_layout()\n\n\n\n\n\n\n\n\n\nGli indici in plt.subplot() sono utilizzati per specificare come dividere una figura in diverse aree di tracciamento, chiamate ‚Äúsubplots‚Äù. La funzione plt.subplot(nrows, ncols, index) prende tre argomenti principali:\n\nnrows: Numero di righe in cui la figura sar√† suddivisa.\nncols: Numero di colonne in cui la figura sar√† suddivisa.\nindex: Indice del subplot su cui operare, partendo dall‚Äôangolo in alto a sinistra e proseguendo da sinistra a destra e dall‚Äôalto in basso.\n\nNel codice precedente, plt.subplot(2, 2, 1) indica che la figura sar√† divisa in una griglia 2x2 (2 righe e 2 colonne) e che la funzione plt.hist() agir√† sul primo subplot, che si trover√† nell‚Äôangolo in alto a sinistra.\nGli altri indici (2, 3, 4) selezionano rispettivamente il secondo subplot (in alto a destra), il terzo subplot (in basso a sinistra) e il quarto subplot (in basso a destra) della griglia 2x2.\nEcco come i subplot sono organizzati sulla figura:\n+---------------------+----------------------+\n|  plt.subplot(2,2,1) |  plt.subplot(2,2,2)  |\n+---------------------+----------------------+\n|  plt.subplot(2,2,3) |  plt.subplot(2,2,4)  |\n+---------------------+----------------------+\nOgni volta che si chiama plt.subplot() con un nuovo indice, il ‚Äúcurrent axes‚Äù cambia per puntare al subplot specificato. Quindi, le funzioni di tracciamento come plt.hist(), sns.kdeplot(), plt.scatter() e plt.boxplot() saranno applicate al subplot attualmente selezionato.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Matplotlib</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/07_matplotlib.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_1/07_matplotlib.html#informazioni-sullambiente-di-sviluppo",
    "title": "8¬† Matplotlib",
    "section": "8.8 Informazioni sull‚ÄôAmbiente di Sviluppo",
    "text": "8.8 Informazioni sull‚ÄôAmbiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m\n\nLast updated: Sat Feb 03 2024\n\nPython implementation: CPython\nPython version       : 3.11.7\nIPython version      : 8.19.0\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.3.0\nMachine     : x86_64\nProcessor   : i386\nCPU cores   : 8\nArchitecture: 64bit\n\nnumpy     : 1.26.2\narviz     : 0.17.0\nmatplotlib: 3.8.2\npandas    : 2.1.4\nseaborn   : 0.13.0\n\nWatermark: 2.4.3",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Matplotlib</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/08_seaborn.html",
    "href": "chapters/chapter_1/08_seaborn.html",
    "title": "9¬† Seaborn",
    "section": "",
    "text": "9.1 Elevare la Visualizzazione dei Dati con Seaborn\nNel capitolo precedente, abbiamo esaminato Matplotlib, una libreria estremamente versatile per la visualizzazione dei dati in Python. Ora esamineremo le funzionalit√† di Seabonrn. Seaborn, che si basa su Matplotlib, arricchisce l‚Äôesperienza di visualizzazione dei dati offrendo una gamma pi√π ampia e specializzata di opzioni grafiche, particolarmente utili nel campo della data science.\nIl vero punto di forza di Seaborn √® la sua capacit√† di migliorare non solo l‚Äôaspetto estetico dei grafici ma anche di facilitare la creazione di visualizzazioni pi√π complesse. Questo rende il processo pi√π diretto e intuitivo. La libreria √® dotata di un‚Äôampia variet√† di strumenti, dalle mappe di calore ai grafici a violino, permettendo agli utenti di esplorare e rappresentare i dati in modi innovativi e informativi.\nPer chi vuole approfondire ulteriormente, i tutorial presenti sul sito ufficiale di Seaborn sono una risorsa preziosa e facilmente accessibile.",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Seaborn</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/08_seaborn.html#preparazione-del-notebook",
    "href": "chapters/chapter_1/08_seaborn.html#preparazione-del-notebook",
    "title": "9¬† Seaborn",
    "section": "9.2 Preparazione del Notebook",
    "text": "9.2 Preparazione del Notebook\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport arviz as az\n\n\n# set seed to make the results fully reproducible\nseed: int = sum(map(ord, \"seaborn\"))\nrng: np.random.Generator = np.random.default_rng(seed=seed)\n\naz.style.use(\"arviz-darkgrid\")\nplt.rcParams[\"figure.dpi\"] = 100\nplt.rcParams[\"figure.facecolor\"] = \"white\"\n\n%config InlineBackend.figure_format = \"retina\"",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Seaborn</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/08_seaborn.html#visualizzare-la-distribuzione-dei-dati",
    "href": "chapters/chapter_1/08_seaborn.html#visualizzare-la-distribuzione-dei-dati",
    "title": "9¬† Seaborn",
    "section": "9.3 Visualizzare la distribuzione dei dati",
    "text": "9.3 Visualizzare la distribuzione dei dati\nVediamo alcuni esempi pratici per scoprire come Seaborn possa trasformare il modo in cui visualizziamo i dati.\nConsideriamo nuovamente i dati Palmer penguin.\n\ndf = pd.read_csv(\"../data/penguins.csv\")\n\nUna delle forme di visualizzazione pi√π comuni e informative nel campo dell‚Äôanalisi dei dati √® l‚Äôistogramma, e la sua variante pi√π sofisticata, l‚Äôistogramma lisciato. Vediamo dunque come generare istogrammi che, per il DataFrame df, sono stratificati sia in base alla specie che al genere dei pinguini.\n\nsns.displot(\n    df, x=\"flipper_length_mm\", col=\"species\", row=\"sex\",\n    binwidth=3, height=3, facet_kws=dict(margin_titles=True),\n);\n\nGeneriamo la stessa figura usando questa volta gli istogrammi lisciati.\n\nsns.displot(\n    df, x=\"flipper_length_mm\", col=\"species\", row=\"sex\",\n    height=3, kind=\"kde\", facet_kws=dict(margin_titles=True),\n);\n\n/opt/anaconda3/envs/pymc_env/lib/python3.12/site-packages/seaborn/axisgrid.py:123: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Seaborn</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/08_seaborn.html#visualizzazione-di-dati-categoriali",
    "href": "chapters/chapter_1/08_seaborn.html#visualizzazione-di-dati-categoriali",
    "title": "9¬† Seaborn",
    "section": "9.4 Visualizzazione di dati categoriali",
    "text": "9.4 Visualizzazione di dati categoriali\nConsideriamo ora il caso in cui si vuole rappresentare la relazione tra una variabile numerica e una o pi√π variabili categoriali.\nConsideriamo, ad esempio, la massa corporea in relazione alla specie, differenziando le osservazioni per genere. Creiamo il grafico utilizzando i boxplot.\n\nsns.catplot(df, x=\"species\", y=\"body_mass_g\", hue=\"sex\", kind=\"box\")\n\n/opt/anaconda3/envs/pymc_env/lib/python3.12/site-packages/seaborn/axisgrid.py:123: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\n\n\nDai diagrammi risulta evidente che i pinguini maschi hanno un peso maggiore rispetto alle femmine in tutte le specie, e che i pinguini Gentoo hanno un peso superiore rispetto ad Adelie e Chinstrap.\nCome alternativa, possiamo utilizzare il violinplot per la rappresentazione grafica dei dati.\n\nsns.catplot(df, x=\"species\", y=\"body_mass_g\", hue=\"sex\", kind=\"violin\")\n\n/opt/anaconda3/envs/pymc_env/lib/python3.12/site-packages/seaborn/axisgrid.py:123: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Seaborn</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/08_seaborn.html#relazioni-tra-variabili",
    "href": "chapters/chapter_1/08_seaborn.html#relazioni-tra-variabili",
    "title": "9¬† Seaborn",
    "section": "9.5 Relazioni tra variabili",
    "text": "9.5 Relazioni tra variabili\nCalcoliamo la correlazione tra le variabili.\n\nvars = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\ncorr_matrix = df[vars].corr().round(2)\ncorr_matrix\n\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\nbill_length_mm\n1.00\n-0.24\n0.66\n0.60\n\n\nbill_depth_mm\n-0.24\n1.00\n-0.58\n-0.47\n\n\nflipper_length_mm\n0.66\n-0.58\n1.00\n0.87\n\n\nbody_mass_g\n0.60\n-0.47\n0.87\n1.00\n\n\n\n\n\n\n\n\nQueste informazioni possono essere comunicate in forma pi√π diretta se usiamo una rappresentazione grafica.\n\nsns.heatmap(corr_matrix, annot=True, linecolor=\"white\", linewidths=5);\n\n\n\n\n\n\n\n\nLa lunghezza della pinna e la massa corporea mostrano un forte legame, con una correlazione di 0.87. Ci√≤ indica che i pinguini con pinne pi√π lunghe tendono a pesare di pi√π.\nDi seguito √® riportato un esempio di diagramma a dispersione che illustra questa relazione.\n\nsns.scatterplot(df, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\");\n\n\n\n\n\n\n\n\nEvidentemente, le osservazioni delle tre specie formano cluster distinti. Per ciascuna specie, la lunghezza e la larghezza del becco presentano un intervallo specifico.\nSpesso √® vantaggioso creare grafici separati in base a diverse dimensioni dei dati; nell‚Äôesempio seguente, suddividiamo i dati in base all‚Äôisola di appartenenza.\n\ng = sns.relplot(\n    data=df,\n    x=\"bill_length_mm\",\n    y=\"bill_depth_mm\",\n    hue=\"species\",\n    row=\"sex\",\n    col=\"island\",\n    height=3,\n    facet_kws=dict(margin_titles=True),\n)\ng.set_axis_labels(\n    \"Bill length (mm)\",\n    \"Bill depth (mm)\",\n);\n\n/opt/anaconda3/envs/pymc_env/lib/python3.12/site-packages/seaborn/axisgrid.py:123: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Seaborn</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_1/08_seaborn.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_1/08_seaborn.html#informazioni-sullambiente-di-sviluppo",
    "title": "9¬† Seaborn",
    "section": "9.6 Informazioni sull‚ÄôAmbiente di Sviluppo",
    "text": "9.6 Informazioni sull‚ÄôAmbiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m\n\nLast updated: Sat Jun 08 2024\n\nPython implementation: CPython\nPython version       : 3.12.3\nIPython version      : 8.25.0\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.4.0\nMachine     : arm64\nProcessor   : arm\nCPU cores   : 8\nArchitecture: 64bit\n\nmatplotlib: 3.8.4\narviz     : 0.18.0\npandas    : 2.2.2\nnumpy     : 1.26.4\nseaborn   : 0.13.2\n\nWatermark: 2.4.3",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Seaborn</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/00_scientific_method.html",
    "href": "chapters/chapter_2/00_scientific_method.html",
    "title": "10¬† La scienza dei dati e il metodo scientifico",
    "section": "",
    "text": "10.1 Strumenti per Comprendere il Reale\nI modelli scientifici sono strumenti essenziali per la comprensione del mondo che ci circonda. Essi non sono delle descrizioni ‚Äúvere‚Äù della realt√†, ma piuttosto rappresentazioni che ci permettono di esplorare, testare e approfondire la nostra conoscenza dei fenomeni naturali.\nQueste domande devono essere sempre in primo piano nella nostra mente quando lavoriamo con i modelli scientifici (Johnson, Ott, and Dogucu 2022).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>La scienza dei dati e il metodo scientifico</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/00_scientific_method.html#strumenti-per-comprendere-il-reale",
    "href": "chapters/chapter_2/00_scientific_method.html#strumenti-per-comprendere-il-reale",
    "title": "10¬† La scienza dei dati e il metodo scientifico",
    "section": "",
    "text": "Natura esplorativa dei modelli: Utilizziamo i modelli per sondare, valutare e testare i limiti della nostra comprensione. Li costruiamo, ammiriamo la loro bellezza, ma anche cerchiamo di comprenderne i limiti e, in ultima analisi, di superarli. √à questo processo di costruzione, test e revisione che ci permette di migliorare la nostra comprensione del mondo, non necessariamente il risultato finale, anche se talvolta possono coincidere (Vasishth and Gelman 2021).\nDualit√† tra mondo del modello e mondo reale: Nella costruzione dei modelli, √® fondamentale tenere presente sia il ‚Äúmondo del modello‚Äù - cio√® il contesto specifico in cui il modello opera - sia il mondo reale pi√π ampio di cui vogliamo parlare (McElreath 2020). I dataset su cui addestriamo i modelli spesso non sono rappresentativi delle popolazioni reali in certi aspetti. I modelli addestrati su tali dati non sono privi di valore, ma non sono nemmeno infallibili.\nQuestioni critiche da considerare:\n\nIn che misura il modello ci insegna qualcosa sui dati che abbiamo?\nIn che misura i dati che abbiamo riflettono il mondo su cui vorremmo trarre conclusioni?",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>La scienza dei dati e il metodo scientifico</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/00_scientific_method.html#il-ruolo-della-statistica-e-della-data-science",
    "href": "chapters/chapter_2/00_scientific_method.html#il-ruolo-della-statistica-e-della-data-science",
    "title": "10¬† La scienza dei dati e il metodo scientifico",
    "section": "10.2 Il Ruolo della Statistica e della Data Science",
    "text": "10.2 Il Ruolo della Statistica e della Data Science\nLa statistica e la data science giocano un ruolo fondamentale nel processo scientifico moderno. Tuttavia, √® importante comprendere alcune sfumature:\n\nValidit√† statistica e assunzioni: La validit√† statistica si basa su determinate assunzioni. Mentre ci√≤ che viene insegnato nella teoria statistica √® corretto, le nostre circostanze specifiche potrebbero non soddisfare i criteri di partenza. √à essenziale essere consapevoli di queste limitazioni quando applichiamo metodi statistici.\nProcesso scientifico reale vs.¬†ideale: Un ulteriore aspetto da considerare riguarda la discrepanza tra l‚Äôinsegnamento teorico della statistica e la sua applicazione pratica nella ricerca. L‚Äôapproccio didattico tradizionale presenta spesso un processo lineare e ordinato: formulazione dell‚Äôipotesi, raccolta dati, analisi statistica e conclusione. La realt√† della ricerca, tuttavia, si rivela notevolmente pi√π complessa e dinamica. Gli scienziati reagiscono a incentivi, fanno tentativi, formulano ipotesi e seguono la loro intuizione, colmando le lacune quando necessario. Questo approccio euristico, sebbene possa sembrare caotico, √® spesso il terreno fertile per scoperte innovative. Questa metodologia pi√π flessibile, tuttavia, pone sfide significative all‚Äôapplicazione rigorosa dei concetti statistici tradizionali. Anche se √® fondamentale una comprensione approfondita di questi concetti, √® altrettanto importante sviluppare la capacit√† di riconoscere quando √® necessario andare oltre le metodologie convenzionali.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>La scienza dei dati e il metodo scientifico</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/00_scientific_method.html#implicazioni-epistemologiche",
    "href": "chapters/chapter_2/00_scientific_method.html#implicazioni-epistemologiche",
    "title": "10¬† La scienza dei dati e il metodo scientifico",
    "section": "10.3 Implicazioni Epistemologiche",
    "text": "10.3 Implicazioni Epistemologiche\n\nModelli come strumenti di apprendimento: I modelli non sono rappresentazioni definitive della realt√†, ma strumenti che ci permettono di apprendere le caratteristiche del reale. Il loro valore risiede nel processo di costruzione, test e revisione, piuttosto che nel risultato finale.\nLimiti della rappresentazione: √à fondamentale riconoscere che i modelli, per quanto sofisticati, sono sempre approssimazioni della realt√†. La loro utilit√† non sta nella perfezione della rappresentazione, ma nella capacit√† di generare intuizioni e previsioni utili.\nIterazione e miglioramento continuo: Il processo scientifico, attraverso l‚Äôuso di modelli, √® caratterizzato da un ciclo continuo di ipotesi, test e revisione. Questo approccio iterativo permette un raffinamento progressivo della nostra comprensione.\nConsapevolezza dei limiti: Una comprensione critica dei modelli richiede una costante consapevolezza dei loro limiti, delle assunzioni sottostanti e del contesto in cui sono stati sviluppati.\n\nIn conclusione, i modelli scientifici sono strumenti potenti per esplorare e comprendere la realt√†, ma la loro vera forza risiede nel processo di indagine che stimolano, piuttosto che nelle singole rappresentazioni che producono. La data science e la statistica forniscono il quadro metodologico per questo processo, ma √® fondamentale mantenere una prospettiva critica e consapevole dei limiti e delle assunzioni su cui si basano i nostri modelli e le nostre analisi.\n\n\n\n\nJohnson, Alicia A., Miles Ott, and Mine Dogucu. 2022. Bayes Rules! An Introduction to Bayesian Modeling with R. CRC Press.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd Edition. Boca Raton, Florida: CRC Press.\n\n\nVasishth, Shravan, and Andrew Gelman. 2021. ‚ÄúHow to Embrace Variation and Accept Uncertainty in Linguistic and Psycholinguistic Data Analysis.‚Äù Linguistics 59 (5): 1311‚Äì42.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>La scienza dei dati e il metodo scientifico</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html",
    "href": "chapters/chapter_2/01_key_notions.html",
    "title": "11¬† Concetti chiave",
    "section": "",
    "text": "11.1 Introduzione\nL‚Äôanalisi dei dati si colloca all‚Äôintersezione tra statistica, teoria della probabilit√† e informatica. Questa disciplina multidisciplinare richiede una solida comprensione dei concetti fondamentali provenienti da ciascuna di queste tre aree.\nLa statistica fornisce gli strumenti e le tecniche per raccogliere, analizzare e interpretare i dati. Attraverso metodi descrittivi e inferenziali, la statistica permette di trarre conclusioni dai dati e di prendere decisioni informate.\nLa teoria della probabilit√† costituisce la base matematica della statistica. Essa consente di modellare l‚Äôincertezza e di comprendere i fenomeni aleatori, fornendo i fondamenti per sviluppare metodi statistici rigorosi.\nL‚Äôinformatica gioca un ruolo cruciale nell‚Äôanalisi dei dati, offrendo gli strumenti necessari per la gestione, l‚Äôelaborazione e la visualizzazione dei dati su larga scala. Conoscere i principi dell‚Äôinformatica √® essenziale per sfruttare appieno le tecnologie moderne, come il machine learning e l‚Äôintelligenza artificiale, nell‚Äôanalisi dei dati. Ad esempio, l‚Äôuso di linguaggi di programmazione come Python e R, insieme a librerie specializzate, permette di eseguire analisi complesse e di visualizzare i dati in modo efficace.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#introduzione",
    "href": "chapters/chapter_2/01_key_notions.html#introduzione",
    "title": "11¬† Concetti chiave",
    "section": "",
    "text": "Statistica\n\n\n\nIl termine ‚Äústatistica‚Äù pu√≤ assumere diversi significati, a seconda del contesto in cui viene utilizzato.\n\nNel primo senso, la statistica √® una scienza e una disciplina che si occupa dello studio e dell‚Äôapplicazione di metodi e tecniche per la raccolta, l‚Äôorganizzazione, l‚Äôanalisi, l‚Äôinterpretazione e la presentazione di dati.\nNel secondo senso, il termine ‚Äústatistica‚Äù si riferisce a una singola misura o un valore numerico che √® stato calcolato a partire da un campione di dati. Questo tipo di statistica rappresenta una caratteristica specifica del campione. Esempi comuni di statistiche in questo senso includono la media campionaria, la deviazione standard campionaria o il coefficiente di correlazione campionario.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#popolazioni-e-campioni",
    "href": "chapters/chapter_2/01_key_notions.html#popolazioni-e-campioni",
    "title": "11¬† Concetti chiave",
    "section": "11.2 Popolazioni e Campioni",
    "text": "11.2 Popolazioni e Campioni\nPer iniziare l‚Äôanalisi dei dati, √® fondamentale individuare le unit√† che contengono le informazioni rilevanti per il fenomeno di interesse. Questo insieme di unit√† costituisce la popolazione o universo, che rappresenta l‚Äôinsieme completo di entit√† capaci di fornire informazioni per l‚Äôindagine statistica in questione. Possiamo rappresentare la popolazione come $ N $ o $ $ nel caso di popolazioni finite o infinite, rispettivamente. Le singole unit√† dell‚Äôinsieme sono chiamate unit√† statistiche.\nNella ricerca psicologica, sia nelle ricerche sperimentali che in quelle osservazionali, l‚Äôobiettivo principale √® studiare i fenomeni psicologici all‚Äôinterno di una specifica popolazione. Pertanto, √® essenziale definire con chiarezza la popolazione di interesse, ovvero l‚Äôinsieme di individui ai quali verranno applicati i risultati della ricerca. Tale popolazione pu√≤ essere reale, come ad esempio tutte le persone sopravvissute per un anno dopo il bombardamento atomico di Hiroshima, o ipotetica, come ad esempio tutte le persone depresse che potrebbero beneficiare di un intervento psicologico. Il ricercatore deve sempre essere in grado di identificare se un individuo specifico appartiene o meno alla popolazione in questione.\n\n11.2.1 Sotto-popolazioni e Campioni\nUna sotto-popolazione √® un sottoinsieme di individui che possiedono propriet√† specifiche ben definite. Ad esempio, potremmo essere interessati alla sotto-popolazione degli uomini di et√† inferiore ai 30 anni o alla sotto-popolazione dei pazienti depressi che hanno ricevuto uno specifico intervento psicologico. Molte questioni scientifiche cercano di descrivere le differenze tra sotto-popolazioni, come ad esempio il confronto tra un gruppo di pazienti sottoposti a psicoterapia e un gruppo di controllo per valutare l‚Äôefficacia di un trattamento.\nIl campione √® un sottoinsieme della popolazione composto da un insieme di elementi, ognuno dei quali rappresenta un‚Äôunit√† statistica (abbreviata con u.s.) portatrice delle informazioni che verranno rilevate tramite un‚Äôoperazione di misurazione. Il campione viene utilizzato per ottenere informazioni sulla popolazione di riferimento.\n\n\n11.2.2 Metodi di Campionamento\nIl campionamento pu√≤ avvenire in diversi modi. Il campionamento casuale consente al ricercatore di trarre conclusioni sulla popolazione e di quantificare l‚Äôincertezza dei risultati. Un esempio di campione casuale √® quello utilizzato in un sondaggio. Tuttavia, esistono anche altre forme di campionamento, come il campione di convenienza, in cui si seleziona una coorte di studenti da un unico istituto, o il campionamento stratificato, dove la popolazione viene divisa in gruppi o strati, e vengono selezionati campioni proporzionali da ciascuno strato.\nIl ricercatore, indipendentemente dal metodo di acquisizione dei dati, deve sempre considerare la questione della rappresentativit√† statistica del campione, ovvero se il campione scelto √® in grado di riflettere in modo accurato e privo di distorsioni le caratteristiche di interesse della popolazione. Selezionare le unit√† statistiche in modo casuale rappresenta il metodo pi√π semplice per garantire la rappresentativit√† del campione. Tuttavia, in molti casi, soprattutto in psicologia, i ricercatori possono non avere a disposizione le risorse necessarie, inclusi i fondi, per utilizzare la tecnica del campionamento casuale nelle loro ricerche. In tali situazioni, possono ricorrere ad altri metodi di campionamento, come il campionamento di convenienza, a seconda delle esigenze e delle risorse disponibili.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#variabili-e-costanti",
    "href": "chapters/chapter_2/01_key_notions.html#variabili-e-costanti",
    "title": "11¬† Concetti chiave",
    "section": "11.3 Variabili e Costanti",
    "text": "11.3 Variabili e Costanti\nNell‚Äôambito dell‚Äôanalisi statistica, le variabili rappresentano concetti centrali, denotando le caratteristiche o gli attributi che possono assumere una variet√† di valori, numerici o categoriali. Essi incarnano gli elementi quantificabili o osservabili ai quali le unit√† statistiche danno riscontro. Ad esempio, ponendo la domanda ‚ÄúQual √® l‚Äôet√† di questo partecipante?‚Äù e ottenendo come risposta ‚Äú19 anni‚Äù, si identifica ‚Äúet√†‚Äù come la variabile, e ‚Äú19‚Äù come il valore corrispondente.\n\n11.3.1 Tipi di Variabili\nIn pratica, nel contesto della ricerca empirica, una variabile rappresenta un insieme di osservazioni relative alla stessa misurazione, come ad esempio il punteggio di ‚Äúnevrosismo‚Äù ottenuto da interviste condotte su 744 bambini. Descrivere una variabile significa essere in grado di prendere tali osservazioni e comunicare chiaramente il loro significato, senza obbligare chi legge a dover esaminare ciascuno dei 744 punteggi di nevrosismo separatamente. Questo compito non √® affatto semplice.\nPossiamo distinguere varie classi di variabili:\n\nVariabili continue: Possono assumere valori in un intervallo potenzialmente infinito.\nVariabili di conteggio: Rappresentano la frequenza con cui si verificano eventi o la quantit√† di oggetti in una categoria specifica.\nVariabili ordinali: Caratterizzate da un ordine intrinseco tra i loro valori, ma non esiste una scala standard per quantificare la differenza tra essi.\nVariabili categoriche: Utilizzate per assegnare categorie o classi a un‚Äôosservazione, indicando a quale categoria appartiene.\nVariabili binarie: Una sottocategoria di variabili categoriche che possono assumere solo due valori distinti.\n\nLe modalit√† descrivono le diverse forme che una variabile statistica pu√≤ assumere. L‚Äôinsieme delle modalit√† di una variabile √® rappresentato dall‚Äôinsieme $ $, che include tutte le possibili manifestazioni della variabile. Le modalit√† presenti nel campione vengono etichettate come dati.\nIn statistica, la nozione di ‚Äúvariabile‚Äù si distingue da quella di ‚Äúcostante‚Äù, che rimane immutabile attraverso tutte le unit√† statistiche.\n\nEsempio 1: In uno studio relativo all‚Äôintelligenza degli adulti italiani, la variabile di interesse √® il punteggio nel test WAIS-IV, con modalit√† quali 112, 92, 121 ecc. Questa variabile √® classificata come quantitativa discreta.\nEsempio 2: Nell‚Äôanalisi del compito Stroop, focalizzata su bambini di et√† 6-8 anni, la variabile in esame √® l‚Äôinverso dei tempi di reazione, misurati in secondi, con modalit√† come 1.93, 2.35, 1.32 ecc. Questa variabile √® classificata come quantitativa continua.\nEsempio 3: In uno studio del disturbo di personalit√† condotto tra i detenuti nelle carceri italiane, la variabile scrutinata √® l‚Äôassessment del disturbo di personalit√†, valutato attraverso interviste cliniche strutturate. Le modalit√† sono i Cluster A, Cluster B, Cluster C, secondo la classificazione del DSM-V, e questa variabile √® classificata come qualitativa.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#la-distribuzione-delle-variabili",
    "href": "chapters/chapter_2/01_key_notions.html#la-distribuzione-delle-variabili",
    "title": "11¬† Concetti chiave",
    "section": "11.4 La Distribuzione delle Variabili",
    "text": "11.4 La Distribuzione delle Variabili\nUna volta compreso il tipo di variabile con cui lavoriamo, √® fondamentale esaminare la distribuzione di tale variabile. La distribuzione di una variabile rappresenta la frequenza con cui si verificano i diversi valori. Nel caso delle variabili discrete, la distribuzione √® semplicemente un elenco delle modalit√† (valori distinti) e delle relative frequenze. Ad esempio, se consideriamo la variabile ‚Äúgenere‚Äù all‚Äôinterno di un campione di studenti, possiamo affermare che l‚Äô82% sono donne e il 18% sono uomini.\nLe distribuzioni delle variabili sono descritte in termini di probabilit√†. Le frequenze relative possono essere interpretate come ‚Äúprobabilit√† empiriche‚Äù. Nell‚Äôesempio precedente, la probabilit√† di ‚Äúessere una donna‚Äù nel campione specifico √® 0.82, mentre la probabilit√† di ‚Äúessere un uomo‚Äù √® 0.18. La probabilit√† che una variabile $ X $ (come il genere) assuma un valore specifico $ x $ (ad esempio, ‚Äúdonna‚Äù) viene indicata come $ P(X = x) $, o pi√π semplicemente $ P(x) $.\nLe distribuzioni delle variabili continue sono pi√π complesse da descrivere. Per le variabili continue, non descriviamo la probabilit√† che la variabile assuma un valore specifico, ma la probabilit√† che essa cada in un intervallo vicino a quel valore specifico. In futuro, esploreremo come rappresentare graficamente la distribuzione di una variabile continua utilizzando un istogramma o un grafico della densit√† di probabilit√† chiamato ‚ÄúKernel Density Plot‚Äù.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#la-matrice-dei-dati",
    "href": "chapters/chapter_2/01_key_notions.html#la-matrice-dei-dati",
    "title": "11¬† Concetti chiave",
    "section": "11.5 La Matrice dei Dati",
    "text": "11.5 La Matrice dei Dati\nNell‚Äôambito dell‚Äôanalisi statistica, la matrice dei dati svolge un ruolo fondamentale nell‚Äôorganizzazione delle informazioni relative alle variabili. Si tratta di una tabella strutturata con righe e colonne, dove ogni riga individua un‚Äôunit√† statistica specifica e ogni colonna rappresenta una diversa variabile statistica in esame.\nVa enfatizzato che, all‚Äôinterno della matrice dei dati, le unit√† statistiche non seguono generalmente un ordine progressivo o gerarchico. L‚Äôindice attribuito a ciascuna unit√† statistica indica semplicemente la posizione che essa occupa all‚Äôinterno della tabella, senza implicare un valore intrinseco o una relazione ordinale. Tale strutturazione metodica offre un mezzo efficace per raccogliere, visualizzare e analizzare le informazioni ottenute durante lo studio statistico, permettendo una gestione chiara e sistematica dei dati raccolti.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#capire-i-dati",
    "href": "chapters/chapter_2/01_key_notions.html#capire-i-dati",
    "title": "11¬† Concetti chiave",
    "section": "11.6 Capire i dati",
    "text": "11.6 Capire i dati\nA scopo illustrativo, prendiamo in considerazione il dataset contenuto nel file STAR.csv. Questi dati sono parte integrante del progetto STAR (Student-Teacher Achievement Ratio), un esperimento pedagogico sviluppato negli Stati Uniti nel periodo tra il 1985 e il 1990. La finalit√† centrale di questo studio era indagare l‚Äôimpatto delle dimensioni delle classi sulle performance accademiche degli studenti. In questo contesto, gli studenti venivano distribuiti casualmente in classi di dimensioni ridotte (13-17 studenti) o pi√π ampie (22-25 studenti).\nDopo aver preparato l‚Äôambiente di lavoro caricando i pacchetti necessari, √® possibile procedere all‚Äôimportazione dei dati in Python. Si pu√≤ fare ci√≤ utilizzando il seguente codice, prestando attenzione al fatto che l‚Äôargomento di read_csv() deve specificare il percorso relativo del file rispetto alla directory in cui √® situato lo script .ipynb:\n\ndf_star = pd.read_csv(\"../data/STAR.csv\")\n\nQuesto codice importa i dati dal file STAR.csv e li memorizza in un DataFrame di pandas. Questo passo √® fondamentale per consentire un‚Äôanalisi e una manipolazione efficiente delle informazioni relative all‚Äôesperimento STAR. In Python, il DataFrame rappresenta la struttura dati principale per la gestione e l‚Äôelaborazione dei dati. Il DataFrame attualizza il concetto di ‚Äúmatrice di dati‚Äù che abbiamo introdotto in precedenza.\n\ndf_star.shape\n\n(1274, 4)\n\n\nDato che il DataFrame √® troppo grande (1274 righe e 4 colonne), stampiamo sullo schermo le prime 5 righe.\n\ndf_star.head()\n\n\n\n\n\n\n\n\n\nclasstype\nreading\nmath\ngraduated\n\n\n\n\n0\nsmall\n578\n610\n1\n\n\n1\nregular\n612\n612\n1\n\n\n2\nregular\n583\n606\n1\n\n\n3\nsmall\n661\n648\n1\n\n\n4\nsmall\n614\n636\n1\n\n\n\n\n\n\n\n\nNella terminologia statistica, l‚Äôosservazione √® l‚Äôinformazione raccolta da un individuo o un‚Äôentit√† specifica che partecipa allo studio. Considerando il dataset STAR, l‚Äôunit√† di osservazione √® costituita dagli studenti. Pertanto, nel DataFrame denominato df_star, ogni riga simboleggia uno studente distinto coinvolto nell‚Äôindagine.\nLe variabili, d‚Äôaltro canto, sono espressioni delle diverse caratteristiche degli individui o delle entit√† analizzate. Nel contesto del progetto STAR, questo concetto si traduce in:\n\nOgni colonna di df_star rappresenta una variabile che incarna una particolare propriet√† condivisa da tutti gli studenti partecipanti.\nLe variabili sono identificate attraverso etichette collegate alle colonne, come classtype (il tipo di classe assegnata, con modalit√† small e regular), reading (il punteggio nel test di lettura standardizzato), math (il punteggio nel test di matematica standardizzato) e graduated (indicazione se lo studente ha conseguito o meno il diploma di scuola superiore, con ‚Äú1‚Äù o ‚Äú0‚Äù rispettivamente).\n\nPer rappresentare un‚Äôosservazione singola della variabile generica \\(X\\), si utilizza la notazione \\(X_i\\), dove \\(i\\) rappresenta l‚Äôindice dell‚Äôosservazione. Questo indice significa che abbiamo un valore differente di \\(X\\) per ogni valore distinto di \\(i\\). Ad esempio, nel caso di 1274 osservazioni, \\(i\\) pu√≤ variare da 1 a 1274. Pertanto, per simboleggiare la seconda osservazione (quella con \\(i=2\\)), useremo la notazione \\(X_2\\). √à fondamentale tener presente che, mentre in Python gli indici iniziano da 0, nella notazione matematica tradizionale, come quella rappresentata da \\(X_i\\), l‚Äôindice ha inizio da 1. Questa differenza tra le convenzioni di indicizzazione pu√≤ essere un aspetto cruciale da considerare durante l‚Äôanalisi dei dati.\n\ndf_star[\"reading\"][1]\n\n612\n\n\nUna delle prime cose da fare, quando esaminiamo un dataset, √® capire che tipo di variabili sono incluse.\n\ndf_star.dtypes\n\nclasstype    object\nreading       int64\nmath          int64\ngraduated     int64\ndtype: object\n\n\nNel caso specifico, notiamo che la variabile classtype √® di tipo object, quindi √® una variabile qualitativa, mentre le altre variabili sono numeriche, rappresentate come numeri interi (int64). Se elenchiamo le modalit√† presenti in classtype utilizzando il metodo unique(), scopriamo che corrispondono a ‚Äúsmall‚Äù e ‚Äúregular‚Äù.\n\ndf_star[\"classtype\"].unique()\n\narray(['small', 'regular'], dtype=object)\n\n\nCon l‚Äôistruzione seguente verifichiamo che la variabile graduated sia una variabile binaria.\n\ndf_star[\"graduated\"].unique()\n\narray([1, 0])",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#variabili-indipendenti-e-variabili-dipendenti",
    "href": "chapters/chapter_2/01_key_notions.html#variabili-indipendenti-e-variabili-dipendenti",
    "title": "11¬† Concetti chiave",
    "section": "11.7 Variabili Indipendenti e Variabili Dipendenti",
    "text": "11.7 Variabili Indipendenti e Variabili Dipendenti\nNell‚Äôambito della ricerca statistica, √® fondamentale distinguere tra variabili indipendenti e dipendenti. Questa distinzione si basa sulla domanda di ricerca e sulla comprensione del fenomeno che si sta studiando.\n\n11.7.1 Variabili Indipendenti\nLe variabili indipendenti, a volte chiamate variabili predittive, rappresentano i fattori che si ipotizza influenzino l‚Äôesito di interesse. Esse sono spesso manipolate o controllate dal ricercatore.\n\n\n11.7.2 Variabili Dipendenti\nLe variabili dipendenti, d‚Äôaltra parte, rappresentano l‚Äôesito o il risultato che si sta cercando di spiegare o prevedere. Esse sono ci√≤ che il ricercatore sta cercando di capire e sono influenzate dalle variabili indipendenti.\nIn molti studi, √® abbastanza chiaro quali sono le variabili indipendenti e dipendenti. Tuttavia, in alcuni casi, la relazione pu√≤ essere pi√π sfumata o complessa. Ad esempio, nell‚Äôanalizzare la correlazione tra l‚Äôesercizio fisico e l‚Äôinsonnia, pu√≤ non essere immediatamente evidente quale sia la causa e quale l‚Äôeffetto. In tali circostanze, una comprensione delle relazioni causali inerenti il fenomeno considerato √® necessaria per una corretta distinzione tra variabili indipendenti (cause) e dipendenti (effetti).\nEsempio 5 Prendiamo in considerazione un esperimento in cui uno psicologo ha chiamato 120 studenti universitari per un test di memoria. Prima di iniziare, met√† dei partecipanti √® stata informata che il compito era particolarmente difficile, mentre all‚Äôaltra met√† non √® stata fornita alcuna indicazione sulla difficolt√†. Successivamente, √® stato misurato il punteggio ottenuto nella prova di memoria da ciascun partecipante.\nIn questo esperimento: - La variabile indipendente √® l‚Äôinformazione sulla difficolt√† del compito, che √® stata manipolata dallo sperimentatore attraverso l‚Äôassegnazione casuale dei soggetti alle due diverse condizioni (‚Äúinformazione fornita‚Äù e ‚Äúinformazione non fornita‚Äù). - La variabile dipendente √® il punteggio ottenuto nella prova di memoria, ovvero l‚Äôoutcome che lo sperimentatore sta cercando di capire e che potrebbe essere influenzato dalla variabile indipendente.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#effetto",
    "href": "chapters/chapter_2/01_key_notions.html#effetto",
    "title": "11¬† Concetti chiave",
    "section": "11.8 Effetto",
    "text": "11.8 Effetto\nIl concetto di ‚Äúeffetto‚Äù √® fondamentale nell‚Äôanalisi dei dati e nella statistica, poich√© rappresenta una misura del cambiamento o dell‚Äôinfluenza tra le variabili. Ad esempio, consideriamo uno studio sulla memoria che indaga l‚Äôeffetto delle mnemotecniche sul miglioramento della memoria. In questo studio, un gruppo riceve un intervento relativo al rilassamento, mentre un altro partecipa a un workshop di riorganizzazione mnemonica. Alla fine, i partecipanti vengono sottoposti a test di memoria e l‚Äôeffetto delle mnemotecniche √® determinato dalla differenza tra i punteggi medi dei due gruppi. Se il gruppo che ha seguito il workshop mostra un punteggio medio superiore, si pu√≤ affermare che le mnemotecniche hanno un effetto positivo sulla memoria.\nL‚Äôeffetto viene misurato attraverso diverse statistiche, come la differenza di medie, il rapporto di probabilit√†, ecc. Quando si analizzano i dati con metodi statistici, l‚Äôobiettivo √® determinare se l‚Äôeffetto osservato √® credibile, ovvero se ci sono evidenze che l‚Äôeffetto sia generalizzabile alla popolazione nel suo insieme. Questo aiuta a valutare l‚Äôimportanza dell‚Äôeffetto e a trarre conclusioni sulle relazioni tra le variabili nello studio.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#variabili-casuali",
    "href": "chapters/chapter_2/01_key_notions.html#variabili-casuali",
    "title": "11¬† Concetti chiave",
    "section": "11.9 Variabili Casuali",
    "text": "11.9 Variabili Casuali\nIl concetto di variabile nella statistica trova un corrispondente nella teoria delle probabilit√†, dove √® chiamato variabile casuale. Quando ci occupiamo di studi come quelli sugli interventi psicologici, le variabili casuali vengono utilizzate per rappresentare e misurare i risultati di tali interventi. In sostanza, una variabile casuale descrive una caratteristica specifica degli individui all‚Äôinterno di una popolazione, e i suoi valori possono variare tra gli individui. Teoricamente, una variabile casuale pu√≤ assumere una gamma di valori possibili, ma in pratica si osserva un valore specifico per ogni individuo.\nUtilizziamo notazioni particolari per riferirci alle variabili casuali e ai loro valori specifici. Ad esempio, le lettere maiuscole come $ X $ e $ Y $ denotano le variabili casuali, mentre le lettere minuscole come $ x $ e $ y $ si riferiscono ai valori che queste variabili possono assumere in circostanze specifiche.\n\n11.9.1 Differenza tra Variabili Casuali e Variabili Statistiche\nLa chiave per comprendere la differenza tra questi due concetti risiede nell‚Äôincertezza epistemica che il ricercatore affronta. Immaginiamo un esperimento casuale, come il lancio di un dado. Supponiamo che la variabile di interesse $ X $ rappresenti l‚Äôesito del lancio. Prima del lancio, $ X $ √® una variabile casuale, poich√© conosciamo i possibili valori che pu√≤ assumere (da 1 a 6), ma non sappiamo quale valore specifico si manifester√†. In questo stadio, $ X $ rappresenta una quantit√† incognita, suscettibile di variazione casuale.\nDopo il lancio, supponiamo che l‚Äôesito sia 5. A questo punto, la variabile $ X $ diventa una variabile statistica, poich√© rappresenta un dato osservato e concreto all‚Äôinterno del campione di osservazioni. L‚Äôincertezza √® risolta, e il valore di $ X $ √® ora noto e fisso.\nIn sintesi, una variabile casuale rappresenta una quantit√† che pu√≤ assumere diversi valori con una certa probabilit√†, mentre una variabile statistica √® una realizzazione specifica di quella quantit√† incognita. La transizione da una variabile casuale a una variabile statistica avviene attraverso l‚Äôosservazione e la misurazione, che trasformano un‚Äôincertezza teorica in una certezza empirica.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#stima-e-inferenza",
    "href": "chapters/chapter_2/01_key_notions.html#stima-e-inferenza",
    "title": "11¬† Concetti chiave",
    "section": "11.10 Stima e Inferenza",
    "text": "11.10 Stima e Inferenza\n\n11.10.1 Stima\nLa stima √® un concetto centrale in statistica che si riferisce al processo attraverso il quale si ottengono informazioni sulle caratteristiche di una popolazione intera basandosi sui dati di un campione estratto da essa. Ad esempio, calcolando la media o la mediana dei dati all‚Äôinterno del campione, possiamo derivare stime per la media o la mediana della popolazione complessiva. Le caratteristiche che vogliamo conoscere della popolazione sono spesso chiamate ‚Äúparametri‚Äù, e la stima pu√≤ riguardare sia questi parametri sia la distribuzione di una variabile casuale nella popolazione.\n\n\n11.10.2 Inferenza Statistica\nDopo aver ottenuto queste stime, si passa al passaggio successivo: l‚Äôinferenza statistica. Questo processo va oltre la semplice stima e ci permette di trarre conclusioni pi√π ampie sulla popolazione. L‚Äôinferenza statistica riguarda la valutazione di specifiche ipotesi o risposte a domande di ricerca relative alla popolazione, utilizzando le stime ottenute dal campione.\nAd esempio, se abbiamo stimato la media dei redditi in un campione di famiglie, possiamo utilizzare l‚Äôinferenza statistica per testare se c‚Äô√® una differenza significativa nei redditi tra diverse regioni o gruppi demografici all‚Äôinterno della popolazione. In questo modo, l‚Äôinferenza statistica ci fornisce gli strumenti per fare previsioni e trarre conclusioni riguardanti la popolazione intera.\n\n\n11.10.3 Approcci all‚ÄôInferenza Statistica\nEsistono vari approcci e metodologie per condurre l‚Äôinferenza statistica, tra cui due dei pi√π comuni sono l‚Äôinferenza bayesiana e l‚Äôapproccio frequentista. L‚Äôinferenza bayesiana si basa sull‚Äôuso di probabilit√† a priori e a posteriori, mentre l‚Äôapproccio frequentista si basa su tecniche come i test d‚Äôipotesi e gli intervalli di confidenza.\nIn sintesi, la stima e l‚Äôinferenza statistica sono due fasi cruciali nell‚Äôanalisi statistica. La stima ci permette di utilizzare i dati del campione per ottenere informazioni su specifiche caratteristiche della popolazione, mentre l‚Äôinferenza statistica ci consente di utilizzare quelle stime per fare affermazioni pi√π generali e valutare ipotesi sulla popolazione nel suo insieme. Entrambi questi processi sono fondamentali per comprendere e interpretare i fenomeni che stiamo studiando.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#modelli-psicologici",
    "href": "chapters/chapter_2/01_key_notions.html#modelli-psicologici",
    "title": "11¬† Concetti chiave",
    "section": "11.11 Modelli Psicologici",
    "text": "11.11 Modelli Psicologici\n\n11.11.1 Concetto di Modello\nIn ambito statistico e nel campo della data science, un ‚Äúmodello‚Äù rappresenta una formulazione matematica semplificata di un fenomeno reale che si desidera studiare. Si tratta di un insieme di equazioni e ipotesi che delineano la struttura probabilistica e le relazioni tra le variabili, cercando di catturare gli aspetti essenziali del fenomeno senza rappresentarlo in ogni dettaglio. La scelta del modello specifico pu√≤ dipendere dai dati disponibili, dalla domanda di ricerca e dall‚Äôobiettivo dell‚Äôanalisi. Poich√© spesso esistono diversi modelli che possono essere applicati allo stesso problema, la data science si pone il problema dell‚Äôidentificazione del modello che meglio si adatta ai dati e che soddisfa certi criteri di validit√† e bont√†.\n\n\n11.11.2 Modelli in Psicologia\nNel contesto della psicologia, la modellazione assume un ruolo fondamentale e particolarmente delicato. I modelli psicologici sono strumenti concettuali utilizzati per descrivere, spiegare e prevedere il comportamento umano e i processi mentali. Data la complessit√† e la variabilit√† dei fenomeni psicologici, la costruzione di modelli efficaci richiede un approccio rigoroso e multidimensionale. Un modello psicologico robusto e valido deve soddisfare diverse caratteristiche essenziali:\n\nCoerenza descrittiva: Il modello deve fornire una rappresentazione logica e internamente coerente del fenomeno studiato. Deve catturare gli elementi essenziali del processo psicologico in esame, offrendo una struttura concettuale che organizzi le osservazioni in modo significativo e comprensibile.\nCapacit√† predittiva: Un aspetto cruciale di un modello psicologico efficace √® la sua abilit√† di formulare predizioni accurate sulle manifestazioni future del fenomeno. Questa caratteristica non solo aumenta l‚Äôutilit√† pratica del modello, ma fornisce anche un mezzo per testarne la validit√†.\nSupporto empirico: Il modello deve essere ancorato a solide prove empiriche. Ci√≤ implica che le sue assunzioni e previsioni devono essere confermate da dati osservabili raccolti attraverso ricerche sistematiche e metodologicamente rigorose.\nFalsificabilit√†: Forse la caratteristica pi√π critica, la falsificabilit√†, richiede che il modello sia costruito in modo da poter essere sottoposto a verifica o confutazione attraverso l‚Äôosservazione e l‚Äôesperimento. Questo principio, fondamentale per il metodo scientifico, assicura che il modello rimanga aperto al scrutinio critico e alla revisione basata su nuove evidenze.\nParsimonia: Un buon modello psicologico dovrebbe essere parsimonioso. Dovrebbe spiegare il fenomeno nel modo pi√π semplice possibile, evitando complessit√† non necessarie.\nGeneralizzabilit√†: Il modello dovrebbe essere applicabile a una vasta gamma di situazioni e contesti, non solo a specifici casi o condizioni sperimentali.\nUtilit√† pratica: Infine, un modello psicologico efficace dovrebbe avere implicazioni pratiche, fornendo insights utili per interventi, terapie o applicazioni nel mondo reale.\n\nLa modellazione in psicologia si trova spesso di fronte a sfide uniche dovute alla natura soggettiva e variabile dell‚Äôesperienza umana. I ricercatori devono bilanciare la necessit√† di precisione scientifica con la flessibilit√† richiesta per catturare la ricchezza e la complessit√† dei fenomeni psicologici. Inoltre, devono essere consapevoli dei limiti etici nella sperimentazione e delle potenziali implicazioni sociali dei loro modelli.\nLa creazione e l‚Äôutilizzo di modelli in psicologia √® un processo dinamico e iterativo. I modelli sono costantemente raffinati, testati e, se necessario, rivisti o sostituiti man mano che emergono nuove evidenze. Questo approccio assicura che la comprensione dei processi psicologici continui a evolvere e migliorare nel tempo.\nL‚Äôanalisi dei dati, attraverso l‚Äôapplicazione di tecniche statistiche, √® il mezzo attraverso il quale un modello psicologico viene valutato. Oltre a determinare se il modello √® in grado di spiegare i dati osservati, l‚Äôanalisi pu√≤ anche verificare la capacit√† del modello di fare previsioni accurate su dati non ancora osservati. In questo modo, la modellazione diventa uno strumento potente non solo per comprendere i fenomeni psicologici ma anche per prevedere e, in alcuni casi, influenzare il comportamento e le dinamiche mentali.\nIn sintesi, un modello, sia in statistica che in psicologia, √® uno strumento teorico che cerca di rappresentare un fenomeno complesso in una forma semplificata ma informativa, guidando la comprensione, la previsione e, in ultima analisi, l‚Äôintervento efficace su quel fenomeno. La scelta e la valutazione del modello giusto sono fondamentali per garantire che le conclusioni derivanti dall‚Äôanalisi siano valide e utili nel contesto specifico.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/01_key_notions.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_2/01_key_notions.html#informazioni-sullambiente-di-sviluppo",
    "title": "11¬† Concetti chiave",
    "section": "11.12 Informazioni sull‚ÄôAmbiente di Sviluppo",
    "text": "11.12 Informazioni sull‚ÄôAmbiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m\n\nThe watermark extension is already loaded. To reload it, use:\n  %reload_ext watermark\nLast updated: Tue Jul 23 2024\n\nPython implementation: CPython\nPython version       : 3.12.4\nIPython version      : 8.26.0\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.5.0\nMachine     : arm64\nProcessor   : arm\nCPU cores   : 8\nArchitecture: 64bit\n\nnumpy : 1.26.4\npandas: 2.2.2\n\nWatermark: 2.4.3",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Concetti chiave</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/02_measurement.html",
    "href": "chapters/chapter_2/02_measurement.html",
    "title": "12¬† La misurazione in psicologia",
    "section": "",
    "text": "12.1 Introduzione\nIl problema dello scaling psicologico riguarda la trasformazione dei dati osservati in misure o punteggi che rappresentino accuratamente le caratteristiche psicologiche misurate. Quando conduciamo ricerche in psicologia, spesso vogliamo assegnare numeri ai comportamenti o alle risposte degli individui per analizzarli in modo oggettivo. Tuttavia, questa trasformazione √® complessa e richiede attenzione a diverse considerazioni.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/02_measurement.html#introduzione",
    "href": "chapters/chapter_2/02_measurement.html#introduzione",
    "title": "12¬† La misurazione in psicologia",
    "section": "",
    "text": "12.1.1 Scaling di Guttman\nUno dei metodi di scaling pi√π noti √® il ¬´Scaling di Guttman¬ª, utilizzato per rappresentare relazioni ordinate tra elementi di una scala. Per esempio, in un questionario sui sintomi di ansia, le domande sono ordinate per intensit√† crescente. Se un partecipante risponde ‚Äús√¨‚Äù a una domanda pi√π intensa, dovrebbe aver risposto ‚Äús√¨‚Äù anche a tutte le domande meno intense precedenti, creando una scala ordinata di gravit√† dei sintomi.\n\n\n12.1.2 Scaling Thurstoniano\nLo ¬´Scaling Thurstoniano¬ª misura le preferenze o i giudizi soggettivi. Ad esempio, per valutare la preferenza per diversi tipi di cibi, i partecipanti confrontano due cibi alla volta e esprimono una preferenza. Queste risposte vengono usate per assegnare punteggi basati sulla preferenza media.\n\n\n12.1.3 Questionari Likert\nI questionari Likert richiedono ai partecipanti di esprimere il grado di accordo con affermazioni su una scala a pi√π livelli (da ¬´fortemente in disaccordo¬ª a ¬´fortemente d‚Äôaccordo¬ª). I punteggi ottenuti vengono sommati per rappresentare la posizione dell‚Äôindividuo rispetto all‚Äôoggetto di studio.\n\n\n12.1.4 Metodi di Valutazione delle Scale Psicologiche\nPer valutare le propriet√† delle scale psicologiche, si utilizzano vari metodi. Ad esempio, l‚Äôaffidabilit√† delle misure pu√≤ essere analizzata con il coefficiente alpha di Cronbach o il coefficiente Omega di McDonald, che misurano la coerenza interna delle risposte ai vari item del questionario. Inoltre, la validit√† delle scale pu√≤ essere esaminata confrontando i risultati con misure simili o utilizzando analisi statistiche per verificare se la scala cattura accuratamente il costrutto che si intende misurare. La validit√† di costrutto √® cruciale, poich√© riguarda la capacit√† della scala di misurare effettivamente il concetto psicologico che si intende esaminare.\n\n\n12.1.5 Prospettive Moderne\nNegli ultimi anni, il dibattito sulla misurazione psicologica si √® arricchito di nuove prospettive, grazie all‚Äôavvento di tecnologie avanzate e all‚Äôintegrazione di approcci interdisciplinari. Ecco alcune delle tendenze pi√π rilevanti:\n\n12.1.5.1 Teoria della Risposta agli Item (IRT)\nLa Teoria della Risposta agli Item (IRT) ha guadagnato popolarit√† per la sua capacit√† di fornire stime pi√π precise delle abilit√† latenti rispetto ai modelli classici. La IRT considera la probabilit√† che un individuo risponda correttamente a un item in funzione della sua abilit√† e delle caratteristiche dell‚Äôitem stesso, offrendo una visione pi√π dettagliata delle propriet√† psicometriche degli strumenti di misurazione.\n\n\n12.1.5.2 Approcci Bayesiani\nGli approcci bayesiani stanno rivoluzionando il campo della psicometria, permettendo di incorporare informazioni a priori nelle stime e di aggiornare le credenze sulla base di nuovi dati. Questi metodi sono particolarmente utili per affrontare la complessit√† e l‚Äôincertezza inerenti alla misurazione psicologica.\n\n\n12.1.5.3 Analisi di Rete\nL‚Äôanalisi di rete √® un‚Äôaltra metodologia emergente che vede i costrutti psicologici non come variabili latenti indipendenti, ma come reti di sintomi interconnessi. Questo approccio pu√≤ offrire nuove intuizioni sulla struttura delle psicopatologie e sulla dinamica dei sintomi.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/02_measurement.html#le-scale-di-misurazione",
    "href": "chapters/chapter_2/02_measurement.html#le-scale-di-misurazione",
    "title": "12¬† La misurazione in psicologia",
    "section": "12.2 Le scale di misurazione",
    "text": "12.2 Le scale di misurazione\nLe scale di misurazione sono strumenti fondamentali per assegnare numeri ai dati osservati, rappresentando le propriet√† psicologiche. La teoria delle scale di Stevens {cite:p}stevens_46 identifica quattro tipi di scale di misurazione: nominali, ordinali, a intervalli e di rapporti. Ognuna di queste scale consente di effettuare operazioni aritmetiche diverse, poich√© ciascuna di esse √® in grado di ‚Äúcatturare‚Äù solo alcune delle propriet√† dei fenomeni psicologici che si intende misurare.\n\n\n\nScale di misurazione.\n\n\n\n12.2.1 Scala nominale\nILa scala nominale √® il livello di misurazione pi√π semplice e corrisponde ad una tassonomia o classificazione delle categorie che utilizziamo per descrivere i fenomeni psicologici. I simboli o numeri che costituiscono questa scala rappresentano i nomi delle categorie e non hanno alcun valore numerico intrinseco. Con la scala nominale possiamo solo distinguere se una caratteristica psicologica √® uguale o diversa da un‚Äôaltra.\nI dati raccolti con la scala nominale sono suddivisi in categorie qualitative e mutuamente esclusive, in cui ogni dato appartiene ad una sola categoria. In questa scala, esiste solo la relazione di equivalenza tra le misure delle unit√† di studio: gli elementi del campione appartenenti a classi diverse sono differenti, mentre tutti quelli della stessa classe sono tra loro equivalenti.\nL‚Äôunica operazione algebrica consentita dalla scala nominale √® quella di contare le unit√† di studio che appartengono ad ogni categoria e il numero totale di categorie. Di conseguenza, la descrizione dei dati avviene tramite le frequenze assolute e le frequenze relative.\nDalla scala nominale √® possibile costruire altre scale nominali equivalenti alla prima, trasformando i valori della scala di partenza in modo tale da cambiare i nomi delle categorie, ma lasciando inalterata la suddivisione delle unit√† di studio nelle medesime classi di equivalenza. In altre parole, cambiando i nomi delle categorie di una variabile misurata su scala nominale, si ottiene una nuova variabile esattamente equivalente alla prima.\n\n\n12.2.2 Scala ordinale\nLa scala ordinale mantiene la caratteristica della scala nominale di classificare ogni unit√† di misura all‚Äôinterno di una singola categoria, ma introduce la relazione di ordinamento tra le categorie. In quanto basata su una relazione di ordine, una scala ordinale descrive solo il rango di ordine tra le categorie e non fornisce informazioni sulla distanza tra di esse. Non ci dice, ad esempio, se la distanza tra le categorie \\(a\\) e \\(b\\) √® uguale, maggiore o minore della distanza tra le categorie \\(b\\) e \\(c\\).\nUn esempio classico di scala ordinale √® quello della scala Mohs per la determinazione della durezza dei minerali. Per stabilire la durezza dei minerali si usa il criterio empirico della scalfittura. Vengono stabiliti livelli di durezza crescente da 1 a 10 con riferimento a dieci minerali: talco, gesso, calcite, fluorite, apatite, ortoclasio, quarzo, topazio, corindone e diamante. Un minerale appartenente ad uno di questi livelli se scalfisce quello di livello inferiore ed √® scalfito da quello di livello superiore.\n\n\n\nLa scala di durezza dei minerali di Mohs. Un oggetto √® considerato pi√π duro di X se graffia X. Sono incluse anche misure di durezza relativa utilizzando uno sclerometro, da cui emerge la non linearit√† della scala di Mohs (Burchard, 2004).\n\n\n\n\n12.2.3 Scala ad intervalli\nLa scala ad intervalli di misurazione include le propriet√† della scala nominale e della scala ordinale e permette di misurare le distanze tra le coppie di unit√† statistiche in termini di un intervallo costante, chiamato ‚Äúunit√† di misura‚Äù, a cui viene attribuito il valore ‚Äú1‚Äù. L‚Äôorigine della scala, ovvero il punto zero, √® scelta arbitrariamente e non indica l‚Äôassenza della propriet√† che si sta misurando. Ci√≤ significa che la scala ad intervalli consente anche valori negativi e lo zero non viene attribuito all‚Äôunit√† statistica in cui la propriet√† risulta assente.\nLa scala ad intervalli equivalenti consente l‚Äôesecuzione di operazioni algebriche basate sulla differenza tra i numeri associati ai diversi punti della scala, operazioni algebriche non possibili con le scale di misura nominale o ordinale. Tuttavia, il limite della scala ad intervalli √® che non consente di calcolare il rapporto tra coppie di misure. √à possibile affermare la differenza tra \\(a\\) e \\(b\\) come la met√† della differenza tra \\(c\\) e \\(d\\) o che le due differenze sono uguali, ma non √® possibile affermare che \\(a\\) abbia una propriet√† misurata in quantit√† doppia rispetto a \\(b\\). In altre parole, non √® possibile stabilire rapporti diretti tra le misure ottenute. Solo le differenze tra le modalit√† permettono tutte le operazioni aritmetiche, come la somma, l‚Äôelevazione a potenza o la divisione, che sono alla base della statistica inferenziale.\nNelle scale ad intervalli equivalenti, l‚Äôunit√† di misura √® arbitraria e pu√≤ essere cambiata attraverso una dilatazione, ovvero la moltiplicazione di tutti i valori della scala per una costante positiva. Inoltre, la traslazione, ovvero l‚Äôaggiunta di una costante a tutti i valori della scala, √® ammessa poich√© non altera le differenze tra i valori della scala. La scala rimane invariata rispetto a traslazioni e dilatazioni e dunque le uniche trasformazioni ammissibili sono le trasformazioni lineari:\n\\[\ny' = a + by, \\quad b &gt; 0.\n\\]\nInfatti, l‚Äôuguaglianza dei rapporti fra gli intervalli rimane invariata a seguito di una trasformazione lineare.\nEsempio di scala ad intervalli √® la temperatura misurata in gradi Celsius o Fahrenheit, ma non Kelvin. Come per la scala nominale, √® possibile stabilire se due modalit√† sono uguali o diverse: 30\\(^\\circ\\)C \\(\\neq\\) 20\\(^\\circ\\)C. Come per la scala ordinale √® possibile mettere due modalit√† in una relazione d‚Äôordine: 30\\(^\\circ\\)C \\(&gt;\\) 20\\(^\\circ\\)C. In aggiunta ai casi precedenti, per√≤, √® possibile definire una unit√† di misura per cui √® possibile dire che tra 30\\(^\\circ\\)C e 20\\(^\\circ\\)C c‚Äô√® una differenza di 30\\(^\\circ\\) - 20\\(^\\circ\\) = 10\\(^\\circ\\)C. I valori di temperatura, oltre a poter essere ordinati secondo l‚Äôintensit√† del fenomeno, godono della propriet√† che le differenze tra loro sono direttamente confrontabili e quantificabili.\nIl limite della scala ad intervalli √® quello di non consentire il calcolo del rapporto tra coppie di misure. Ad esempio, una temperatura di 80\\(^\\circ\\)C non √® il doppio di una di 40\\(^\\circ\\)C. Se infatti esprimiamo le stesse temperature nei termini della scala Fahrenheit, allora i due valori non saranno in rapporto di 1 a 2 tra loro. Infatti, 20\\(^\\circ\\)C = 68\\(^\\circ\\)F e 40\\(^\\circ\\)C = 104\\(^\\circ\\)F. Questo significa che la relazione ‚Äúil doppio di‚Äù che avevamo individuato in precedenza si applicava ai numeri della scala centigrada, ma non alla propriet√† misurata (cio√® la temperatura). La decisione di che scala usare (Centigrada vs.¬†Fahrenheit) √® arbitraria. Ma questa arbitrariet√† non deve influenzare le inferenze che traiamo dai dati. Queste inferenze, infatti, devono dirci qualcosa a proposito della realt√† empirica e non possono in nessun modo essere condizionate dalle nostre scelte arbitrarie che ci portano a scegliere la scala Centigrada piuttosto che quella Fahrenheit.\nConsideriamo ora l‚Äôaspetto invariante di una trasformazione lineare, ovvero l‚Äôuguaglianza dei rapporti fra intervalli. Prendiamo in esame, ad esempio, tre temperature: \\(20^\\circ C = 68^\\circ F\\), \\(15^\\circ C = 59^\\circ F\\), \\(10^\\circ C = 50 ^\\circ F\\).\n√à facile rendersi conto del fatto che i rapporti fra intervalli restano costanti indipendentemente dall‚Äôunit√† di misura che √® stata scelta:\n\\[\n  \\frac{20^\\circ C - 10^\\circ C}{20^\\circ C - 15^\\circ C} =\n  \\frac{68^\\circ F - 50^\\circ F}{68^\\circ F-59^\\circ F} = 2.\n\\]\n\n\n12.2.4 Scala di rapporti\nNella scala a rapporti equivalenti, lo zero non √® arbitrario e rappresenta l‚Äôelemento che ha intensit√† nulla rispetto alla propriet√† misurata. Per costruire questa scala, si associa il numero 0 all‚Äôelemento con intensit√† nulla e si sceglie un‚Äôunit√† di misura \\(u\\). Ad ogni elemento si assegna un numero \\(a\\) definito come \\(a=d/u\\), dove \\(d\\) rappresenta la distanza dall‚Äôorigine. In questo modo, i numeri assegnati riflettono le differenze e i rapporti tra le intensit√† della propriet√† misurata.\nIn questa scala, √® possibile effettuare operazioni aritmetiche non solo sulle differenze tra i valori della scala, ma anche sui valori stessi della scala. L‚Äôunica scelta arbitraria √® l‚Äôunit√† di misura, ma lo zero deve sempre rappresentare l‚Äôintensit√† nulla della propriet√† considerata.\nLe trasformazioni ammissibili in questa scala sono chiamate trasformazioni di similarit√† e sono del tipo \\(y' = by\\), dove \\(b&gt;0\\). In questa scala, i rapporti tra i valori rimangono invariati dopo le trasformazioni. In altre parole, se rapportiamo due valori originali e due valori trasformati, il rapporto rimane lo stesso: \\(\\frac{y_i}{y_j} = \\frac{y'_i}{y'_j}\\).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/02_measurement.html#gerarchia-dei-livelli-delle-scale-di-misurazione",
    "href": "chapters/chapter_2/02_measurement.html#gerarchia-dei-livelli-delle-scale-di-misurazione",
    "title": "12¬† La misurazione in psicologia",
    "section": "12.3 Gerarchia dei livelli delle scale di misurazione",
    "text": "12.3 Gerarchia dei livelli delle scale di misurazione\nSecondo {cite:t}stevens_46, esiste una gerarchia dei livelli delle scale di misurazione, denominati ‚Äúlivelli di scala‚Äù. Questi livelli sono organizzati in modo gerarchico, in cui la scala nominale rappresenta il livello pi√π basso della misurazione, mentre la scala a rapporti equivalenti rappresenta il livello pi√π alto. - La scala nominale √® il livello pi√π elementare, in cui le categorie o le etichette vengono assegnate agli oggetti o agli individui senza alcuna valutazione di grandezza o ordine. - Al livello successivo si trova la scala ordinale, in cui le categorie sono ordinate in base a una qualche qualit√† o caratteristica. Qui, √® possibile stabilire un ordine di preferenza o gerarchia tra le categorie, ma non √® possibile quantificare la differenza tra di esse in modo preciso. - La scala intervallo rappresenta un livello successivo, in cui le categorie sono ordinate e la differenza tra di esse √® quantificabile in modo preciso. In questa scala, √® possibile effettuare operazioni matematiche come l‚Äôaddizione e la sottrazione tra i valori, ma non √® possibile stabilire un vero e proprio punto zero significativo. - Infine, la scala a rapporti equivalenti rappresenta il livello pi√π alto. In questa scala, le categorie sono ordinate, la differenza tra di esse √® quantificabile in modo preciso e esiste un punto zero assoluto che rappresenta l‚Äôassenza totale della grandezza misurata. Questo livello di scala permette di effettuare tutte le operazioni matematiche, compresa la moltiplicazione e la divisione.\nPassando da un livello di misurazione ad uno pi√π alto aumenta il numero di operazioni aritmetiche che possono essere compiute sui valori della scala, come indicato nella figura seguente.\n\n\n\nRelazioni tra i livelli di misurazione.\n\n\nPer ci√≤ che riguarda le trasformazioni ammissibili, pi√π il livello di scala √® basso, pi√π le funzioni sono generali (sono minori cio√® i vincoli per passare da una rappresentazione numerica ad un‚Äôaltra equivalente). Salendo la gerarchia, la natura delle funzioni di trasformazione si fa pi√π restrittiva.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/02_measurement.html#variabili-discrete-o-continue",
    "href": "chapters/chapter_2/02_measurement.html#variabili-discrete-o-continue",
    "title": "12¬† La misurazione in psicologia",
    "section": "12.4 Variabili discrete o continue",
    "text": "12.4 Variabili discrete o continue\nLe variabili possono essere classificate come variabili a livello di intervalli o di rapporti e possono essere sia discrete che continue. - Le variabili discrete assumono valori specifici ma non possono assumere valori intermedi. Una volta che l‚Äôelenco dei valori accettabili √® stato definito, non vi sono casi che si trovano tra questi valori. In genere, le variabili discrete assumono valori interi, come il numero di eventi, il numero di persone o il numero di oggetti. - D‚Äôaltra parte, le variabili continue possono assumere qualsiasi valore all‚Äôinterno di un intervallo specificato. Teoricamente, ci√≤ significa che √® possibile utilizzare frazioni e decimali per ottenere qualsiasi grado di precisione.\n\n\n\nVariabili discrete e continue.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/02_measurement.html#comprendere-gli-errori-nella-misurazione",
    "href": "chapters/chapter_2/02_measurement.html#comprendere-gli-errori-nella-misurazione",
    "title": "12¬† La misurazione in psicologia",
    "section": "12.5 Comprendere gli errori nella misurazione",
    "text": "12.5 Comprendere gli errori nella misurazione\nGli errori di misurazione possono essere casuali o sistematici. Gli errori casuali sono fluttuazioni aleatorie, mentre gli errori sistematici sono costanti e derivano da problemi nel metodo di misurazione o negli strumenti.\n\n12.5.1 Precisione e Accuratezza\nLa precisione indica la coerenza tra misurazioni ripetute, mentre l‚Äôaccuratezza si riferisce alla vicinanza del valore misurato al valore reale. Entrambi i concetti sono cruciali per l‚Äôassessment psicometrico.\nUtilizzando l‚Äôanalogia del tiro al bersaglio, si pu√≤ avere una serie di colpi vicini tra loro ma lontani dal centro (precisione senza accuratezza) oppure colpi distribuiti in modo sparso ma in media vicini al centro (accuratezza senza precisione).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/02_measurement.html#assessment-psicometrico",
    "href": "chapters/chapter_2/02_measurement.html#assessment-psicometrico",
    "title": "12¬† La misurazione in psicologia",
    "section": "12.6 Assessment psicometrico",
    "text": "12.6 Assessment psicometrico\nL‚Äôassessment psicometrico valuta la qualit√† delle misurazioni psicologiche, considerando la validit√† e l‚Äôaffidabilit√†.\n\n12.6.1 Validit√† nella Misurazione Psicologica\nLa validit√† √® una propriet√† psicometrica fondamentale dei test psicologici. Secondo gli Standards for Educational and Psychological Testing (2014), la validit√† si riferisce al grado in cui evidenza e teoria supportano le interpretazioni dei punteggi dei test per gli usi proposti. Questo concetto evidenzia che la validit√† riguarda sia il significato dei punteggi sia il loro utilizzo, rendendola ‚Äúla considerazione pi√π fondamentale nello sviluppo e nella valutazione dei test‚Äù.\n\n\n12.6.2 Evoluzione del Concetto di Validit√†\nTradizionalmente, la validit√† era suddivisa in tre categorie:\n\nValidit√† di Contenuto: Si riferisce alla corrispondenza tra il contenuto degli item di un test e il dominio dell‚Äôattributo psicologico che il test intende misurare. √à importante che gli item siano pertinenti e rappresentativi dell‚Äôattributo misurato.\nValidit√† di Criterio: Valuta il grado di concordanza tra i risultati ottenuti tramite lo strumento di misurazione e i risultati ottenuti da altri strumenti che misurano lo stesso costrutto o da un criterio esterno. Include validit√† concorrente e predittiva.\nValidit√† di Costrutto: Riguarda il grado in cui un test misura effettivamente il costrutto che si intende misurare. Si suddivide in validit√† convergente (accordo con strumenti che misurano lo stesso costrutto) e validit√† divergente (capacit√† di discriminare tra costrutti diversi).\n\nLa moderna teoria della validit√† non adotta pi√π questa visione tripartita. Gli Standards del 2014 descrivono la validit√† come un concetto unitario, dove diverse forme di evidenza concorrono a supportare l‚Äôinterpretazione dei punteggi del test per il loro utilizzo previsto.\n\n\n12.6.3 Tipologie di Prove di Validit√†\nGli Standards del 2014 identificano cinque categorie principali di prove di validit√†:\n\nProve Basate sul Contenuto del Test: Valutano quanto il contenuto del test rappresenti adeguatamente il dominio del costrutto da misurare.\nProve Basate sui Processi di Risposta: Analizzano se i processi cognitivi e comportamentali degli esaminandi riflettono il costrutto valutato.\nProve Basate sulla Struttura Interna: Esaminano la coerenza tra gli elementi del test e la struttura teorica del costrutto. L‚Äôanalisi fattoriale √® uno strumento chiave in questo contesto.\nProve Basate sulle Relazioni con Altre Variabili: Studiano la correlazione tra i punteggi del test e altre variabili teoricamente correlate, utilizzando metodi come la validit√† convergente e divergente.\nProve Basate sulle Conseguenze del Test: Considerano le implicazioni e gli effetti dell‚Äôuso del test, sia intenzionali che non intenzionali.\n\n\n\n12.6.4 Minacce alla Validit√†\nLa validit√† pu√≤ essere compromessa quando un test non misura integralmente il costrutto di interesse (sotto-rappresentazione del costrutto) o quando include varianza estranea al costrutto. Inoltre, fattori esterni come l‚Äôansia o la bassa motivazione degli esaminandi, e deviazioni nelle procedure di amministrazione e valutazione, possono influenzare negativamente la validit√† delle interpretazioni dei risultati.\n\n\n12.6.5 Integrazione delle Prove di Validit√†\nLa validit√† di un test si costruisce attraverso l‚Äôintegrazione di diverse linee di evidenza. Ogni interpretazione o uso di un test deve essere validato specificamente, richiedendo una valutazione continua e accurata delle prove disponibili. Questo processo implica la costruzione di un argomento di validit√† che consideri attentamente la qualit√† tecnica del test e l‚Äôadeguatezza delle sue interpretazioni per gli scopi previsti.\nIn conclusione, la validit√† √® un concetto complesso e integrato che richiede un‚Äôanalisi continua e multidimensionale delle evidenze. La moderna teoria della validit√† enfatizza l‚Äôimportanza di considerare diverse forme di evidenza per supportare le interpretazioni dei punteggi dei test, garantendo che siano utilizzati in modo appropriato e significativo. Gli sviluppatori e gli utilizzatori di test devono impegnarsi a valutare costantemente la validit√† per assicurare misurazioni psicologiche accurate e affidabili.\n\n\n12.6.6 Affidabilit√†\nL‚Äôaffidabilit√† concerne la consistenza e stabilit√† delle misurazioni, verificata attraverso metodi come l‚Äôaffidabilit√† test-retest, inter-rater, intra-rater e l‚Äôaffidabilit√† interna.\n\nAffidabilit√† Test-Retest: Questa forma di affidabilit√† verifica la consistenza delle misurazioni nel tempo. Se un individuo viene testato in due momenti diversi, i risultati dovrebbero essere simili, assumendo che non ci siano stati cambiamenti significativi nel costrutto misurato.\nAffidabilit√† Inter-rater: In questo caso, l‚Äôaffidabilit√† √® determinata dalla concordanza tra le valutazioni di diversi esaminatori. Ad esempio, se pi√π psicologi dovessero valutare un individuo utilizzando lo stesso strumento, le loro valutazioni dovrebbero essere simili.\nAffidabilit√† Intra-rater: Questa misura dell‚Äôaffidabilit√† si riferisce alla consistenza delle valutazioni dello stesso esaminatore in momenti diversi.\nAffidabilit√† Interna: Si riferisce alla coerenza delle risposte all‚Äôinterno dello stesso test. Ad esempio, se un test misura un costrutto come l‚Äôansia, gli item che misurano l‚Äôansia dovrebbero correlare positivamente l‚Äôuno con l‚Äôaltro. Un modo comune per valutare l‚Äôaffidabilit√† interna √® utilizzare il coefficiente \\(\\omega\\) di McDonald.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/02_measurement.html#commenti-e-considerazioni-finali",
    "href": "chapters/chapter_2/02_measurement.html#commenti-e-considerazioni-finali",
    "title": "12¬† La misurazione in psicologia",
    "section": "12.7 Commenti e considerazioni finali",
    "text": "12.7 Commenti e considerazioni finali\nLa teoria della misurazione √® fondamentale nella ricerca empirica per valutare l‚Äôattendibilit√† e la validit√† delle misurazioni. √à cruciale valutare l‚Äôerrore nella misurazione per garantire la precisione e l‚Äôaccuratezza delle misure. L‚Äôassessment psicometrico si occupa di valutare la qualit√† delle misurazioni psicologiche, considerando l‚Äôaffidabilit√† e la validit√† per garantire misure accurate dei costrutti teorici. Le moderne tecnologie e metodologie stanno continuamente arricchendo questo campo, offrendo strumenti sempre pi√π raffinati per la comprensione delle caratteristiche psicologiche.\n\n\n\n\nStevens, Stanley Smith. 1946. ‚ÄúOn the Theory of Scales of Measurement.‚Äù Science 103 (2684): 677‚Äì80.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>La misurazione in psicologia</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/03_freq_distr.html",
    "href": "chapters/chapter_2/03_freq_distr.html",
    "title": "13¬† Distribuzioni di Frequenze",
    "section": "",
    "text": "13.1 Introduzione\nIn questo capitolo, esploreremo le strategie per sintetizzare grandi quantit√† di dati, soffermandoci su concetti fondamentali come le distribuzioni di frequenza, i quantili e le tecniche di visualizzazione. Discuteremo sia il calcolo che l‚Äôinterpretazione di queste misure, fornendo strumenti utili per rappresentare graficamente le sintesi di dati. Prima di procedere, √® indispensabile leggere l‚Äôappendice {ref}sec-appendix-sums per comprendere appieno le operazioni descritte.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Distribuzioni di Frequenze</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/03_freq_distr.html#i-dati-grezzi",
    "href": "chapters/chapter_2/03_freq_distr.html#i-dati-grezzi",
    "title": "13¬† Distribuzioni di Frequenze",
    "section": "13.2 I dati grezzi",
    "text": "13.2 I dati grezzi\nPer illustrare i principali strumenti della statistica descrittiva, analizzeremo i dati raccolti da Zetsche, Buerkner, and Renneberg (2019) in uno studio che indaga le aspettative negative come meccanismo chiave nel mantenimento della depressione. I ricercatori hanno confrontato 30 soggetti con episodi depressivi con un gruppo di controllo di 37 individui sani, utilizzando il Beck Depression Inventory (BDI-II) per misurare la depressione.\nCarichiamo quindi i dati dal file data.mood.csv.\n\ndf = pd.read_csv(\"../data/data.mood.csv\")\n\nPer conoscere le dimensioni del DataFrame utilizzo il metodo shape().\n\ndf.shape\n\n(1188, 44)\n\n\nIl DataFrame ha 1188 righe e 44 colonne. Visualizzo il nome delle colonne con il metodo .columns.\n\ndf.columns\n\nIndex(['Unnamed: 0', 'vpn_nr', 'esm_id', 'group', 'bildung', 'bdi',\n       'nr_of_episodes', 'nobs_mood', 'trigger_counter', 'form', 'traurig_re',\n       'niedergeschlagen_re', 'unsicher_re', 'nervos_re', 'glucklich_re',\n       'frohlich_re', 'mood_sad.5', 'mood_fearful.5', 'mood_neg.5',\n       'mood_happy.5', 'cesd_sum', 'rrs_sum', 'rrs_brood', 'rrs_reflect',\n       'forecast_sad', 'forecast_fear', 'forecast_neg', 'forecast_happy',\n       'recall_sad', 'recall_fear', 'recall_neg', 'recall_happy',\n       'diff_neg.fore.5', 'diff_sad.fore.5', 'diff_fear.fore.5',\n       'diff_happy.fore.5', 'diff_neg.retro.5', 'diff_sad.retro.5',\n       'diff_fear.retro.5', 'diff_happy.retro.5', 'mood_sad5_tm1',\n       'mood_neg5_tm1', 'mood_fearful5_tm1', 'mood_happy5_tm1'],\n      dtype='object')\n\n\nPer questo esercizio, ci concentriamo sulle colonne esm_id (il codice del soggetto), group (il gruppo) e bdi (il valore BDI-II).\n\ndf = df[[\"esm_id\", \"group\", \"bdi\"]]\ndf.head()\n\n\n\n\n\n\n\n\n\nesm_id\ngroup\nbdi\n\n\n\n\n0\n10\nmdd\n25.0\n\n\n1\n10\nmdd\n25.0\n\n\n2\n10\nmdd\n25.0\n\n\n3\n10\nmdd\n25.0\n\n\n4\n10\nmdd\n25.0\n\n\n\n\n\n\n\n\nRimuoviamo i duplicati per ottenere un unico valore BDI-II per ogni soggetto:\n\ndf = df.drop_duplicates(keep=\"first\")\n\nVerifichiamo di avere ottenuto il risultato desiderato.\n\ndf.shape\n\n(67, 3)\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nesm_id\ngroup\nbdi\n\n\n\n\n0\n10\nmdd\n25.0\n\n\n14\n9\nmdd\n30.0\n\n\n29\n6\nmdd\n26.0\n\n\n45\n7\nmdd\n35.0\n\n\n64\n12\nmdd\n44.0\n\n\n\n\n\n\n\n\nSi noti che il nuovo DataFrame (con 67 righe) conserva il ‚Äúnome‚Äù delle righe (ovvero, l‚Äôindice di riga) del DataFrame originario (con 1188 righe). Per esempio, il secondo soggetto (con codice identificativo 9) si trova sulla seconda riga del DataFrame, ma il suo indice di riga √® 15. Questo non ha nessuna conseguenza perch√© non useremo l‚Äôindice di riga nelle analisi seguenti.\nEliminiamo eventuali valori mancanti:\n\ndf = df[pd.notnull(df[\"bdi\"])]\n\nOtteniamo cos√¨ il DataFrame finale per gli scopi presenti (66 righe e 3 colonne):\n\ndf.shape\n\n(66, 3)\n\n\nStampiamo i valori BDI-II presentandoli ordinati dal pi√π piccolo al pi√π grande:\n\nprint(df[\"bdi\"].sort_values())\n\n682     0.0\n455     0.0\n465     0.0\n485     0.0\n540     0.0\n       ... \n190    39.0\n810    41.0\n150    43.0\n135    43.0\n64     44.0\nName: bdi, Length: 66, dtype: float64\n\n\n√à chiaro dall‚Äôelenco precedente che i dati grezzi non sono molto informativi. Nella sezione successiva vedremo come creare una rappresentazione sintetica e comprensibile di questi dati.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Distribuzioni di Frequenze</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/03_freq_distr.html#distribuzioni-di-frequenze",
    "href": "chapters/chapter_2/03_freq_distr.html#distribuzioni-di-frequenze",
    "title": "13¬† Distribuzioni di Frequenze",
    "section": "13.3 Distribuzioni di frequenze",
    "text": "13.3 Distribuzioni di frequenze\nUna distribuzione di frequenze rappresenta il conteggio delle volte in cui i valori di una variabile si verificano all‚Äôinterno di un intervallo. Per i nostri dati BDI-II, categorizziamo i punteggi in quattro classi:\n\n0‚Äì13: depressione minima\n14‚Äì19: depressione lieve-moderata\n20‚Äì28: depressione moderata-severa\n29‚Äì63: depressione severa\n\nOgni classe \\(\\Delta_i\\) rappresenta un intervallo di valori aperto a destra \\([a_i, b_i)\\) o aperto a sinistra \\((a_i, b_i]\\). Ad ogni classe \\(\\Delta_i\\), con limiti inferiori e superiori \\(a_i\\) e \\(b_i\\), vengono associati un‚Äôampiezza \\(b_i - a_i\\) (che non √® necessariamente uguale per ogni classe) e un valore centrale \\(\\bar{x}_i\\). Poich√© ogni osservazione \\(x_i\\) appartiene a una sola classe \\(\\Delta_i\\), √® possibile calcolare le seguenti quantit√†.\n\nLa frequenza assoluta \\(n_i\\) di ciascuna classe, ovvero il numero di osservazioni che ricadono nella classe \\(\\Delta_i\\).\n\nPropriet√†: \\(n_1 + n_2 + \\dots + n_m = n\\).\n\nLa frequenza relativa \\(f_i = n_i/n\\) di ciascuna classe.\n\nPropriet√†: \\(f_1+f_2+\\dots+f_m =1\\).\n\nLa frequenza cumulata \\(N_i\\), ovvero il numero totale delle osservazioni che ricadono nelle classi fino alla \\(i\\)-esima compresa: \\(N_i = \\sum_{i=1}^m n_i.\\)\nLa frequenza cumulata relativa \\(F_i\\), ovvero \\(F_i = f_1+f_2+\\dots+f_m = \\frac{N_i}{n} = \\frac{1}{n} \\sum_{i=1}^m f_i.\\)\n\n\n13.3.1 Frequenze Assolute e Relative\nPer ottenere la distribuzione di frequenza assoluta e relativa dei valori BDI-II nel dataset di zetsche_2019future, √® necessario prima aggiungere al DataFrame df una colonna che contenga una variabile categoriale che classifichi ciascuna osservazione in una delle quattro classi che descrivono la gravit√† della depressione. Questo risultato si ottiene con il metodo pandas.cut().\nIn pandas.cut(), il primo argomento x √® un array unidimensionale (lista python, numpy.ndarray o pandas.Series) che contiene i dati e il secondo argomento bins specifica gli intervalli delle classi. La funzione restituisce un array che specifica la classe di appartenenza di ogni elemento dell‚Äôarray x. L‚Äôargomento include_lowest=True specifica classi chiuse a destra (nel nostro caso √® irrilevante dato che nessuna osservazione coincide con il limite di una classe).\n\n13.3.1.1 Frequenze assolute\n\ndf[\"bdi_class\"] = pd.cut(df[\"bdi\"], bins=[0, 13.5, 19.5, 28.5, 63], include_lowest=True)\ndf[\"bdi_class\"].value_counts()\n\nbdi_class\n(-0.001, 13.5]    36\n(28.5, 63.0]      17\n(19.5, 28.5]      12\n(13.5, 19.5]       1\nName: count, dtype: int64\n\n\n\n\n13.3.1.2 Frequenze relative\n\nabs_freq = pd.crosstab(index=df[\"bdi_class\"], columns=[\"Abs. freq.\"])\nrel_freq = abs_freq / abs_freq.sum()\nrel_freq = rel_freq.round(2)\nrel_freq\n\n\n\n\n\n\n\n\ncol_0\nAbs. freq.\n\n\nbdi_class\n\n\n\n\n\n(-0.001, 13.5]\n0.55\n\n\n(13.5, 19.5]\n0.02\n\n\n(19.5, 28.5]\n0.18\n\n\n(28.5, 63.0]\n0.26\n\n\n\n\n\n\n\n\nControlliamo\n\nrel_freq.sum()\n\ncol_0\nAbs. freq.    1.01\ndtype: float64\n\n\n\ngrp_freq = pd.crosstab(index=df[\"group\"], columns=[\"Abs. freq.\"], colnames=[\"\"])\ngrp_freq\n\n\n\n\n\n\n\n\n\nAbs. freq.\n\n\ngroup\n\n\n\n\n\nctl\n36\n\n\nmdd\n30\n\n\n\n\n\n\n\n\nVolendo modificare tale ordine √® possibile accedere al DataFrame tramite loc e specificando come secondo argomento una lista dei valori nell‚Äôordine desiderato:\n\ngrp_freq.loc[[\"mdd\", \"ctl\"], :]\n\n\n\n\n\n\n\n\n\nAbs. freq.\n\n\ngroup\n\n\n\n\n\nmdd\n30\n\n\nctl\n36\n\n\n\n\n\n\n\n\nIn Python, il simbolo : utilizzato all‚Äôinterno delle parentesi quadre permette di ottenere uno slicing corrispondente all‚Äôintera lista.\n\n\n\n13.3.2 Distribuzioni congiunte\nLe variabili possono anche essere analizzate insieme tramite le distribuzioni congiunte di frequenze. Queste distribuzioni rappresentano l‚Äôinsieme delle frequenze assolute o relative ad ogni possibile combinazione di valori delle variabili. Ad esempio, se l‚Äôinsieme di variabili \\(V\\) √® composto da due variabili, \\(X\\) e \\(Y\\), ciascuna delle quali pu√≤ assumere due valori, 1 e 2, allora una possibile distribuzione congiunta di frequenze relative per \\(V\\) potrebbe essere espressa come \\(f(X = 1, Y = 1) = 0.2\\), \\(f(X = 1, Y = 2) = 0.1\\), \\(f(X = 2, Y = 1) = 0.5\\), e \\(f(X = 2, Y = 2) = 0.2\\). Come nel caso delle distribuzioni di frequenze relative di una singola variabile, le frequenze relative di una distribuzione congiunta devono sommare a 1.\nPer i dati dell‚Äôesempio precedente, la funzione pd.crosstab pu√≤ essere utilizzata anche per produrre questo tipo di tabella: basta indicare le serie corrispondenti alle variabili considerate come valori degli argomenti index e columns.\n\nbdi_group_abs_freq = pd.crosstab(index=df[\"bdi_class\"], columns=df[\"group\"])\nbdi_group_abs_freq\n\n\n\n\n\n\n\n\ngroup\nctl\nmdd\n\n\nbdi_class\n\n\n\n\n\n\n(-0.001, 13.5]\n36\n0\n\n\n(13.5, 19.5]\n0\n1\n\n\n(19.5, 28.5]\n0\n12\n\n\n(28.5, 63.0]\n0\n17\n\n\n\n\n\n\n\n\nOppure:\n\nbdi_group_rel_freq = pd.crosstab(index=df[\"bdi_class\"], columns=df[\"group\"], normalize=True)\nbdi_group_rel_freq\n\n\n\n\n\n\n\n\ngroup\nctl\nmdd\n\n\nbdi_class\n\n\n\n\n\n\n(-0.001, 13.5]\n0.545455\n0.000000\n\n\n(13.5, 19.5]\n0.000000\n0.015152\n\n\n(19.5, 28.5]\n0.000000\n0.181818\n\n\n(28.5, 63.0]\n0.000000\n0.257576\n\n\n\n\n\n\n\n\nInvocando il metodo plot.bar sulla tabella, otteniamo un grafico a barre nel quale le barre relative a uno stesso valore bdi_class risultino affiancate. Nel caso presente, le due distribuzioni sono completamente separate, quindi non abbiamo mai due barre affiancate:\n\nbdi_group_rel_freq.plot.bar();",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Distribuzioni di Frequenze</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/03_freq_distr.html#istogramma",
    "href": "chapters/chapter_2/03_freq_distr.html#istogramma",
    "title": "13¬† Distribuzioni di Frequenze",
    "section": "13.4 Istogramma",
    "text": "13.4 Istogramma\nUn istogramma rappresenta graficamente una distribuzione di frequenze. Un istogramma mostra sulle ascisse i limiti delle classi \\(\\Delta_i\\) e sulle ordinate la densit√† della frequenza relativa della variabile \\(X\\) nella classe \\(\\Delta_i\\). La densit√† della frequenza relativa √® misurata dalla funzione costante a tratti \\(\\varphi_n(x)= \\frac{f_i}{b_i-a_i}\\), dove \\(f_i\\) √® la frequenza relativa della classe \\(\\Delta_i\\) e \\(b_i - a_i\\) rappresenta l‚Äôampiezza della classe. In questo modo, l‚Äôarea del rettangolo associato alla classe \\(\\Delta_i\\) sull‚Äôistogramma sar√† proporzionale alla frequenza relativa \\(f_i\\). √à importante notare che l‚Äôarea totale dell‚Äôistogramma delle frequenze relative √® uguale a 1.0, poich√© rappresenta la somma delle aree dei singoli rettangoli.\nPer fare un esempio, costruiamo un istogramma per i valori BDI-II di {cite:t}zetsche_2019future. Con i quattro intervalli individuati dai cut-off del BDI-II creo una prima versione dell‚Äôistogramma ‚Äì si notino le frequenze assolute sull‚Äôasse delle ordinate.\n\nplt.hist(df[\"bdi\"], bins=[0, 13.5, 19.5, 28.5, 63], density=True)\nplt.xlabel(\"BDI\")\nplt.ylabel(\"Density\")\nplt.title(\"Density Histogram of BDI Scores\")\nplt.show()\n\n\n\n\n\n\n\n\nAnche se nel caso presente √® sensato usare ampiezze diverse per gli intervalli delle classi, in generale gli istogrammi si costruiscono utilizzando intervalli riportati sulle ascisse con un‚Äôampiezza uguale.\n\nplt.hist(df[\"bdi\"], density=True)\nplt.xlabel(\"BDI\")\nplt.ylabel(\"Density\")\nplt.title(\"Density Histogram of BDI Scores\")\nplt.show()",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Distribuzioni di Frequenze</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/03_freq_distr.html#kernel-density-plot",
    "href": "chapters/chapter_2/03_freq_distr.html#kernel-density-plot",
    "title": "13¬† Distribuzioni di Frequenze",
    "section": "13.5 Kernel density plot",
    "text": "13.5 Kernel density plot\nConfrontando le due figure precedenti, emerge chiaramente una limitazione dell‚Äôistogramma: la sua forma dipende dall‚Äôarbitrariet√† con cui vengono scelti il numero e l‚Äôampiezza delle classi, rendendo difficile interpretare correttamente la distribuzione dei dati.\nPer superare questa difficolt√†, possiamo utilizzare una tecnica alternativa chiamata stima della densit√† kernel (KDE). Mentre l‚Äôistogramma utilizza barre per rappresentare i dati, la KDE crea un profilo smussato che fornisce una visione pi√π continua e meno dipendente dall‚Äôarbitrariet√† delle classi.\nImmaginiamo un istogramma con classi di ampiezza molto piccola, tanto da avere una curva continua invece di barre discrete. Questo √® ci√≤ che fa la KDE: smussa il profilo dell‚Äôistogramma per ottenere una rappresentazione continua dei dati. Invece di utilizzare barre, la KDE posiziona una piccola curva (detta kernel) su ogni osservazione nel dataset. Queste curve possono essere gaussiane (a forma di campana) o di altro tipo. Ogni kernel ha un‚Äôaltezza e una larghezza determinate da parametri di smussamento (o bandwidth), che controllano quanto deve essere larga e alta la curva. Tutte le curve kernel vengono sommate per creare una singola curva complessiva. Questa curva rappresenta la densit√† dei dati, mostrando come i dati sono distribuiti lungo il range dei valori.\nLa curva risultante dal KDE mostra la proporzione di casi per ciascun intervallo di valori. L‚Äôarea sotto la curva in un determinato intervallo rappresenta la proporzione di casi della distribuzione che ricadono in quell‚Äôintervallo. Per esempio, se un intervallo ha un‚Äôarea maggiore sotto la curva rispetto ad altri, significa che in quell‚Äôintervallo c‚Äô√® una maggiore concentrazione di dati.\nLa curva di densit√† ottenuta tramite KDE fornisce dunque un‚Äôidea chiara di come i dati sono distribuiti senza dipendere dall‚Äôarbitrariet√† della scelta delle classi dell‚Äôistogramma.\nCrediamo un kernel density plot per ciascuno dei due gruppi di valori BDI-II riportati da {cite:t}zetsche_2019future.\n\nsns.kdeplot(data=df, x=\"bdi\", hue=\"group\", common_norm=False)\nplt.xlabel(\"BDI-II\")\nplt.ylabel(\"Density\")\nplt.title(\"Density Histogram of BDI-II Scores\")\nplt.show()",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Distribuzioni di Frequenze</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/03_freq_distr.html#forma-di-una-distribuzione",
    "href": "chapters/chapter_2/03_freq_distr.html#forma-di-una-distribuzione",
    "title": "13¬† Distribuzioni di Frequenze",
    "section": "13.6 Forma di una distribuzione",
    "text": "13.6 Forma di una distribuzione\nIn generale, la forma di una distribuzione descrive come i dati si distribuiscono intorno ai valori centrali. Distinguiamo tra distribuzioni simmetriche e asimmetriche, e tra distribuzioni unimodali o multimodali. Un‚Äôillustrazione grafica √® fornita nella figura seguente. Nel pannello 1 la distribuzione √® unimodale con asimmetria negativa; nel pannello 2 la distribuzione √® unimodale con asimmetria positiva; nel pannello 3 la distribuzione √® simmetrica e unimodale; nel pannello 4 la distribuzione √® bimodale.\nhatygzz ../images/shape_distribution.png :alt: shape :class: bg-primary mb-1 :width: 420px :align: center\n\nIl kernel density plot dei valori BDI-II nel campione di {cite:t}zetsche_2019future √® bimodale. Ci√≤ indica che le osservazioni della distribuzione si addensano in due cluster ben distinti: un gruppo di osservazioni tende ad avere valori BDI-II bassi, mentre l‚Äôaltro gruppo tende ad avere BDI-II alti. Questi due cluster di osservazioni corrispondono al gruppo di controllo e al gruppo clinico nel campione di dati esaminato da {cite:t}zetsche_2019future.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Distribuzioni di Frequenze</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/03_freq_distr.html#indici-di-posizione",
    "href": "chapters/chapter_2/03_freq_distr.html#indici-di-posizione",
    "title": "13¬† Distribuzioni di Frequenze",
    "section": "13.7 Indici di posizione",
    "text": "13.7 Indici di posizione\n\n13.7.1 Quantili\nLa distribuzione dei valori BDI-II di {cite:t}zetsche_2019future pu√≤ essere sintetizzata attraverso l‚Äôuso dei quantili, che sono valori caratteristici che suddividono i dati in parti ugualmente numerose. I quartili sono tre quantili specifici: il primo quartile, \\(q_1\\), divide i dati in due parti, lasciando a sinistra il 25% del campione; il secondo quartile, \\(q_2\\), corrisponde alla mediana e divide i dati in due parti uguali; il terzo quartile lascia a sinistra il 75% del campione.\nInoltre, ci sono altri indici di posizione chiamati decili e percentili che suddividono i dati in parti di dimensioni uguali a 10% e 1%, rispettivamente.\nPer calcolare i quantili, i dati vengono prima ordinati in modo crescente e poi viene determinato il valore di \\(np\\), dove \\(n\\) √® la dimensione del campione e \\(p\\) √® l‚Äôordine del quantile. Se \\(np\\) non √® un intero, il valore del quantile corrisponde al valore del dato che si trova alla posizione successiva alla parte intera di \\(np\\). Se \\(np\\) √® un intero, il valore del quantile corrisponde alla media dei dati nelle posizioni \\(k\\) e \\(k+1\\), dove \\(k\\) √® la parte intera di \\(np\\).\nGli indici di posizione possono essere utilizzati per creare un box-plot, una rappresentazione grafica della distribuzione dei dati che √® molto popolare e pu√≤ essere utilizzata in alternativa ad un istogramma.\nAd esempio, per calcolare la mediana della distribuzione dei nove soggetti con un unico episodio di depressione maggiore del campione clinico di Zetsche et al.¬†(2019), si determina il valore di \\(np = 9 \\cdot 0.5 = 4.5\\), che non √® un intero. Pertanto, il valore del secondo quartile √® pari al valore del dato che si trova alla posizione successiva alla parte intera di \\(np\\), ovvero \\(q_2 = x_{4 + 1} = 27\\). Per calcolare il quantile di ordine \\(2/3\\), si determina il valore di \\(np = 9 \\cdot 2/3 = 6\\), che √® un intero. Quindi, il valore del quantile corrisponde alla media dei dati nelle posizioni \\(6\\) e \\(7\\), ovvero \\(q_{\\frac{2}{3}} = \\frac{1}{2} (x_{6} + x_{7}) = \\frac{1}{2} (33 + 33) = 33\\).\nUsiamo numpy per trovare la soluzione dell‚Äôesercizio precedente.\n\nx = [19, 26, 27, 28, 28, 33, 33, 41, 43]\nnp.quantile(x, 2 / 3)\n\n33.0",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Distribuzioni di Frequenze</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/03_freq_distr.html#mostrare-i-dati",
    "href": "chapters/chapter_2/03_freq_distr.html#mostrare-i-dati",
    "title": "13¬† Distribuzioni di Frequenze",
    "section": "13.8 Mostrare i dati",
    "text": "13.8 Mostrare i dati\n\n13.8.1 Diagramma a scatola\nIl box plot √® uno strumento grafico che visualizza la dispersione di una distribuzione. Per creare un box plot, si disegna un rettangolo (la ‚Äúscatola‚Äù) di altezza arbitraria, basato sulla distanza interquartile (IQR), che corrisponde alla differenza tra il terzo quartile (\\(q_{0.75}\\)) e il primo quartile (\\(q_{0.25}\\)). La mediana (\\(q_{0.5}\\)) √® rappresentata da una linea all‚Äôinterno del rettangolo.\nAi lati della scatola, vengono tracciati due segmenti di retta, detti ‚Äúbaffi‚Äù, che rappresentano i valori adiacenti inferiore e superiore. Il valore adiacente inferiore √® il valore pi√π basso tra le osservazioni che √® maggiore o uguale al primo quartile meno 1.5 volte la distanza interquartile. Il valore adiacente superiore √® il valore pi√π alto tra le osservazioni che √® minore o uguale al terzo quartile pi√π 1.5 volte la distanza interquartile.\nSe ci sono dei valori che cadono al di fuori dei valori adiacenti, vengono chiamati ‚Äúvalori anomali‚Äù e sono rappresentati individualmente nel box plot per evidenziare la loro presenza e posizione. In questo modo, il box plot fornisce una rappresentazione visiva della distribuzione dei dati, permettendo di individuare facilmente eventuali valori anomali e di comprendere la dispersione dei dati.\nhatygzz ../images/boxplot.png :alt: fishy :class: bg-primary mb-1 :width: 640px :align: center\n\nUtilizziamo un box-plot per rappresentare graficamente la distribuzione dei punteggi BDI-II nel gruppo dei pazienti e nel gruppo di controllo.\n\nsns.boxplot(x=\"group\", y=\"bdi\", data=df)\nplt.xlabel(\"Group\")\nplt.ylabel(\"BDI-II\")\nplt.title(\"Boxplot of BDI-II Scores by Group\")\nplt.show()\n\n\n\n\n\n\n\n\nUn risultato migliore si ottiene usando un violin plot e mostrando anche i dati grezzi.\n\n\n13.8.2 Grafico a violino\nI violin plot combinano box plot e KDE plot per una rappresentazione pi√π dettagliata. Al grafico sono sovrapposti i dati grezzi.\n\nsns.violinplot(x=\"group\", y=\"bdi\", data=df, color=\"lightgray\")\nsns.stripplot(x=\"group\", y=\"bdi\", data=df, color=\"black\", size=5, jitter=True, alpha=0.3)\nplt.ylabel(\"BDI-II\")\nplt.xlabel(\"Group\")\nplt.title(\"Violin Plot with Overlay of Individual Data Points of BDI-II Scores by Group\")\nplt.show()",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Distribuzioni di Frequenze</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/03_freq_distr.html#commenti-e-considerazioni-finali",
    "href": "chapters/chapter_2/03_freq_distr.html#commenti-e-considerazioni-finali",
    "title": "13¬† Distribuzioni di Frequenze",
    "section": "13.9 Commenti e considerazioni finali",
    "text": "13.9 Commenti e considerazioni finali\nAbbiamo esplorato diverse tecniche per sintetizzare e visualizzare i dati, includendo distribuzioni di frequenze, istogrammi e grafici di densit√†. Questi strumenti sono essenziali per comprendere meglio i dati e presentare risultati in modo chiaro e informativo.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Distribuzioni di Frequenze</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/03_freq_distr.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_2/03_freq_distr.html#informazioni-sullambiente-di-sviluppo",
    "title": "13¬† Distribuzioni di Frequenze",
    "section": "13.10 Informazioni sull‚ÄôAmbiente di Sviluppo",
    "text": "13.10 Informazioni sull‚ÄôAmbiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m\n\nLast updated: Mon Jul 22 2024\n\nPython implementation: CPython\nPython version       : 3.12.4\nIPython version      : 8.26.0\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.5.0\nMachine     : arm64\nProcessor   : arm\nCPU cores   : 8\nArchitecture: 64bit\n\narviz     : 0.18.0\nnumpy     : 1.26.4\nseaborn   : 0.13.2\nmatplotlib: 3.9.1\npandas    : 2.2.2\n\nWatermark: 2.4.3\n\n\n\n\n\n\n\nZetsche, Ulrike, Paul-Christian Buerkner, and Babette Renneberg. 2019. ‚ÄúFuture Expectations in Clinical Depression: Biased or Realistic?‚Äù Journal of Abnormal Psychology 128 (7): 678.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Distribuzioni di Frequenze</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/04_loc_scale.html",
    "href": "chapters/chapter_2/04_loc_scale.html",
    "title": "14¬† Indici di posizione e di scala",
    "section": "",
    "text": "14.1 Introduzione\nLa visualizzazione grafica dei dati rappresenta il pilastro fondamentale di ogni analisi quantitativa. Grazie alle rappresentazioni grafiche adeguate, √® possibile individuare importanti caratteristiche di una distribuzione, quali la simmetria o l‚Äôasimmetria, nonch√© la presenza di una o pi√π mode. Successivamente, al fine di descrivere sinteticamente le principali caratteristiche dei dati, si rende necessario l‚Äôutilizzo di specifici indici numerici. In questo capitolo, verranno presentati i principali indicatori della statistica descrittiva.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Indici di posizione e di scala</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/04_loc_scale.html#indici-di-tendenza-centrale",
    "href": "chapters/chapter_2/04_loc_scale.html#indici-di-tendenza-centrale",
    "title": "14¬† Indici di posizione e di scala",
    "section": "14.2 Indici di tendenza centrale",
    "text": "14.2 Indici di tendenza centrale\nGli indici di tendenza centrale sono misure statistiche che cercano di rappresentare un valore tipico o centrale all‚Äôinterno di un insieme di dati. Sono utilizzati per ottenere una comprensione immediata della distribuzione dei dati senza dover analizzare l‚Äôintero insieme. Gli indici di tendenza centrale sono fondamentali nell‚Äôanalisi statistica, in quanto forniscono una sintesi semplice e comprensibile delle caratteristiche principali di un insieme di dati. I principali indici di tendenza centrale sono:\n\nMedia: La media √® la somma di tutti i valori divisa per il numero totale di valori. √à spesso utilizzata come misura generale di tendenza centrale, ma √® sensibile agli estremi (valori molto alti o molto bassi).\nMediana: La mediana √® il valore che divide l‚Äôinsieme di dati in due parti uguali. A differenza della media, non √® influenzata da valori estremi ed √® quindi pi√π robusta in presenza di outlier.\nModa: La moda √® il valore che appare pi√π frequentemente in un insieme di dati. In alcuni casi, pu√≤ non essere presente o esserci pi√π di una moda.\n\nLa scelta dell‚Äôindice di tendenza centrale appropriato dipende dalla natura dei dati e dall‚Äôobiettivo dell‚Äôanalisi. Ad esempio, la mediana potrebbe essere preferita alla media se l‚Äôinsieme di dati contiene valori anomali che potrebbero distorcere la rappresentazione centrale. La conoscenza e l‚Äôapplicazione corretta di questi indici possono fornire una preziosa intuizione sulle caratteristiche centrali di una distribuzione di dati.\n\n14.2.1 Media\nLa media aritmetica di un insieme di valori rappresenta il punto centrale o il baricentro della distribuzione dei dati. √à calcolata come la somma di tutti i valori divisa per il numero totale di valori, ed √® espressa dalla formula:\n\\[\n\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i,\n\\tag{14.1}\\]\ndove \\(x_i\\) rappresenta i valori nell‚Äôinsieme, \\(n\\) √® il numero totale di valori, e \\(\\sum\\) indica la sommatoria.\n\n14.2.1.1 Propriet√† della media\nUna propriet√† fondamentale della media √® che la somma degli scarti di ciascun valore dalla media √® zero:\n\\[\n\\sum_{i=1}^n (x_i - \\bar{x}) = 0.\\notag\n\\tag{14.2}\\]\nInfatti,\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n (x_i - \\bar{x}) &= \\sum_i x_i - \\sum_i \\bar{x}\\notag\\\\\n&= \\sum_i x_i - n \\bar{x}\\notag\\\\\n&= \\sum_i x_i - \\sum_i x_i = 0.\\notag\n\\end{aligned}\n\\]\nQuesta propriet√† implica che i dati sono equamente distribuiti intorno alla media.\n\n\n14.2.1.2 La media come centro di gravit√† dell‚Äôistogramma\nLa media aritmetica pu√≤ essere interpretata come il centro di gravit√† o il punto di equilibrio della distribuzione dei dati. In termini fisici, il centro di gravit√† √® il punto in cui la massa di un sistema √® equilibrata o concentrata.\nIn termini statistici, possiamo considerare la media come il punto in cui la distribuzione dei dati √® in equilibrio. Ogni valore dell‚Äôinsieme di dati pu√≤ essere visto come un punto materiale con una massa proporzionale al suo valore. Se immaginiamo questi punti disposti su una linea, con valori pi√π grandi a destra e pi√π piccoli a sinistra, la media corrisponder√† esattamente al punto in cui la distribuzione sarebbe in equilibrio.\n\n\n14.2.1.3 Principio dei minimi quadrati\nLa posizione della media minimizza la somma delle distanze quadrate dai dati, un principio noto come ‚Äúmetodo dei minimi quadrati‚Äù. Matematicamente, questo si traduce nel fatto che la somma dei quadrati degli scarti tra ciascun valore e la media √® minima. Questo principio √® alla base dell‚Äôanalisi statistica dei modelli di regressione e conferma l‚Äôinterpretazione della media come centro di gravit√† dell‚Äôistogramma.\n\n\n14.2.1.4 Calcolo della media con NumPy\nPer calcolare la media di un piccolo numero di valori in Python, possiamo utilizzare la somma di questi valori e dividerla per il numero totale di elementi. Consideriamo ad esempio i valori 12, 44, 21, 62, 24:\n\n(12 + 44 + 21 + 62 + 24) / 5\n\n32.6\n\n\novvero\n\nx = np.array([12, 44, 21, 62, 24])\nnp.mean(x)\n\n32.6\n\n\n\nnp.average(x)\n\n32.6\n\n\n\n\n14.2.1.5 Le proporzioni sono medie\nSe una collezione consiste solo di uni e zeri, allora la somma della collezione √® il numero di uni in essa, e la media della collezione √® la proporzione di uni.\n\nzero_one = np.array([1, 1, 1, 0])\nresult = sum(zero_one)\nprint(result) \n\n3\n\n\n\nnp.mean(zero_one)\n\n0.75\n\n\n√à possibile sostituire 1 con il valore booleano True e 0 con False:\n\nnp.mean(np.array([(True, True, True, False)]))\n\n0.75\n\n\n\n\n14.2.1.6 Limiti della media aritmetica\nLa media aritmetica, tuttavia, ha alcune limitazioni: non sempre √® l‚Äôindice pi√π adeguato per descrivere accuratamente la tendenza centrale della distribuzione, specialmente quando si verificano asimmetrie o valori anomali (outlier). In queste situazioni, √® pi√π indicato utilizzare la mediana o la media spuntata (come spiegheremo successivamente).\n\n\n14.2.1.7 Medie per gruppi\nMolto spesso per√≤ i nostri dati sono contenuti in file e inserire i dati manualmente non √® fattibile. Per fare un esempio, considereremo i dati del Progetto STAR, contenuti nel file STAR.csv, che rappresentano un‚Äôimportante indagine sulle prestazioni degli studenti in relazione alla dimensione delle classi. Negli anni ‚Äô80, i legislatori del Tennessee considerarono la possibilit√† di ridurre le dimensioni delle classi per migliorare il rendimento degli studenti. Al fine di prendere decisioni informate, commissionarono lo studio multimilionario ‚ÄúProgetto Student-Teacher Achievement Ratio‚Äù (Project STAR). Lo studio coinvolgeva bambini della scuola materna assegnati casualmente a classi piccole, con 13-17 studenti, o classi di dimensioni regolari, con 22-25 studenti, fino alla fine della terza elementare. I ricercatori hanno seguito il progresso degli studenti nel tempo, concentrandosi su variabili di risultato, come i punteggi dei test standardizzati di lettura (reading) e matematica (math) alla terza elementare, oltre ai tassi di diploma di scuola superiore (graduated, con valore 1 per s√¨ e 0 per no).\nPoniamoci il problema di calcolare la media dei punteggi math calcolata separatamente per i due gruppi di studenti: coloro che hanno completato la scuola superiore e coloro che non l‚Äôhanno completata.\nProcediamo all‚Äôimportazione dei dati per iniziare l‚Äôanalisi.\n\ndf = pd.read_csv(\"../../data/STAR.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\nclasstype\nreading\nmath\ngraduated\n\n\n\n\n0\nsmall\n578\n610\n1\n\n\n1\nregular\n612\n612\n1\n\n\n2\nregular\n583\n606\n1\n\n\n3\nsmall\n661\n648\n1\n\n\n4\nsmall\n614\n636\n1\n\n\n\n\n\n\n\n\nEsaminiamo la numerosit√† di ciascun gruppo.\n\ndf.groupby(\"graduated\").size()\n\ngraduated\n0     166\n1    1108\ndtype: int64\n\n\nOra procediamo al calcolo delle medie dei punteggi math all‚Äôinterno dei due gruppi. Per rendere la risposta pi√π concisa, useremo la funzione round() per stampare solo 2 valori decimali.\n\ndf.groupby(\"graduated\")[\"math\"].mean().round(2)\n\ngraduated\n0    606.64\n1    635.33\nName: math, dtype: float64\n\n\nIn alternativa, possiamo usare il metodo .describe():\n\ndf.groupby(\"graduated\")[\"math\"].describe().round(1)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\ngraduated\n\n\n\n\n\n\n\n\n\n\n\n\n0\n166.0\n606.6\n34.1\n526.0\n580.5\n606.0\n629.0\n711.0\n\n\n1\n1108.0\n635.3\n38.1\n515.0\n609.5\n634.0\n659.0\n774.0\n\n\n\n\n\n\n\n\n\n\n\n14.2.2 Media spuntata\nLa media spuntata, indicata come \\(\\bar{x}_t\\) o trimmed mean, √® un metodo di calcolo della media che prevede l‚Äôeliminazione di una determinata percentuale di dati estremi prima di effettuare la media aritmetica. Solitamente, viene eliminato il 10% dei dati, ovvero il 5% all‚Äôinizio e alla fine della distribuzione. Per ottenere la media spuntata, i dati vengono ordinati in modo crescente, \\(x_1 \\leq x_2 \\leq x_3 \\leq \\dots \\leq x_n\\), e quindi viene eliminato il primo 5% e l‚Äôultimo 5% dei dati nella sequenza ordinata. Infine, la media spuntata √® calcolata come la media aritmetica dei dati rimanenti. Questo approccio √® utile quando ci sono valori anomali o quando la distribuzione √® asimmetrica e la media aritmetica non rappresenta adeguatamente la tendenza centrale dei dati.\nA titolo di esempio, procediamo al calcolo della media spuntata dei valori math per i due gruppi definiti dalla variabile graduated, escludendo il 10% dei valori pi√π estremi.\n\nnot_graduated = df[df[\"graduated\"] == 0].math\nstats.trim_mean(not_graduated, 0.10)\n\n605.6492537313433\n\n\n\ngraduated = df[df[\"graduated\"] == 1].math\nstats.trim_mean(graduated, 0.10)\n\n634.4403153153153\n\n\n\n\n14.2.3 Quantili\nIl quantile non interpolato di ordine \\(p\\) \\((0 &lt; p &lt; 1)\\) rappresenta il valore che divide la distribuzione dei dati in modo tale che una frazione \\(p\\) dei dati si trovi al di sotto di esso.\nLa formula per calcolare il quantile non interpolato √® la seguente:\n\\[\n    q_p = x_{(k)},\n\\]\ndove \\(x_{(k)}\\) √® l‚Äôelemento \\(k\\)-esimo nell‚Äôinsieme di dati ordinato in modo crescente, e \\(k\\) √® calcolato come:\n\\[\nk = \\lceil p \\cdot n \\rceil,\n\\]\ndove \\(n\\) √® il numero totale di dati nel campione, e \\(\\lceil \\cdot \\rceil\\) rappresenta la funzione di arrotondamento all‚Äôintero successivo. In questa definizione, il quantile non interpolato corrisponde al valore effettivo nell‚Äôinsieme di dati, senza effettuare alcuna interpolazione tra i valori circostanti.\nAd esempio, consideriamo il seguente insieme di dati: \\(\\{ 15, 20, 23, 25, 28, 30, 35, 40, 45, 50 \\}\\). Supponiamo di voler calcolare il quantile non interpolato di ordine \\(p = 0.3\\) (cio√® il 30¬∞ percentile).\nOrdiniamo i dati in modo crescente: \\(\\{ 15, 20, 23, 25, 28, 30, 35, 40, 45, 50 \\}.\\) Calcoliamo \\(k\\) utilizzando la formula \\(k = \\lceil p \\cdot n \\rceil\\), dove \\(n\\) √® il numero totale di dati nel campione. Nel nostro caso, \\(n = 10\\) e \\(p = 0.3\\):\n\\[\nk = \\lceil 0.3 \\cdot 10 \\rceil = \\lceil 3 \\rceil = 3.\n\\]\nIl quantile non interpolato corrisponde al valore \\(x_{(k)}\\), ovvero l‚Äôelemento \\(k\\)-esimo nell‚Äôinsieme ordinato: \\(q_{0.3} = x_{(3)} = 23.\\)\nOltre al quantile non interpolato, esiste anche il concetto di quantile interpolato. A differenza del quantile non interpolato, il quantile interpolato pu√≤ essere calcolato anche per percentili che non corrispondono esattamente a valori presenti nell‚Äôinsieme di dati. Per ottenere il valore del quantile interpolato, viene utilizzato un procedimento di interpolazione lineare tra i valori adiacenti. In genere, il calcolo del quantile interpolato viene eseguito mediante l‚Äôuso di software dedicati.\nOra, procediamo al calcolo dei quantili di ordine 0.10 e 0.90 per i valori math all‚Äôinterno dei due gruppi. I quantili sono dei valori che dividono la distribuzione dei dati in parti specifiche. Ad esempio, il quantile di ordine 0.10 corrisponde al valore al di sotto del quale si trova il 10% dei dati, mentre il quantile di ordine 0.90 rappresenta il valore al di sotto del quale si trova il 90% dei dati.\nCalcoliamo i quantili di ordine 0.1 e 0.9 della distribuzione dei punteggi math nei due gruppi definiti dalla variabile graduated.\n\n# Quantili di ordine 0.1 e 0.9 per il gruppo di studenti che hanno completato la scuola superiore\n[\n    df[df[\"graduated\"] == 1][\"math\"].quantile(0.1), \n    df[df[\"graduated\"] == 1][\"math\"].quantile(0.9)\n]\n\n[588.0, 684.0]\n\n\n\n# Quantili di ordine 0.1 e 0.9 per il gruppo di studenti che non hanno completato la scuola superiore\n[\n    df[df[\"graduated\"] == 0][\"math\"].quantile(0.1),\n    df[df[\"graduated\"] == 0][\"math\"].quantile(0.9),\n]\n\n[564.5, 651.0]\n\n\n\n\n14.2.4 Moda e mediana\nIn precedenza abbiamo gi√† incontrato altri due popolari indici di tendenza centrale: la moda (Mo), che rappresenta il valore centrale della classe con la frequenza massima (in alcune distribuzioni pu√≤ esserci pi√π di una moda, rendendola multimodale e facendo perdere a questo indice il suo significato di indicatore di tendenza centrale); e la mediana (\\(\\tilde{x}\\)), che rappresenta il valore corrispondente al quantile di ordine 0.5 della distribuzione.\n\n\n14.2.5 Quando usare media, moda, mediana\nLa moda pu√≤ essere utilizzata per dati a livello nominale o ordinale ed √® l‚Äôunica tra le tre statistiche che pu√≤ essere calcolata in questi casi.\nLa media, d‚Äôaltra parte, √® una buona misura di tendenza centrale solo se la distribuzione dei dati √® simmetrica, ossia se i valori sono distribuiti uniformemente a sinistra e a destra della media. Tuttavia, se ci sono valori anomali o se la distribuzione √® asimmetrica, la media pu√≤ essere influenzata in modo significativo e, pertanto, potrebbe non essere la scelta migliore come misura di tendenza centrale.\nIn queste situazioni, la mediana pu√≤ fornire una misura migliore di tendenza centrale rispetto alla media poich√© √® meno influenzata dai valori anomali e si basa esclusivamente sul valore centrale dell‚Äôinsieme di dati. Di conseguenza, la scelta tra media e mediana dipende dal tipo di distribuzione dei dati e dagli obiettivi dell‚Äôanalisi.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Indici di posizione e di scala</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/04_loc_scale.html#indici-di-dispersione",
    "href": "chapters/chapter_2/04_loc_scale.html#indici-di-dispersione",
    "title": "14¬† Indici di posizione e di scala",
    "section": "14.3 Indici di dispersione",
    "text": "14.3 Indici di dispersione\nLe misure di posizione descritte in precedenza, come le medie e gli indici di posizione, offrono una sintesi dei dati mettendo in evidenza la tendenza centrale delle osservazioni. Tuttavia, trascurano un aspetto importante della distribuzione dei dati: la variabilit√† dei valori numerici della variabile statistica. Pertanto, √® essenziale completare la descrizione della distribuzione di una variabile statistica utilizzando anche indicatori che valutino la dispersione delle unit√† statistiche. In questo modo, otterremo una visione pi√π completa e approfondita delle caratteristiche del campione analizzato.\n\n14.3.1 Indici basati sull‚Äôordinamento dei dati\nPer valutare la variabilit√† dei dati, √® possibile utilizzare indici basati sull‚Äôordinamento dei dati. L‚Äôindice pi√π semplice √® l‚Äôintervallo di variazione, che corrisponde alla differenza tra il valore massimo e il valore minimo di una distribuzione di dati. Tuttavia, questo indice ha il limite di essere calcolato basandosi solo su due valori della distribuzione, e non tiene conto di tutte le informazioni disponibili. Inoltre, l‚Äôintervallo di variazione pu√≤ essere fortemente influenzato dalla presenza di valori anomali.\nUn altro indice basato sull‚Äôordinamento dei dati √® la differenza interquartile, gi√† incontrata in precedenza. Anche se questo indice utilizza pi√π informazioni rispetto all‚Äôintervallo di variazione, presenta comunque il limite di essere calcolato basandosi solo su due valori della distribuzione, ossia il primo quartile \\(Q_1\\) e il terzo quartile \\(Q_3\\).\nPer valutare la variabilit√† in modo pi√π completo, √® necessario utilizzare altri indici di variabilit√† che tengano conto di tutti i dati disponibili. In questo modo, si otterr√† una valutazione pi√π accurata della dispersione dei valori nella distribuzione e si potranno individuare eventuali pattern o tendenze nascoste.\n\n\n14.3.2 Varianza\nDate le limitazioni delle statistiche descritte in precedenza, √® pi√π comune utilizzare una misura di variabilit√† che tenga conto della dispersione dei dati rispetto a un indice di tendenza centrale. La varianza √® la misura di variabilit√† pi√π utilizzata per valutare la variabilit√† di una variabile statistica. Essa √® definita come la media dei quadrati degli scarti \\(x_i - \\bar{x}\\) tra ogni valore e la media della distribuzione, come segue:\n\\[\n\\begin{equation}\nS^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2.\n\\end{equation}\n\\tag{14.3}\\]\nLa varianza √® una misura di dispersione pi√π completa rispetto a quelle descritte in precedenza. Tuttavia, √® appropriata solo nel caso di distribuzioni simmetriche ed √® fortemente influenzata dai valori anomali, come altre misure di dispersione. Inoltre, la varianza √® espressa in un‚Äôunit√† di misura che √® il quadrato dell‚Äôunit√† di misura dei dati originali, pertanto, potrebbe non essere facilmente interpretata in modo intuitivo.\nCalcoliamo la varianza dei valori math per i dati del progetto STAR. Applicando l‚Äôequazione della varianza, otteniamo:\n\nsum((df[\"math\"] - np.mean(df[\"math\"])) ** 2) / len(df[\"math\"])\n\n1507.2328523125227\n\n\nPi√π semplicemente, possiamo usare la funzione np.var():\n\nnp.var(df[\"math\"])\n\n1507.2328523125227\n\n\n\n14.3.2.1 Stima della varianza della popolazione\nSi noti il denominatore della formula della varianza. Nell‚ÄôEquation¬†14.3, ho utilizzato \\(n\\) come denominatore (l‚Äôampiezza campionaria, ovvero il numero di osservazioni nel campione). In questo modo, otteniamo la varianza come statistica descrittiva del campione. Tuttavia, √® possibile utilizzare \\(n-1\\) come denominatore alternativo:\n\\[\n\\begin{equation}\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\n\\end{equation}\n\\tag{14.4}\\]\nIn questo secondo caso, otteniamo la varianza come stimatore della varianza della popolazione. Si pu√≤ dimostrare che l‚ÄôEquation¬†14.4 fornisce una stima corretta (ovvero, non distorta) della varianza della popolazione da cui abbiamo ottenuto il campione, mentre l‚ÄôEquation¬†14.3 fornisce (in media) una stima troppo piccola della varianza della popolazione. Si presti attenzione alla notazione: \\(S^2\\) rappresenta la varianza come statistica descrittiva, mentre \\(s^2\\) rappresenta la varianza come stimatore.\nPer illustrare questo punto, svolgiamo una simulazione. Consideriamo la distribuzione dei punteggi del quoziente di intelligenza (QI). I valori del QI seguono una particolare distribuzione chiamata distribuzione normale, con media 100 e deviazione standard 15. La forma di questa distribuzione √® illustrata nella figura seguente.\n\nx = np.arange(100 - 4 * 15, 100 + 4 * 15, 0.001)\n\nmu = 100\nsigma = 15\n\npdf = stats.norm.pdf(x, mu, sigma)\nplt.plot(x, pdf)\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.show()\n\n\n\n\n\n\n\n\nSupponiamo di estrarre un campione casuale di 4 osservazioni dalla popolazione del quoziente di intelligenza ‚Äì in altre parole, supponiamo di misurare il quoziente di intelligenza di 4 persone prese a caso dalla popolazione.\n\nx = rng.normal(loc=100, scale=15, size=4)\nprint(x)\n\n[ 96.66036673 119.88488115  91.43544977 110.02373805]\n\n\nCalcoliamo la varianza usando \\(n\\) al denominatore. Si noti che la vera varianza del quoziente di intelligenza √® \\(15^2\\) = 225.\n\nnp.var(x)\n\n134.65656223872708\n\n\nConsideriamo ora 10 campioni casuali del QI, ciascuno di ampiezza 4.\n\nmu = 100\nsigma = 15\nsize = 4\nniter = 10\nrandom_samples = []\n\nfor i in range(niter):\n    one_sample = rng.normal(loc=mu, scale=sigma, size=size)\n    random_samples.append(one_sample)\n\nIl primo campione √®\n\nrandom_samples[0]\n\narray([ 70.73447217,  80.4673074 , 101.91760605,  95.25636111])\n\n\nIl decimo campione √®\n\nrandom_samples[9]\n\narray([111.14881257, 108.14731402,  90.01735439, 103.48241985])\n\n\nStampiamo i valori di tutti i 10 campioni.\n\nrs = np.array(random_samples)\nrs\n\narray([[117.18071524, 108.81016091,  99.22835071,  72.32428097],\n       [ 95.39169462,  99.82359162, 109.53158888, 101.69933684],\n       [121.28194403, 111.99123812,  93.5863066 , 123.02032138],\n       [ 66.58961856, 104.23622977,  83.17674426, 119.51304508],\n       [107.69111686,  84.07010216,  98.52662251, 105.83838496],\n       [ 70.66964863, 105.06034379, 107.1614251 ,  83.45087842],\n       [ 92.51928633, 113.73691601,  90.51622696, 107.85154079],\n       [105.74034675, 102.67262674,  88.66272413,  87.55578622],\n       [108.18945322, 119.3681818 , 107.45218178,  81.44893886],\n       [ 84.56836494,  94.18203683,  97.52873515, 100.41911991]])\n\n\nPer ciascun campione (ovvero, per ciascuna riga della matrice precedente), calcoliamo la varianza usando la formula con \\(n\\) al denominatore. Otteniamo cos√¨ 10 stime della varianza della popolazione del QI.\n\nx_var = np.var(rs, axis=1)  # applichiamo la funzione su ciascuna riga\nprint(x_var)\n\n[284.45704733  26.15452963 136.44568199 405.65618265  86.35524345\n 231.95644125  97.72682684  66.1097433  193.53695841  35.63101473]\n\n\nNotiamo due cose:\n\nle stime sono molto diverse tra loro; questo fenomeno √® noto con il nome di variabilit√† campionaria;\nin media le stime sembrano troppo piccole.\n\nPer aumentare la sicurezza riguardo al secondo punto menzionato in precedenza, ripeteremo la simulazione utilizzando un numero di iterazioni maggiore.\n\nmu = 100\nsigma = 15\nsize = 4\nniter = 10000\nrandom_samples = []\n\nfor i in range(niter):\n    one_sample = rng.normal(loc=mu, scale=sigma, size=size)\n    random_samples.append(one_sample)\n\nrs = np.array(random_samples)\nx_var = np.var(rs, ddof=0, axis=1)\n\nEsaminiamo la distribuzione dei valori ottenuti.\n\nplt.hist(x_var, bins=10, color=\"skyblue\", edgecolor=\"black\")\nplt.xlabel(\"Varianza\")\nplt.ylabel(\"Frequenza\")\nplt.title(\"Varianza del QI in campioni di n = 4\")\nplt.show()\n\n\n\n\n\n\n\n\nLa stima pi√π verosimile della varianza del QI √® dato dalla media di questa distribuzione.\n\nnp.mean(x_var)\n\n170.04960311858687\n\n\nSi noti che il nostro spospetto √® stato confermato: il valore medio della stima della varianza ottenuta con l‚ÄôEquation¬†14.3 √® troppo piccolo rispetto al valore corretto di \\(15^2 = 225\\).\nRipetiamo ora la simulazione usando la formula della varianza con \\(n-1\\) al denominatore.\n\nmu = 100\nsigma = 15\nsize = 4\nniter = 10000\nrandom_samples = []\n\nfor i in range(niter):\n    one_sample = rng.normal(loc=mu, scale=sigma, size=size)\n    random_samples.append(one_sample)\n\nrs = np.array(random_samples)\nx_var = np.var(rs, ddof=1, axis=1)\n\nnp.mean(x_var)\n\n222.53361717582456\n\n\nNel secondo caso, se utilizziamo \\(n-1\\) come denominatore per calcolare la stima della varianza, il valore atteso di questa stima √® molto vicino al valore corretto di 225. Se il numero di campioni fosse infinito, i due valori sarebbero identici.\nIn conclusione, le due formule della varianza hanno scopi diversi. La formula della varianza con \\(n\\) al denominatore viene utilizzata come statistica descrittiva per descrivere la variabilit√† di un particolare campione di osservazioni. D‚Äôaltro canto, la formula della varianza con \\(n-1\\) al denominatore viene utilizzata come stimatore per ottenere la migliore stima della varianza della popolazione da cui quel campione √® stato estratto.\n\n\n\n14.3.3 Deviazione standard\nPer interpretare la varianza in modo pi√π intuitivo, si pu√≤ calcolare la deviazione standard (o scarto quadratico medio o scarto tipo) prendendo la radice quadrata della varianza. La deviazione standard √® espressa nell‚Äôunit√† di misura originaria dei dati, a differenza della varianza che √® espressa nel quadrato dell‚Äôunit√† di misura dei dati. La deviazione standard fornisce una misura della dispersione dei dati attorno alla media, rendendo pi√π facile la comprensione della variabilit√† dei dati.\nLa deviazione standard (o scarto quadratico medio, o scarto tipo) √® definita come:\n\\[\ns^2 = \\sqrt{(n-1)^{-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}.\n\\tag{14.5}\\]\nQuando tutte le osservazioni sono uguali, \\(s = 0\\), altrimenti \\(s &gt; 0\\).\n\n\n\n\n\n\nIl termine standard deviation √® stato introdotto in statistica da Pearson nel 1894 assieme alla lettera greca \\(\\sigma\\) che lo rappresenta. Il termine italiano ‚Äúdeviazione standard‚Äù ne √® la traduzione pi√π utilizzata nel linguaggio comune; il termine dell‚ÄôEnte Nazionale Italiano di Unificazione √® tuttavia ‚Äúscarto tipo‚Äù, definito come la radice quadrata positiva della varianza.\n\n\n\nLa deviazione standard \\(s\\) dovrebbe essere utilizzata solo quando la media √® una misura appropriata per descrivere il centro della distribuzione, ad esempio nel caso di distribuzioni simmetriche. Tuttavia, √® importante tener conto che, come la media \\(\\bar{x}\\), anche la deviazione standard √® fortemente influenzata dalla presenza di dati anomali, ovvero pochi valori che si discostano notevolmente dalla media rispetto agli altri dati della distribuzione. In presenza di dati anomali, la deviazione standard pu√≤ risultare ingannevole e non rappresentare accuratamente la variabilit√† complessiva della distribuzione. Pertanto, √® fondamentale considerare attentamente il contesto e le caratteristiche dei dati prima di utilizzare la deviazione standard come misura di dispersione. In alcune situazioni, potrebbe essere pi√π appropriato ricorrere a misure di dispersione robuste o ad altre statistiche descrittive per caratterizzare la variabilit√† dei dati in modo pi√π accurato e affidabile.\nPer fare un esempio, calcoliamo la deviazione standard per i valori math del campione di dati del progetto STAR. Applicando l‚ÄôEquation¬†14.5, per tutto il campione abbiamo\n\nnp.std(df.math)\n\n38.82309689234648\n\n\nPer ciascun gruppo, abbiamo:\n\ndf.groupby(\"graduated\")[\"math\"].std()\n\ngraduated\n0    34.105746\n1    38.130136\nName: math, dtype: float64\n\n\n\n14.3.3.1 Interpretazione\nLa deviazione standard pu√≤ essere interpretata in modo semplice: essa rappresenta la dispersione dei dati rispetto alla media aritmetica. √à simile allo scarto semplice medio campionario, cio√® alla media aritmetica dei valori assoluti degli scarti tra ciascuna osservazione e la media, anche se non √® identica. La deviazione standard ci fornisce un‚Äôindicazione di quanto, in media, le singole osservazioni si discostino dal centro della distribuzione.\nPer verificare l‚Äôinterpretazione della deviazione standard, utilizziamo i valori math del campione di dati del progetto STAR.\n\nnp.std(df[\"math\"])\n\n38.82309689234648\n\n\nLa deviazione standard calcolata per questi dati √® \\(\\approx 38.8\\). Questo valore ci indica che, in media, ogni osservazione si discosta di circa 38.8 punti dalla media aritmetica dei punteggi math. Maggiore √® il valore della deviazione standard, maggiore √® la dispersione dei dati attorno alla media, mentre un valore pi√π piccolo indica che i dati sono pi√π concentrati vicino alla media. La deviazione standard ci offre quindi una misura quantitativa della variabilit√† dei dati nella distribuzione.\nPer questi dati, lo scarto semplice medio campionario √®\n\nnp.mean(np.abs(df.math - np.mean(df.math)))\n\n30.9682664274501\n\n\nSi noti che i due valori sono simili, ma non identici.\n\n\n\n14.3.4 Deviazione mediana assoluta\nUna misura robusta della dispersione statistica di un campione √® la deviazione mediana assoluta (Median Absolute Deviation, MAD) definita come la mediana del valore assoluto delle deviazioni dei dati dalla mediana. Matematicamente, la formula per calcolare la MAD √®:\n\\[\n\\text{MAD} = \\text{median} \\left( |X_i - \\text{median}(X)| \\right)\n\\tag{14.6}\\]\nLa deviazione mediana assoluta √® particolarmente utile quando si affrontano distribuzioni con presenza di dati anomali o asimmetrie, poich√© √® meno influenzata da questi valori estremi rispetto alla deviazione standard.\nQuando i dati seguono una distribuzione gaussiana (normale), esiste una relazione specifica tra MAD e la deviazione standard (si veda il Capitolo {ref}cont-rv-distr-notebook). In una distribuzione normale, la MAD √® proporzionale alla deviazione standard. La costante di proporzionalit√† dipende dalla forma esatta della distribuzione normale, ma in generale, la relazione √® data da:\n\\[\n\\sigma \\approx k \\times \\text{MAD},\n\\]\ndove: - $ $ √® la deviazione standard. - MAD √® la Mediana della Deviazione Assoluta. - $ k $ √® una costante che, per una distribuzione normale, √® tipicamente presa come circa 1.4826.\nQuesta costante di 1.4826 √® derivata dal fatto che, in una distribuzione normale, circa il 50% dei valori si trova entro 0.6745 deviazioni standard dalla media. Quindi, per convertire la MAD (basata sulla mediana) nella deviazione standard (basata sulla media), si usa il reciproco di 0.6745, che √® approssimativamente 1.4826.\nLa formula completa per convertire la MAD in una stima della deviazione standard in una distribuzione normale √®:\n\\[\n\\sigma \\approx 1.4826 \\times \\text{MAD}\n\\]\nQuesta relazione √® utile per stimare la deviazione standard in modo pi√π robusto, specialmente quando si sospetta la presenza di outlier o si ha a che fare con campioni piccoli. Di conseguenza, molti software restituiscono il valore MAD moltiplicato per questa costante per fornire un‚Äôindicazione pi√π intuitiva della variabilit√† dei dati. Tuttavia, √® importante notare che questa relazione si mantiene accurata solo per le distribuzioni che sono effettivamente normali. In presenza di distribuzioni fortemente asimmetriche o con elevati outlier, la deviazione standard e la MAD possono fornire indicazioni molto diverse sulla variabilit√† dei dati.\nPer verificare questo principio, calcoliamo la deviazione mediana assoluta dei valori math del campione di dati del progetto STAR.\n\n1.4826 * np.median(np.abs(df[\"math\"] - np.median(df[\"math\"])))\n\n41.5128\n\n\nIn questo caso, la MAD per i punteggi di matematica √® simile alla deviazione standard.\n\nnp.std(df[\"math\"])\n\n38.82309689234648\n\n\nInfatti, la distribuzione dei punteggi math √® approssimativamente gaussiana.\n\nplt.hist(df[\"math\"], bins=10, color=\"skyblue\", edgecolor = \"black\")\nplt.xlabel(\"math\")\nplt.ylabel(\"Frequenza\")\nplt.title(\"Distribuzione dei Punteggi di Matematica\")\nplt.show()\n\n\n\n\n\n\n\n\nVerifichiamo nuovamente il principio usando un campione di dati estratto da una popolazione normale. Usiamo, ad esempio, la distribuzione \\(\\mathcal{N}(100, 15)\\):\n\nx = np.random.normal(loc=100, scale=15, size=10000)\n1.4826 * np.median(np.abs(x - np.median(x)))\n\n15.179389799831695\n\n\n\n\n14.3.5 Quando usare la deviazione standard e MAD\nLa deviazione standard e la MAD sono entrambe misure di dispersione che forniscono informazioni su quanto i dati in un insieme si discostano dalla tendenza centrale. Tuttavia, ci sono alcune differenze tra le due misure e situazioni in cui pu√≤ essere pi√π appropriato utilizzare una rispetto all‚Äôaltra.\n\nDeviazione standard: Questa misura √® particolarmente utile per descrivere la dispersione dei dati in una distribuzione normale. La deviazione standard √® una scelta appropriata se si vuole sapere quanto i dati sono distribuiti intorno alla media, o se si vuole confrontare la dispersione di due o pi√π set di dati. Tuttavia, la deviazione standard √® fortemente influenzata dalla presenza di dati anomali, e questo pu√≤ rappresentare una limitazione in casi in cui sono presenti valori estremi nell‚Äôinsieme di dati.\nDeviazione mediana assoluta (MAD): La MAD √® meno sensibile ai valori anomali rispetto alla deviazione standard, il che la rende una scelta migliore quando ci sono valori anomali nell‚Äôinsieme di dati. Inoltre, la MAD pu√≤ essere una buona scelta quando si lavora con dati non normalmente distribuiti, poich√© non assume una distribuzione specifica dei dati. La MAD √® calcolata utilizzando la mediana e i valori assoluti delle deviazioni dei dati dalla mediana, il che la rende una misura robusta di dispersione.\n\nIn sintesi, se si sta lavorando con dati normalmente distribuiti, la deviazione standard √® la misura di dispersione pi√π appropriata. Se si lavora con dati non normalmente distribuiti o si hanno valori anomali nell‚Äôinsieme di dati, la MAD pu√≤ essere una scelta migliore. In ogni caso, la scelta tra le due misure dipende dal tipo di dati che si sta analizzando e dall‚Äôobiettivo dell‚Äôanalisi.\n\n\n14.3.6 Indici di variabilit√† relativi\nA volte pu√≤ essere necessario confrontare la variabilit√† di grandezze incommensurabili, ovvero di caratteri misurati con differenti unit√† di misura. In queste situazioni, le misure di variabilit√† descritte in precedenza diventano inadeguate poich√© dipendono dall‚Äôunit√† di misura utilizzata. Per superare questo problema, si ricorre a specifici numeri adimensionali chiamati indici relativi di variabilit√†.\nIl pi√π importante di questi indici √® il coefficiente di variazione (\\(C_v\\)), definito come il rapporto tra la deviazione standard (\\(\\sigma\\)) e la media dei dati (\\(\\bar{x}\\)):\n\\[\nC_v = \\frac{\\sigma}{\\bar{x}}.\n\\tag{14.7}\\]\nIl coefficiente di variazione √® un numero puro e permette di confrontare la variabilit√† di distribuzioni con unit√† di misura diverse.\nUn altro indice relativo di variabilit√† √® la differenza interquartile rapportata a uno dei tre quartili (primo quartile, terzo quartile o mediana). Questo indice √® definito come:\n\\[\n\\frac{x_{0.75} - x_{0.25}}{x_{0.25}}, \\qquad \\frac{x_{0.75} - x_{0.25}}{x_{0.75}}, \\qquad \\frac{x_{0.75} - x_{0.25}}{x_{0.50}}.\n\\]\nQuesti indici relativi di variabilit√† forniscono una misura adimensionale della dispersione dei dati, rendendo possibile il confronto tra grandezze con diverse unit√† di misura e facilitando l‚Äôanalisi delle differenze di variabilit√† tra i dati.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Indici di posizione e di scala</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/04_loc_scale.html#la-fallacia-ergodica",
    "href": "chapters/chapter_2/04_loc_scale.html#la-fallacia-ergodica",
    "title": "14¬† Indici di posizione e di scala",
    "section": "14.4 La fallacia ergodica",
    "text": "14.4 La fallacia ergodica\nSebbene il concetto di ‚Äúmedia‚Äù possa sembrare chiaro, ci√≤ non implica che il suo utilizzo non presenti delle problematiche nell‚Äôambito della pratica psicologica. Un aspetto su cui vale la pena soffermarsi √® ci√≤ che viene definito ‚Äúfallacia ergodica‚Äù.\nIl concetto di ‚Äúfallacia ergodica‚Äù (Speelman et al. 2024) si riferisce all‚Äôerrore compiuto dai ricercatori quando assumono che le caratteristiche medie di un gruppo di individui possano essere applicate a ciascun individuo all‚Äôinterno di quel gruppo, senza considerare le differenze individuali o le variazioni nel tempo. Questa fallacia emerge dalla pratica comune nella ricerca psicologica di raccogliere dati aggregati da gruppi di persone per stimare parametri della popolazione, al fine di confrontare comportamenti in condizioni diverse o esplorare associazioni tra diverse misurazioni della stessa persona.\nIl problema di questo approccio √® che l‚Äôuso dei risultati basati sul gruppo per caratterizzare le caratteristiche degli individui o per estrapolare a persone simili a quelle del gruppo √® ingiustificato, poich√© le medie di gruppo possono fornire informazioni solo sui risultati collettivi, come la performance media del gruppo, e non consentono di fare affermazioni accurate sugli individui che compongono quel gruppo. La fallacia ergodica si basa sull‚Äôassunzione che per utilizzare legittimamente una statistica aggregata (ad esempio, la media) derivata da un gruppo per descrivere un individuo di quel gruppo, due condizioni devono essere soddisfatte: gli individui devono essere cos√¨ simili da essere praticamente interscambiabili, e le caratteristiche degli individui devono essere temporalmente stabili.\nTuttavia, i fenomeni e i processi psicologici di interesse per i ricercatori sono per natura non uniformi tra gli individui e variabili nel tempo, sia all‚Äôinterno degli individui che tra di loro. Di conseguenza, i risultati ottenuti dalla media di misure di comportamenti, cognizioni o stati emotivi di pi√π individui non descrivono accuratamente nessuno di quegli individui in un dato momento, n√© possono tenere conto dei cambiamenti in quelle variabili per un individuo nel tempo.\nSpeelman et al. (2024) osservano che la stragrande maggioranza degli articoli che hanno analizzato include conclusioni nelle sezioni degli Abstract e/o delle Discussioni che implicano che i risultati trovati con dati aggregati di gruppo si applichino anche agli individui in quei gruppi e/o si applichino agli individui nella popolazione. Questa pratica riflette la fallacia ergodica, che consiste nell‚Äôassumere che i campioni siano sistemi ergodici quando non lo sono.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Indici di posizione e di scala</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/04_loc_scale.html#commenti-e-considerazioni-finali",
    "href": "chapters/chapter_2/04_loc_scale.html#commenti-e-considerazioni-finali",
    "title": "14¬† Indici di posizione e di scala",
    "section": "14.5 Commenti e considerazioni finali",
    "text": "14.5 Commenti e considerazioni finali\nLe statistiche descrittive ci permettono di ottenere indicatori sintetici che riassumono i dati di una popolazione o di un campione estratto da essa. Questi indicatori includono misure di tendenza centrale, come la media, la mediana e la moda, che ci forniscono informazioni sulla posizione centrale dei dati rispetto alla distribuzione. Inoltre, ci sono gli indici di dispersione, come la deviazione standard e la varianza, che ci indicano quanto i dati si disperdono attorno alla tendenza centrale. Questi indici ci aiutano a comprendere quanto i valori si discostano dalla media, e quindi ci forniscono un‚Äôidea della variabilit√† dei dati. In sintesi, le statistiche descrittive ci offrono un quadro chiaro e sintetico delle caratteristiche principali dei dati, consentendoci di comprendere meglio la loro distribuzione e variabilit√†.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Indici di posizione e di scala</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/04_loc_scale.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_2/04_loc_scale.html#informazioni-sullambiente-di-sviluppo",
    "title": "14¬† Indici di posizione e di scala",
    "section": "14.6 Informazioni sull‚ÄôAmbiente di Sviluppo",
    "text": "14.6 Informazioni sull‚ÄôAmbiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m\n\nLast updated: Fri Feb 16 2024\n\nPython implementation: CPython\nPython version       : 3.11.7\nIPython version      : 8.21.0\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.3.0\nMachine     : x86_64\nProcessor   : i386\nCPU cores   : 8\nArchitecture: 64bit\n\nmatplotlib: 3.8.2\npandas    : 2.2.0\narviz     : 0.17.0\nnumpy     : 1.26.4\nscipy     : 1.12.0\nseaborn   : 0.13.2\n\nWatermark: 2.4.3\n\n\n\n\n\n\n\nSpeelman, Craig P, Laura Parker, Benjamin J Rapley, and Marek McGann. 2024. ‚ÄúMost Psychological Researchers Assume Their Samples Are Ergodic: Evidence from a Year of Articles in Three Major Journals.‚Äù Collabra: Psychology 10 (1).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Indici di posizione e di scala</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/05_correlation.html",
    "href": "chapters/chapter_2/05_correlation.html",
    "title": "15¬† Le relazioni tra variabili",
    "section": "",
    "text": "15.1 Introduzione\nNel linguaggio comune, termini quali ‚Äúdipendenza‚Äù, ‚Äúassociazione‚Äù e ‚Äúcorrelazione‚Äù sono spesso utilizzati in modo intercambiabile. Tuttavia, da un punto di vista tecnico, ‚Äúassociazione‚Äù e ‚Äúdipendenza‚Äù sono sinonimi e entrambi si distinguono dalla ‚Äúcorrelazione‚Äù. L‚Äôassociazione implica una relazione molto ampia: conoscere il valore di una variabile ci fornisce informazioni su un‚Äôaltra variabile. Al contrario, la correlazione descrive una relazione pi√π specifica e quantificabile, indicando se due variabili tendono a variare insieme in modo sistematico; ad esempio, in una tendenza crescente, se $ X &gt; _X $ allora √® probabile che anche $ Y &gt; _Y $.\n√à cruciale comprendere che non tutte le associazioni sono correlazioni e che, crucialmente, la correlazione non implica causalit√†. Questa distinzione √® vitale per interpretare accuratamente i dati e per evitare conclusioni errate sulle relazioni tra variabili.\nIn questo capitolo, ci concentreremo su due misure statistiche fondamentali per valutare la relazione lineare tra due variabili: la covarianza e la correlazione. Questi strumenti sono essenziali per analizzare il grado e la direzione dell‚Äôassociazione lineare tra le variabili, permettendoci di quantificare in che modo le variabili variano congiuntamente.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Le relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/05_correlation.html#i-dati-grezzi",
    "href": "chapters/chapter_2/05_correlation.html#i-dati-grezzi",
    "title": "15¬† Le relazioni tra variabili",
    "section": "15.2 I dati grezzi",
    "text": "15.2 I dati grezzi\nPer illustrare la correlazione e la covarianza, analizzeremo i dati raccolti da Zetsche, Buerkner, and Renneberg (2019) in uno studio che indaga le aspettative negative come meccanismo chiave nel mantenimento e nella reiterazione della depressione. Nello specifico, i ricercatori si sono proposti di determinare se gli individui depressi sviluppano aspettative accurate riguardo al loro umore futuro o se tali aspettative sono distortamente negative.\nUno dei loro studi ha coinvolto un campione di 30 soggetti con almeno un episodio depressivo maggiore, confrontati con un gruppo di controllo composto da 37 individui sani. La misurazione del livello di depressione √® stata effettuata tramite il Beck Depression Inventory (BDI-II).\nIl BDI-II √® uno strumento di autovalutazione utilizzato per valutare la gravit√† della depressione in adulti e adolescenti. Il test √® stato sviluppato per identificare e misurare l‚Äôintensit√† dei sintomi depressivi sperimentati nelle ultime due settimane. I 21 item del test sono valutati su una scala a 4 punti, dove 0 rappresenta il grado pi√π basso e 3 il grado pi√π elevato di sintomatologia depressiva.\nNell‚Äôesercizio successivo, ci proponiamo di analizzare i punteggi di depressione BDI-II nel campione di dati fornito da Zetsche, Buerkner, and Renneberg (2019).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Le relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/05_correlation.html#definizione-delle-relazioni-tra-variabili",
    "href": "chapters/chapter_2/05_correlation.html#definizione-delle-relazioni-tra-variabili",
    "title": "15¬† Le relazioni tra variabili",
    "section": "15.3 Definizione delle relazioni tra variabili",
    "text": "15.3 Definizione delle relazioni tra variabili\nNel contesto delle indagini statistiche, spesso non ci limitiamo a esaminare la distribuzione di una singola variabile. Invece, il nostro interesse si concentra sulla relazione che emerge nei dati tra due o pi√π variabili. Ma cosa significa esattamente quando diciamo che due variabili hanno una relazione?\nPer comprendere ci√≤, prendiamo ad esempio l‚Äôaltezza e l‚Äôet√† tra un gruppo di bambini. In generale, √® possibile notare che all‚Äôaumentare dell‚Äôet√† di un bambino, aumenta anche la sua altezza. Pertanto, conoscere l‚Äôet√† di un bambino, ad esempio tredici anni, e l‚Äôet√† di un altro, sei anni, ci fornisce un‚Äôindicazione su quale dei due bambini sia pi√π alto.\nNel linguaggio statistico, definiamo questa relazione tra altezza e et√† come positiva, il che significa che all‚Äôaumentare dei valori di una delle variabili (in questo caso, l‚Äôet√†), ci aspettiamo di vedere valori pi√π elevati anche nell‚Äôaltra variabile (l‚Äôaltezza). Tuttavia, esistono anche relazioni negative, in cui l‚Äôaumento di una variabile √® associato a un diminuzione dell‚Äôaltra (ad esempio, pi√π et√† √® correlata a meno pianto).\nNon si tratta solo di relazioni positive o negative; ci sono anche situazioni in cui le variabili non hanno alcuna relazione tra loro, definendo cos√¨ una relazione nulla. Inoltre, le relazioni possono variare nel tempo, passando da positive a negative o da fortemente positive a appena positiva. In alcuni casi, una delle variabili pu√≤ essere categorica, rendendo difficile parlare di ‚Äúmaggioranza‚Äù o ‚Äúminoranza‚Äù ma piuttosto di ‚Äúdifferente‚Äù (ad esempio, i bambini pi√π grandi potrebbero semplicemente avere diverse preferenze rispetto ai bambini pi√π piccoli, senza necessariamente essere ‚Äúmigliori‚Äù o ‚Äúpeggiori‚Äù).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Le relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/05_correlation.html#sec-scatter-plot",
    "href": "chapters/chapter_2/05_correlation.html#sec-scatter-plot",
    "title": "15¬† Le relazioni tra variabili",
    "section": "15.4 Grafico a dispersione",
    "text": "15.4 Grafico a dispersione\nIl metodo pi√π diretto per visualizzare la relazione tra due variabili continue √® tramite un grafico a dispersione, comunemente noto come ‚Äúscatterplot‚Äù. Questo tipo di diagramma rappresenta le coppie di dati ottenute da due variabili, posizionandole sull‚Äôasse delle ascisse (orizzontale) e delle ordinate (verticale).\nPer rendere l‚Äôidea pi√π chiara, consideriamo i dati dello studio condotto da Zetsche, Buerkner, and Renneberg (2019), in cui i ricercatori hanno utilizzato due scale psicometriche, il Beck Depression Inventory II (BDI-II) e la Center for Epidemiologic Studies Depression Scale (CES-D), per misurare il livello di depressione nei partecipanti. Il BDI-II √® uno strumento di autovalutazione che valuta la presenza e l‚Äôintensit√† dei sintomi depressivi in pazienti adulti e adolescenti con diagnosi psichiatrica, mentre la CES-D √® una scala di autovalutazione progettata per misurare i sintomi depressivi sperimentati nella settimana precedente nella popolazione generale, in particolare negli adolescenti e nei giovani adulti. Poich√© entrambe le scale misurano lo stesso costrutto, ovvero la depressione, ci aspettiamo una relazione tra i punteggi ottenuti dal BDI-II e dalla CES-D. Un diagramma a dispersione ci consente di esaminare questa relazione in modo visuale e intuitivo.\n\n# Leggi i dati dal file CSV\ndf = pd.read_csv(\"../../data/data.mood.csv\", index_col=0)\n\n# Seleziona le colonne di interesse\ndf = df[[\"esm_id\", \"group\", \"bdi\", \"cesd_sum\"]]\n\n# Rimuovi le righe duplicate\ndf = df.drop_duplicates(keep=\"first\")\n\n# Rimuovi le righe con valori mancanti nella colonna \"bdi\"\ndf = df.dropna(subset=[\"bdi\"])\n\nPosizionando i valori del BDI-II sull‚Äôasse delle ascisse e quelli del CES-D sull‚Äôasse delle ordinate, ogni punto sul grafico rappresenta un individuo, di cui conosciamo il livello di depressione misurato dalle due scale. √à evidente che i valori delle scale BDI-II e CES-D non possono coincidere per due motivi principali: (1) la presenza di errori di misurazione e (2) l‚Äôutilizzo di unit√† di misura arbitrarie per le due variabili. L‚Äôerrore di misurazione √® una componente inevitabile che influisce in parte su qualsiasi misurazione, ed √® particolarmente rilevante in psicologia, dove la precisione degli strumenti di misurazione √® generalmente inferiore rispetto ad altre discipline, come la fisica. Il secondo motivo per cui i valori delle scale BDI-II e CES-D non possono essere identici √® che l‚Äôunit√† di misura della depressione √® una questione arbitraria e non standardizzata. Tuttavia, nonostante le differenze dovute agli errori di misurazione e all‚Äôuso di unit√† di misura diverse, ci aspettiamo che, se le due scale misurano lo stesso costrutto (la depressione), i valori prodotti dalle due scale dovrebbero essere associati linearmente tra di loro. Per comprendere meglio il concetto di ‚Äúassociazione lineare‚Äù, √® possibile esaminare i dati attraverso l‚Äôutilizzo di un diagramma a dispersione.\n\n# Crea uno scatterplot con colori diversi per i due gruppi\nplt.scatter(df[df[\"group\"] == \"mdd\"][\"bdi\"], df[df[\"group\"] == \"mdd\"][\"cesd_sum\"], label=\"Pazienti\", c=\"C0\")\nplt.scatter(df[df[\"group\"] == \"ctl\"][\"bdi\"], df[df[\"group\"] == \"ctl\"][\"cesd_sum\"], label=\"Controlli\", c=\"C2\")\n\n# Calcola i coefficienti della retta dei minimi quadrati\ncoeff_combined = np.polyfit(df[\"bdi\"], df[\"cesd_sum\"], 1)\n\n# Calcola la retta dei minimi quadrati\nline_combined = np.poly1d(coeff_combined)\n\n# Disegna la retta dei minimi quadrati\nx_values = np.linspace(df[\"bdi\"].min(), df[\"bdi\"].max(), 100)\nplt.plot(x_values, line_combined(x_values), linestyle='--', color='C3')\n\n# Etichette degli assi\nplt.xlabel(\"BDI-II\")\nplt.ylabel(\"CESD\")\n\n# Linee verticali ed orizzontali per le medie\nplt.axvline(np.mean(df[df[\"group\"] == \"mdd\"][\"bdi\"]), alpha=0.2, color=\"blue\")\nplt.axvline(np.mean(df[df[\"group\"] == \"ctl\"][\"bdi\"]), alpha=0.2, color=\"red\")\nplt.axhline(np.mean(df[df[\"group\"] == \"mdd\"][\"cesd_sum\"]), alpha=0.2, color=\"blue\")\nplt.axhline(np.mean(df[df[\"group\"] == \"ctl\"][\"cesd_sum\"]), alpha=0.2, color=\"red\")\n\n# Legenda\nplt.legend()\n\n# Mostra il grafico\nplt.show()\n\n\n\n\n\n\n\n\nOsservando il grafico a dispersione, √® evidente che i dati mostrano una tendenza a distribuirsi in modo approssimativamente lineare. In termini statistici, ci√≤ suggerisce una relazione di associazione lineare tra i punteggi CES-D e BDI-II.\nTuttavia, √® importante notare che la relazione lineare tra le due variabili √® lontana dall‚Äôessere perfetta. In una relazione lineare perfetta, tutti i punti nel grafico sarebbero allineati in modo preciso lungo una retta. Nella realt√†, la dispersione dei punti dal comportamento lineare ideale √® evidente.\nDi conseguenza, sorge la necessit√† di quantificare numericamente la forza e la direzione della relazione lineare tra le due variabili e di misurare quanto i punti si discostino da una relazione lineare ideale. Esistono vari indici statistici a disposizione per raggiungere questo obiettivo.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Le relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/05_correlation.html#covarianza",
    "href": "chapters/chapter_2/05_correlation.html#covarianza",
    "title": "15¬† Le relazioni tra variabili",
    "section": "15.5 Covarianza",
    "text": "15.5 Covarianza\nIniziamo a considerare il pi√π importante di tali indici, chiamato covarianza. In realt√† la definizione di questo indice non ci sorprender√† pi√π di tanto in quanto, in una forma solo apparentemente diversa, l‚Äôabbiamo gi√† incontrata in precedenza. Ci ricordiamo infatti che la varianza di una generica variabile \\(X\\) √® definita come la media degli scarti quadratici di ciascuna osservazione dalla media:\n\\[\nS_{XX} = \\frac{1}{n} \\sum_{i=1}^n(X_i - \\bar{X}) (X_i - \\bar{X}).\n\\]\nLa varianza viene talvolta descritta come la ‚Äúcovarianza di una variabile con s√© stessa‚Äù. Adesso facciamo un passo ulteriore. Invece di valutare la dispersione di una sola variabile, ci chiediamo come due variabili \\(X\\) e \\(Y\\) ‚Äúvariano insieme‚Äù (co-variano). √à facile capire come una risposta a tale domanda possa essere fornita da una semplice trasformazione della formula precedente che diventa:\n\\[\nS_{XY} = \\frac{1}{n} \\sum_{i=1}^n(X_i - \\bar{X}) (Y_i - \\bar{Y}).\n\\tag{15.1}\\]\nL‚ÄôEquation¬†15.1 ci fornisce la definizione della covarianza.\n\n15.5.1 Interpretazione\nPer capire il significato dell‚ÄôEquation¬†15.1, supponiamo di dividere il grafico riportato nella ?sec-introduction in quattro quadranti definiti da una retta verticale passante per la media dei valori BDI-II e da una retta orizzontale passante per la media dei valori CES-D. Numeriamo i quadranti partendo da quello in basso a sinistra e muovendoci in senso antiorario.\nSe prevalgono punti nel I e III quadrante, allora la nuvola di punti avr√† un andamento crescente (per cui a valori bassi di \\(X\\) tendono ad associarsi valori bassi di \\(Y\\) e a valori elevati di \\(X\\) tendono ad associarsi valori elevati di \\(Y\\)) e la covarianza avr√† segno positivo. Mentre se prevalgono punti nel II e IV quadrante la nuvola di punti avr√† un andamento decrescente (per cui a valori bassi di \\(X\\) tendono ad associarsi valori elevati di \\(Y\\) e a valori elevati di \\(X\\) tendono ad associarsi valori bassi di \\(Y\\)) e la covarianza avr√† segno negativo. Dunque, il segno della covarianza ci informa sulla direzione della relazione lineare tra due variabili: l‚Äôassociazione lineare si dice positiva se la covarianza √® positiva, negativa se la covarianza √® negativa.\nEsercizio. Implemento l‚ÄôEquation¬†15.1 in Python.\n\ndef cov_value(x, y):\n\n    mean_x = sum(x) / float(len(x))\n    mean_y = sum(y) / float(len(y))\n\n    sub_x = [i - mean_x for i in x]\n    sub_y = [i - mean_y for i in y]\n\n    sum_value = sum([sub_y[i] * sub_x[i] for i in range(len(x))])\n    denom = float(len(x))\n\n    cov = sum_value / denom\n    return cov\n\nPer i dati mostrati nel diagramma, la covarianza tra BDI-II e CESD √® 207.4\n\nx = df[\"bdi\"]\ny = df[\"cesd_sum\"]\n\ncov_value(x, y)\n\n207.42653810835637\n\n\nOppure, in maniera pi√π semplice:\n\nnp.mean((x - np.mean(x)) * (y - np.mean(y)))\n\n207.42653810835628\n\n\nLo stesso risultato si ottiene con la funzione cov di NumPy.\n\nnp.cov(x, y, ddof=0)\n\narray([[236.23875115, 207.42653811],\n       [207.42653811, 222.83379247]])\n\n\nLa funzione np.cov(x, y, ddof=0) in Python, utilizzata tramite la libreria NumPy, calcola la covarianza tra due array, x e y. L‚Äôargomento ddof (Delta Degrees of Freedom) specifica il ‚Äúcorrettore‚Äù da applicare al denominatore della formula di covarianza.\nQuando si imposta ddof=0, la formula utilizzata per il calcolo della covarianza divide la somma dei prodotti delle deviazioni dalla media per n, dove n √® il numero totale degli elementi nel campione (ovvero, la dimensione del campione). Questo approccio assume che i dati forniti rappresentino l‚Äôintera popolazione da cui si vuole stimare la covarianza, producendo una stima non corretta (bias) se i dati sono effettivamente un campione di una popolazione pi√π ampia. Il ‚Äúbias‚Äù in questo contesto si riferisce al fatto che la stima tende sistematicamente a essere pi√π piccola rispetto alla vera covarianza della popolazione da cui il campione √® stato estratto.\nPer correggere questo errore sistematico e ottenere una stima non distorta (unbiased) della covarianza di una popolazione pi√π ampia basandosi su un campione, si utilizza ddof=1. Questo significa che al denominatore della formula si sottrae 1 a n, dividendo quindi per n-1. Il correttore n-1 √® noto come correttore di Bessel, e l‚Äôuso di ddof=1 rende la stima della covarianza non distorta nel contesto di un campione prelevato da una popolazione. La correzione √® importante in statistica perch√© fornisce una stima pi√π accurata delle propriet√† della popolazione, soprattutto quando la dimensione del campione √® piccola.\nIn sintesi: - Con ddof=0, si divide per n, assumendo che i dati rappresentino l‚Äôintera popolazione. Questo pu√≤ introdurre un bias nella stima della covarianza se i dati sono in realt√† un campione. - Con ddof=1, si divide per n-1, correggendo il bias e ottenendo una stima non distorta (unbiased) della covarianza se i dati rappresentano un campione di una popolazione pi√π grande. Questo approccio √® generalmente preferito per la stima delle propriet√† della popolazione basata su campioni.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Le relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/05_correlation.html#correlazione",
    "href": "chapters/chapter_2/05_correlation.html#correlazione",
    "title": "15¬† Le relazioni tra variabili",
    "section": "15.6 Correlazione",
    "text": "15.6 Correlazione\nLa direzione della relazione tra le variabili √® indicata dal segno della covarianza, ma il valore assoluto di questo indice non fornisce informazioni utili poich√© dipende dall‚Äôunit√† di misura delle variabili. Ad esempio, considerando l‚Äôaltezza e il peso delle persone, la covarianza sar√† pi√π grande se l‚Äôaltezza √® misurata in millimetri e il peso in grammi, rispetto al caso in cui l‚Äôaltezza √® in metri e il peso in chilogrammi. Pertanto, per descrivere la forza e la direzione della relazione lineare tra due variabili in modo adimensionale, si utilizza l‚Äôindice di correlazione.\nLa correlazione √® ottenuta standardizzando la covarianza tramite la divisione delle deviazioni standard (\\(s_X\\), \\(s_Y\\)) delle due variabili:\n\\[\nr_{XY} = \\frac{S_{XY}}{S_X S_Y}.\n\\tag{15.2}\\]\nLa quantit√† che si ottiene dall‚ÄôEquation¬†15.2 viene chiamata correlazione di Bravais-Pearson (dal nome degli autori che, indipendentemente l‚Äôuno dall‚Äôaltro, l‚Äôhanno introdotta).\n\n15.6.1 Propriet√†\nIl coefficiente di correlazione ha le seguenti propriet√†:\n\nha lo stesso segno della covarianza, dato che si ottiene dividendo la covarianza per due numeri positivi;\n√® un numero puro, cio√® non dipende dall‚Äôunit√† di misura delle variabili;\nassume valori compresi tra -1 e +1.\n\n\n\n15.6.2 Interpretazione\nAll‚Äôindice di correlazione possiamo assegnare la seguente interpretazione:\n\n\\(r_{XY} = -1\\) \\(\\rightarrow\\) perfetta relazione negativa: tutti i punti si trovano esattamente su una retta con pendenza negativa (dal quadrante in alto a sinistra al quadrante in basso a destra);\n\\(r_{XY} = +1\\) \\(\\rightarrow\\) perfetta relazione positiva: tutti i punti si trovano esattamente su una retta con pendenza positiva (dal quadrante in basso a sinistra al quadrante in alto a destra);\n\\(-1 &lt; r_{XY} &lt; +1\\) \\(\\rightarrow\\) presenza di una relazione lineare di intensit√† diversa;\n\\(r_{XY} = 0\\) \\(\\rightarrow\\) assenza di relazione lineare tra \\(X\\) e \\(Y\\).\n\nEsercizio. Per i dati riportati nel diagramma della sezione {ref}sec-zetsche-scatter, la covarianza √® 207.4. Il segno positivo della covarianza ci dice che tra le due variabili c‚Äô√® un‚Äôassociazione lineare positiva. Per capire quale sia l‚Äôintensit√† della relazione lineare calcoliamo la correlazione. Essendo le deviazioni standard del BDI-II e del CES-D rispettavamente uguali a 15.37 e 14.93, la correlazione diventa uguale a \\(\\frac{207.426}{15.38 \\cdot 14.93} = 0.904.\\) Tale valore √® prossimo a 1.0, il che vuol dire che i punti del diagramma a dispersione non si discostano troppo da una retta con una pendenza positiva.\nTroviamo la correlazione con la funzione corrcoef():\n\nnp.corrcoef(x, y)\n\narray([[1.        , 0.90406202],\n       [0.90406202, 1.        ]])\n\n\nReplichiamo il risultato implementando l‚Äôeq. {eq}eq-cor-def:\n\ns_xy = np.mean((x - np.mean(x)) * (y - np.mean(y)))\ns_x = x.std(ddof=0)\ns_y = y.std(ddof=0)\nr_xy = s_xy / (s_x * s_y)\nprint(r_xy)\n\n0.9040620189474861\n\n\nUn altro modo ancora per trovare la correlazione tra i punteggi BDI-II e CESD √® quello di standardizzare le due variabili per poi applicare la formula della covarianza:\n\nz_x = (x - np.mean(x)) / np.std(x, ddof=0)\nz_y = (y - np.mean(y)) / np.std(y, ddof=0)\nnp.mean(z_x * z_y)\n\n0.9040620189474862\n\n\nEsempio. Un uso interessante delle correlazioni viene fatto in un recente articolo di Guilbeault et al.¬†(2024). Il concetto di ‚Äúgender bias‚Äù si riferisce alla tendenza sistematica di favorire un sesso rispetto all‚Äôaltro, spesso a scapito delle donne. Lo studio di Guilbeault et al.¬†(2024) analizza come le immagini online influenzino la diffusione su vasta scala di questo preconcetto di genere.\nAttraverso un vasto insieme di immagini e testi raccolti online, gli autori dimostrano che sia le misurazioni basate sulle immagini che quelle basate sui testi catturano la frequenza con cui varie categorie sociali sono associate a rappresentazioni di genere, valutate su una scala da -1 (femminile) a 1 (maschile), con 0 che indica una neutralit√† di genere. Questo consente di quantificare il preconcetto di genere come una forma di bias statistico lungo tre dimensioni: la tendenza delle categorie sociali ad associarsi a un genere specifico nelle immagini e nei testi, la rappresentazione relativa delle donne rispetto agli uomini in tutte le categorie sociali nelle immagini e nei testi, e il confronto tra le associazioni di genere nei dati delle immagini e dei testi con la distribuzione empirica delle donne e degli uomini nella societ√†. Il lavoro di Guilbeault et al.¬†(2024) evidenzia che il preconcetto di genere √® molto pi√π evidente nelle immagini rispetto ai testi, come mostrato nella {numref}gender-bias-1-fig C.\nSi noti che, nel grafico della {numref}gender-bias-1-fig C, ogni punto pu√≤ essere interpretato come una misura di correlazione. La misura utilizzata da Guilbeault et al.¬†(2024) riflette il grado di associazione tra le categorie sociali e le rappresentazioni di genere presenti nelle immagini e nei testi analizzati. Quando la misura √® vicina a +1, indica una forte associazione positiva tra una categoria sociale specifica e una rappresentazione di genere maschile, mentre un valore vicino a -1 indica una forte associazione negativa con una rappresentazione di genere femminile. Un valore di 0, invece, suggerisce che non vi √® alcuna associazione tra la categoria sociale considerata e un genere specifico, indicando una sorta di neutralit√† di genere. In sostanza, questa misura di frequenza pu√≤ essere interpretata come una correlazione che riflette la tendenza delle categorie sociali a essere rappresentate in un modo o nell‚Äôaltro nelle immagini e nei testi analizzati, rispetto ai concetti di genere femminile e maschile.\n\n\n\nIl preconcetto di genere √® pi√π prevalente nelle immagini online (da Google Immagini) e nei testi online (da Google News). A. La correlazione tra le associazioni di genere nelle immagini da Google Immagini e nei testi da Google News per tutte le categorie sociali (n = 2.986), organizzate per decili. B. La forza dell‚Äôassociazione di genere in queste immagini e testi online per tutte le categorie (n = 2.986), suddivisa in base al fatto che queste categorie siano inclinate verso il femminile o il maschile. C. Le associazioni di genere per un campione di occupazioni secondo queste immagini e testi online; questo campione √® stato selezionato manualmente per evidenziare i tipi di categorie sociali e preconcetti di genere esaminati. (Figura tratta da Guilbeault et al.¬†(2024)).",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Le relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/05_correlation.html#correlazione-di-spearman",
    "href": "chapters/chapter_2/05_correlation.html#correlazione-di-spearman",
    "title": "15¬† Le relazioni tra variabili",
    "section": "15.7 Correlazione di Spearman",
    "text": "15.7 Correlazione di Spearman\nUn‚Äôalternativa per valutare la relazione lineare tra due variabili √® il coefficiente di correlazione di Spearman, che si basa esclusivamente sull‚Äôordine dei dati e non sugli specifici valori. Questo indice di associazione √® particolarmente adatto quando gli psicologi sono in grado di misurare solo le relazioni di ordine tra diverse modalit√† di risposta dei soggetti, ma non l‚Äôintensit√† della risposta stessa. Tali variabili psicologiche che presentano questa caratteristica sono definite come ‚Äúordinali‚Äù.\n\n\n\n\n\n\n√à importante ricordare che, nel caso di una variabile ordinale, non √® possibile utilizzare le statistiche descrittive convenzionali come la media e la varianza per sintetizzare le osservazioni. Tuttavia, √® possibile riassumere le osservazioni attraverso una distribuzione di frequenze delle diverse modalit√† di risposta. Come abbiamo appena visto, la direzione e l‚Äôintensit√† dell‚Äôassociazione tra due variabili ordinali possono essere descritte utilizzando il coefficiente di correlazione di Spearman.\n\n\n\nPer fornire un esempio, consideriamo due variabili di scala ordinale e calcoliamo la correlazione di Spearman tra di esse.\n\nstats.spearmanr([1, 2, 3, 4, 5], [5, 6, 7, 8, 7])\n\nSignificanceResult(statistic=0.8207826816681233, pvalue=0.08858700531354381)",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Le relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/05_correlation.html#correlazione-nulla",
    "href": "chapters/chapter_2/05_correlation.html#correlazione-nulla",
    "title": "15¬† Le relazioni tra variabili",
    "section": "15.8 Correlazione nulla",
    "text": "15.8 Correlazione nulla\nUn aspetto finale da sottolineare riguardo alla correlazione √® che essa descrive la direzione e l‚Äôintensit√† della relazione lineare tra due variabili. Tuttavia, la correlazione non cattura relazioni non lineari tra le variabili, anche se possono essere molto forti. √à fondamentale comprendere che una correlazione pari a zero non implica l‚Äôassenza di una relazione tra le due variabili, ma indica solamente l‚Äôassenza di una relazione lineare tra di esse.\nLa figura seguente fornisce tredici esempi di correlazione nulla in presenza di una chiara relazione (non lineare) tra due variabili. In questi tredici insiemi di dati i coefficienti di correlazione di Pearson sono sempre uguali a 0. Ma questo non significa che non vi sia alcuna relazione tra le variabili.\n\ndatasaurus_data = pd.read_csv(\"../data/datasaurus.csv\")\ndatasaurus_data.groupby(\"dataset\").agg(\n    {\"x\": [\"count\", \"mean\", \"std\"], \"y\": [\"count\", \"mean\", \"std\"]}\n)\n\n\n\n\n\n\n\n\n\nx\ny\n\n\n\ncount\nmean\nstd\ncount\nmean\nstd\n\n\ndataset\n\n\n\n\n\n\n\n\n\n\naway\n142\n54.266100\n16.769825\n142\n47.834721\n26.939743\n\n\nbullseye\n142\n54.268730\n16.769239\n142\n47.830823\n26.935727\n\n\ncircle\n142\n54.267320\n16.760013\n142\n47.837717\n26.930036\n\n\ndino\n142\n54.263273\n16.765142\n142\n47.832253\n26.935403\n\n\ndots\n142\n54.260303\n16.767735\n142\n47.839829\n26.930192\n\n\nh_lines\n142\n54.261442\n16.765898\n142\n47.830252\n26.939876\n\n\nhigh_lines\n142\n54.268805\n16.766704\n142\n47.835450\n26.939998\n\n\nslant_down\n142\n54.267849\n16.766759\n142\n47.835896\n26.936105\n\n\nslant_up\n142\n54.265882\n16.768853\n142\n47.831496\n26.938608\n\n\nstar\n142\n54.267341\n16.768959\n142\n47.839545\n26.930275\n\n\nv_lines\n142\n54.269927\n16.769959\n142\n47.836988\n26.937684\n\n\nwide_lines\n142\n54.266916\n16.770000\n142\n47.831602\n26.937902\n\n\nx_shape\n142\n54.260150\n16.769958\n142\n47.839717\n26.930002\n\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(4, 4, figsize=(15, 15))\ndatasets = datasaurus_data[\"dataset\"].unique()\n\nfor i, dataset in enumerate(datasets):\n    row = i // 4\n    col = i % 4\n    ax = axs[row, col]\n    subset = datasaurus_data[datasaurus_data[\"dataset\"] == dataset]\n    ax.scatter(subset[\"x\"], subset[\"y\"], alpha=0.7)\n    ax.set_title(dataset)\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n\nplt.tight_layout()\nplt.show()\n\n/var/folders/s7/z86r4t9j6yx376cm120nln6w0000gn/T/ipykernel_14701/188333220.py:14: UserWarning: The figure layout has changed to tight\n  plt.tight_layout()",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Le relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/05_correlation.html#considerazioni-conclusive",
    "href": "chapters/chapter_2/05_correlation.html#considerazioni-conclusive",
    "title": "15¬† Le relazioni tra variabili",
    "section": "15.9 Considerazioni conclusive",
    "text": "15.9 Considerazioni conclusive\nNel concludere questo capitolo dedicato allo studio di correlazione e covarianza, √® essenziale mettere in evidenza alcuni principi fondamentali. Il concetto di ‚Äúassociazione‚Äù equivale a quello di ‚Äúdipendenza‚Äù e pu√≤ manifestarsi sia attraverso cause dirette che indirette. Ci√≤ significa che variazioni in una variabile possono influenzare o essere influenzate da un‚Äôaltra, indipendentemente dalla natura diretta o mediata di questo legame.\nPer quanto riguarda la correlazione, questa descrive specifici tipi di associazione, come tendenze monotone. Ad esempio, la presenza di un incremento congiunto o di raggruppamenti in determinati intervalli tra due variabili √® indicativa di correlazione. √à per√≤ cruciale ricordare che correlazione non equivale a causalit√†; osservare una correlazione non implica necessariamente che una variabile causi l‚Äôaltra.\nNei contesti in cui il numero di variabili √® ampio rispetto alla grandezza del campione, possono emergere correlazioni elevate ma spurie. Questo accade perch√© un gran numero di variabili pu√≤ accidentalmente generare correlazioni elevate, non riflettendo un legame sostanziale tra di esse.\nD‚Äôaltra parte, con un numero elevato di osservazioni, anche le correlazioni pi√π deboli possono sembrare robuste. In campioni di grandi dimensioni, anche le pi√π piccole variazioni possono apparire accentuate rispetto alla variabilit√† generale del campione, bench√© queste correlazioni possano non avere una rilevanza pratica.\nIn conclusione, √® fondamentale adottare un approccio critico e ben informato nell‚Äôinterpretare i risultati delle analisi di correlazione e covarianza, tenendo sempre in considerazione le dimensioni del campione e il numero di variabili coinvolte. Tale prudenza aiuta a prevenire interpretazioni errate delle relazioni tra le variabili, specialmente riguardo alla causalit√†.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Le relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/05_correlation.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_2/05_correlation.html#informazioni-sullambiente-di-sviluppo",
    "title": "15¬† Le relazioni tra variabili",
    "section": "15.10 Informazioni sull‚ÄôAmbiente di Sviluppo",
    "text": "15.10 Informazioni sull‚ÄôAmbiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m\n\nLast updated: Sat Jun 08 2024\n\nPython implementation: CPython\nPython version       : 3.12.3\nIPython version      : 8.25.0\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.4.0\nMachine     : arm64\nProcessor   : arm\nCPU cores   : 8\nArchitecture: 64bit\n\nWatermark: 2.4.3\n\n\n\n\n\n\n\nZetsche, Ulrike, Paul-Christian Buerkner, and Babette Renneberg. 2019. ‚ÄúFuture Expectations in Clinical Depression: Biased or Realistic?‚Äù Journal of Abnormal Psychology 128 (7): 678.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Le relazioni tra variabili</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/06_causality.html",
    "href": "chapters/chapter_2/06_causality.html",
    "title": "16¬† Causalit√† dai dati osservazionali",
    "section": "",
    "text": "16.1 Introduzione\nPer assicurare l‚Äôintegrit√† e la validit√† scientifica dei modelli statistici, √® essenziale abbinarli a un‚Äôanalisi causale. La pura osservazione dei dati pu√≤ rivelare correlazioni e pattern nei dati, ma senza un‚Äôindagine sulle cause che stanno dietro a queste correlazioni, le conclusioni tratte possono essere fuorvianti o incomplete.\nAnche nell‚Äôambito di una discussione sull‚Äôanalisi descrittiva dei dati (e, forse, soprattutto in un tale contesto), √® importante sottolineare le limitazioni di un tale approccio. L‚Äôaspetto cruciale da tenere a mente √® che le cause dei fenomeni non possono essere inferite solamente dall‚Äôanalisi dei dati. N√© dall‚Äôanalisi descrittiva dei dati che stiamo discutendo adesso, n√© dall‚Äôanalisi inferenziale dei dati che discuteremo in seguito. Pertanto, per giungere ad una comprensione dei fenomeni √® necessario integrare il processo di modellazione statistica con una comprensione delle cause sottostanti del fenomeno oggetto d‚Äôesame. In altre parole, per una comprensione scientificamente valida, √® necessario combinare la modellazione statistica con l‚Äôanalisi causale.\nIn questo capitolo, ci concentreremo sull‚Äôintroduzione dei concetti fondamentali dell‚Äôanalisi causale, i quali costituiscono una base cruciale per l‚Äôinterpretazione dei dati. Cominceremo con la distinzione tra correlazione e causalit√†. La correlazione indica una relazione tra due variabili, mettendo in evidenza la loro forza e direzione, mentre la causalit√† implica un legame di causa ed effetto tra le variabili. √à di importanza cruciale comprendere che il fatto che due variabili siano correlate non implica automaticamente l‚Äôesistenza di un rapporto causale tra di esse. Il noto principio ‚Äúcorrelazione non implica causalit√†‚Äù sottolinea questa distinzione critica. Numerosi esempi di correlazioni che non sono basate su una relazione causale diretta possono essere esaminati sul sito spurious correlations.\nOltre a questo concetto fondamentale, procederemo ad esplorare una serie di concetti introdotti da Judea Pearl nel suo testo ‚ÄúCausality‚Äù (Pearl 2009). Questi concetti sono essenziali per descrivere le relazioni tra variabili. Essi includono strutture di relazione come biforcazioni, catene, collider e strutture discendenti. Questa cornice concettuale ci permetter√† di distinguere tra i diversi tipi di legami causali tra le variabili, ponendo cos√¨ le basi per condurre un‚Äôanalisi dei dati pi√π completa e accurata.\nMediante esempi numerici, dimostreremo l‚Äôimportanza dei fattori confondenti nell‚Äôanalisi della correlazione e della covarianza. Questo passo ci condurr√† oltre l‚Äôanalisi delle relazioni bivariate di base, introducendoci alle dinamiche pi√π complesse che regolano le relazioni tra le variabili.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Causalit√† dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/06_causality.html#inferenza-causale",
    "href": "chapters/chapter_2/06_causality.html#inferenza-causale",
    "title": "16¬† Causalit√† dai dati osservazionali",
    "section": "16.2 Inferenza Causale",
    "text": "16.2 Inferenza Causale\nL‚Äôinferenza causale si pone l‚Äôobiettivo di rappresentare il processo sottostante a un fenomeno, permettendo di prevedere gli effetti di un intervento. Oltre a prevedere le conseguenze di una causa, consente di esplorare scenari controfattuali, immaginando gli esiti alternativi che si sarebbero verificati con decisioni diverse. Questo tipo di ragionamento √® cruciale sia in contesti descrittivi che inferenziali.\nMcElreath (2020) utilizza l‚Äôanalogia dei Golem, potenti ma privi di saggezza e previsione, per descrivere un approccio limitato che √® stato a lungo lo standard in psicologia. Questo approccio si concentra sul semplice test delle ipotesi nulle e non stabilisce una relazione chiara tra le problematiche causali della ricerca e i test statistici. Tale limitazione √® stata identificata come una delle principali cause della crisi di replicabilit√† dei risultati nella ricerca psicologica (si veda il capitolo {ref}generalizability-crisis-notebook) e, di conseguenza, della crisi della psicologia stessa.\n\n\n\nEsempio di albero decisionale per la selezione di una procedura statistica appropriata. Iniziando dall‚Äôalto, l‚Äôutente risponde a una serie di domande riguardanti la misurazione e l‚Äôintento, arrivando infine al nome di una procedura. Sono possibili molti alberi decisionali simili. (Figura tratta da McElreath (2020)).\n\n\nUn problema evidenziato da McElreath (2020) √® che processi causali completamente distinti possono generare la stessa distribuzione di risultati osservati. Pertanto, un approccio focalizzato esclusivamente sul test dell‚Äôipotesi nulla non √® in grado di distinguere tra questi diversi scenari. Questa limitazione √® dimostrata numericamente nel capitolo {ref}causal-inference-notebook.\nIl test dell‚Äôipotesi nulla, ovvero l‚Äôapproccio frequentista, nonostante sia stato ampiamente utilizzato in psicologia per decenni, ha mostrato una bassa sensibilit√† nel rilevare le caratteristiche cruciali dei fenomeni studiati e un alto tasso di falsi positivi (Zwet et al. 2023).\nLa ricerca scientifica richiede una metodologia pi√π sofisticata rispetto all‚Äôapproccio che si limita a confutare ipotesi nulle. √à essenziale sviluppare modelli causali che rispondano direttamente alle domande di ricerca. Inoltre, √® fondamentale avere una strategia razionale per l‚Äôestrazione delle stime dei modelli e per la quantificazione dell‚Äôincertezza associata ad esse. In questo contesto, l‚Äôanalisi bayesiana dei dati emerge come un approccio altamente efficace. Anche se in analisi semplici le differenze rispetto all‚Äôapproccio frequentista potrebbero sembrare minime o addirittura introdurre alcune complicazioni, quando ci si trova ad affrontare analisi pi√π realistiche e complesse, la differenza diventa sostanziale.\n\n16.2.1 Passaggi Chiave per un‚ÄôAnalisi Causale\nPer condurre un‚Äôanalisi scientifica dei dati che superi l‚Äôapproccio ‚Äúamatoriale‚Äù frequentista, McElreath (2020) suggerisce di seguire una serie di passaggi chiave:\n\nComprendere il concetto teorico del fenomeno oggetto dell‚Äôanalisi.\nSviluppare modelli causali che descrivano accuratamente le relazioni tra le variabili coinvolte nel problema di ricerca, basandosi sulla teoria sottostante.\nFormulare modelli statistici appropriati che riflettano fedelmente il contesto scientifico e le relazioni causali identificate.\nEseguire simulazioni basate sui modelli causali per verificare se i modelli statistici sviluppati siano in grado di stimare correttamente ci√≤ che √® teoricamente atteso. Questa fase di verifica √® cruciale per garantire la validit√† dei modelli.\nCondurre l‚Äôanalisi dei dati effettivi utilizzando i modelli statistici sviluppati, avendo la fiducia che riflettano accuratamente le teorie sottostanti e le relazioni causali.\n\n\n\n16.2.2 Grafi Aciclici Direzionati (DAG)\nUn elemento chiave sottolineato da McElreath (2020) nel processo di analisi √® l‚Äôutilizzo dei Grafi Aciclici Direzionati (DAG), che rappresentano uno strumento essenziale per realizzare il flusso di lavoro descritto. I DAG forniscono una rappresentazione grafica dei modelli causali, consentendo una visualizzazione chiara delle relazioni tra le variabili coinvolte. Questi grafici rendono trasparenti le assunzioni alla base dell‚Äôanalisi, creando una connessione evidente tra le teorie sottostanti e l‚Äôanalisi statistica. In contrasto, l‚Äôapproccio frequentista √® caratterizzato dall‚Äôassenza di ipotesi sulle relazioni sottostanti tra le variabili, rendendo difficile comprendere e interpretare le implicazioni scientifiche dei risultati ottenuti.\nPer un approfondimento di questi temi, consiglio fortemente la lettura del primo capitolo di Statistical Rethinking.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Causalit√† dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/06_causality.html#due-paradossi-comuni",
    "href": "chapters/chapter_2/06_causality.html#due-paradossi-comuni",
    "title": "16¬† Causalit√† dai dati osservazionali",
    "section": "16.3 Due Paradossi Comuni",
    "text": "16.3 Due Paradossi Comuni\nEsistono due situazioni comuni in cui i dati possono ingannarci, e che vale la pena esaminare esplicitamente. Questi sono:\n\nIl paradosso di Simpson\nIl paradosso di Berkson\n\n\n16.3.1 Paradosso di Simpson\nIl paradosso di Simpson si verifica quando stimiamo una relazione per sottoinsiemi dei nostri dati, ma otteniamo una relazione diversa considerando l‚Äôintero dataset (Simpson 1951). √à un caso particolare della fallacia ecologica, che si verifica quando cerchiamo di fare affermazioni sugli individui basandoci sui loro gruppi. Ad esempio, potrebbe esserci una relazione positiva tra i voti universitari e la performance alla scuola di specializzazione in due dipartimenti considerati individualmente. Tuttavia, se i voti universitari tendono a essere pi√π alti in un dipartimento rispetto all‚Äôaltro, mentre la performance alla scuola di specializzazione tende a essere opposta, potremmo trovare una relazione negativa tra i voti universitari e la performance alla scuola di specializzazione.\n\n\n16.3.2 Paradosso di Berkson\nIl paradosso di Berkson si verifica quando stimiamo una relazione basandoci sul dataset che abbiamo, ma a causa della selezione del dataset, la relazione risulta diversa in un dataset pi√π generale (Berkson 1946). Ad esempio, se abbiamo un dataset di ciclisti professionisti, potremmo non trovare una relazione tra il loro VO2 max e la possibilit√† di vincere una gara di ciclismo (Coyle et al.¬†1988; Podlogar, Leo, and Spragg 2022). Tuttavia, se avessimo un dataset della popolazione generale, potremmo trovare una relazione tra queste due variabili. Il dataset professionale √® cos√¨ selezionato che la relazione scompare; non si pu√≤ diventare ciclisti professionisti senza avere un VO2 max adeguato, ma tra i ciclisti professionisti, tutti hanno un VO2 max sufficiente.\n\n\n16.3.3 I Disegni di Ricerca e la Causalit√†\nPer comprendere meglio l‚Äôuso dei DAG nell‚Äôinferenza causale, √® necessario distinguere tra ricerche basate su disegni osservazionali e sperimentali (Rohrer 2018).\nNei disegni osservazionali i ricercatori osservano e registrano gli eventi senza intervenire direttamente. A differenza degli esperimenti controllati, qui non si manipolano le variabili studiate, ma si osservano le relazioni naturali che emergono. Questi studi sono preziosi quando gli esperimenti non sono fattibili per motivi etici o pratici. Tuttavia, sebbene siano efficaci nell‚Äôidentificare correlazioni e tendenze, i disegni osservazionali spesso mancano della capacit√† di stabilire con certezza relazioni causali, a causa di variabili confondenti e bias di selezione. Esempi comuni di disegni osservazionali includono studi trasversali e longitudinali.\nI disegni sperimentali, d‚Äôaltra parte, si basano sulla manipolazione controllata delle variabili e sulla randomizzazione. Quest‚Äôultima, ovvero l‚Äôassegnazione casuale dei partecipanti ai gruppi, √® fondamentale per ridurre i bias e garantire la validit√† delle conclusioni causali. Gli esperimenti randomizzati sono considerati il gold standard per indagare le relazioni causali, poich√© la randomizzazione consente di bilanciare gli effetti delle variabili osservate e non osservate tra i diversi gruppi di trattamento.\nNonostante gli esperimenti offrano un elevato grado di certezza nella determinazione delle relazioni causali, i disegni osservazionali presentano vantaggi in termini di flessibilit√† e applicabilit√† in situazioni in cui gli esperimenti non possono essere condotti per ragioni etiche o pratiche. {cite:p}rohrer2018thinking nota che, mentre i disegni osservazionali spesso si basano su relazioni associative piuttosto che causali, gli esperimenti potrebbero non consentire una generalizzazione al di l√† delle condizioni artificiali del laboratorio.\nNel contesto della ricerca, il concetto di causalit√† si riferisce alla relazione tra una variabile (X) e un‚Äôaltra (Y), in cui X pu√≤ influenzare Y. Il linguaggio causale utilizza termini come ‚Äúcausa‚Äù, ‚Äúinfluenza‚Äù e ‚Äúdetermina‚Äù, a differenza di termini descrittivi come ‚Äúassociato‚Äù o ‚Äúcorrelato‚Äù, che indicano semplicemente relazioni senza implicare un legame causale diretto. √à importante ribadire che la causalit√† √® concepita come un concetto probabilistico, il che significa che una variazione in X aumenta la probabilit√† di un certo risultato in Y, piuttosto che determinarlo in modo assoluto.\nNel campo della ricerca psicologica, spesso √® necessario inferire relazioni causali da dati osservazionali poich√© gli esperimenti randomizzati non sono sempre praticabili o eticamente accettabili. Tuttavia, questo approccio presenta diverse sfide che possono influenzare l‚Äôintegrit√† e l‚Äôaccuratezza delle inferenze tratte dagli studi. Per affrontare tali sfide e migliorare la solidit√† delle loro analisi, i ricercatori impiegano una serie di strategie metodologiche.\n\nUtilizzo di interventi surrogati: questa tecnica √® preziosa in situazioni in cui la manipolazione diretta di una variabile di interesse √® impraticabile o eticamente inaccettabile. Piuttosto che intervenire direttamente, i ricercatori si avvalgono di variabili surrogate (o proxy) che presumibilmente sono associate alla variabile di interesse principale. L‚Äôobiettivo √® esplorare e dedurre gli effetti indiretti di una variabile su un‚Äôaltra, stabilendo associazioni che, sebbene non dirette, possono fornire importanti informazioni sul fenomeno in esame. Tuttavia, questa strategia presenta delle sfide intrinseche, come la difficolt√† nel garantire una relazione diretta tra la variabile surrogata e quella di interesse principale.\nLinguaggio cauto nell‚Äôinterpretazione dei dati: i ricercatori adottano un linguaggio prudente quando interpretano dati osservazionali, evitando di fare affermazioni causali dirette quando i dati mostrano solo correlazioni.\nUtilizzo di metodi statistici avanzati per controllare variabili confondenti: Le variabili confondenti sono fattori esterni che possono influenzare la relazione tra le variabili studiate. L‚Äôutilizzo di tecniche statistiche avanzate, come la regressione multipla o l‚Äôanalisi di matching, aiuta a controllare l‚Äôeffetto di queste variabili confondenti, consentendo una stima pi√π accurata della relazione tra le variabili di interesse. Tuttavia, questi metodi hanno limitazioni (si veda ?sec-causal-inference) e potrebbero non riuscire a controllare completamente tutte le variabili confondenti, specialmente in situazioni complesse.\n\nRohrer (2018) sottolinea che, nonostante l‚Äôadozione di queste strategie, persistono sfide in termini di validit√† esterna (cio√® la generalizzabilit√† dei risultati) e interpretazione dei dati. La validit√† esterna potrebbe essere limitata poich√© le condizioni specifiche dello studio potrebbero non rappresentare situazioni pi√π ampie o diverse, mentre l‚Äôinterpretazione dei risultati pu√≤ essere complicata dalla natura indiretta o surrogata delle evidenze raccolte.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Causalit√† dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/06_causality.html#modelli-causali-grafici",
    "href": "chapters/chapter_2/06_causality.html#modelli-causali-grafici",
    "title": "16¬† Causalit√† dai dati osservazionali",
    "section": "16.4 Modelli Causali Grafici",
    "text": "16.4 Modelli Causali Grafici\nL‚Äôanalisi dei dati osservazionali per trarre inferenze causali √® intrinsecamente complessa, ma con precauzioni adeguate e approcci metodologici appropriati, √® possibile estrarre preziosi insight causali anche da questo tipo di dati. In questo contesto, Rohrer (2018) sottolinea l‚Äôimportanza dei DAG come strumento fondamentale per la rappresentazione visiva delle ipotesi causali. Questi diagrammi non solo agevolano la comprensione delle relazioni causali tra le variabili, ma forniscono anche una guida per l‚Äôidentificazione e il trattamento corretto delle variabili esterne, comunemente note come ‚Äúvariabili di confondimento‚Äù.\nUna delle funzioni pi√π cruciali dei DAG √® la loro capacit√† di individuare quali variabili di confondimento debbano essere considerate nell‚Äôanalisi e quali possano essere trascurate senza compromettere l‚Äôintegrit√† dell‚Äôinferenza causale. Questa caratteristica √® essenziale poich√© sia il controllo insufficiente sia quello eccessivo delle variabili di confondimento possono condurre a conclusioni erronee. In particolare, i DAG aiutano a determinare quando l‚Äôaggiustamento per determinate variabili di confondimento pu√≤ migliorare l‚Äôaccuratezza delle inferenze causali e quando, al contrario, tale aggiustamento potrebbe introdurre distorsioni (questo punto verr√† approfondito nel capitolo sec-causal-inference).\nGrazie ai DAG, i ricercatori possono esplicitare le loro ipotesi sulle relazioni causali tra le variabili, riducendo cos√¨ il rischio di interpretare erroneamente i dati osservazionali. Questo approccio consente di sviluppare strategie di analisi pi√π informate e metodologicamente solide, mirate a comprendere al meglio le dinamiche causali sottostanti.\n\n16.4.1 Importanza delle Assunzioni\n√à cruciale comprendere che l‚Äôestrazione di conclusioni causali da dati correlazionali non pu√≤ essere basata esclusivamente sui dati stessi. √à invece necessario possedere delle conoscenze sulle dinamiche causali del fenomeno esaminato. Questo principio deriva dalla consapevolezza che una correlazione osservata pu√≤ essere attribuita a una vasta gamma di fattori, inclusa l‚Äôinfluenza potenziale di variabili terze non considerate nell‚Äôanalisi. Di conseguenza, qualsiasi tentativo di inferenza causale privo di una comprensione preliminare delle possibili relazioni causali rischia di essere fallace.\nNonostante queste sfide, √® fondamentale non rinunciare davanti alla complessit√† intrinseca della ricerca osservazionale. Pur non potendo dimostrare inequivocabilmente la causalit√†, per via della loro incapacit√† di manipolare direttamente le variabili di interesse, gli studi osservazionali svolgono un ruolo irrinunciabile nel contesto scientifico. Questi studi sono fondamentali per la generazione di ipotesi e per guidare la ricerca futura verso domande scientifiche rilevanti. Allo stesso modo, gli studi sperimentali, bench√© considerati il gold standard per la determinazione delle relazioni causali, non sono esenti da limitazioni. Gli esperimenti, soprattutto quelli condotti in ambienti altamente controllati come i laboratori, presuppongono la generalizzabilit√† dei risultati al di l√† dei confini dello studio, un‚Äôassunzione che pu√≤ non sempre trovare riscontro nella realt√†.\nIn questo contesto, ci√≤ che rende un‚Äôindagine rigorosa, sia essa osservazionale che sperimentale, √® la capacit√† di riconoscere, articolare e comunicare chiaramente le premesse su cui si basa. Questo approccio non solo facilita una valutazione critica delle conclusioni causali da parte della comunit√† scientifica, ma promuove anche un dialogo costruttivo e produttivo all‚Äôinterno della stessa, fondamentale per il progresso della conoscenza. La trasparenza nell‚Äôesposizione delle premesse rappresenta un prerequisito essenziale per un avanzamento scientifico informato, consapevole e, soprattutto, veritiero.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Causalit√† dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/06_causality.html#introduzione-ai-dag",
    "href": "chapters/chapter_2/06_causality.html#introduzione-ai-dag",
    "title": "16¬† Causalit√† dai dati osservazionali",
    "section": "16.5 Introduzione ai DAG",
    "text": "16.5 Introduzione ai DAG\nI Grafi Aciclici Diretti (DAG) offrono una rappresentazione grafica delle relazioni causali ipotizzate tra le variabili. In un DAG, le frecce vengono utilizzate per indicare le relazioni causali tra diverse variabili.\nSono chiamati cos√¨ perch√©:\n\nLe variabili presenti nel diagramma (nodi) possono essere collegate tra loro solo attraverso frecce, anzich√© semplici linee di collegamento, da cui il termine ‚Äúdiretti‚Äù.\nPartendo da un nodo, non √® possibile ritornare allo stesso nodo seguendo il percorso delle frecce, da cui il termine ‚Äúaciclici‚Äù.\n\nLe variabili sono chiamate nodi del DAG. Un cammino tra due nodi √® una sequenza di frecce che collegano i due nodi, indipendentemente dalla direzione delle frecce.\nAll‚Äôinterno di un DAG, la freccia che va dal nodo \\(X\\) al nodo \\(Y\\) evidenzia una relazione causale, implicando che \\(X\\) abbia un‚Äôinfluenza su \\(Y\\). Tuttavia, questa relazione non √® deterministica; piuttosto, le variazioni nel livello di \\(X\\) sono associate a cambiamenti in \\(Y\\) su base probabilistica. Pertanto, la presenza della freccia nel DAG causale indica che un aumento di \\(X\\) aumenta la probabilit√† che \\(Y\\) aumenti, ma non lo garantisce.\nSe tra due nodi c‚Äô√® una sola freccia, il nodo dal quale parte la freccia √® detto ‚Äúgenitore‚Äù, mentre quello di arrivo √® detto ‚Äúfiglio‚Äù. Se, partendo da un nodo A, si arriva a un nodo B seguendo il verso di una successione di frecce, allora il nodo A √® detto ‚Äúantenato‚Äù, mentre B √® detto ‚Äúdiscendente‚Äù.\nI DAG permettono di identificare quali variabili agiscono come confondenti e quali no, basandosi sulla teoria sviluppata da Judea Pearl (Pearl 2009). √à importante rappresentare tutte le possibili relazioni nel DAG, poich√© l‚Äôassenza di una freccia tra due variabili implica una certezza dell‚Äôassenza di relazione tra di esse.\nDue dei concetti fondamentali della teoria dei DAG sono la d-separazione e il back-door path.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Causalit√† dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/06_causality.html#concetto-di-d-separazione",
    "href": "chapters/chapter_2/06_causality.html#concetto-di-d-separazione",
    "title": "16¬† Causalit√† dai dati osservazionali",
    "section": "16.6 Concetto di d-separazione",
    "text": "16.6 Concetto di d-separazione\nLa d-separazione √® un concetto fondamentale per comprendere come le variabili all‚Äôinterno di un DAG possano influenzarsi reciprocamente. Essa definisce se un insieme di variabili pu√≤ bloccare l‚Äôinfluenza di una variabile su un‚Äôaltra. In altre parole, la d-separazione ci aiuta a capire se un percorso che collega due variabili √® attivo o inattivo, considerando un insieme di nodi specifico.\n\n16.6.1 Definizione di d-separazione\nUn cammino tra due nodi √® d-separato (o bloccato) da un insieme Œõ di nodi se e solo se si verificano le seguenti condizioni:\n\nCatena (Sequenza di frecce): Se il cammino √® costituito da una sequenza di frecce nella stessa direzione (ad esempio, \\(X \\rightarrow Z \\rightarrow Y\\)), il nodo intermedio (\\(Z\\)) deve appartenere all‚Äôinsieme Œõ affinch√© il cammino sia bloccato. Questo significa che la variabile \\(Z\\) deve essere inclusa nell‚Äôanalisi per bloccare l‚Äôinfluenza di \\(X\\) su \\(Y\\).\nFork (Nodo con due frecce in uscita): Se nel cammino c‚Äô√® un nodo da cui partono due frecce (ad esempio, \\(X \\leftarrow Z \\rightarrow Y\\)), il nodo di partenza delle frecce (\\(Z\\)) deve appartenere all‚Äôinsieme Œõ affinch√© il cammino sia bloccato. In questo caso, \\(Z\\) agisce come una variabile confondente, e includerla nell‚Äôanalisi blocca il percorso.\nCollider (Nodo con due frecce in entrata): Se nel cammino c‚Äô√® un nodo in cui convergono due frecce (ad esempio, \\(X \\rightarrow Z \\leftarrow Y\\)), n√© il nodo di convergenza (\\(Z\\)) n√© i suoi discendenti devono appartenere all‚Äôinsieme Œõ affinch√© il cammino sia bloccato. Un collider introduce un‚Äôassociazione tra \\(X\\) e \\(Y\\) solo se \\(Z\\) o i suoi discendenti sono inclusi nell‚Äôanalisi.\n\nLa d-separazione √® uno strumento potente per identificare i percorsi di confondimento e determinare quali variabili devono essere considerate nel modello statistico per ottenere inferenze causali accurate.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Causalit√† dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/06_causality.html#il-criterio-del-back-door",
    "href": "chapters/chapter_2/06_causality.html#il-criterio-del-back-door",
    "title": "16¬† Causalit√† dai dati osservazionali",
    "section": "16.7 Il criterio del back-door",
    "text": "16.7 Il criterio del back-door\nIl criterio del back-door √® uno strumento grafico utile per individuare le variabili confondenti che devono essere considerate per ottenere stime causali corrette. Questo criterio si basa sulla verifica dei percorsi non causali, detti ‚Äúback-door paths‚Äù, che collegano l‚Äôesposizione all‚Äôoutcome.\nIl criterio del back-door pu√≤ essere applicato mediante i seguenti passaggi:\n\nEliminare le frecce dirette: Rimuovere tutte le frecce che vanno dall‚Äôesposizione all‚Äôoutcome nel grafo.\nVerificare i percorsi non bloccati: Controllare se esistono percorsi non bloccati tra l‚Äôesposizione e l‚Äôoutcome nel grafo modificato.\n\nSe tutti i percorsi sono bloccati, significa che non c‚Äô√® confondimento, e quindi l‚Äôesposizione e l‚Äôoutcome sono indipendenti. Se invece esistono percorsi non bloccati, √® necessario individuare e aggiustare per le variabili che possono bloccarli.\n\n16.7.1 Esempi di applicazione del criterio del back-door\nConsideriamo un DAG che rappresenta la relazione tra autostima (A) e performance accademica (P), tenendo conto di variabili come il supporto familiare (S) e il livello di stress (L). Il DAG √® strutturato come segue:\n\nA ‚Üí P\nS ‚Üí A\nS ‚Üí P\nL ‚Üê S\n\nRimuovendo la freccia diretta A ‚Üí P, rimangono i percorsi:\n\nA ‚Üê S ‚Üí P\nA ‚Üê S ‚Üí L ‚Üí P\n\nPer bloccare questi percorsi, √® necessario condizionare su S, che blocca entrambi i percorsi. Pertanto, per ottenere una stima non distorta della relazione tra autostima e performance accademica, √® sufficiente aggiustare per il supporto familiare (S).\n\n16.7.1.1 Esempio 2: Relazione tra attivit√† fisica e benessere psicologico\nConsideriamo un DAG con le seguenti relazioni:\n\nAttivit√† fisica (F) ‚Üí Benessere psicologico (B)\nGenere (G) ‚Üí F\nGenere (G) ‚Üí B\nEt√† (E) ‚Üí F\nEt√† (E) ‚Üí B\n\nIn questo caso, abbiamo i percorsi:\n\nF ‚Üê G ‚Üí B\nF ‚Üê E ‚Üí B\n\nRimuovendo la freccia diretta F ‚Üí B, rimangono i percorsi non bloccati:\n\nF ‚Üê G ‚Üí B\nF ‚Üê E ‚Üí B\n\nPer bloccare questi percorsi, √® necessario condizionare sia su G (genere) che su E (et√†). Pertanto, per ottenere una stima non distorta della relazione tra attivit√† fisica e benessere psicologico, √® necessario aggiustare per le variabili genere ed et√†.\n\n\n\n16.7.2 Procedura per determinare l‚Äôinsieme sufficiente di variabili\nPer decidere se un insieme di variabili √® sufficiente per l‚Äôaggiustamento, √® possibile seguire questi passaggi:\n\nEliminare le frecce dell‚Äôesposizione: Rimuovere tutte le frecce che partono dalla variabile di esposizione.\nAggiungere archi di controllo: Aggiungere tutti gli archi necessari generati dal controllo sulle variabili dell‚Äôinsieme considerato.\nVerificare i percorsi non bloccati: Se tutti i percorsi non bloccati tra l‚Äôesposizione e l‚Äôoutcome contengono almeno una variabile dell‚Äôinsieme considerato, allora l‚Äôinsieme √® sufficiente per il controllo.\n\nL‚Äôinsieme pu√≤ anche essere vuoto se non √® necessario aggiustare per alcuna variabile.\nIn sintesi, il criterio del back-door consente di ridurre il numero di variabili da considerare per l‚Äôaggiustamento, risolvendo un importante problema pratico. Identificare l‚Äôinsieme sufficiente minimale di variabili aiuta a mantenere l‚Äôanalisi gestibile e precisa, migliorando la validit√† delle inferenze causali. Questo √® particolarmente utile in psicologia, dove le variabili confondenti sono spesso numerose e complesse.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Causalit√† dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/06_causality.html#strutture-causali-elementari",
    "href": "chapters/chapter_2/06_causality.html#strutture-causali-elementari",
    "title": "16¬† Causalit√† dai dati osservazionali",
    "section": "16.8 Strutture Causali Elementari",
    "text": "16.8 Strutture Causali Elementari\nAll‚Äôinterno dei DAG causali, si possono riconoscere quattro tipologie fondamentali di strutture causali: confondente, catena, collider e discendenti. La comprensione di queste strutture causali di base √® fondamentale per analizzare i percorsi causali pi√π complessi.\n\n\n\nI quattro confondenti di base. (Figura tratta da McElreath (2020)).\n\n\n\n16.8.1 Catena\nIl concetto di catena (pipe) in un DAG descrive una sequenza in cui una variabile \\(X\\) influisce su una variabile \\(Y\\) attraverso una variabile intermedia \\(Z\\), formando una catena causale del tipo \\(X \\rightarrow Z \\rightarrow Y\\). In questo scenario, \\(Z\\) agisce come mediatore della relazione causale tra \\(X\\) e \\(Y\\). Controllando per \\(Z\\), la relazione diretta osservata tra \\(X\\) e \\(Y\\) scompare perch√© \\(X\\) influisce su \\(Y\\) esclusivamente attraverso \\(Z\\).\nIn sintesi,\n\n\\(X\\) e \\(Y\\) sono associati: \\(X \\not\\perp Y\\).\nL‚Äôinfluenza di \\(X\\) su \\(Y\\) viene trasmessa tramite \\(Z\\).\nStratificando i dati in base a \\(Z\\), l‚Äôassociazione tra \\(X\\) e \\(Y\\) scompare: \\(X \\perp Y \\mid Z\\).\n\nEsempio. Consideriamo l‚Äôeffetto dell‚Äôeducazione sull‚Äôoccupabilit√† attraverso l‚Äôacquisizione di competenze specifiche.\nVariabili: - \\(X\\): Livello di Educazione (ad es., nessun diploma, diploma di scuola superiore, laurea) - \\(Z\\): Competenze Specifiche Acquisite (ad es., competenze informatiche, competenze linguistiche, competenze professionali specifiche) - \\(Y\\): Occupabilit√† (misurata come probabilit√† di ottenere un lavoro nel proprio campo di studio entro un certo periodo dopo il completamento degli studi)\nRelazione Causale: Il livello di educazione (\\(X\\)) influisce sull‚Äôoccupabilit√† (\\(Y\\)) attraverso l‚Äôacquisizione di competenze specifiche (\\(Z\\)). In altre parole, non √® tanto il titolo di studio in s√© a determinare direttamente l‚Äôoccupabilit√†, quanto le competenze specifiche acquisite durante il percorso educativo. Questo √® un classico esempio di relazione di tipo ‚Äúpipe‚Äù.\nScenario senza Stratificazione: Senza stratificare per \\(Z\\), si potrebbe osservare una correlazione positiva tra \\(X\\) e \\(Y\\), suggerendo che un maggiore livello di educazione √® associato a una maggiore occupabilit√†. Tuttavia, questa osservazione non chiarisce se il titolo di studio in s√© sia il fattore determinante o se ci√≤ dipenda dalle competenze acquisite.\nScenario con Stratificazione per \\(Z\\): Quando si stratifica per le competenze specifiche acquisite (\\(Z\\)), controllando quindi per il tipo e il livello di competenze ottenute, la relazione diretta tra il livello di educazione e l‚Äôoccupabilit√† potrebbe scomparire o indebolirsi fortemente. Questo indica che, una volta considerate le competenze acquisite, il livello di educazione in s√© non ha un impatto diretto sull‚Äôoccupabilit√†. L‚Äôeffetto osservato del livello di educazione sull‚Äôoccupabilit√† √® quindi mediato completamente dalle competenze specifiche acquisite.\nQuesto esempio mostra come, in una relazione a catena, la variabile mediatrice (\\(Z\\)) spiega completamente la relazione tra la variabile indipendente (\\(X\\)) e la variabile dipendente (\\(Y\\)).\n\n\n16.8.2 Confondente\nIl concetto di confondente (fork) si riferisce a una struttura in cui una variabile comune \\(Z\\) causa due variabili \\(X\\) e \\(Y\\), formando una relazione del tipo \\(X \\leftarrow Z \\rightarrow Y\\). In questo scenario, \\(Z\\) √® una variabile confondente che pu√≤ generare una correlazione apparente tra \\(X\\) e \\(Y\\), anche se non esiste una relazione causale diretta tra di loro. Analizzare adeguatamente il ruolo di \\(Z\\) √® fondamentale per comprendere le vere relazioni causali tra \\(X\\) e \\(Y\\).\nEsempio. Consideriamo l‚Äôinfluenza dell‚Äôet√† sul numero di scarpe e abilit√† matematiche.\nVariabili: - \\(Z\\): Et√† (variabile confondente) - \\(X\\): Numero di Scarpe - \\(Y\\): Abilit√† Matematiche\nRelazione Causale: In questo esempio, l‚Äôet√† (\\(Z\\)) √® la variabile confondente che influisce sia sul numero di scarpe (\\(X\\)) che sulle abilit√† matematiche (\\(Y\\)). Con l‚Äôaumentare dell‚Äôet√†, generalmente aumenta il numero di scarpe a causa della crescita fisica e migliorano le abilit√† matematiche grazie all‚Äôapprendimento scolastico e allo sviluppo cognitivo.\nSenza Considerare l‚ÄôEt√†: Se non consideriamo l‚Äôet√† (\\(Z\\)) nell‚Äôanalisi, potremmo erroneamente concludere che esiste una relazione diretta tra il numero di scarpe (\\(X\\)) e le abilit√† matematiche (\\(Y\\)), dato che entrambi aumentano nel tempo. Tuttavia, questa correlazione non implica una relazione causale diretta tra il numero di scarpe e le abilit√† matematiche, ma √® piuttosto il risultato dell‚Äôinfluenza dell‚Äôet√†.\nConsiderando l‚ÄôEt√†: Una volta che l‚Äôet√† viene inclusa nell‚Äôanalisi come variabile confondente, diventa chiaro che qualsiasi correlazione osservata tra il numero di scarpe e le abilit√† matematiche √® spiegata dalla loro relazione comune con l‚Äôet√†. Stratificando per et√† o utilizzando metodi statistici per controllare l‚Äôeffetto dell‚Äôet√†, possiamo correttamente interpretare che non esiste una relazione causale diretta tra il numero di scarpe e le abilit√† matematiche.\nQuesto esempio illustra l‚Äôimportanza di identificare e controllare le variabili confondenti nelle analisi causali.\n\n\n16.8.3 Collider\nLa struttura causale nota come collider si verifica quando due variabili, \\(X\\) e \\(Y\\), convergono influenzando una terza variabile, \\(Z\\), formando una configurazione del tipo \\(X \\rightarrow Z \\leftarrow Y\\). In questo schema, \\(X\\) e \\(Y\\) sono indipendenti, il che significa che non esiste una relazione causale diretta o un‚Äôinfluenza reciproca evidente tra di loro. Sebbene entrambe influenzino \\(Z\\), non si pu√≤ dedurre l‚Äôesistenza di un legame causale diretto tra \\(X\\) e \\(Y\\) basandosi esclusivamente sulla loro comune influenza su \\(Z\\).\nEsempio. Consideriamo quali variabili, il talento musicale, il supporto familiare e il successo in una carriera musicale.\nVariabili: - \\(X\\): Talento Musicale - \\(Y\\): Supporto Familiare - \\(Z\\): Successo nella Carriera Musicale\nRelazione Causale: In questo esempio, sia il talento musicale (\\(X\\)) che il supporto familiare (\\(Y\\)) influenzano il successo nella carriera musicale (\\(Z\\)). Tuttavia, il talento musicale e il supporto familiare sono indipendenti l‚Äôuno dall‚Äôaltro. Non esiste una relazione causale diretta tra il talento musicale e il supporto familiare.\nSenza Considerare \\(Z\\): \\(X\\) e \\(Y\\) sono indipendenti (\\(X \\perp Y\\)). Il talento musicale e il supporto familiare non hanno alcuna influenza reciproca diretta.\nConsiderando \\(Z\\): Quando ci si concentra esclusivamente sulle persone che hanno raggiunto il successo musicale (\\(Z\\)), si potrebbe erroneamente percepire un‚Äôassociazione tra talento musicale e supporto familiare. Questo √® conosciuto come bias del collider. Questa associazione artificiale emerge perch√© stiamo selezionando un sottogruppo basato su \\(Z\\), che √® influenzato sia da \\(X\\) che da \\(Y\\). Pertanto, quando si stratifica per \\(Z\\), \\(X\\) e \\(Y\\) appaiono associati (\\(X \\not\\perp Y \\mid Z\\)).\nIn sintesi,\n\nIndipendenza: \\(X\\) e \\(Y\\) sono indipendenti: \\(X \\perp Y\\).\nInfluenza: Sia \\(X\\) che \\(Y\\) influenzano \\(Z\\).\nAssociazione Spuria: Stratificando i dati in base a \\(Z\\), emerge un‚Äôassociazione spuria: \\(X \\not\\perp Y \\mid Z\\).\n\nIl bias del collider si verifica quando la selezione di casi basata su \\(Z\\) introduce un‚Äôassociazione artificiale tra \\(X\\) e \\(Y\\), che non riflette le relazioni presenti nell‚Äôintera popolazione. Questo pu√≤ portare a interpretazioni errate e conclusioni fuorvianti.\n\n\n16.8.4 Il Discendente\nLa configurazione del discendente √® caratterizzata dalla presenza di una variabile, \\(Z\\), che riceve l‚Äôinfluenza da una variabile \\(X\\) e trasmette tale effetto a una variabile \\(Y\\). Inoltre, \\(Z\\) esercita un‚Äôinfluenza diretta su un‚Äôaltra variabile, \\(A\\). Pertanto, \\(Z\\) agisce sia come mediatore nella relazione causale tra \\(X\\) e \\(Y\\), sia come collegamento causale diretto con \\(A\\). La struttura risultante pu√≤ essere rappresentata come \\(X \\rightarrow Z \\rightarrow Y\\), con una ramificazione aggiuntiva da \\(Z\\) ad \\(A\\) (\\(Z \\rightarrow A\\)).\nEsempio. Consideriamo l‚Äôeffetto del supporto sociale sulla felicit√† attraverso l‚Äôautostima.\nVariabili: - \\(X\\): Supporto Sociale - \\(Z\\): Autostima - \\(Y\\): Felicit√† - \\(A\\): Livello di Stress\nRelazione Causale: Il supporto sociale (\\(X\\)) migliora l‚Äôautostima (\\(Z\\)), che a sua volta influisce sulla felicit√† (\\(Y\\)) e sul livello di stress (\\(A\\)). In questo caso, \\(Z\\) agisce come mediatore tra \\(X\\) e \\(Y\\) e ha un effetto diretto su \\(A\\).\nSenza Considerare \\(A\\): Concentrandosi sul percorso \\(X \\rightarrow Z \\rightarrow Y\\), si pu√≤ isolare l‚Äôeffetto del supporto sociale sulla felicit√† attraverso l‚Äôautostima, mantenendo chiara la relazione causale.\nConsiderando \\(A\\): Quando si include \\(A\\) nell‚Äôanalisi, si introduce complessit√† poich√© \\(A\\) pu√≤ aprire nuovi percorsi causali o introdurre bias, complicando l‚Äôinterpretazione dell‚Äôeffetto diretto di \\(X\\) su \\(Y\\). Se \\(A\\) ha un discendente \\(D\\) che funge da collider, influenzato sia da \\(A\\) che da una causa comune non osservata con \\(Y\\) (\\(U\\)), condizionare su \\(D\\) pu√≤ introdurre un bias aprendo un percorso non causale (\\(A \\rightarrow D \\leftarrow U \\rightarrow Y\\)). Questo esemplifica come l‚Äôinclusione di discendenti e colliders possa alterare l‚Äôinterpretazione degli effetti causali.\nIn sintesi,\n\n\\(X\\) e \\(Y\\) sono associati causalmente attraverso \\(Z\\): \\(X \\not\\perp Y\\).\n\\(A\\) fornisce informazioni su \\(Z\\).\nStratificando i dati in base ad \\(A\\), l‚Äôassociazione tra \\(X\\) e \\(Y\\) pu√≤ indebolirsi o scomparire: \\(X \\perp Y \\mid A\\).\n\nComprendere il ruolo dei discendenti e dei collider √® cruciale per evitare interpretazioni errate dei dati e per fare inferenze causali accurate.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Causalit√† dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/06_causality.html#considerazioni-conclusive",
    "href": "chapters/chapter_2/06_causality.html#considerazioni-conclusive",
    "title": "16¬† Causalit√† dai dati osservazionali",
    "section": "16.9 Considerazioni Conclusive",
    "text": "16.9 Considerazioni Conclusive\nNegli studi basati su dati osservazionali, districarsi tra le complesse maglie delle inferenze causali richiede un‚Äôacuta analisi e una particolare sensibilit√† verso vari aspetti critici del processo di ricerca. Ogni fase, dall‚Äôidentificazione dei fattori confondenti all‚Äôanalisi accurata dei percorsi causali, passando per la gestione dei mediatori e dei collider, fino alla valutazione della robustezza interna ed esterna degli studi, presenta sfide e opportunit√†.\nIl primo passo verso un‚Äôinterpretazione corretta dei dati osservazionali consiste nel saper riconoscere e gestire le variabili confondenti. Queste variabili, agendo su entrambe le variabili indipendenti e dipendenti, possono generare correlazioni ingannevoli che mascherano la vera natura delle relazioni in esame.\nL‚Äôuso dei DAG (Grafi Aciclici Diretti) emerge come uno strumento molto utile in questo contesto, offrendo una rappresentazione visiva dei percorsi causali e fornendo indicazioni preziose su come evitare trappole analitiche quali il sovracontrollo o il controllo inappropriato di variabili post-trattamento.\nQuando si tratta di mediatori e collider, il terreno si fa ancora pi√π insidioso. I mediatori, essendo ponti tra cause ed effetti, necessitano di una gestione oculata per non perdere di vista l‚Äôeffetto causale che si intende esplorare. Allo stesso tempo, √® fondamentale resistere alla tentazione di controllare indiscriminatamente i collider, poich√© ci√≤ pu√≤ aprire la porta a correlazioni spurie che distorcono la realt√† dei fenomeni studiati.\nLa questione del bilanciamento tra validit√† interna ed esterna ricorda che, sebbene gli esperimenti randomizzati possano offrire garanzie solide di validit√† interna, la loro capacit√† di generalizzare i risultati al mondo esterno ‚Äì la validit√† esterna ‚Äì non √® sempre garantita. Solo un approccio che valorizza la diversit√† metodologica, abbracciando tanto gli studi sperimentali quanto quelli osservazionali, pu√≤ rispondere in modo completo alle domande di ricerca.\nIn conclusione, per studiare con successo le dinamiche nascoste che regolano le relazioni tra le variabili, √® essenziale non solo un‚Äôattenta valutazione dei dati disponibili ma anche una profonda riflessione sulle strutture causali in gioco. Integrare diversi approcci di ricerca arricchisce la nostra comprensione e ci avvicina a conclusioni causali solide e generalizzabili.\nUn sommario ironico di questi concetti √® fornito nella vignetta di xkcd.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Causalit√† dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_2/06_causality.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_2/06_causality.html#informazioni-sullambiente-di-sviluppo",
    "title": "16¬† Causalit√† dai dati osservazionali",
    "section": "16.10 Informazioni sull‚ÄôAmbiente di Sviluppo",
    "text": "16.10 Informazioni sull‚ÄôAmbiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m\n\nLast updated: Sat Feb 03 2024\n\nPython implementation: CPython\nPython version       : 3.11.7\nIPython version      : 8.19.0\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.3.0\nMachine     : x86_64\nProcessor   : i386\nCPU cores   : 8\nArchitecture: 64bit\n\nnumpy     : 1.26.2\nseaborn   : 0.13.0\nscipy     : 1.11.4\nmatplotlib: 3.8.2\narviz     : 0.17.0\ngraphviz  : 0.20.1\npandas    : 2.1.4\n\nWatermark: 2.4.3\n\n\n\n\n\n\n\nAlexander, Rohan. 2023. Telling Stories with Data: With Applications in r. Chapman; Hall/CRC.\n\n\nDagan, Noa, Noam Barda, Eldad Kepten, Oren Miron, Shay Perchik, Mark Katz, Miguel Hern√°n, Marc Lipsitch, Ben Reis, and Ran Balicer. 2021. ‚ÄúBNT162b2 mRNA Covid-19 Vaccine in a Nationwide Mass Vaccination Setting.‚Äù New England Journal of Medicine 384 (15): 1412‚Äì23. https://doi.org/10.1056/NEJMoa2101765.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. 1st ed. Chapman & Hall. https://theeffectbook.net.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd Edition. Boca Raton, Florida: CRC Press.\n\n\nPearl, Judea. 2009. Causality. Cambridge University Press.\n\n\nRiederer, Emily. 2021. ‚ÄúCausal Design Patterns for Data Analysts,‚Äù January. https://emilyriederer.netlify.app/post/causal-design-patterns/.\n\n\nRohrer, Julia M. 2018. ‚ÄúThinking Clearly about Correlations and Causation: Graphical Causal Models for Observational Data.‚Äù Advances in Methods and Practices in Psychological Science 1 (1): 27‚Äì42.\n\n\nZwet, Erik van, Andrew Gelman, Sander Greenland, Guido Imbens, Simon Schwab, and Steven N Goodman. 2023. ‚ÄúA New Look at p Values for Randomized Clinical Trials.‚Äù NEJM Evidence 3 (1): EVIDoa2300003.",
    "crumbs": [
      "Fondamenti",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Causalit√† dai dati osservazionali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_3/01_intro_prob.html",
    "href": "chapters/chapter_3/01_intro_prob.html",
    "title": "17¬† Introduzione al calcolo delle probabilit√†",
    "section": "",
    "text": "17.1 Introduzione\nIn questo capitolo, esamineremo alcuni concetti generali e fondamentali che sono cruciali per una comprensione dei metodi bayesiani. Ulteriori concetti legati alla probabilit√† verranno introdotti o approfonditi nei capitoli successivi, a seconda delle necessit√†. Tuttavia, per uno studio dettagliato della teoria delle probabilit√†, consiglio vivamente il libro ‚ÄúIntroduction to Probability‚Äù di Blitzstein e Hwang (2019).\nNel corso di questo capitolo, esploreremo varie concezioni della probabilit√†, tra cui la visione classica, frequentista e bayesiana. Approfondiremo anche argomenti come le variabili casuali, le funzioni di massa di probabilit√† e le funzioni di ripartizione. Inoltre, introdurremo la simulazione con Python per una migliore comprensione della legge dei grandi numeri, un concetto fondamentale nell‚Äôambito della probabilit√†.",
    "crumbs": [
      "Probabilit√†",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduzione al calcolo delle probabilit√†</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_3/01_intro_prob.html#storia-e-definizioni-della-probabilit√†",
    "href": "chapters/chapter_3/01_intro_prob.html#storia-e-definizioni-della-probabilit√†",
    "title": "17¬† Introduzione al calcolo delle probabilit√†",
    "section": "17.2 Storia e definizioni della probabilit√†",
    "text": "17.2 Storia e definizioni della probabilit√†\nLa probabilit√† √® un modo formale di quantificare l‚Äôincertezza, assegnando plausibilit√† o credibilit√† a un insieme di possibilit√† mutuamente esclusive o risultati di un esperimento o osservazione.\n\n17.2.1 Che cos‚Äô√® la probabilit√†?\nCi sono due modi principali per interpretare la probabilit√†:\n\nFrequentista: Secondo il framework frequentista, la probabilit√† rappresenta il limite della frequenza relativa con cui un evento di interesse si verifica quando il numero di esperimenti condotti ripetutamente nelle stesse condizioni tende all‚Äôinfinito. In questa visione, chiamata ‚Äúontologica‚Äù, la probabilit√† √® considerata una propriet√† intrinseca del mondo, indipendente dalla nostra esperienza. La probabilit√† √® quindi vista come una caratteristica oggettiva della realt√†.\nBayesiana: Al contrario, il framework bayesiano interpreta la probabilit√† come una credenza soggettiva riguardo alla probabilit√† di accadimento di un evento. In questa prospettiva ‚Äúepistemica‚Äù, la probabilit√† √® una misura della nostra conoscenza del mondo piuttosto che una propriet√† oggettiva. Questa visione soggettiva della probabilit√† dipende dalle informazioni disponibili e dal punto di vista dell‚Äôosservatore.\n\n\n\n17.2.2 Storia della probabilit√†\nLa storia della probabilit√† √® lunga e complessa, come illustrato in varie opere (Tabak 2004, Stigler 1986, Weisberg 2014). L‚Äôorigine della probabilit√† moderna risale a una domanda posta da Antoine Gombaud (Chevalier de M√©r√©) a Blaise Pascal (1623‚Äì1662) su come dividere equamente le puntate di un gioco di carte interrotto.\n\n17.2.2.1 Problema dei punti\nIl problema pu√≤ essere formulato cos√¨:\n\nImmaginiamo due persone che partecipano a un gioco a pi√π round. In ogni round, entrambe le persone hanno la stessa probabilit√† di vincere. La prima persona che vince sei round consecutivi si aggiudicher√† un ricco premio in denaro. Supponiamo che A e B abbiano gi√† disputato sei round, con A che ha vinto cinque volte e B una volta. In quel momento, il gioco √® interrotto. Poich√© n√© A n√© B hanno raggiunto le sei vittorie, hanno deciso di dividere il premio. Ma qual √® il modo pi√π equo per farlo?\n\nLa discussione tra Pierre de Fermat (1607‚Äì1665) e Pascal ha portato alla formalizzazione dell‚Äôutilizzo della matematica per risolvere questo problema, proponendo di considerare le probabilit√† di vincita di ciascun giocatore. Ad esempio, se A ha una probabilit√† del 97% di vincere il premio e B ha una probabilit√† del 3%, sembrerebbe equo assegnare ad A il 97% del premio. L‚Äôinteresse pubblico per la loro corrispondenza √® sopravvissuto grazie al libro di Christian Huygens del 1657 ‚ÄúDe Ratiociniis in Ludo Aleae‚Äù (Sul Ragionamento nei Giochi di Dadi), che √® rimasto il riferimento per la probabilit√† per circa 50 anni.\n\n\n17.2.2.2 Sviluppi successivi\nIl libro postumo di Jacob Bernoulli, ‚ÄúL‚ÄôArte della Congettura‚Äù (1713), ha segnato una svolta nella storia della probabilit√†. Bernoulli ha definito la probabilit√† come un indice di incertezza compreso tra 0 e 1 e ha collegato il calcolo della probabilit√† ai dati e alla frequenza a lungo termine di un evento, noto come legge dei grandi numeri. Bernoulli ha applicato la probabilit√† anche a settori diversi dal gioco d‚Äôazzardo, come la mortalit√† umana e la giustizia penale, creando la cosiddetta ‚Äúprobabilit√† soggettiva‚Äù.\n\n\n\n17.2.3 Interpretazione ‚Äúclassica‚Äù\nStoricamente, la prima definizione di probabilit√† √® stata proposta da Pierre-Simon Laplace (1749-1827), che si √® avvalso del calcolo combinatorio. Secondo Laplace, la probabilit√†\\(P\\)di un evento √® definita come il rapporto tra il numero di casi in cui l‚Äôevento si verifica e il numero totale di casi possibili. In questa definizione, un evento √® qualcosa a cui √® possibile assegnare un valore di verit√†, ovvero qualcosa che pu√≤ essere vero o falso. Ad esempio, la probabilit√† di ottenere un 3 in un lancio di un singolo dado √® 1/6 ‚âÉ 0.17, poich√© c‚Äô√® un solo caso favorevole (il lancio ha prodotto un 3) su sei casi possibili (i numeri da 1 a 6). Tuttavia, questa definizione √® insoddisfacente in quanto si basa sull‚Äôassunzione che ogni evento sia equiprobabile, il che non √® sempre vero. Inoltre, questa definizione √® circolare poich√© per definire il concetto di probabilit√†, √® necessario prima definire cosa significa che gli eventi siano equiprobabili, e quindi si deve gi√† conoscere il concetto di probabilit√†.\n\n\n17.2.4 Interpretazione frequentista\nUn secondo tentativo di definire la probabilit√† (dopo quello ‚Äúclassico‚Äù di Laplace) si basa sull‚Äôapproccio frequentista, che pu√≤ essere attribuito a molti autori. In questo approccio, la probabilit√† √® definita sulla base delle frequenze osservate dell‚Äôoccorrenza di un evento. Questo approccio nasce dalla difficolt√† di assegnare una probabilit√† agli eventi assumendo il principio di equiprobabilit√†, come nel caso delle monete, dei dadi o delle carte di un mazzo. Sebbene la probabilit√† di ottenere testa come risultato del lancio di un dado sia 1/2 se crediamo che la moneta sia bilanciata, se cos√¨ non fosse non potremmo assegnare la stessa probabilit√† a tutti i risultati possibili. Tuttavia, possiamo stimare le probabilit√† come la frequenza\\(f_t\\), definita come il rapporto tra il numero di volte in cui un lancio ha prodotto ‚Äútesta‚Äù e il numero totale di lanci.\nSi osservi che l‚Äôosservazione della frequenza \\(f_t\\) √® solo un‚Äô approssimazione della probabilit√†, ma l‚Äôaccuratezza migliora all‚Äôaumentare del numero totale di lanci, \\(N\\). In linea di principio, la probabilit√† di ottenere ‚Äútesta‚Äù, \\(P(T)\\), √® il limite della frequenza \\(f_t\\) quando il numero totale di lanci \\(N\\) tende all‚Äôinfinito. Tuttavia, questa definizione richiede l‚Äôinfinita ripetizione di un esperimento, il che pu√≤ essere impraticabile o impossibile in molti casi. Inoltre, questa definizione assume che gli eventi futuri siano simili agli eventi passati, il che non √® sempre garantito.\n\ndef coin_flips(n, run_label):\n    # Genera un array di 0 e 1 dove 1 rappresenta 'testa' e 0 'croce'\n    # usando una distribuzione binomiale.\n    heads = np.random.binomial(1, 0.5, n)\n    \n    # Calcola la proporzione cumulativa di teste.\n    flips = np.arange(1, n + 1) \n    proportion_heads = np.cumsum(heads) / flips\n    \n    # Crea un DataFrame per un facile accesso e visualizzazione dei dati.\n    df = pd.DataFrame({'flips': flips, 'proportion_heads': proportion_heads, 'run': run_label})\n\n    return df\n\nn = 1000\n\ndf = pd.concat([coin_flips(n, f'run{i+1}') for i in range(4)], axis=0)\nax = sns.lineplot(data = df, x = 'flips', y = 'proportion_heads', hue = 'run')\n\n\n\n\n\n\n\n\n\n\n17.2.5 La Legge dei Grandi Numeri\nLa simulazione precedente fornisce un esempio della Legge dei grandi numeri. La Legge dei Grandi Numeri afferma che, man mano che il numero di esperimenti casuali ripetuti aumenta, la stima della probabilit√† di un evento \\(P(Y=y)\\) diventa sempre pi√π accurata.\nIl teorema sostiene che, con l‚Äôaumento del numero di ripetizioni di un esperimento casuale, la media dei risultati osservati tende a convergere al valore atteso teorico della variabile casuale. In altre parole, la media empirica dei risultati osservati si avvicina sempre di pi√π al valore medio teorico.\nQuesta legge √® cruciale perch√© garantisce che, con un numero sufficientemente grande di prove, la stima empirica della probabilit√† di un evento si avvicina al valore reale. Questo rende le stime probabilistiche pi√π precise e affidabili.\nDal punto di vista pratico, la Legge dei Grandi Numeri consente di utilizzare modelli probabilistici per interpretare fenomeni reali. Anche se le osservazioni singole possono variare in modo casuale, la media delle osservazioni su un ampio numero di ripetizioni rifletter√† fedelmente le probabilit√† teoriche.\nFormalmente, data una serie di variabili casuali indipendenti \\(X_1, X_2, \\ldots, X_n\\), ciascuna con media \\(\\mu\\), la Legge dei Grandi Numeri √® espressa come:\n\\[\n\\lim_{{n \\to \\infty}} P\\left(\\left|\\frac{X_1 + X_2 + \\ldots + X_n}{n} - \\mu\\right| &lt; \\epsilon\\right) = 1,\n\\]\ndove \\(\\epsilon\\) √® un valore positivo arbitrariamente piccolo e \\(P(\\cdot)\\) indica la probabilit√†. Questo significa che, con un numero molto grande di ripetizioni, la media campionaria osservata sar√† vicina alla media teorica attesa, permettendo inferenze affidabili sulla probabilit√† degli eventi.\nIn sintesi, la Legge dei Grandi Numeri assicura che, aumentando il numero di prove, le stime empiriche delle probabilit√† diventano sempre pi√π precise, allineandosi con i valori teorici attesi.\n\n17.2.5.1 Problema del caso singolo\nNell‚Äôambito dell‚Äôapproccio frequentista alla probabilit√†, basato sulla concezione delle frequenze relative di eventi osservati su lunghe serie di ripetizioni, emerge un limite concettuale nel trattare la probabilit√† di eventi singolari e non ripetibili. Secondo questa prospettiva, infatti, non risulta rigorosamente appropriato discutere di probabilit√† relative a eventi unici e non replicabili nel tempo. Esempi emblematici di tali eventi includono la possibilit√† che Alcaraz vinca contro Djokovic nella finale di Wimbledon del 2023 o che si verifichi pioggia a Firenze il giorno di Ferragosto del 2024. Questi scenari, essendo unici e circoscritti a un preciso momento storico, sfuggono alla logica frequentista che richiede, per definizione, la possibilit√† di osservazione ripetuta degli eventi per valutarne la probabilit√†. Nonostante ci√≤, nel linguaggio comune non specialistico, √® comune l‚Äôuso del termine ‚Äúprobabilit√†‚Äù per riferirsi anche a tali eventi specifici e non ripetibili, evidenziando cos√¨ una discrepanza tra l‚Äôuso tecnico e quello colloquiale del concetto di probabilit√†.\n\n\n\n17.2.6 Collegamento tra probabilit√† e statistica\nDurante gli anni ‚Äô20 del Novecento, Ronald A. Fisher propose un nuovo framework teorico per l‚Äôinferenza statistica, basato sulla concettualizzazione della frequenza. Fisher introdusse concetti chiave come la massima verosimiglianza, i test di significativit√†, i metodi di campionamento, l‚Äôanalisi della varianza e il disegno sperimentale.\nNegli anni ‚Äô30, Jerzy Neyman ed Egon Pearson fecero ulteriori progressi nel campo con lo sviluppo di una teoria della decisione statistica, basata sul principio della verosimiglianza e sull‚Äôinterpretazione frequentista della probabilit√†. Definirono due tipologie di errori decisionali e utilizzarono il test di significativit√† di Fisher, interpretando i valori\\(p\\)come indicatori dei tassi di errore a lungo termine.\n\n\n17.2.7 La riscoperta dei metodi Monte Carlo Markov chain\nFisher assunse una prospettiva critica nei confronti della ‚Äúprobabilit√† inversa‚Äù (ossia, i metodi bayesiani), nonostante questa fosse stata la metodologia predominante per l‚Äôinferenza statistica per quasi un secolo e mezzo. Il suo approccio frequentista ebbe un profondo impatto sullo sviluppo della statistica sia teorica che sperimentale, contribuendo a un decremento nell‚Äôutilizzo dell‚Äôinferenza basata sul metodo della probabilit√† inversa, originariamente proposto da Laplace.\nNel 1939, il libro di Harold Jeffreys intitolato ‚ÄúTheory of Probability‚Äù rappresent√≤ una delle prime esposizioni moderne dei metodi bayesiani. Tuttavia, la rinascita del framework bayesiano fu rinviata fino alla scoperta dei metodi Monte Carlo Markov chain alla fine degli anni ‚Äô80. Questi metodi hanno reso fattibile il calcolo di risultati precedentemente non ottenibili, consentendo un rinnovato interesse e sviluppo nei metodi bayesiani. Per una storia dell‚Äôapproccio bayesiano, si veda Bayesian Methods: General Background oppure Philosophy of Statistics.\n\n\n17.2.8 Interpretazione soggettivista\nUna visione alternativa della probabilit√† la considera come una credenza soggettiva. De Finetti (2017) ha proposto un‚Äôinterpretazione in cui la probabilit√† non √® vista come una caratteristica oggettiva degli eventi, ma piuttosto come una misura della credenza soggettiva, suggerendo di trattare \\(p(¬∑)\\) come una probabilit√† soggettiva. √à interessante notare che de Finetti era un soggettivista radicale. Infatti, la frase di apertura del suo trattato in due volumi sulla probabilit√† afferma che ‚ÄúLa probabilit√† non esiste‚Äù, intendendo che la probabilit√† non ha uno status oggettivo, ma rappresenta piuttosto la quantificazione della nostra esperienza di incertezza. Riteneva che l‚Äôidea di una probabilit√† esterna all‚Äôindividuo, con uno status oggettivo, fosse pura superstizione, paragonabile al credere in ‚ÄúEtere cosmico, Spazio e Tempo assoluti, ‚Ä¶, o Fate e Streghe‚Ä¶‚Äù. Secondo de Finetti, ‚Äú‚Ä¶ esistono solo probabilit√† soggettive - cio√®, il grado di credenza nell‚Äôoccorrenza di un evento attribuito da una determinata persona in un dato momento con un dato insieme di informazioni.‚Äù\nCome sottolineato da Press (2009), la prima menzione della probabilit√† come grado di credenza soggettiva fu fatta da Ramsey (1926), ed √® questa nozione di probabilit√† come credenza soggettiva che ha portato a una notevole resistenza alle idee bayesiane. Una trattazione dettagliata degli assiomi della probabilit√† soggettiva si trova in Fishburn (1986).\nLa denominazione ‚Äúsoggettivo‚Äù legata alla probabilit√† potrebbe risultare infelice, poich√© potrebbe suggerire un ragionamento vago o non scientifico. Lindley (2013) condivide queste riserve, proponendo l‚Äôalternativa ‚Äúprobabilit√† personale‚Äù rispetto a ‚Äúprobabilit√† soggettiva‚Äù. Analogamente, Howson and Urbach (2006) preferiscono utilizzare l‚Äôespressione ‚Äúprobabilit√† epistemica‚Äù, che riflette il grado di incertezza di un individuo di fronte al problema trattato. In sostanza, la probabilit√† epistemica si riferisce all‚Äôincertezza personale riguardo a variabili sconosciute. Questa terminologia viene adottata anche nel testo di Kaplan (2023), fornendo un linguaggio pi√π neutro per discutere di questi concetti.\nVa inoltre notato che l‚Äôinterpretazione soggettiva si adatta bene a eventi singoli, permettendo di esprimere una convinzione su eventi specifici, come la probabilit√† di pioggia in un dato giorno o l‚Äôesito di una competizione sportiva.\n\n\n17.2.9 Nota\nPer chi desidera approfondire, il primo capitolo del testo Bernoulli‚Äôs Fallacy (Clayton 2021) offre un‚Äôintroduzione molto leggibile alle tematiche della definizione della probabilit√† nella storia della scienza.",
    "crumbs": [
      "Probabilit√†",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduzione al calcolo delle probabilit√†</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_3/01_intro_prob.html#definizioni-e-assiomi",
    "href": "chapters/chapter_3/01_intro_prob.html#definizioni-e-assiomi",
    "title": "17¬† Introduzione al calcolo delle probabilit√†",
    "section": "17.3 Definizioni e assiomi",
    "text": "17.3 Definizioni e assiomi\nIndipendentemente dalla controversia in corso sulla sua interpretazione, la probabilit√† √® stata stabilita come teoria matematica dal matematico sovietico Andrey Kolmogorov all‚Äôinizio del XX secolo. Poich√© sia i frequentisti che i bayesiani utilizzano questa teoria matematica, il disaccordo riguarda l‚Äôinterpretazione e non la matematica.\n√à possibile definire la probabilit√† utilizzando i seguenti tre concetti: esperimento, spazio campionario ed evento.\n\n17.3.1 Esperimenti casuali ed eventi\nSia i fenomeni deterministici che quelli stocastici influenzano la nostra vita quotidiana, determinando le dinamiche degli eventi che ci circondano.\n\nUn fenomeno deterministico produce sempre lo stesso risultato ogni volta che viene ripetuto nelle stesse condizioni. Ad esempio, se riscaldiamo l‚Äôacqua a 100¬∞C a livello del mare, essa bollir√† sempre.\nUn fenomeno casuale √® caratterizzato da condizioni in cui il risultato non pu√≤ essere determinato con certezza prima che si verifichi. In altre parole, ogni volta che il processo o l‚Äôesperimento viene ripetuto, si osserva uno tra i vari possibili risultati. Ad esempio, quando si lancia una moneta, l‚Äôesito pu√≤ essere testa (T) o croce (C), ma non si pu√≤ conoscere in anticipo quale sar√†.\n\nNella teoria delle probabilit√†, un esperimento casuale √® un processo o una situazione in cui il risultato non pu√≤ essere previsto con certezza prima dell‚Äôesecuzione dell‚Äôesperimento. Per poter analizzare correttamente gli esperimenti casuali, sono fondamentali alcune nozioni chiave, come indicato nelle sezioni successive.\n\n\n17.3.2 Spazi Campionari ed Eventi\nLo spazio campionario \\(\\Omega\\) √® l‚Äôinsieme degli esiti possibili di un esperimento. I punti\\(\\omega\\)in\\(\\Omega\\)sono chiamati esiti campionari o realizzazioni.\nAd esempio, se lanciamo un dado a 6 facce, lo spazio campionario √® costituito dai sei possibili risultati, Œ© = {1, 2, 3, 4, 5, 6}. Diversi esperimenti casuali hanno spazi campionari differenti che possono essere rappresentati in modo equivalente. Ad esempio:\n\nLancio di una moneta: Œ© = {T, C}\nLancio di due monete: Œ© = {TT, TC, CT, CC}\n\nGli eventi sono sottoinsiemi di\\(\\Omega\\).\nUn evento √® denotato da una lettera maiuscola come A, B, o C. Ad esempio, nel caso del lancio di un dado, l‚Äôevento ‚Äúnumero pari‚Äù pu√≤ essere rappresentato da A = {2, 4, 6}, che √® un sottoinsieme di Œ© (A ‚äÇ Œ©), e l‚Äôevento ‚Äúnumero dispari‚Äù da B = {1, 3, 5}, anch‚Äôesso un sottoinsieme di Œ© (B ‚äÇ Œ©).\nSe un evento √® costituito da un singolo risultato dello spazio campionario, √® detto evento elementare. Ad esempio, l‚Äôevento di ottenere il numero 1 nel lancio di un dado, denotato come A = {1}. Se un evento √® costituito da pi√π risultati dello spazio campionario, √® detto evento composto, come l‚Äôevento di ottenere un numero pari nel lancio di un dado, A = {2, 4, 6}.\nDato un evento\\(A\\), sia \\(A^c = \\{ \\omega \\in \\Omega : \\text{non } (\\omega \\in A) \\}\\) il complemento di \\(A\\). Il complemento di\\(\\Omega\\)√® l‚Äôinsieme vuoto \\(\\varnothing\\).\nL‚Äôunione degli eventi\\(A\\)e\\(B\\)√® definita come \\(A \\cup B = \\{ \\omega \\in \\Omega : \\omega \\in A \\text{ o } \\omega \\in B \\}\\). Se\\(A_1, A_2, \\dots\\)√® una sequenza di insiemi, allora\n\\[\\cup_{i=1}^\\infty A_i = \\left\\{ \\omega \\in \\Omega : \\omega \\in A_i \\text{ per qualche } i \\right\\}\\]\nL‚Äôintersezione di \\(A\\) e \\(B\\) √® \\(A \\cap B = \\{ \\omega \\in \\Omega : \\omega \\in A \\text{ e } \\omega \\in B \\}\\). Se \\(A_1, A_2, \\dots\\) √® una sequenza di insiemi, allora\n\\[\\cap_{i=1}^\\infty A_i = \\left\\{ \\omega \\in \\Omega : \\omega \\in A_i \\text{ per tutti } i \\right\\}\\]\nSia \\(A - B = \\left\\{ \\omega \\in \\Omega : \\omega \\in A \\text{ e non } (\\omega \\in B) \\right\\}\\). Se ogni elemento di \\(A\\) √® contenuto in \\(B\\), scriviamo \\(A \\subset B\\)o\\(B \\supset A\\). Se \\(A\\) √® un insieme finito, sia \\(|A|\\) il numero di elementi in \\(A\\).\n\n\n\n\n\n\n\nNotazione\nSignificato\n\n\n\n\n\\(\\Omega\\)\nspazio campionario\n\n\n\\(\\omega\\)\nesito\n\n\n\\(A\\)\nevento (sottoinsieme di\\(\\Omega\\))\n\n\n\\(\\vert A \\vert\\)\nnumero di elementi in\\(A\\)(se finito)\n\n\n\\(A^c\\)\ncomplemento di\\(A\\)(non\\(A\\))\n\n\n\\(A \\cup B\\)\nunione (\\(A\\)o\\(B\\))\n\n\n\\(A \\cap B\\)o\\(AB\\)\nintersezione (\\(A\\)e\\(B\\))\n\n\n\\(A - B\\)\ndifferenza insiemistica (punti in\\(A\\)ma non in\\(B\\))\n\n\n\\(A \\subset B\\)\ninclusione insiemistica (\\(A\\)√® un sottoinsieme di\\(B\\))\n\n\n\\(\\varnothing\\)\nevento nullo (sempre falso)\n\n\n\\(\\Omega\\)\nevento certo (sempre vero)\n\n\n\nDiciamo che\\(A_1, A_2, \\dots\\)sono disgiunti o mutuamente esclusivi se\\(A_i \\cap A_j = \\varnothing\\)ogni volta che\\(i \\neq j\\).\nUna partizione di\\(\\Omega\\)√® una sequenza di insiemi disgiunti\\(A_1, A_2, \\dots\\)tale che\\(\\cup_{i=1}^\\infty A_i = \\Omega\\).\nDato un evento\\(A\\), definiamo la funzione indicatrice di\\(A\\) come\n\\[I_A(\\omega) = I(\\omega \\in A) = \\begin{cases}\n1 &\\text{se } \\omega \\in A \\\\\n0 &\\text{altrimenti}\n\\end{cases}\n\\]\nUna sequenza di insiemi \\(A_1, A_2, \\dots\\) √® monotona crescente se \\(A_1 \\subset A_2 \\subset \\dots\\), e definiamo \\(\\lim_{n \\rightarrow \\infty} A_n = \\cup_{i=1}^\\infty A_i\\). Una sequenza di insiemi \\(A_1, A_2, \\dots\\) √® monotona decrescente se \\(A_1 \\supset A_2 \\supset \\dots\\) e allora definiamo \\(\\lim_{n \\rightarrow \\infty} A_n = \\cap_{i=1}^n A_i\\). In entrambi i casi, scriveremo \\(A_n \\rightarrow A\\).\n\n\n17.3.3 Probabilit√†\nLa probabilit√† di un evento √® una misura numerica che indica la possibilit√† che tale evento si verifichi. Nel caso del lancio di una moneta, la probabilit√† di ottenere testa √® 1/2, cos√¨ come la probabilit√† di ottenere croce. I valori di probabilit√† P(A) = 0 rappresentano eventi impossibili, mentre P(A) = 1 rappresentano eventi certi.\nPer denotare la probabilit√† che un evento A non si verifichi, possiamo usare la notazione P(¬¨A) o P(AÃÖ), dove P(AÃÖ) = 1 - P(A).\n\n\n17.3.4 Gli Assiomi di Kolmogorov\nUna funzione \\(\\mathbb{P}\\) che assegna un numero reale \\(\\mathbb{P}(A)\\) a ogni evento \\(A\\) √® una distribuzione di probabilit√† o una misura di probabilit√† se soddisfa i seguenti tre assiomi:\n\nAssioma 1. La probabilit√† di un evento \\(A\\) √® un numero non negativo: \\(\\mathbb{P}(A) \\geq 0\\) per ogni \\(A\\).\nAssioma 2. La probabilit√† di tutti i possibili esiti, o spazio campionario \\(\\Omega\\), √® pari a uno: \\(\\mathbb{P}(\\Omega) = 1\\).\nAssioma 3. Se \\(A_1, A_2, \\dots\\) sono disgiunti (mutuamente esclusivi) allora\n\n\\[\\mathbb{P} \\left( \\cup_{i=1}^\\infty A_i \\right) = \\sum_{i=1}^\\infty \\mathbb{P}(A_i).\\]\nI primi due assiomi insieme implicano che la probabilit√† varia da 0 a 1. Per comprendere l‚Äôultimo assioma, consideriamo il concetto di eventi mutuamente esclusivi, ossia due eventi, \\(A\\) e \\(B\\), che non condividono alcun risultato. In questo caso, possiamo applicare la regola dell‚Äôaddizione per concludere che \\(P(A \\cup B) = P(A) + P(B)\\).\n\n# Creare una nuova figura\nfig, ax = plt.subplots()\n\n# Creare cerchi per A e B\ncircle_A = patches.Circle((0.3, 0.5), 0.2, edgecolor='blue', facecolor='lightblue', alpha=0.5)\ncircle_B = patches.Circle((0.5, 0.5), 0.2, edgecolor='blue', facecolor='lightblue', alpha=0.5)\n\n# Aggiungere cerchi alla trama\nax.add_patch(circle_A)\nax.add_patch(circle_B)\n\n# Aggiungere etichette per A, B e le regioni\nax.text(0.2, 0.7, 'A', fontsize=12, ha='center')\nax.text(0.6, 0.7, 'B', fontsize=12, ha='center')\nax.text(0.4, 0.5, 'A ‚à© B', fontsize=12, ha='center', color='black')\nax.text(0.15, 0.5, 'A ‚à© B^c', fontsize=12, ha='center', color='black')\nax.text(0.7, 0.5, 'B ‚à© A^c', fontsize=12, ha='center', color='black')\n\n# Impostare i limiti dell'asse\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\n\n# Rimuovere assi\nax.axis('off')\n\n# Mostrare la trama\nplt.show()\n\n\n\n\n\n\n\n\nOra, consideriamo due eventi che non sono mutuamente esclusivi perch√© condividono un risultato. In questo caso, la regola dell‚Äôaddizione non si applica perch√© sia \\(A\\) che \\(B\\) contengono alcuni stessi risultati. Per eventi che non sono mutuamente esclusivi, possiamo applicare la seguente regola generale dell‚Äôaddizione:\nPer qualsiasi evento dato\\(A\\)e\\(B\\), la regola dell‚Äôaddizione √® data da:\n\\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B).\\]\nUsando il terzo assioma della probabilit√†, abbiamo:\n\\[P(A \\cup B) = P(A \\cap B^c) + P(B \\cap A^c) + P(A \\cap B).\\]\nQuando \\(A\\) e \\(B\\) sono mutuamente esclusivi, \\(P(A \\cap B^c)\\) e \\(P(B \\cap A^c)\\) si riducono a \\(P(A)\\) e \\(P(B)\\) rispettivamente. Inoltre, abbiamo \\(P(A \\cap B) = 0\\) in questo caso di mutua esclusivit√†.\nInfine, notiamo che l‚Äôevento \\(A\\) pu√≤ essere scomposto in due eventi mutuamente esclusivi, \\({A \\cap B}\\) (regione sovrapposta) e \\({A \\cap B^c}\\) (regione non sovrapposta). Questo √® chiamato legge della probabilit√† totale. Per qualsiasi evento dato \\(A\\) e \\(B\\), la legge della probabilit√† totale √® data da:\n\\[P(A) = P(A \\cap B) + P(A \\cap B^c).\\]\nSecondo la legge della probabilit√† totale, possiamo scrivere \\(P(A \\cap B^c) = P(A) - P(A \\cap B)\\) sottraendo \\(P(A \\cap B)\\) da entrambi i lati dell‚Äôequazione. Analogamente, la legge della probabilit√† totale pu√≤ essere applicata all‚Äôevento \\(B\\), ottenendo \\(P(B \\cap A^c) = P(B) - P(A \\cap B)\\). Sostituendo questi risultati nell‚Äôequazione precedente e semplificando l‚Äôespressione, si arriva alla regola generale dell‚Äôaddizione. Sottolineiamo che questo risultato √® ottenuto utilizzando solo gli assiomi della probabilit√†.\nAlcune propriet√† che possono essere derivate dagli assiomi:\n\n\\(\\mathbb{P}(\\varnothing) = 0\\),\n\\(A \\subset B \\Rightarrow \\mathbb{P}(A) \\leq \\mathbb{P}(B)\\),\n\\(0 \\leq \\mathbb{P}(A) \\leq 1\\),\n\\(\\mathbb{P}\\left(A^c\\right) = 1 - \\mathbb{P}(A)\\),\n\\(A \\cap B = \\varnothing \\Rightarrow \\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B)\\).",
    "crumbs": [
      "Probabilit√†",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduzione al calcolo delle probabilit√†</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_3/01_intro_prob.html#elementi-di-calcolo-combinatorio",
    "href": "chapters/chapter_3/01_intro_prob.html#elementi-di-calcolo-combinatorio",
    "title": "17¬† Introduzione al calcolo delle probabilit√†",
    "section": "17.4 Elementi di calcolo combinatorio",
    "text": "17.4 Elementi di calcolo combinatorio\n\n17.4.1 Permutazioni\nQuando ogni risultato √® ugualmente probabile, per calcolare la probabilit√† di un evento \\(A\\), √® necessario contare il numero di elementi nell‚Äôevento \\(A\\) e il numero totale di elementi nello spazio campionario:\n\\[\\mathbb{P}(A) = \\frac{|A|}{|\\Omega|}.\\]\nOra introduciamo una tecnica di conteggio utile, chiamata permutazioni. Le permutazioni si riferiscono al numero di modi in cui gli oggetti possono essere disposti. Ad esempio, consideriamo tre oggetti distinti A, B e C. Ci sono 6 modi diversi per disporli: {ABC, ACB, BAC, BCA, CAB, CBA}.\nCome possiamo calcolare il numero di permutazioni senza enumerare ogni disposizione, specialmente quando il numero di oggetti √® grande? C‚Äô√® un modo semplice per farlo. Consideriamo l‚Äôesempio di disporre tre oggetti, A, B e C. Prima di tutto, ci sono tre modi per scegliere il primo oggetto: A, B o C. Una volta scelto il primo oggetto, ci sono due modi per scegliere il secondo oggetto. Infine, rimane un solo modo per scegliere l‚Äôultimo oggetto. Possiamo concettualizzare questo processo come un albero, dove il numero totale di foglie √® uguale al numero di permutazioni. Per calcolare il numero di foglie, basta moltiplicare sequenzialmente il numero di rami a ogni livello, cio√® \\(3 \\times 2 \\times 1\\).\nGeneralizzando questa idea, possiamo calcolare il numero di permutazioni di\\(k\\)oggetti su un insieme di \\(n\\) oggetti unici, denotato da \\(nPk\\) dove\\(k \\leq n\\), usando la seguente formula:\nIl numero di permutazioni di\\(k\\)oggetti su\\(n\\)oggetti unici √® dato da:\n\\[nPk = \\frac{n!}{(n - k)!}.\\]\nUn fattoriale √® definito come:\n\\[n! = n \\times (n - 1) \\times \\cdots \\times 2 \\times 1.\\]\nSi noti che \\(0!\\) √® definito come 1.\n\n17.4.1.1 Problema dei Compleanni\nIl problema dei compleanni √® un noto esempio controintuitivo di permutazioni. Il problema chiede quanti individui sono necessari affinch√© la probabilit√† che almeno due persone abbiano lo stesso compleanno superi 0.5, assumendo che ogni compleanno sia ugualmente probabile. Sorprendentemente, la risposta √® solo 23 persone, molto meno di quanto la maggior parte delle persone immagina.\nPer risolvere questo problema usando le permutazioni, notiamo la seguente relazione:\n\\[P(\\text{almeno due persone hanno lo stesso compleanno}) = 1 - P(\\text{nessuno ha lo stesso compleanno}).\\]\nQuesta uguaglianza vale perch√© l‚Äôevento ‚Äúnessuno ha lo stesso compleanno‚Äù √® il complemento dell‚Äôevento ‚Äúalmeno due persone hanno lo stesso compleanno‚Äù. Questo significa che dobbiamo solo calcolare la probabilit√† che nessuno abbia lo stesso compleanno.\nSia \\(k\\) il numero di persone. Per calcolare la probabilit√† che nessuno abbia lo stesso compleanno, contiamo il numero di modi in cui \\(k\\) persone possono avere compleanni diversi. Poich√© ogni compleanno √® ugualmente probabile, possiamo usare le permutazioni per contare il numero di modi in cui \\(k\\) compleanni unici possono essere disposti su 365 giorni:\n\\[365Pk = \\frac{365!}{(365 - k)!}.\\]\nDividiamo questo numero per il numero totale di elementi nello spazio campionario, che √® il numero totale di modi in cui \\(k\\) compleanni non unici possono essere disposti su 365 giorni:\n\\[365^k .\\]\nQuindi, abbiamo:\n\\[P(\\text{nessuno ha lo stesso compleanno}) = \\frac{365Pk}{365^k} = \\frac{365!}{365^k (365 - k)!} .\\]\nInsieme alla precedente equazione, la soluzione al problema dei compleanni √®:\n\\[1 - \\frac{365!}{365^k (365 - k)!} .\\]\n\ndef birthday(k):\n    logdenom = k * math.log(365) + math.lgamma(365 - k + 1) # log denominatore\n    lognumer = math.lgamma(366) # log numeratore\n    pr = 1 - np.exp(lognumer - logdenom) # trasformazione inversa\n    return pr\n\nk = np.arange(1, 51)\nbday = [birthday(i) for i in k]\n\nplt.plot(k, bday, marker='o')\nplt.xlabel('Numero di persone')\nplt.ylabel('Probabilit√† che almeno due persone\\nabbiano lo stesso compleanno')\nplt.axhline(y=0.5, color='r', linestyle='-')\nplt.xlim(0, 50)\nplt.ylim(0, 1)\nplt.grid(True)\nplt.title('Probabilit√† del Problema dei Compleanni')\nplt.show()\n\nprint(\"Probabilit√† per 20-25 persone:\", bday[19:25])\n\n\n\n\n\n\n\n\nProbabilit√† per 20-25 persone: [0.41143838358049944, 0.44368833516523465, 0.47569530766240553, 0.507297234324024, 0.5383442579144757, 0.5686997039694264]\n\n\nOsserviamo che quando il numero di persone √® 23, la probabilit√† che almeno due persone abbiano lo stesso compleanno supera 0.5. Quando il numero di persone √® pi√π di 50, questa probabilit√† √® quasi 1.\n\n\n\n17.4.2 Campionamento con e senza reinserimento\nAbbiamo derivato una soluzione analitica esatta per il problema dei compleanni, ma possiamo anche produrre una soluzione approssimata utilizzando il metodo della simulazione Monte Carlo. Il nome deriva dal Casin√≤ di Monte Carlo a Monaco, ma possiamo semplicemente chiamarlo metodo di simulazione. La simulazione Monte Carlo √® una classe generale di metodi stocastici (contrariamente ai metodi deterministici) che possono essere utilizzati per risolvere approssimativamente problemi analitici generando casualmente le quantit√† di interesse.\nPer il problema dei compleanni, campioniamo\\(k\\)compleanni potenzialmente non unici su 365 giorni e verifichiamo se i\\(k\\) compleanni campionati sono tutti diversi. Utilizziamo il campionamento con reinserimento perch√© ad ogni estrazione ogni giorno dei 365 √® ugualmente probabile, indipendentemente dai giorni estratti in precedenza. In altre parole, il fatto che una persona sia nata in un certo giorno dell‚Äôanno non esclude che qualcun altro possa essere nato lo stesso giorno. Dopo aver ripetuto questa procedura di campionamento molte volte, calcoliamo la frazione di prove di simulazione in cui almeno due compleanni sono uguali, e questa frazione serve come stima della probabilit√† corrispondente. Questa procedura di simulazione √® intuitiva perch√© emula il processo di generazione dei dati descritto nel problema dei compleanni.\nPer implementare il campionamento con o senza reinserimento in Python, utilizziamo la funzione numpy.random.choice. Nel caso del campionamento con reinserimento, impostiamo l‚Äôargomento replace su True. Il campionamento senza reinserimento significa che, una volta campionato un elemento, questo non sar√† disponibile per estrazioni successive.\n\nk = 23  # numero di persone\nsims = 1000  # numero di simulazioni\nevent = 0  # contatore eventi\n\nfor _ in range(sims):\n    days = np.random.choice(365, k, replace=True)\n    unique_days = np.unique(days)\n    if len(unique_days) &lt; k:\n        event += 1\n\n# frazione di prove in cui almeno due compleanni sono uguali\nanswer = event / sims\nprint(f\"Stima della probabilit√†: {answer}\")\n\n# Aumentare il numero di simulazioni a un milione per maggiore accuratezza\nsims_large = 1000000\nevent_large = 0\n\nfor _ in range(sims_large):\n    days = np.random.choice(365, k, replace=True)\n    unique_days = np.unique(days)\n    if len(unique_days) &lt; k:\n        event_large += 1\n\nanswer_large = event_large / sims_large\nprint(f\"Stima con un milione di simulazioni: {answer_large}\")\n\nStima della probabilit√†: 0.509\nStima con un milione di simulazioni: 0.506191\n\n\nNel codice sopra, abbiamo impostato il numero di simulazioni a 1000. Aumentando il numero di simulazioni a un milione, otteniamo una stima pi√π accurata. Osserviamo che quando il numero di persone √® 23, la probabilit√† che almeno due persone abbiano lo stesso compleanno √® superiore a 0.5. Quando il numero di persone supera 50, questa probabilit√† √® vicina a 1.\nLa simulazione Monte Carlo √® una classe generale di procedure di campionamento casuale ripetuto utilizzate per risolvere approssimativamente problemi analitici. I metodi comunemente utilizzati includono il campionamento con reinserimento, in cui la stessa unit√† pu√≤ essere campionata ripetutamente, e il campionamento senza reinserimento, in cui ogni unit√† pu√≤ essere campionata al massimo una volta.\n\n\n17.4.3 Combinazioni\nIntroduciamo un‚Äôaltra utile tecnica di conteggio chiamata combinazioni. Le combinazioni sono simili alle permutazioni, ma ignorano l‚Äôordine degli elementi. In altre parole, le combinazioni rappresentano i modi di scegliere \\(k\\) elementi distinti da\\(n\\)elementi senza considerare l‚Äôordine. Ad esempio, scegliendo 2 elementi da 3 (A, B e C), le permutazioni sono 6 (AB, BA, AC, CA, BC, CB), mentre le combinazioni sono 3 (AB, AC, BC).\nPer calcolare le combinazioni, prima calcoliamo le permutazioni\\(nPk\\)e poi dividiamo per \\(k!\\). Questo perch√© ci sono \\(k!\\) modi per disporre \\(k\\) elementi in ordine diverso, ma tutte queste disposizioni contano come una singola combinazione. La formula generale per le combinazioni √®:\n\\[\nnCk = \\frac{n!}{k!(n-k)!} .\n\\]\n\n17.4.3.1 Esempio: Problema di una Commissione Psicologica\nSupponiamo di dover formare una commissione di 5 psicologi su un gruppo di 20 persone (10 psicologi clinici e 10 psicologi del lavoro). Qual √® la probabilit√† che almeno 2 psicologi clinici siano nella commissione? Per calcolare questa probabilit√†, notiamo prima la seguente uguaglianza:\n\\[\nP(\\text{almeno 2 psicologi clinici}) = 1 - P(\\text{nessun psicologo clinico}) - P(\\text{esattamente 1 psicologo clinico}).\n\\]\nIl numero totale di modi per selezionare 5 persone dalla commissione su 20 √® dato da:\n\\[\n20C5 = \\frac{20!}{5!(15!)} = 15,504 .\n\\]\nIl numero di modi per avere nessun psicologo clinico nella commissione √®:\n\\[\n10C0 \\times 10C5 = 1 \\times 252 = 252 .\n\\]\nQuindi, la probabilit√† di avere nessun psicologo clinico √® \\(\\frac{252}{15,504} \\approx 0.016\\).\nIl numero di modi per avere esattamente 1 psicologo clinico nella commissione √®:\n\\[\n10C1 \\times 10C4 = 10 \\times 210 = 2,100 .\n\\]\nQuindi, la probabilit√† di avere esattamente 1 psicologo clinico √® \\(\\frac{2,100}{15,504} \\approx 0.135\\).\nLa probabilit√† di avere almeno 2 psicologi clinici nella commissione √® quindi:\n\\[\nP(\\text{almeno 2 psicologi clinici nella commissione}) = 1 - 0.016 - 0.135 = 0.849 .\n\\]\n\n# Funzione per calcolare le combinazioni\ndef nCk(n, k):\n    return math.factorial(n) // (math.factorial(k) * math.factorial(n - k))\n\n# Calcolo delle probabilit√† per il problema della commissione\ntotal_ways = nCk(20, 5)\nno_clinical = nCk(10, 0) * nCk(10, 5)\none_clinical = nCk(10, 1) * nCk(10, 4)\n\np_no_clinical = no_clinical / total_ways\np_one_clinical = one_clinical / total_ways\n\np_at_least_two_clinical = 1 - p_no_clinical - p_one_clinical\n\nprint(f\"Probabilit√† di almeno 2 psicologi clinici: {p_at_least_two_clinical:.3f}\")\n\nProbabilit√† di almeno 2 psicologi clinici: 0.848\n\n\n\n\n17.4.3.2 Esempio: Problema del Messaggio di Schwarzenegger\nConsideriamo un incidente del 2009 quando il Governatore della California Arnold Schwarzenegger invi√≤ un messaggio all‚Äôassemblea statale riguardo il veto al disegno di legge 1176. Questo messaggio formava un‚Äôacrostico volgare con le prime lettere di ogni riga.\n\n\n\n\n\n\nFigure¬†17.1: Il Messaggio di Schwarzenegger.\n\n\n\nQual √® la probabilit√† che questo acrostico sia stato casuale?\nSupponiamo che il messaggio sia stato diviso in 7 righe in modo casuale. Per ottenere 7 righe, devono essere inseriti 6 interruzioni di riga in 84 spazi possibili (prima della seconda parola, terza parola, ecc.). Il numero di modi per inserire 6 interruzioni in 84 spazi √® dato da:\n\\[\n84C6 = \\frac{84!}{6!(78!)} \\approx 406,481,544 .\n\\]\nTuttavia, ci sono solo 12 modi per produrre questo particolare acrostico. Quindi, la probabilit√† che questo acrostico si verifichi casualmente √®:\n\\[\n\\frac{12}{84C6} \\approx \\frac{12}{406,481,544} \\approx 1 \\text{ su } 34,000,000 .\n\\]\n\n# Calcolo del numero di combinazioni\ntotal_ways_acrostic = nCk(84, 6)\nacrostic_ways = 12\n\np_acrostic = acrostic_ways / total_ways_acrostic\nprint(f\"Probabilit√† dell'acrostico: {p_acrostic:.2e}\")\n\nProbabilit√† dell'acrostico: 2.95e-08\n\n\nQuesta analisi suggerisce che, secondo questo modello probabilistico, la ‚Äúcoincidenza‚Äù √® un evento altamente improbabile.",
    "crumbs": [
      "Probabilit√†",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduzione al calcolo delle probabilit√†</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_3/01_intro_prob.html#commenti-e-considerazioni-finali",
    "href": "chapters/chapter_3/01_intro_prob.html#commenti-e-considerazioni-finali",
    "title": "17¬† Introduzione al calcolo delle probabilit√†",
    "section": "17.5 Commenti e considerazioni finali",
    "text": "17.5 Commenti e considerazioni finali\nIn questo capitolo, abbiamo esplorato i fondamenti della teoria delle probabilit√†, tra cui la costruzione dello spazio campione per gli esperimenti casuali e le propriet√† fondamentali della probabilit√†. Abbiamo imparato a calcolare le probabilit√† degli eventi in uno spazio campione discreto. Inoltre, abbiamo introdotto il concetto di simulazione come metodo per approssimare le distribuzioni di probabilit√† empiriche quando non √® possibile ottenere soluzioni analitiche.\nLa teoria delle probabilit√† √® essenziale per la statistica e ha diverse applicazioni pratiche, tra cui la psicologia. Comprendere le probabilit√† ci consente di prendere decisioni informate in situazioni incerte e di sviluppare previsioni affidabili. Con una solida comprensione delle nozioni di base della probabilit√†, possiamo affrontare una vasta gamma di problemi e prendere decisioni basate sulla probabilit√† dei risultati possibili. Tuttavia, √® fondamentale ricordare che i modelli probabilistici sono solo approssimazioni della realt√† e possono essere influenzati da semplificazioni e limitazioni dei dati disponibili. Pertanto, √® importante esercitare cautela nell‚Äôinterpretazione dei risultati e comprendere le assunzioni alla base delle analisi.",
    "crumbs": [
      "Probabilit√†",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduzione al calcolo delle probabilit√†</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_3/01_intro_prob.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_3/01_intro_prob.html#informazioni-sullambiente-di-sviluppo",
    "title": "17¬† Introduzione al calcolo delle probabilit√†",
    "section": "17.6 Informazioni sull‚ÄôAmbiente di Sviluppo",
    "text": "17.6 Informazioni sull‚ÄôAmbiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m\n\nLast updated: Tue May 21 2024\n\nPython implementation: CPython\nPython version       : 3.12.3\nIPython version      : 8.22.2\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.4.0\nMachine     : arm64\nProcessor   : arm\nCPU cores   : 8\nArchitecture: 64bit\n\nscipy     : 1.13.0\narviz     : 0.18.0\nmatplotlib: 3.8.4\npandas    : 2.2.2\nseaborn   : 0.13.2\nnumpy     : 1.26.4\n\nWatermark: 2.4.3\n\n\n\n\n\n\n\nClayton, Aubrey. 2021. Bernoulli‚Äôs Fallacy: Statistical Illogic and the Crisis of Modern Science. Columbia University Press.\n\n\nDe Finetti, Bruno. 2017. Theory of Probability: A Critical Introductory Treatment. Vol. 6. John Wiley & Sons.\n\n\nFishburn, Peter C. 1986. ‚ÄúThe Axioms of Subjective Probability.‚Äù Statistical Science 1 (3): 335‚Äì45.\n\n\nHowson, Colin, and Peter Urbach. 2006. Scientific Reasoning: The Bayesian Approach. Open Court Publishing.\n\n\nKaplan, David. 2023. Bayesian Statistics for the Social Sciences. Guilford Publications.\n\n\nLindley, Dennis V. 2013. Understanding Uncertainty. John Wiley & Sons.\n\n\nPress, S James. 2009. Subjective and Objective Bayesian Statistics: Principles, Models, and Applications. John Wiley & Sons.\n\n\nRamsey, Frank P. 1926. ‚ÄúTruth and Probability.‚Äù In Readings in Formal Epistemology: Sourcebook, 21‚Äì45. Springer.",
    "crumbs": [
      "Probabilit√†",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduzione al calcolo delle probabilit√†</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_3/02_conditional_prob.html",
    "href": "chapters/chapter_3/02_conditional_prob.html",
    "title": "18¬† Probabilit√† condizionata",
    "section": "",
    "text": "18.1 Introduzione\nUn principio fondamentale nel campo della probabilit√† √® il concetto di condizionamento. Il condizionamento si verifica quando, all‚Äôinterno di un esperimento aleatorio, le probabilit√† vengono calcolate focalizzandosi esclusivamente su un sottoinsieme specifico dei risultati possibili. In pratica, questo significa che la probabilit√† viene determinata tenendo conto solo di quei risultati che rientrano in un certo criterio o condizione predefinita.",
    "crumbs": [
      "Probabilit√†",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Probabilit√† condizionata</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_3/02_conditional_prob.html#indipendenza-stocastica",
    "href": "chapters/chapter_3/02_conditional_prob.html#indipendenza-stocastica",
    "title": "18¬† Probabilit√† condizionata",
    "section": "18.2 Indipendenza Stocastica",
    "text": "18.2 Indipendenza Stocastica\nNel contesto della probabilit√† condizionata, il concetto di indipendenza gioca un ruolo fondamentale. Questa caratteristica permette di semplificare notevolmente il calcolo delle probabilit√† in molti problemi, evidenziando come la conoscenza di un evento non fornisca alcuna informazione aggiuntiva sull‚Äôaltro.\n\n18.2.1 Indipendenza di Due Eventi\nDue eventi \\(A\\) e \\(B\\) sono detti indipendenti se il verificarsi di uno non influenza la probabilit√† di verificarsi dell‚Äôaltro. Formalmente, questa condizione √® espressa come:\n\\[\\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\mathbb{P}(B),\\]\ndove \\(\\mathbb{P}(A \\cap B)\\) rappresenta la probabilit√† che entrambi gli eventi \\(A\\) e \\(B\\) si verifichino simultaneamente.\nSe questa condizione √® soddisfatta, scriviamo \\(A \\text{ ‚´´ } B\\), il che significa ‚ÄúA √® indipendente da B‚Äù.\n\n\n18.2.2 Indipendenza di un Insieme di Eventi\nL‚Äôindipendenza stocastica √® un concetto fondamentale nell‚Äôapplicazione della probabilit√† in campo statistico. Un insieme di eventi \\(\\{ A_i : i \\in I \\}\\) √® detto indipendente se per ogni sottoinsieme finito \\(J\\) di \\(I\\), la probabilit√† dell‚Äôintersezione degli eventi nel sottoinsieme \\(J\\) √® uguale al prodotto delle loro singole probabilit√†. Formalmente:\n\\[\\mathbb{P} \\left( \\cap_{i \\in J} A_i \\right) = \\prod_{i \\in J} \\mathbb{P}(A_i).\\]\nQuesto significa che ogni combinazione finita di eventi nell‚Äôinsieme √® indipendente.\nL‚Äôindipendenza pu√≤ essere assunta o derivata a seconda del contesto. In alcuni modelli o situazioni, assumiamo che certi eventi siano indipendenti perch√© questa assunzione semplifica i calcoli o riflette una conoscenza previa. In altri casi, l‚Äôindipendenza pu√≤ essere derivata dai dati o da altre propriet√† del modello.\n\n\n18.2.3 Eventi Disgiunti e Indipendenza\nEventi disgiunti (o mutuamente esclusivi) sono quelli che non possono verificarsi simultaneamente, cio√® \\(\\mathbb{P}(A \\cap B) = 0\\). Se due eventi disgiunti hanno una probabilit√† positiva di verificarsi, allora non possono essere indipendenti. Questo perch√© per eventi disgiunti con \\(\\mathbb{P}(A) &gt; 0\\) e \\(\\mathbb{P}(B) &gt; 0\\), l‚Äôequazione di indipendenza \\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\mathbb{P}(B)\\) non pu√≤ essere soddisfatta, dato che \\(\\mathbb{P}(A \\cap B) = 0\\) e \\(\\mathbb{P}(A) \\mathbb{P}(B) &gt; 0\\).",
    "crumbs": [
      "Probabilit√†",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Probabilit√† condizionata</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_3/02_conditional_prob.html#teorema-della-probabilit√†-composta",
    "href": "chapters/chapter_3/02_conditional_prob.html#teorema-della-probabilit√†-composta",
    "title": "18¬† Probabilit√† condizionata",
    "section": "18.4 Teorema della probabilit√† composta",
    "text": "18.4 Teorema della probabilit√† composta\n√à possibile scrivere l‚ÄôEquation¬†18.1 nella forma:\n\\[\nP(A \\cap B) = P(B)P(A \\mid B) = P(A)P(B \\mid A).\n\\tag{18.2}\\]\nQuesto secondo modo di scrivere l‚ÄôEquation¬†18.1 √® chiamato teorema della probabilit√† composta (o regola moltiplicativa, o regola della catena). La legge della probabilit√† composta ci dice che la probabilit√† che si verifichino contemporaneamente due eventi \\(A\\) e \\(B\\) √® pari alla probabilit√† di uno dei due eventi moltiplicata per la probabilit√† dell‚Äôaltro evento condizionata al verificarsi del primo.\nL‚Äôl‚ÄôEquation¬†18.2 si estende al caso di \\(n\\) eventi \\(A_1, \\dots, A_n\\) nella forma seguente:\n\\[\nP\\left( \\bigcap_{k=1}^n A_k \\right) = \\prod_{k=1}^n P\\left(  A_k  \\ \\Biggl\\lvert \\ \\bigcap_{j=1}^{k-1} A_j \\right).\n\\tag{18.3}\\]\nPer esempio, nel caso di quattro eventi abbiamo\n\\[\n\\begin{split}\nP(&A_1 \\cap A_2 \\cap A_3 \\cap A_4) =  \\\\\n& P(A_1) \\cdot P(A_2 \\mid A_1) \\cdot  P(A_3 \\mid A_1 \\cap A_2) \\cdot P(A_4 \\mid A_1 \\cap A_2 \\cap A_{3}).\\notag\n\\end{split}\n\\]\n\nExample 18.5 Per fare un esempio, consideriamo il problema seguente. Da un‚Äôurna contenente 6 palline bianche e 4 nere si estrae una pallina per volta, senza reintrodurla nell‚Äôurna. Indichiamo con \\(B_i\\) l‚Äôevento: ‚Äúesce una pallina bianca alla \\(i\\)-esima estrazione‚Äù e con \\(N_i\\) l‚Äôestrazione di una pallina nera. L‚Äôevento: ‚Äúescono due palline bianche nelle prime due estrazioni‚Äù √® rappresentato dalla intersezione \\(\\{B_1 \\cap B_2\\}\\) e, per l‚ÄôEquation¬†18.2, la sua probabilit√† vale\n\\[\nP(B_1 \\cap B_2) = P(B_1)P(B_2 \\mid B_1).\n\\]\n\\(P(B_1)\\) vale 6/10, perch√© nella prima estrazione \\(\\Omega\\) √® costituito da 10 elementi: 6 palline bianche e 4 nere. La probabilit√† condizionata \\(P(B_2 \\mid B_1)\\) vale 5/9, perch√© nella seconda estrazione, se √® verificato l‚Äôevento \\(B_1\\), lo spazio campionario consiste di 5 palline bianche e 4 nere. Si ricava pertanto:\n\\[\nP(B_1 \\cap B_2) = \\frac{6}{10} \\cdot \\frac{5}{9} = \\frac{1}{3}.\n\\]\nIn modo analogo si ha che\n\\[\nP(N_1 \\cap N_2) = P(N_1)P(N_2 \\mid N_1) = \\frac{4}{10} \\cdot \\frac{3}{9} = \\frac{4}{30}.\n\\]\nSe l‚Äôesperimento consiste nell‚Äôestrazione successiva di 3 palline, la probabilit√† che queste siano tutte bianche, per l‚ÄôEquation¬†18.3, vale\n\\[\n\\begin{aligned}\nP(B_1 \\cap B_2 \\cap B_3) &=P(B_1)P(B_2 \\mid B_1)P(B_3 \\mid B_1 \\cap B_2) \\notag\\\\\n&=\\frac{6}{10}\\cdot\\frac{5}{9} \\cdot\\frac{4}{8} \\notag\\\\\n&= \\frac{1}{6}.\n\\end{aligned}\n\\]\nLa probabilit√† dell‚Äôestrazione di tre palline nere √® invece:\n\\[\n\\begin{aligned}\nP(N_1 \\cap N_2 \\cap N_3) &= P(N_1)P(N_2 \\mid N_1)P(N_3 \\mid N_1 \\cap N_2)\\notag\\\\\n&= \\frac{4}{10} \\cdot \\frac{3}{9} \\cdot \\frac{2}{8} \\notag\\\\\n&= \\frac{1}{30}.\\notag\n\\end{aligned}\n\\]",
    "crumbs": [
      "Probabilit√†",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Probabilit√† condizionata</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_3/02_conditional_prob.html#il-teorema-della-probabilit√†-totale",
    "href": "chapters/chapter_3/02_conditional_prob.html#il-teorema-della-probabilit√†-totale",
    "title": "18¬† Probabilit√† condizionata",
    "section": "18.5 Il teorema della probabilit√† totale",
    "text": "18.5 Il teorema della probabilit√† totale\nIl teorema della probabilit√† totale (detto anche teorema delle partizioni) afferma che se abbiamo una partizione di uno spazio campionario \\(\\Omega\\) in \\(n\\) eventi mutualmente esclusivi e tali che la loro unione formi \\(\\Omega\\), allora la probabilit√† di un qualsiasi evento in \\(\\Omega\\) pu√≤ essere calcolata sommando la probabilit√† dell‚Äôevento su ciascun sottoinsieme della partizione, pesata in base alla probabilit√† del sottoinsieme.\nIn altre parole, se \\(H_1, H_2, \\dots, H_n\\) sono eventi mutualmente esclusivi e tali che \\(\\bigcup_{i=1}^n H_i = \\Omega\\), allora per ogni evento \\(E \\subseteq \\Omega\\), la probabilit√† di \\(E\\) √® data dalla formula:\n\\[\nP(E) = \\sum_{i=1}^n P(E \\mid H_i)P(H_i),\n\\tag{18.4}\\]\ndove \\(P(E \\mid H_i)\\) rappresenta la probabilit√† condizionata di \\(E\\) dato che si √® verificato l‚Äôevento \\(H_i\\), e \\(P(H_i)\\) √® la probabilit√† dell‚Äôevento \\(H_i\\).\nIl teorema della probabilit√† totale riveste un ruolo fondamentale in quanto fornisce il denominatore nel teorema di Bayes, svolgendo la funzione di costante di normalizzazione. Questa costante di normalizzazione √® di vitale importanza per assicurare che la distribuzione a posteriori sia una distribuzione di probabilit√† valida. Per ulteriori dettagli e approfondimenti, √® possibile fare riferimento al ?sec-subj-prop.\nNell‚Äôambito della probabilit√† discreta, questo teorema viene usato quando abbiamo una partizione dello spazio campionario e vogliamo calcolare la probabilit√† di un evento, sfruttando le probabilit√† dei singoli eventi della partizione. Il caso pi√π semplice √® quello di una partizione dello spazio campione in due sottoinsiemi: \\(P(E) = P(E \\cap H_1) + P(E \\cap H_2)\\).\n\n\n\n\n\n\nFigure¬†18.2: Partizione dello spazio campionario per il teorema di Bayes.\n\n\n\nIn tali circostanza abbiamo che\n\\[\nP(E) = P(E \\mid H_1) P(H_1) + P(E \\mid H_2) P(H_2).\n\\]\nL‚ÄôEquation¬†18.4 √® utile per calcolare \\(P(E)\\), se \\(P(E \\mid H_i)\\) e \\(P(H_i)\\) sono facili da trovare.\n\nExample 18.6 Abbiamo tre urne, ciascuna delle quali contiene 100 palline:\n\nUrna 1: 75 palline rosse e 25 palline blu,\nUrna 2: 60 palline rosse e 40 palline blu,\nUrna 3: 45 palline rosse e 55 palline blu.\n\nUna pallina viene estratta a caso da un‚Äôurna anch‚Äôessa scelta a caso. Qual √® la probabilit√† che la pallina estratta sia di colore rosso?\nSia \\(R\\) l‚Äôevento ‚Äúla pallina estratta √® rossa‚Äù e sia \\(U_i\\) l‚Äôevento che corrisponde alla scelta dell‚Äô\\(i\\)-esima urna. Sappiamo che\n\\[\nP(R \\mid U_1) = 0.75, \\quad P(R \\mid U_2) = 0.60, \\quad P(R \\mid U_3) = 0.45.\n\\]\nGli eventi \\(U_1\\), \\(U_2\\) e \\(U_3\\) costituiscono una partizione dello spazio campione in quanto \\(U_1\\), \\(U_2\\) e \\(U_3\\) sono eventi mutualmente esclusivi ed esaustivi, ovvero \\(P(U_1 \\cup U_2 \\cup U_3) = 1.0\\). In base al teorema della probabilit√† totale, la probabilit√† di estrarre una pallina rossa √® dunque\n\\[\n\\begin{split}\nP(R) &= P(R \\mid U_1)P(U_1) + P(R \\mid U_2)P(U_2) + P(R \\mid U_3)P(U_3) \\\\\n&= 0.75 \\cdot \\frac{1}{3}+0.60 \\cdot \\frac{1}{3}+0.45 \\cdot \\frac{1}{3} \\\\\n&=0.60.\n\\end{split}\n\\]\n\n\n18.5.1 Indipendenza e probabilit√† condizionata\nL‚Äôindipendenza tra due eventi \\(A\\) e \\(B\\) pu√≤ essere espressa in modo intuitivo utilizzando la probabilit√† condizionata. Se \\(A\\) e \\(B\\) sono indipendenti, il verificarsi di uno degli eventi non influisce sulla probabilit√† del verificarsi dell‚Äôaltro. In altre parole, la probabilit√† che \\(A\\) accada non cambia se sappiamo che \\(B\\) √® avvenuto, e viceversa.\nPossiamo esprimere questa idea con le seguenti equazioni:\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)} = P(A),\n\\]\n\\[\nP(B \\mid A) = \\frac{P(A \\cap B)}{P(A)} = P(B).\n\\]\nQuindi, due eventi \\(A\\) e \\(B\\) sono indipendenti se soddisfano le condizioni:\n\\[\nP(A \\mid B) = P(A),\n\\]\n\\[\nP(B \\mid A) = P(B).\n\\]\nQuesto significa che la probabilit√† di \\(A\\) rimane invariata indipendentemente dal fatto che \\(B\\) sia accaduto o meno, e lo stesso vale per \\(B\\).\n\n18.5.1.1 Indipendenza di Tre Eventi\nTre eventi \\(A\\), \\(B\\) e \\(C\\) sono indipendenti se soddisfano le seguenti condizioni:\n\\[\n\\begin{align}\nP(A \\cap B) &= P(A) P(B), \\\\\nP(A \\cap C) &= P(A) P(C), \\\\\nP(B \\cap C) &= P(B) P(C), \\\\\nP(A \\cap B \\cap C) &= P(A) P(B) P(C).\n\\end{align}\n\\]\nLe prime tre condizioni verificano l‚Äôindipendenza a due a due, ovvero l‚Äôindipendenza di ciascuna coppia di eventi. Tuttavia, per essere completamente indipendenti, deve essere soddisfatta anche l‚Äôultima condizione, che riguarda l‚Äôintersezione di tutti e tre gli eventi. Solo se tutte queste condizioni sono soddisfatte possiamo dire che \\(A\\), \\(B\\) e \\(C\\) sono completamente indipendenti.\nIn sintesi, l‚Äôindipendenza tra eventi implica che la conoscenza del verificarsi di uno non fornisce alcuna informazione sulla probabilit√† del verificarsi degli altri.\n\nExample 18.7 Consideriamo un esempio utilizzando un mazzo di 52 carte. Ogni seme contiene 13 carte e ci sono 4 regine in totale. Definiamo i seguenti eventi:\n\nEvento A: pescare una carta di picche,\nEvento B: pescare una regina.\n\nProbabilit√† con un mazzo completo\nIn un mazzo completo, la probabilit√† di pescare una carta di picche (\\(P(A)\\)) √® $ = \\(, poich√© ci sono 13 picche su 52 carte totali. La probabilit√† di pescare una regina (\\)P(B)$) √® $ = $, poich√© ci sono 4 regine su 52 carte.\nOra consideriamo la probabilit√† congiunta di pescare la regina di picche (\\(P(AB)\\)). Poich√© esiste solo una regina di picche nel mazzo, la probabilit√† di pescare questa specifica carta √® $ $.\nSecondo la definizione di indipendenza, se gli eventi \\(A\\) e \\(B\\) sono indipendenti, allora:\n\\[ P(AB) = P(A)P(B) \\]\nCalcoliamo \\(P(A)P(B)\\):\n\\[ P(A)P(B) = \\left( \\frac{1}{4} \\right) \\left( \\frac{1}{13} \\right) = \\frac{1}{52} \\]\nPoich√© \\(P(AB) = \\frac{1}{52}\\) √® uguale a \\(P(A)P(B)\\), possiamo affermare che gli eventi \\(A\\) e \\(B\\) sono indipendenti con un mazzo completo di 52 carte.\nProbabilit√† dopo la rimozione di una carta\nConsideriamo ora un mazzo con una carta in meno, ad esempio il due di quadri, riducendo il numero totale di carte a 51. Ricalcoliamo le probabilit√† con questo mazzo ridotto:\nLa probabilit√† di pescare la regina di picche (\\(P(AB)\\)) √® ora $ $, poich√© ci sono 51 carte nel mazzo.\nRicalcoliamo anche \\(P(A)\\) e \\(P(B)\\):\n\n\\(P(A)\\) diventa $ $, poich√© ci sono ancora 13 picche, ma su 51 carte.\n\\(P(B)\\) diventa $ $, poich√© ci sono ancora 4 regine, ma su 51 carte.\n\nOra calcoliamo il prodotto \\(P(A)P(B)\\) con queste nuove probabilit√†:\n\\[ P(A)P(B) = \\left( \\frac{13}{51} \\right) \\left( \\frac{4}{51} \\right) = \\frac{52}{2601} \\]\nConfrontiamo \\(P(AB)\\) e \\(P(A)P(B)\\):\n\\[ \\frac{1}{51} \\neq \\frac{52}{2601} \\]\nPoich√© $ $, gli eventi \\(A\\) e \\(B\\) non sono pi√π indipendenti dopo la rimozione del due di quadri.\nQuesto esempio mostra come l‚Äôindipendenza tra due eventi dipenda dal contesto. Con un mazzo completo, i due eventi sono indipendenti. Tuttavia, rimuovendo una carta dal mazzo, le probabilit√† cambiano e gli eventi non sono pi√π indipendenti. Questo evidenzia l‚Äôimportanza di considerare la composizione e le condizioni iniziali quando si analizzano probabilit√† e indipendenza. Modifiche nella composizione del mazzo possono alterare le probabilit√†, influenzando le relazioni di indipendenza tra eventi specifici.\nIn generale, l‚Äôindipendenza tra due eventi significa che la probabilit√† di uno non √® influenzata dal verificarsi dell‚Äôaltro. Questo concetto √® cruciale per analisi probabilistiche e modelli statistici pi√π complessi.\n\n\nExample 18.8 Nel lancio di due dadi non truccati, si considerino gli eventi: \\(A\\) = ‚Äúesce un 1 o un 2 nel primo lancio‚Äù e \\(B\\) = ‚Äúil punteggio totale √® 8‚Äù. Gli eventi \\(A\\) e \\(B\\) sono indipendenti?\nCalcoliamo \\(P(A)\\):\n\nr = range(1, 7)\nsample = [(i, j) for i in r for j in r]\nA = [roll for roll in sample if roll[0] == 1 or roll[0] == 2]\nprint(A)\nprint(f\"{len(A)} / {len(sample)}\")\n\n[(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6)]\n12 / 36\n\n\nCalcoliamo \\(P(B)\\):\n\nB = [roll for roll in sample if roll[0] + roll[1] == 8]\nprint(B)\nprint(f\"{len(B)} / {len(sample)}\")\n\n[(2, 6), (3, 5), (4, 4), (5, 3), (6, 2)]\n5 / 36\n\n\nCalcoliamo \\(P(A \\cap B)\\):\n\nI = [\n    roll\n    for roll in sample\n    if (roll[0] == 1 or roll[0] == 2) and (roll[0] + roll[1] == 8)\n]\nprint(I)\nprint(f\"{len(I)} / {len(sample)}\")\n\n[(2, 6)]\n1 / 36\n\n\nGli eventi \\(A\\) e \\(B\\) non sono statisticamente indipendenti dato che \\(P(A \\cap B) \\neq P(A)P(B)\\):\n\n12/36 * 5/36 == 1/36\n\nFalse",
    "crumbs": [
      "Probabilit√†",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Probabilit√† condizionata</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_3/02_conditional_prob.html#commenti-e-considerazioni-finali",
    "href": "chapters/chapter_3/02_conditional_prob.html#commenti-e-considerazioni-finali",
    "title": "18¬† Probabilit√† condizionata",
    "section": "18.6 Commenti e considerazioni finali",
    "text": "18.6 Commenti e considerazioni finali\nLa probabilit√† condizionata riveste un ruolo fondamentale poich√© ci permette di definire in modo preciso il concetto di indipendenza statistica. Uno degli aspetti cruciali dell‚Äôanalisi statistica riguarda la valutazione dell‚Äôassociazione tra due variabili. Nel capitolo attuale, ci siamo concentrati sul concetto di indipendenza, che indica l‚Äôassenza di relazione tra le variabili. Tuttavia, in futuro, esploreremo come fare inferenze sulla correlazione tra variabili, ovvero come determinare se le variabili sono associate tra loro o se esiste una relazione statistica credibile tra di esse.\nNell‚Äôambito dell‚Äôinferenza bayesiana, il condizionamento emerge come uno strumento essenziale. L‚Äôinferenza bayesiana √® un approccio statistico che sfrutta proprio il condizionamento per rivedere e aggiornare le credenze o le incertezze relative a determinate ipotesi, basandosi sull‚Äôintroduzione di nuove informazioni.\nIl processo inizia stabilendo una probabilit√† iniziale, denominata probabilit√† a priori (\\(P(A)\\)), che esprime la nostra convinzione o supposizione iniziale riguardo all‚Äôipotesi \\(A\\), prima di ricevere qualsiasi dato aggiuntivo. Questa probabilit√† a priori si fonda su conoscenze gi√† acquisite o su supposizioni precedentemente formulate.\nIl cuore dell‚Äôinferenza bayesiana si trova nell‚Äôaggiornamento di questa credenza iniziale in risposta all‚Äôacquisizione di nuove informazioni, rappresentate dalla variabile \\(E\\). L‚Äôaggiornamento avviene mediante il condizionamento, culminando nella determinazione di una probabilit√† a posteriori (\\(P(A | E)\\)). Questa nuova probabilit√† rappresenta la nostra credenza aggiornata sull‚Äôipotesi \\(A\\) dopo aver preso in esame l‚Äôevidenza \\(E\\) appena acquisita. In questo modo, l‚Äôinferenza bayesiana permette di affinare le nostre supposizioni e le nostre previsioni su determinati fenomeni, incorporando sistematicamente nuove prove nel nostro quadro di conoscenza.\nLa formula di Bayes governa questo processo di aggiornamento:\n\\[\nP(A | E) = \\frac{P(E | A) \\times P(A)}{P(E)}\n\\]\nIn questa formula, troviamo: - \\(P(A | E)\\): la probabilit√† a posteriori, che √® la probabilit√† dell‚Äôipotesi \\(A\\) date le nuove prove \\(E\\). - \\(P(E | A)\\): la verosimiglianza, ovvero la probabilit√† di osservare le prove \\(E\\) se l‚Äôipotesi \\(A\\) fosse vera. - \\(P(A)\\): la probabilit√† a priori, che indica il nostro livello di convinzione iniziale nell‚Äôipotesi \\(A\\). - \\(P(E)\\): la probabilit√† di osservare le prove \\(E\\), tenendo conto di tutte le ipotesi possibili.\nIn questo capitolo esploreremo alcuni concetti chiave per una comprensione approfondita dell‚Äôaggiornamento bayesiano:\n\nProbabilit√† Congiunta: Questa √® la probabilit√† che due eventi avvengano insieme. Per esempio, potrebbe riferirsi alla probabilit√† di estrarre una pallina rossa e poi una verde da un‚Äôurna in sequenza.\nProbabilit√† Marginale: Si tratta della probabilit√† di verificarsi di un singolo evento, considerato a prescindere da altri eventi. Ad esempio, potremmo voler calcolare la probabilit√† di estrarre una pallina verde da un‚Äôurna, senza considerare altri eventi.\nProbabilit√† Condizionata: Indica la probabilit√† che un evento si verifichi, dato che un altro evento correlato √® gi√† accaduto. Un esempio potrebbe essere la probabilit√† di estrarre una seconda pallina verde, sapendo che la prima estratta era verde.\n\nQuesti concetti sono fondamentali per navigare nel processo di inferenza bayesiana e per comprendere come le probabilit√† si aggiornano in risposta a nuove informazioni. Inoltre, esamineremo i principali teoremi legati alla probabilit√† condizionata.",
    "crumbs": [
      "Probabilit√†",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Probabilit√† condizionata</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_3/02_conditional_prob.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_3/02_conditional_prob.html#informazioni-sullambiente-di-sviluppo",
    "title": "18¬† Probabilit√† condizionata",
    "section": "18.7 Informazioni sull‚ÄôAmbiente di Sviluppo",
    "text": "18.7 Informazioni sull‚ÄôAmbiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m \n\nLast updated: Wed Feb 21 2024\n\nPython implementation: CPython\nPython version       : 3.11.7\nIPython version      : 8.21.0\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.3.0\nMachine     : x86_64\nProcessor   : i386\nCPU cores   : 8\nArchitecture: 64bit\n\npandas: 2.2.0\nnumpy : 1.26.4\n\nWatermark: 2.4.3",
    "crumbs": [
      "Probabilit√†",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Probabilit√† condizionata</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html",
    "title": "19¬† Linguaggio Stan",
    "section": "",
    "text": "19.1 Introduzione\nNel presente capitolo, presenteremo un linguaggio di programmazione probabilistica denominato Stan. Stan consente di estrarre campioni da distribuzioni di probabilit√† mediante la costruzione di una catena di Markov, la cui distribuzione di equilibrio (o stazionaria) coincide con la distribuzione desiderata. Il nome del linguaggio deriva da uno dei pionieri del metodo Monte Carlo, Stanislaw Ulam. Un‚Äôintroduzione dettagliata al linguaggio Stan √® fornita in (appendix-cmdstanpy?). In questo capitolo, utilizzeremo Stan per fare inferenza su una proporzione.\nIl linguaggio di programmazione probabilistica Stan √® compatibile con diverse piattaforme e offre varie interfacce (R, Python, Julia). In questo corso, useremo CmdStanPy, un‚Äôinterfaccia per Stan pensata per gli utenti di Python. CmdStanPy √® un pacchetto puramente in Python3 che √® un wrapper di CmdStan, l‚Äôinterfaccia a riga di comando per Stan scritta in C++. Pertanto, oltre a Python3, CmdStanPy richiede un toolchain C++ per compilare ed eseguire i modelli Stan.\nLa procedura per installare CmdStanPy e i componenti sottostanti di CmdStan dal repository conda-forge √® descritta nel capitolo Appendix E.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#inferenza-bayesiana-e-metodi-mcmc",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#inferenza-bayesiana-e-metodi-mcmc",
    "title": "19¬† Linguaggio Stan",
    "section": "19.2 Inferenza Bayesiana e Metodi MCMC",
    "text": "19.2 Inferenza Bayesiana e Metodi MCMC\nL‚Äôinferenza bayesiana, impiegata per la stima dei parametri, la previsione e la valutazione della probabilit√† di eventi, si basa sulle aspettative a posteriori. Queste aspettative si configurano come integrali multidimensionali nello spazio dei parametri. Stan, un software all‚Äôavanguardia per l‚Äôanalisi statistica, si avvale del metodo Monte Carlo per risolvere questi integrali complessi. I metodi Monte Carlo sfruttano il campionamento casuale per affrontare integrali ad alta dimensionalit√†.\nTuttavia, per la maggior parte dei problemi bayesiani, non √® possibile utilizzare i metodi Monte Carlo standard, poich√© non √® fattibile generare campioni indipendenti dalla densit√† a posteriori di interesse, fatta eccezione per modelli estremamente semplici con prior coniugati. Di conseguenza, √® necessario ricorrere ai metodi Monte Carlo a Catena di Markov (MCMC), che producono campioni correlati tra loro. Stan implementa il Monte Carlo Hamiltoniano (HMC), il metodo MCMC pi√π efficiente e scalabile per le densit√† target. Altri metodi, come il Metropolis-Hastings e il campionamento di Gibbs, risultano pi√π semplici ma meno efficienti dell‚ÄôHMC.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#stan-e-la-programmazione-probabilistica",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#stan-e-la-programmazione-probabilistica",
    "title": "19¬† Linguaggio Stan",
    "section": "19.3 Stan e la Programmazione Probabilistica",
    "text": "19.3 Stan e la Programmazione Probabilistica\nStan si configura come un linguaggio di programmazione probabilistica (PPL) concepito per definire modelli statistici complessi e effettuare inferenze su di essi. Un PPL consente di esprimere modelli probabilistici in modo conciso e di utilizzare algoritmi avanzati per l‚Äôinferenza. Ci√≤ risulta particolarmente utile nell‚Äôinferenza bayesiana, dove si aggiornano le distribuzioni a priori con dati osservati per ottenere distribuzioni a posteriori.\n\n19.3.1 Struttura di un Programma Stan\nUn programma Stan richiede la specificazione di variabili e parametri, definendo le distribuzioni a priori dei parametri del modello statistico e la funzione di verosimiglianza. In sostanza, un programma Stan descrive l‚Äôinterazione tra dati e parametri e le distribuzioni probabilistiche che li governano. Questo consente di effettuare inferenze sulle distribuzioni a posteriori dei parametri del modello, dedotte dai dati osservati e dalle distribuzioni a priori.\n\n\n19.3.2 Esecuzione di un Programma Stan\nUn programma Stan utilizza metodi di inferenza avanzati:\n\nCampionamento MCMC: Stan impiega metodi come il Monte Carlo a Catena di Markov per generare campioni dalle distribuzioni a posteriori.\nInferenza Variazionale: Un metodo approssimativo che fornisce stime delle distribuzioni a posteriori.\nApprossimazione di Laplace: Un ulteriore metodo approssimativo per l‚Äôinferenza.\n\nStan √® accessibile attraverso vari linguaggi di programmazione e strumenti di analisi open-source, tra cui Python, R e Julia, ed √® compatibile con gli strumenti di analisi bayesiana integrati in questi linguaggi. √à inoltre disponibile in ambienti come Mathematica, Stata e MATLAB, sebbene queste interfacce siano meno complete.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#simulazione-in-avanti-e-problema-inverso",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#simulazione-in-avanti-e-problema-inverso",
    "title": "19¬† Linguaggio Stan",
    "section": "19.4 Simulazione in Avanti e Problema Inverso",
    "text": "19.4 Simulazione in Avanti e Problema Inverso\nStan genera dati attraverso procedure pseudo-casuali, applicabili sia per simulazioni in avanti che per risolvere il problema inverso.\n\n19.4.1 Simulazione in Avanti\nLa simulazione in avanti consiste nella generazione di dati simulati a partire da un insieme di parametri noti di un modello probabilistico. In altri termini, date determinate assunzioni sui parametri di un modello, si utilizza la simulazione in avanti per prevedere i possibili risultati.\nAd esempio, consideriamo uno studio clinico con \\(N\\) soggetti e una probabilit√† \\(\\theta\\) di esito positivo per ciascun soggetto. Conoscendo il valore di \\(\\theta\\) e il numero di soggetti \\(N\\), possiamo impiegare una distribuzione binomiale per simulare il numero di pazienti che avranno un esito positivo. Questo processo ci consente di generare dati che riflettono le nostre assunzioni sui parametri del modello.\nIn notazione statistica, questo si esprime come:\n\\[\nY \\sim \\text{Binomiale}(N, \\theta)\n\\]\ndove \\(Y\\) rappresenta il numero di esiti positivi su \\(N\\) pazienti, con probabilit√† \\(\\theta\\) di esito positivo per ciascun paziente.\n\n19.4.1.1 Esempio di Simulazione in Avanti\nSupponiamo di avere \\(N = 100\\) soggetti in uno studio clinico e un tasso di successo \\(\\theta = 0.3\\). Possiamo simulare un risultato \\(Y\\) generando casualmente il numero di soggetti con esito positivo. Utilizzando una distribuzione binomiale, possiamo calcolare la probabilit√† di ottenere esattamente \\(y\\) esiti positivi su \\(N\\) tentativi:\n\\[\np(Y = y \\mid N, \\theta) = \\binom{N}{y} \\cdot \\theta^y \\cdot (1 - \\theta)^{N - y}\n\\]\nQuesta espressione ci permette di calcolare la probabilit√† di ottenere un certo numero di successi, dato il numero di soggetti e la probabilit√† di successo.\n\n\n\n19.4.2 Il Problema Inverso\nIl problema inverso consiste nella stima dei parametri del modello, come la probabilit√† di successo \\(\\theta\\), dato un insieme di dati osservati. Supponiamo di avere i seguenti dati: \\(N = 100\\) soggetti e \\(y = 32\\) esiti positivi. L‚Äôobiettivo √® stimare \\(\\theta\\), la probabilit√† di successo.\nNell‚Äôapproccio bayesiano, si inizia specificando una distribuzione a priori per \\(\\theta\\). Supponiamo di utilizzare una distribuzione Beta(\\(\\alpha\\), \\(\\beta\\)) come prior per \\(\\theta\\), dove \\(\\alpha\\) e \\(\\beta\\) sono parametri scelti in base alle conoscenze precedenti. La distribuzione a posteriori di \\(\\theta\\) data l‚Äôosservazione \\(y\\) √® ancora una distribuzione Beta, ma con parametri aggiornati:\n\\[\n\\theta \\mid y \\sim \\text{Beta}(\\alpha + y, \\beta + N - y)\n\\]\nAd esempio, scegliendo una distribuzione a priori non informativa con \\(\\alpha = 1\\) e \\(\\beta = 1\\), la distribuzione a posteriori diventa:\n\\[\n\\theta \\mid y \\sim \\text{Beta}(1 + 32, 1 + 100 - 32) = \\text{Beta}(33, 69)\n\\]\nQuesta distribuzione a posteriori fornisce una stima aggiornata della probabilit√† di successo \\(\\theta\\) considerando i dati osservati. Utilizzando Stan, √® possibile ottenere campioni da questa distribuzione a posteriori per calcolare statistiche riassuntive e effettuare previsioni.\nIn sintesi, la simulazione in avanti e il problema inverso rappresentano due approcci complementari: la simulazione in avanti genera dati simulati da parametri noti, mentre il problema inverso stima i parametri del modello dai dati osservati.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#un-primo-programma-in-stan",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#un-primo-programma-in-stan",
    "title": "19¬† Linguaggio Stan",
    "section": "19.5 Un Primo Programma in Stan",
    "text": "19.5 Un Primo Programma in Stan\n\n19.5.1 Generazione di Dati Casuali\nSupponiamo di voler generare dei valori casuali $ Y $ da una distribuzione binomiale con parametri $ N $ e $ $. Ad esempio, possiamo impostare $ = 0.3 $, per rappresentare una probabilit√† del 30% di un esito positivo (in statistica, il termine ‚Äòsuccesso‚Äô indica un esito positivo), e possiamo impostare $ N = 100 $. Il seguente programma Stan pu√≤ essere utilizzato per generare valori di $ Y $ compresi tra 0 e 100.\ndata {\n  int&lt;lower=0&gt; N;\n  real&lt;lower=0, upper=1&gt; theta;\n}\n\ngenerated quantities {\n  int&lt;lower=0, upper=N&gt; y;\n  y = binomial_rng(N, theta);\n}\n\n\n19.5.2 Organizzazione di un Programma Stan\nLa prima cosa da notare √® che un programma Stan √® organizzato in blocchi. Qui abbiamo due blocchi: un blocco dei dati contenente le dichiarazioni delle variabili che devono essere fornite come dati e un blocco delle quantit√† generate, che non solo dichiara variabili ma assegna loro un valore. In questo programma Stan, la variabile y viene assegnata come risultato di una singola estrazione da una distribuzione \\(\\textrm{binomiale}(N, \\theta)\\), che Stan fornisce attraverso la funzione binomial_rng.\n\n\n19.5.3 Tipi di Variabili in Stan\nLa seconda cosa da notare √® che tutte le variabili in un programma Stan sono dichiarate con tipi specifici. Stan utilizza la tipizzazione statica, il che significa che, a differenza di Python o R, il tipo di una variabile √® dichiarato nel programma prima del suo utilizzo, piuttosto che essere determinato al momento dell‚Äôesecuzione in base al valore assegnato. Una volta dichiarato, il tipo di una variabile non cambia mai. Stan utilizza anche la tipizzazione forte, il che significa che, a differenza di C o C++, non √® possibile aggirare le restrizioni di tipo per accedere direttamente alla memoria.\nIl programma dichiara tre variabili: N e y di tipo int (interi) e theta di tipo real (numeri reali). Nei computer, gli interi hanno limiti precisi e i numeri reali possono avere errori di calcolo. Stan utilizza numeri reali con precisione doppia (64 bit) secondo lo standard IEEE 754, tranne in alcune operazioni ottimizzate che possono perdere un po‚Äô di precisione.\n\n\n19.5.4 Vincoli sui Tipi\nUn tipo di variabile pu√≤ avere dei vincoli. Poich√© N √® un conteggio, deve essere maggiore o uguale a zero, cosa che indichiamo con il vincolo lower=0. Allo stesso modo, la variabile y, che rappresenta il numero di esiti positivi su N, deve essere compresa tra 0 e N (inclusi); questo √® indicato con il vincolo lower=0, upper=N. Infine, la variabile theta √® un numero reale e deve essere compresa tra 0 e 1, cosa che indichiamo con il vincolo lower=0, upper=1. Anche se tecnicamente i limiti per i numeri reali sono aperti, in pratica possiamo ottenere valori di 0 o 1 a causa di errori di arrotondamento nei calcoli.\n\n\n19.5.5 Esecuzione del Programma Stan\nLa funzione cmdstan_model() crea un nuovo oggetto CmdStanModel a partire da un file contenente un programma Stan. In background, CmdStan traduce un programma Stan in C++ e creare un eseguibile compilato.\n\nmodel = CmdStanModel(stan_file='../../stan/binomial-rng.stan')\n\nDurante l‚Äôesecuzione, il programma Stan compilato richiede i valori di N e theta. Ad ogni iterazione, il programma campiona un valore di y utilizzando il suo generatore di numeri pseudocasuali integrato. I valori di N e theta devono essere forniti in un dizionario Python.\n\nN = 100\ntheta = 0.3\ndata = {'N': N, 'theta': theta}\n\nInfine campioniamo dal modello utilizzando il metodo sample di CmdStanModel.\n\ntrace = model.sample(\n    data=data, seed=123, \n    chains=1,\n    iter_sampling=10, \n    iter_warmup=1,\n    show_progress=False, \n    show_console=False\n)\n\n\n\n19.5.6 Costruzione del Modello\nIl costruttore di CmdStanModel viene utilizzato per creare un modello a partire da un programma Stan presente nel file specificato. Si consiglia vivamente di utilizzare un file separato per i programmi Stan, in modo da facilitarne la condivisione, permettere l‚Äôuso sia di virgolette che di apostrofi e rendere pi√π agevole individuare le linee di riferimento nei messaggi di errore. In pratica, il processo inizia con la traduzione del programma Stan in una classe C++ utilizzando un traduttore specifico. Successivamente, il programma C++ viene compilato, operazione che richiede circa venti secondi.\n\n\n19.5.7 Interfaccia Python\nNell‚Äôinterfaccia Python, il metodo sample() accetta i seguenti argomenti:\n\ndata: i dati letti nel blocco dati del programma Stan,\nseed: generatore di numeri pseudocasuali per la riproducibilit√†,\nchains: il numero di simulazioni da eseguire (parallel_chains indica quante eseguire in parallelo),\niter_sampling: numero di estrazioni (cio√®, dimensione del campione) da restituire,\niter_warmup: numero di iterazioni di riscaldamento per tarare i parametri dell‚Äôalgoritmo di campionamento (non necessari qui, quindi impostato a 0),\nshow_progress: se True, stampa aggiornamenti di progresso,\nshow_console: apre un monitor di progresso GUI.\n\nIl risultato della chiamata a sample() sull‚Äôistanza del modello viene assegnato alla variabile trace e contiene le 10 estrazioni richieste con l‚Äôargomento iter_sampling = 10.\nQuando si chiama model.sample(...), CmdStan esegue Stan come programma C++ autonomo in un processo in background. Questo programma inizia copiando i dati forniti nell‚Äôargomento data di Python in un file, quindi legge quel file di dati per costruire un oggetto C++ che rappresenta il modello statistico. Poich√© il nostro programma Stan ha solo un blocco di quantit√† generate, l‚Äôunico compito rimanente della classe C++ √® generare il numero richiesto di estrazioni. Per ciascuna delle estrazioni specificate da iter_sampling, Stan utilizza un generatore di numeri pseudocasuali per ottenere un valore dalla distribuzione binomiale specificata.\nLa generazione di numeri casuali √® determinata dal valore seed specificato nella chiamata.\n\n\n19.5.8 Estrazione dei Risultati\nUna volta completato il campionamento, possiamo estrarre il campione di 10 valori per la variabile scalare y sotto forma di array e quindi stampare i loro valori insieme ai valori delle variabili di input.\n\ny = trace.stan_variable('y')\nprint(\"N =\", N, \";  theta =\", theta, \";  y(0:10) =\", *y.astype(int))\n\nN = 100 ;  theta = 0.3 ;  y(0:10) = 28 34 31 29 26 25 31 28 30 36",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#integrazione-monte-carlo",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#integrazione-monte-carlo",
    "title": "19¬† Linguaggio Stan",
    "section": "19.6 Integrazione Monte Carlo",
    "text": "19.6 Integrazione Monte Carlo\nIl calcolo bayesiano si basa sulla media delle incertezze nella stima dei parametri. In generale, ci√≤ implica il calcolo di aspettative, che sono medie ponderate con pesi dati dalle densit√† di probabilit√†. In questa sezione, introdurremo i metodi Monte Carlo per calcolare un semplice integrale che corrisponde all‚Äôaspettativa di una variabile indicatrice discreta. Utilizzeremo l‚Äôesempio classico del lancio di freccette su un bersaglio per stimare la costante matematica \\(\\pi\\).\n\n19.6.1 Esempio: Stima di \\(\\pi\\)\nImmaginiamo un quadrato di lato 2 centrato sull‚Äôorigine. Genereremo punti casuali uniformemente distribuiti all‚Äôinterno di questo quadrato. Per ogni punto \\((x, y)\\), verificheremo se cade all‚Äôinterno del cerchio di raggio unitario inscritto nel quadrato, cio√® se la distanza dall‚Äôorigine √® minore di 1:\n\\[\n\\sqrt{x^2 + y^2} &lt; 1,\n\\]\nche si semplifica a:\n\\[\nx^2 + y^2 &lt; 1.\n\\]\nLa proporzione di tali punti rappresenta la proporzione dell‚Äôarea del quadrato occupata dal cerchio. Poich√© il quadrato ha un‚Äôarea di 4, l‚Äôarea del cerchio √® pari a 4 volte la proporzione dei punti che cadono all‚Äôinterno del cerchio.\nQuindi, se generiamo un numero sufficiente di punti casuali e contiamo quanti di essi cadono all‚Äôinterno del cerchio, possiamo stimare \\(\\pi\\) come:\n\\[\n\\pi \\approx 4 \\times \\frac{\\text{numero di punti dentro il cerchio}}{\\text{numero totale di punti}}.\n\\]\n\n\n19.6.2 Codice Stan\nEsaminiamo il corrispondente codice Stan.\ngenerated quantities {\n  real&lt;lower=-1, upper=1&gt; x = uniform_rng(-1, 1);\n  real&lt;lower=-1, upper=1&gt; y = uniform_rng(-1, 1);\n  int&lt;lower=0, upper=1&gt; inside = x^2 + y^2 &lt; 1;\n  real&lt;lower=0, upper=4&gt; pi = 4 * inside;\n}\n\nVariabili x e y:\n\nVengono generate casualmente e uniformemente nell‚Äôintervallo \\((-1, 1)\\). Questo significa che stiamo campionando punti all‚Äôinterno di un quadrato di lato 2 centrato sull‚Äôorigine.\n\nVariabile inside:\n\n√à un indicatore che verifica se il punto \\((x, y)\\) cade all‚Äôinterno del cerchio unitario. La condizione \\(x^2 + y^2 &lt; 1\\) √® vera se il punto \\((x, y)\\) √® all‚Äôinterno del cerchio di raggio 1 centrato sull‚Äôorigine, e falsa altrimenti.\nSe la condizione √® vera, inside √® impostato a 1, altrimenti a 0.\n\nVariabile pi:\n\npi viene calcolata come 4 volte il valore di inside.\n\n\nIl programma Stan genera punti casuali, verifica se cadono all‚Äôinterno del cerchio e usa la proporzione di punti che cadono all‚Äôinterno del cerchio per stimare \\(\\pi\\). Moltiplicando il valore indicatore per 4, otteniamo una stima di \\(\\pi\\) basata su ciascun punto generato. La stima finale di \\(\\pi\\) sar√† la media di queste stime su molti punti campionati.\n\n\n19.6.3 Media Campionaria dell‚ÄôIndicatore\nDopo aver generato un numero sufficiente di punti casuali e aver verificato quanti di essi cadono all‚Äôinterno del cerchio, calcoliamo la media campionaria dell‚Äôindicatore inside. Questo indicatore √® uguale a 1 se il punto √® dentro il cerchio e a 0 se √® fuori. La media di questi valori ci d√† la proporzione dei punti che cadono dentro il cerchio.\nQuesta proporzione √® una stima della probabilit√† che un punto casuale sia all‚Äôinterno del cerchio. Moltiplicando questa proporzione per 4, otteniamo una stima di \\(\\pi\\).\nMatematicamente, possiamo scrivere questo processo come segue:\n\\[\n\\mathbb{E}[4 \\cdot \\textrm{I}(\\sqrt{X^2 + Y^2} \\leq 1)] = \\int_{-1}^1 \\int_{-1}^1 4 \\cdot \\textrm{I}(x^2 + y^2 &lt; 1) \\, \\textrm{d}x \\, \\textrm{d}y = \\pi,\n\\]\ndove \\(\\textrm{I}()\\) √® l‚Äôindicatore che ritorna 1 se il suo argomento √® vero e 0 altrimenti.\nIn altre parole, stiamo calcolando l‚Äôaspettativa di 4 volte l‚Äôindicatore che un punto casuale \\((x, y)\\) cade dentro il cerchio unitario. Questo valore atteso √® uguale a \\(\\pi\\), il che ci permette di stimare \\(\\pi\\) usando i metodi Monte Carlo.\n\n\n19.6.4 Compilazione e Campionamento\nCompiliamo e poi campioniamo dal modello, prendendo un campione di dimensione \\(M = 10,000\\) estrazioni.\n\nM = 10_000\nmodel = CmdStanModel(stan_file='../../stan/monte-carlo-pi.stan')\n\nsample = model.sample(\n    chains=1, iter_warmup=1, iter_sampling=M,\n    show_progress=False, show_console=False,\n    seed=123\n)\n\nx_draws = sample.stan_variable('x')\ny_draws = sample.stan_variable('y')\ninside_draws = sample.stan_variable('inside')\npi_draws = sample.stan_variable('pi')\n\ndf = pd.DataFrame({'N': 1000, 'x': x_draws, 'y': y_draws, 'inside': inside_draws})\n\nplt.figure(figsize=(5, 5))\nplt.scatter(df['x'], df['y'], c=df['inside'], cmap='coolwarm', s=1)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Monte Carlo Simulation of Pi Estimation')\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()\n\n\n\n\n\n\n\n\nSuccessivamente, calcoliamo la media campionaria dell‚Äôindicatore dentro-il-cerchio, che produce una stima della probabilit√† che un punto sia dentro il cerchio:\n\nPr_is_inside = np.mean(inside_draws)\npi_hat = np.mean(pi_draws)\nprint(f\"Pr[Y is inside circle] = {Pr_is_inside:.3f};\")\nprint(f\"estimate for pi = {pi_hat:.3f}\")\n\nPr[Y is inside circle] = 0.786;\nestimate for pi = 3.144\n\n\nIl valore esatto di \\(\\pi\\) fino a tre cifre decimali √® \\(3.142\\). Con il nostro metodo, ci avviciniamo a questo valore, ma non lo raggiungiamo esattamente, il che √® tipico dei metodi Monte Carlo. Aumentando il numero di estrazioni, l‚Äôerrore diminuisce. Teoricamente, con un numero sufficiente di estrazioni, possiamo ottenere qualsiasi precisione desiderata; tuttavia, in pratica, dobbiamo accontentarci di pochi decimali di accuratezza nelle nostre stime Monte Carlo. Questo di solito non √® un problema, poich√© l‚Äôincertezza statistica tende a dominare rispetto all‚Äôimprecisione numerica nella maggior parte delle applicazioni.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#metodi-monte-carlo-a-catena-di-markov",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#metodi-monte-carlo-a-catena-di-markov",
    "title": "19¬† Linguaggio Stan",
    "section": "19.7 Metodi Monte Carlo a Catena di Markov",
    "text": "19.7 Metodi Monte Carlo a Catena di Markov\nNelle sezioni precedenti, abbiamo visto come generare un campione eseguendo una serie di estrazioni indipendenti e calcolare la media dei risultati per ottenere stime delle aspettative.\nNei moderni modelli bayesiani, raramente √® possibile generare estrazioni indipendenti dalle distribuzioni di interesse. Questo rende i semplici metodi Monte Carlo non applicabili. Solo in rari casi, con modelli molto semplici, √® possibile ottenere estrazioni indipendenti, ma questi casi sono limitati (Diaconis e Ylvisaker 1979). Prima della rivoluzione dei metodi Monte Carlo a catena di Markov (MCMC) negli anni ‚Äô90, l‚Äôinferenza bayesiana era per lo pi√π limitata a questi modelli semplici.\nL‚Äôintroduzione dei metodi MCMC ha rivoluzionato l‚Äôinferenza bayesiana. Questi metodi permettono di generare campioni da distribuzioni complesse dove non √® possibile ottenere estrazioni indipendenti. Tra questi, il metodo Hamiltonian Monte Carlo (HMC) √® particolarmente efficiente e scalabile, grazie all‚Äôuso della differenziazione automatica. HMC √® implementato in Stan e ha ampliato notevolmente la gamma di modelli che possono essere analizzati in tempi ragionevoli.\n\n19.7.1 Catene di Markov\nNei metodi Monte Carlo a catena di Markov, ogni estrazione dipende dall‚Äôestrazione precedente. Una sequenza di variabili casuali in cui ciascuna dipende solo dalla variabile precedente √® chiamata catena di Markov. In altre parole, una catena di Markov √® una sequenza di variabili casuali dove ogni variabile √® condizionatamente indipendente dalle precedenti, dato il valore della variabile immediatamente precedente.\n\n\n19.7.2 Esempio di Catena di Markov\nConsideriamo un esempio semplice con tre catene di Markov, tutte con una distribuzione stazionaria di \\(\\theta = 0.5\\). Questo significa che, col tempo, la distribuzione delle estrazioni converge a una distribuzione Bernoulli con parametro \\(\\theta\\) pari a 0.5. La media a lungo termine delle estrazioni si avviciner√† a 0.5, poich√© questo √® il valore atteso di una variabile Bernoulli con parametro \\(\\theta\\).\nEcco un programma Stan che implementa una catena di Markov:\ndata {\n  int&lt;lower=0&gt; M;\n  real&lt;lower=0, upper=1&gt; rho;  // probabilit√† di rimanere nello stesso stato\n}\ngenerated quantities {\n  array[M] int&lt;lower=0, upper=1&gt; y;  // Catena di Markov\n  y[1] = bernoulli_rng(0.5);\n  for (m in 2:M) {\n    y[m] = bernoulli_rng(y[m - 1] ? rho : 1 - rho);\n  } \n}\nIn questo programma, y[m] viene assegnato utilizzando un‚Äôoperazione ternaria. Se y[m - 1] √® 1, y[m] √® assegnato a bernoulli_rng(rho), altrimenti √® assegnato a bernoulli_rng(1 - rho). Questo crea una catena di Markov dove la probabilit√† di rimanere nello stesso stato √® rho.\nIn conclusione, i metodi Monte Carlo a catena di Markov, come l‚ÄôHamiltonian Monte Carlo, hanno ampliato notevolmente le capacit√† dell‚Äôinferenza bayesiana, permettendo di affrontare modelli complessi che non possono essere analizzati con semplici estrazioni indipendenti.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#il-problema-inverso-delle-nascite-di-laplace",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#il-problema-inverso-delle-nascite-di-laplace",
    "title": "19¬† Linguaggio Stan",
    "section": "19.8 Il Problema Inverso delle Nascite di Laplace",
    "text": "19.8 Il Problema Inverso delle Nascite di Laplace\nQuando si dispone di un modello che genera dati a partire dai parametri, il problema inverso consiste nell‚Äôinferire i valori dei parametri dai dati osservati. Questo tipo di problema richiede di ragionare a ritroso dalle osservazioni, attraverso il modello di misurazione e il modello diretto, per stimare parametri come, ad esempio, la probabilit√† di successo. La risoluzione dei problemi inversi √® uno degli ambiti in cui le statistiche bayesiane eccellono.\n\n19.8.1 Storia e Applicazione\nUn decennio dopo la pubblicazione della regola di Bayes, Laplace utilizz√≤ la funzione beta di Eulero per derivare formalmente la distribuzione a posteriori. In questa sezione, analizzeremo il problema di Laplace utilizzando Stan.\nLaplace raccolse dati sul sesso dei bambini nati vivi a Parigi tra il 1745 e il 1770:\n\n\n\nSesso\nNascite vive\n\n\n\n\nFemmina\n105.287\n\n\nMaschio\n110.312\n\n\n\nLaplace si chiese se, sulla base di questi dati, la probabilit√† di nascita dei maschi fosse superiore a quella delle femmine.\n\n\n19.8.2 Modello di Laplace\nLaplace adott√≤ la seguente distribuzione campionaria per modellare il numero di maschi nati su un totale di \\(N\\) nascite:\n\\[\ny \\sim \\text{binomiale}(N, \\theta),\n\\]\ndove \\(N\\) √® il numero totale di nascite, \\(\\theta\\) √® la probabilit√† di nascita di un maschio e \\(y\\) √® il numero di nascite maschili.\n\n\n19.8.3 Distribuzione a Priori\nLaplace utilizz√≤ la seguente distribuzione a priori per \\(\\theta\\):\n\\[\n\\theta \\sim \\text{beta}(1, 1),\n\\]\ndove la distribuzione \\(\\text{beta}(1, 1)\\) √® uniforme sull‚Äôintervallo \\(\\theta \\in (0, 1)\\) poich√© la densit√† √® proporzionale a una costante:\n\\[\n\\text{beta}(\\theta \\mid 1, 1) \\propto \\theta^{1 - 1} \\cdot (1 - \\theta)^{1 - 1} = 1.\n\\]\n\n\n19.8.4 Distribuzione a Posteriori\nIl modello di Laplace √® abbastanza semplice da permettere una soluzione analitica della distribuzione a posteriori:\n\\[\n\\begin{aligned}\n    p(\\theta \\mid y, N) &\\propto p(y \\mid N, \\theta) \\cdot p(\\theta) \\\\\n    &= \\text{binomiale}(y \\mid N, \\theta) \\cdot \\text{beta}(\\theta \\mid 1, 1) \\\\\n    &\\propto \\theta^y \\cdot (1 - \\theta)^{N - y} \\cdot \\theta^{1 - 1} \\cdot (1 - \\theta)^{1 - 1} \\\\\n    &= \\theta^{y} \\cdot (1 - \\theta)^{N - y} \\\\\n    &\\propto \\text{beta}(\\theta \\mid y + 1, N - y + 1).\n\\end{aligned}\n\\]\nQuindi, possiamo concludere che:\n\\[\np(\\theta \\mid y, N) = \\text{beta}(\\theta \\mid y + 1, N - y + 1).\n\\]\n\n\n19.8.5 Implementazione in Stan\nA differenza del primo modello Stan che abbiamo visto, che generava solo dati, il seguente programma Stan richiede che vengano forniti dati, specificamente il numero di nascite maschili (\\(y\\)) e il numero totale di nascite (\\(N\\)). Il modello ci permetter√† di stimare la probabilit√† di nascita di un maschio (\\(\\theta\\)) e la probabilit√† che nascano pi√π maschi che femmine (\\(\\theta &gt; 0.5\\)).\nEcco come possiamo specificare il modello Stan:\n\nstan_file = os.path.join(\n    project_directory, 'stan', 'sex-ratio.stan')\n\nwith open(stan_file, 'r') as f:\n    print(f.read())\n\ndata {\n  int&lt;lower = 0&gt; N;\n  int&lt;lower = 0, upper = N&gt; y;\n  int&lt;lower = 0&gt; alpha_prior;\n  int&lt;lower = 0&gt; beta_prior;\n}\nparameters {\n  real&lt;lower=0, upper=1&gt; theta;\n}\nmodel {\n  theta ~ beta(alpha_prior, beta_prior);\n  y ~ binomial(N, theta);\n}\ngenerated quantities {\n  int&lt;lower=0, upper=1&gt; boys_gt_girls = theta &gt; 0.5;\n}\n\n\n\nIn questo programma Stan, vediamo che sia il numero totale di nascite (\\(N\\)) sia il numero di nascite maschili (\\(y\\)) sono forniti come dati. Poi ci sono due blocchi aggiuntivi: un blocco dei parametri, usato per dichiarare valori sconosciuti (qui, solo il tasso di nascite maschili \\(\\theta\\)), e un blocco del modello, dove specifichiamo la distribuzione a priori e la verosimiglianza. La distribuzione a posteriori viene calcolata da Stan combinando queste due componenti. Inoltre, c‚Äô√® un blocco delle quantit√† generate dove viene calcolata una variabile booleana che indica se la probabilit√† di nascita dei maschi \\(\\theta\\) √® maggiore di 0.5.\nIl modello di Laplace e la sua implementazione in Stan ci permettono di affrontare il problema inverso delle nascite, inferendo la probabilit√† di nascita di un maschio dai dati osservati. Utilizzando le tecniche bayesiane, possiamo stimare non solo la probabilit√† di nascita di un maschio, ma anche la probabilit√† che nascano pi√π maschi che femmine.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#campionare-dalla-distribuzione-a-posteriori",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#campionare-dalla-distribuzione-a-posteriori",
    "title": "19¬† Linguaggio Stan",
    "section": "19.9 Campionare dalla Distribuzione a Posteriori",
    "text": "19.9 Campionare dalla Distribuzione a Posteriori\nQuando eseguiamo un programma Stan, esso genera una serie di campioni casuali che approssimano la distribuzione a posteriori. Con il proseguire delle estrazioni, questi campioni tendono a diventare sempre pi√π simili a veri campioni della distribuzione a posteriori, fino a diventare numericamente indistinguibili da essa.\nStan utilizza un algoritmo Markov Chain Monte Carlo (MCMC), che pu√≤ introdurre autocorrelazione nei campioni della distribuzione a posteriori. In altre parole, i campioni non sono indipendenti tra loro, ma ogni campione √® correlato (o anti-correlato) con il campione precedente.\nL‚Äôautocorrelazione non introduce bias nelle stime Monte Carlo, ma i campioni positivamente autocorrelati, che si osservano nei modelli pi√π complessi, aumentano la varianza delle stime rispetto ai campioni indipendenti. Questo incremento della varianza aumenta l‚Äôerrore quadratico medio atteso, che √® una combinazione di errore dovuto al bias (qui nullo) e alla varianza. Al contrario, nei modelli molto semplici, l‚Äôautocorrelazione negativa riduce la varianza rispetto ai campioni indipendenti, riducendo quindi l‚Äôerrore quadratico medio atteso.\nPer affrontare problemi ad alta dimensionalit√†, Duane et al.¬†(1987) hanno introdotto l‚Äôalgoritmo Hamiltonian Monte Carlo (HMC) che migliora l‚Äôefficienza del campionamento. Per Stan, Hoffman e Gelman (2014) hanno sviluppato una versione adattiva dell‚ÄôHMC chiamata No-U-Turn Sampler (NUTS), successivamente migliorata da Betancourt (2017a). NUTS pu√≤ essere estremamente efficiente, generando campioni anti-correlati che possono portare a stime Monte Carlo pi√π precise rispetto ai campioni indipendenti.\n\n19.9.1 Compilazione del Codice Stan\nPer utilizzare Stan, dobbiamo compilare il codice del modello. Questo crea un file eseguibile che, nel nostro caso, abbiamo chiamato model.\n\nmodel = CmdStanModel(stan_file=stan_file)\n\nI dati devono essere contenuti in un dizionario.\n\nboys = 110312\ngirls = 105287\n\ndata = {\n    'N': boys + girls, \n    'y': boys,\n    \"alpha_prior\" : 1,\n    \"beta_prior\" : 1\n    }\n\nprint(data)\n\n{'N': 215599, 'y': 110312, 'alpha_prior': 1, 'beta_prior': 1}\n\n\nPossiamo ora eseguire il campionamento MCMC con la seguente chiamata.\n\nsample = model.sample(\n    data=data,\n    iter_warmup = 1000,\n    iter_sampling = 10_000,\n    seed = 123,\n    show_progress = False, \n    show_console = False\n)\n\nIl metodo $sample() viene applicato al file eseguibile del modello Stan che abbiamo compilato e nominato model.\nAvendo assunto una distribuzione a priori per il parametro \\(\\theta\\), l‚Äôalgoritmo procede in maniera ciclica, aggiornando la distribuzione a priori di \\(\\theta\\) condizionandola ai valori gi√† generati. Dopo un certo numero di iterazioni, l‚Äôalgoritmo raggiunge la convergenza, e i valori estratti possono essere considerati campioni dalla distribuzione a posteriori di \\(\\theta\\).\nAll‚Äôinizio del campionamento, la distribuzione dei campioni pu√≤ essere significativamente diversa dalla distribuzione stazionaria. Questo periodo iniziale √® chiamato ‚Äúburn-in‚Äù. Durante il burn-in, i campioni possono non rappresentare accuratamente la distribuzione a posteriori e sono tipicamente scartati. Man mano che il numero di iterazioni aumenta, la distribuzione dei campioni si avvicina sempre pi√π alla distribuzione target.\nDopo aver eseguito il modello in Stan, otteniamo una serie di campioni \\(\\theta^{(m)}\\) dalla distribuzione a posteriori \\(p(\\theta \\mid N, y)\\). Ogni campione rappresenta un possibile valore di \\(\\theta\\) compatibile con i dati osservati \\(y\\). Procediamo quindi a estrarre i campioni a posteriori per le variabili theta e boys_gt_girls.\n\ntheta_draws = sample.stan_variable('theta')\nboys_gt_girls_draws = sample.stan_variable('boys_gt_girls')\n\nTracciando un istogramma di questi campioni, possiamo visualizzare dove i valori di \\(\\theta\\) sono pi√π probabili e comprendere meglio la forma della distribuzione a posteriori. L‚Äôistogramma ci fornisce diverse informazioni:\n\nValore pi√π probabile di \\(\\theta\\): Questo √® il valore intorno al quale i campioni sono pi√π concentrati, noto come la moda della distribuzione.\nDistribuzione dei possibili valori di \\(\\theta\\): Questo ci d√† un‚Äôidea dell‚Äôincertezza nella stima di \\(\\theta\\).\n\nSe l‚Äôistogramma √® stretto e concentrato attorno a un valore specifico, significa che c‚Äô√® poca incertezza nella stima di \\(\\theta\\). In altre parole, possiamo essere abbastanza sicuri che il valore vero di \\(\\theta\\) sia vicino a questo valore.\nSe l‚Äôistogramma √® largo e distribuito, significa che c‚Äô√® maggiore incertezza nella stima di \\(\\theta\\). Questo indica che i dati osservati non forniscono una stima precisa e che il valore di \\(\\theta\\) potrebbe variare notevolmente.\n\nplt.hist(theta_draws, bins=30, alpha=0.5, color='b', edgecolor='black', density=True)\n\n# Aggiunta di titolo e etichette agli assi\nplt.title('Istogramma della distribizione a posteriori di theta')\nplt.xlabel('Valori')\nplt.ylabel('Frequenza')\n\nplt.show()",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#stime-puntuali-bayesiane",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#stime-puntuali-bayesiane",
    "title": "19¬† Linguaggio Stan",
    "section": "19.10 Stime Puntuali Bayesiane",
    "text": "19.10 Stime Puntuali Bayesiane\nIn termini bayesiani, una stima puntuale per un parametro \\(\\Theta\\) condizionato sui dati osservati \\(Y = y\\) √® un singolo valore \\(\\hat{\\theta} \\in \\mathbb{R}^D\\) che riassume la distribuzione a posteriori \\(p(\\theta \\mid y)\\). La notazione \\(\\hat{\\theta}\\) √® convenzionale nella statistica per indicare una stima di un parametro \\(\\theta\\). In questa sezione definiamo tre stimatori e discutiamo come i due stimatori bayesiani minimizzino una funzione di perdita tra il valore vero e la stima. Torneremo alla funzione di perdita e alle propriet√† degli stimatori dopo averli definiti.\n\n19.10.1 Stimatore della Media Posteriori\nLa stima puntuale bayesiana pi√π comune per un parametro √® la media posteriori,\n\\[\n\\begin{align}\n\\widehat{\\theta}\n&= \\mathbb{E}[\\Theta \\mid Y = y] \\\\\n&= \\int_{\\Theta} \\theta \\cdot p(\\theta \\mid y) \\, \\textrm{d}\\theta \\\\\n&= \\lim_{M \\rightarrow \\infty} \\, \\frac{1}{M} \\sum_{m=1}^M \\theta^{(m)} \\\\\n&\\approx \\frac{1}{M} \\sum_{m=1}^M \\theta^{(m)},\n\\end{align}\n\\]\ndove nelle ultime due righe, ogni estrazione √® distribuita approssimativamente secondo la distribuzione a posteriori,\n\\[\n\\theta^{(m)} \\sim p(\\theta \\mid y).\n\\]\nAbbiamo introdotto la notazione di aspettativa condizionale nella prima riga di questa definizione. Le aspettative sono semplicemente medie ponderate, con i pesi dati da una densit√† di probabilit√†. L‚Äôinferenza bayesiana coinvolge aspettative sulla distribuzione a posteriori, la cui notazione concisa √® quella dell‚Äôaspettativa condizionale,\n\\[\n\\mathbb{E}\\!\n\\left[ f(\\Theta) \\mid Y = y \\right]\n= \\int_{\\mathbb{R^N}} f(\\theta) \\cdot p_{\\Theta \\mid Y}(\\theta \\mid y) \\, \\textrm{d}\\theta,\n\\]\ndove \\(\\Theta\\) e \\(Y\\) sono variabili casuali, mentre \\(\\theta\\) e \\(y\\) sono variabili vincolate ordinarie.\nPer il modello di Laplace, la stima per il tasso di nascite maschili \\(\\theta\\) condizionata sui dati di nascita \\(y\\) √® calcolata come la media campionaria delle estrazioni per theta.\n\ntheta_hat = np.mean(theta_draws)\nprint(f\"estimated theta = {theta_hat:.3f}\")\n\nestimated theta = 0.512\n\n\n\n\n19.10.2 Stimatore della Mediana Posteriori, Quantili e Intervalli\nUn‚Äôalternativa popolare alla stima puntuale bayesiana √® la mediana posteriori, \\(\\theta^+\\). La mediana √® il valore tale che, per ogni dimensione \\(d \\in 1{:}D\\),\n\\[\n\\Pr[\\Theta_d \\leq \\theta^+_d] = \\frac{1}{2}.\n\\]\nIn altre parole, la mediana √® il valore che divide la distribuzione a posteriori in due parti uguali: il 50% dei campioni √® al di sotto della mediana e il 50% √® al di sopra. La mediana posteriori pu√≤ essere calcolata prendendo la mediana dei campioni dalla distribuzione a posteriori.\nEcco come calcolare la mediana posteriori utilizzando Python:\n\ntheta_plus = np.median(theta_draws)\nprint(f\"estimated (median) theta = {theta_plus:.3f}\")\n\nestimated (median) theta = 0.512\n\n\nPoich√© la distribuzione a posteriori per i dati di Laplace √® quasi simmetrica, la media posteriori e la mediana posteriori sono molto simili.\n\n\n19.10.3 Quantili e Intervalli di Credibilit√†\nOltre alla mediana, possiamo anche calcolare i quantili e gli intervalli di credibilit√† per fornire ulteriori informazioni sulla distribuzione a posteriori. I quantili sono valori che dividono la distribuzione in intervalli con una probabilit√† specificata. Gli intervalli di credibilit√† indicano l‚Äôintervallo entro il quale cade una certa percentuale della distribuzione a posteriori.\n\n19.10.3.1 Quantili\nAd esempio, se vogliamo calcolare il quantile al 95% della distribuzione a posteriori, possiamo semplicemente prendere il valore che si trova al 95¬∞ percentile nella sequenza ordinata dei campioni. Di seguito sono riportati i quantili al 5% e al 95% della distribuzione a posteriori di Laplace, calcolati utilizzando i quantili empirici.\n\nquantile_05 = np.quantile(theta_draws, 0.05)\nquantile_95 = np.quantile(theta_draws, 0.95)\nprint(f\"\"\"0.05 quantile = {quantile_05:.3f};\n0.95 quantile = {quantile_95:.3f}\"\"\")\n\n0.05 quantile = 0.510;\n0.95 quantile = 0.513\n\n\n\n\n19.10.3.2 Intervalli Posteriori\nInsieme, il quantile al 5% e al 95% ci forniscono i limiti del nostro intervallo di probabilit√† centrale al 90%. Questo intervallo √® definito come l‚Äôintervallo che contiene il 90% della massa di probabilit√† a posteriori, con il 5% della massa rimanente al di sotto dell‚Äôintervallo e il 5% al di sopra.\n\n\n\n19.10.4 Errore di Stima e Bias\nL‚Äôerrore di una stima √® la differenza tra la stima stessa e il valore vero del parametro,\n\\[\n\\textrm{err} = \\hat{\\theta} - \\theta.\n\\]\nLa nostra stima \\(\\hat{\\theta}\\) √® implicitamente una funzione dei dati \\(y\\), quindi anche l‚Äôerrore dipende dai dati. Possiamo rendere esplicita questa dipendenza scrivendo\n\\[\n\\text{err}(y) = \\hat{\\theta}(y) - \\theta.\n\\]\nIl bias di uno stimatore √® definito come l‚Äôerrore atteso, cio√® la media dell‚Äôerrore rispetto alla distribuzione dei dati per la variabile casuale \\(Y\\),\n\\[\n\\begin{align}\n\\text{bias}\n&= \\mathbb{E}[\\text{err}(Y)] \\\\\n&= \\mathbb{E}[\\hat{\\theta}(Y) - \\theta] \\\\\n&= \\int_Y (\\hat{\\theta}(y) - \\theta) \\, \\text{d}y.\n\\end{align}\n\\]\nIn altre parole, il bias misura quanto, in media, la stima \\(\\hat{\\theta}\\) si discosta dal valore vero \\(\\theta\\) considerando tutte le possibili realizzazioni dei dati \\(Y\\). Un bias nullo indica che lo stimatore √® corretto in media, cio√® non tende a sovrastimare o sottostimare il valore vero del parametro.\n\n\n19.10.5 Stimatore della Moda Posteriori\nUno stimatore popolare, sebbene non strettamente bayesiano, √® la moda a posteriori, che rappresenta il valore del parametro \\(\\theta\\) per cui la densit√† a posteriori √® massima. Formalmente, √® definita come:\n\\[\n\\theta^* = \\text{arg max}_\\theta \\ p(\\theta \\mid y).\n\\]\nLa stima \\(\\theta^*\\) √® spesso chiamata stima MAP (Maximum A Posteriori). La moda a posteriori non √® considerata un vero stimatore bayesiano perch√© non tiene conto dell‚Äôincertezza nella stessa misura in cui lo fanno altri metodi bayesiani. In altre parole, non minimizza una funzione di perdita basata sui valori veri dei parametri, ma cerca semplicemente il valore pi√π probabile dato i dati osservati.\n\n\n19.10.6 Caratteristiche della Moda Posteriori\n\nNon considera l‚Äôincertezza: La stima MAP si focalizza solo sul valore pi√π probabile della distribuzione a posteriori, senza tenere conto della variabilit√† dei dati.\nMassimo della densit√† a posteriori: La moda a posteriori rappresenta il punto in cui la densit√† a posteriori raggiunge il suo massimo.\nPossibili limitazioni: La stima MAP potrebbe non esistere in alcuni casi, come nei modelli in cui la densit√† cresce senza limiti. Questo pu√≤ accadere, ad esempio, nei modelli bayesiani gerarchici o in distribuzioni semplici come la distribuzione esponenziale con parametro 1 (\\(\\textrm{esponenziale}(1)\\)).\n\n\n\n19.10.7 Funzioni di Perdita e Propriet√† degli Stimatori\nLa media a posteriori √® uno stimatore bayesiano popolare per due ragioni principali. Primo, √® uno stimatore non distorto, il che significa che ha un bias nullo. Secondo, ha l‚Äôerrore quadratico medio atteso minimo tra tutti gli stimatori non distorti. L‚Äôerrore quadratico di una stima √® definito come:\n\\[\n\\text{err}^2(y) = \\left(\\hat{\\theta}(y) - \\theta\\right)^2.\n\\]\nQuesta √® una funzione di perdita, che misura la differenza tra una stima \\(\\hat{\\theta}\\) e il valore vero \\(\\theta\\). Tuttavia, la media a posteriori potrebbe non esistere se la distribuzione a posteriori ha code molto ampie, come accade nella distribuzione di Cauchy standard.\n\n\n19.10.8 Propriet√† della Mediana Posteriori\nLa mediana a posteriori \\(\\theta^+\\) ha tre propriet√† interessanti:\n\nSempre ben definita: La mediana a posteriori √® sempre ben definita, anche per densit√† con poli o code molto ampie.\nMinimizzazione dell‚Äôerrore assoluto atteso: La mediana minimizza l‚Äôerrore assoluto atteso, il che la rende robusta.\nRobustezza ai valori anomali: La mediana √® meno sensibile ai valori anomali rispetto alla media, perch√© minimizza l‚Äôerrore assoluto anzich√© l‚Äôerrore quadrato.\n\n\n\n19.10.9 Concentrazione sulle Medie a Posteriori\nIn questa introduzione a Stan, ci concentreremo principalmente sulle medie a posteriori. La media a posteriori non solo fornisce una stima non distorta, ma minimizza anche l‚Äôerrore quadratico medio atteso, rendendola uno strumento potente per l‚Äôinferenza bayesiana. Tuttavia, √® importante essere consapevoli delle sue limitazioni, specialmente in presenza di distribuzioni a posteriori con code molto ampie.\n\n\n19.10.10 Errore (Markov Chain) Monte Carlo e Dimensione del Campione Effettivo\nQuando utilizziamo un campionatore di catene di Markov per stimare parametri, otteniamo una sequenza di campioni casuali. Questa sequenza √® essa stessa una variabile casuale, perch√© √® composta da molte variabili casuali. A causa di questa natura casuale, ogni esecuzione del campionatore pu√≤ produrre risultati leggermente diversi, introducendo quello che √® noto come errore Monte Carlo.\nL‚Äôerrore Monte Carlo √® l‚Äôerrore introdotto dal fatto che utilizziamo solo un numero finito di campioni ($ M $) per stimare i parametri. Questo tipo di errore si verifica perch√©, con un numero limitato di campioni, non possiamo catturare perfettamente l‚Äôintera distribuzione a posteriori.\n\n19.10.10.1 Errore Standard di Monte Carlo (MCMC)\nStan riporta l‚Äôerrore standard di Monte Carlo (MCMC) insieme alle stime della media. L‚Äôerrore standard MCMC per un parametro scalare $ _d $ √® definito come:\n\\[\n\\text{mcmc-se} = \\frac{\\textrm{sd}[\\Theta_d \\mid Y = y]}{\\sqrt{N^{\\text{eff}}}},\n\\]\ndove: - \\(\\text{sd}[\\Theta_d \\mid Y = y]\\) √® la deviazione standard del parametro $ _d $ nella distribuzione a posteriori. - \\(N^{\\text{eff}}\\) √® la dimensione del campione effettivo, che riflette il numero di campioni indipendenti equivalenti ottenuti dal campionatore.\n\n\n19.10.10.2 Dimensione del Campione Effettivo\nNel classico teorema del limite centrale, la dimensione del campione (numero di estrazioni indipendenti) appare al posto di \\(N^{\\text{eff}}\\). Tuttavia, nel contesto delle catene di Markov, i campioni successivi sono correlati tra loro. La dimensione del campione effettivo (\\(N^{\\text{eff}}\\)) tiene conto di questa correlazione e rappresenta il numero di estrazioni indipendenti che porterebbero allo stesso errore delle nostre estrazioni correlate.\nLa dimensione del campione effettivo per un campione di dimensione $ M $ √® definita come:\n\\[\nN^{\\text{eff}} = \\frac{M}{\\text{IAT}},\n\\]\ndove \\(\\text{IAT}\\) √® il tempo di autocorrelazione integrata. Sebbene non sia definito formalmente qui, pu√≤ essere considerato come l‚Äôintervallo tra estrazioni effettivamente indipendenti nella nostra catena di Markov. Se l‚Äôautocorrelazione √® bassa, \\(\\text{IAT}\\) sar√† vicino a 1; se l‚Äôautocorrelazione √® alta, \\(\\text{IAT}\\) sar√† molto pi√π alto.\nIn sintesi, \\(N^{\\text{eff}}\\) rappresenta il numero di estrazioni indipendenti che porterebbero allo stesso errore delle estrazioni correlate della nostra catena di Markov.\nIn conclusione, l‚Äôerrore standard di Monte Carlo (MCMC) fornisce una misura di quanto varierebbero le nostre stime se ripetessimo il processo di campionamento pi√π volte. √à un indicatore dell‚Äôaffidabilit√† delle nostre stime, tenendo conto della casualit√† introdotta dall‚Äôutilizzo di un numero finito di campioni. Conoscere questo errore ci aiuta a valutare la precisione delle nostre stime e a comprendere meglio l‚Äôincertezza associata ai risultati ottenuti tramite il campionamento di catene di Markov.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#stima-delle-probabilit√†-di-evento",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#stima-delle-probabilit√†-di-evento",
    "title": "19¬† Linguaggio Stan",
    "section": "19.11 Stima delle Probabilit√† di Evento",
    "text": "19.11 Stima delle Probabilit√† di Evento\nLaplace non cercava semplicemente un valore specifico per \\(\\theta\\). Voleva sapere qual era la probabilit√† che \\(\\theta\\) fosse maggiore di \\(\\frac{1}{2}\\) dopo aver osservato \\(y\\) nascite maschili su un totale di \\(N\\) nascite. In termini di teoria della probabilit√†, voleva stimare la probabilit√† di un evento.\nUn sottoinsieme di parametri √® noto come evento. Possiamo convertire le condizioni sui parametri in eventi. Ad esempio, la condizione \\(\\theta &gt; \\frac{1}{2}\\) pu√≤ essere espressa come l‚Äôevento:\n\\[ A = \\left\\{ \\theta \\in \\Theta : \\theta &gt; \\frac{1}{2} \\right\\}. \\]\nData una misura di probabilit√†, la probabilit√† dell‚Äôevento \\(A\\), ossia che il tasso di nascite maschili sia superiore a quello delle nascite femminili, sar√† ben definita. Poich√© possiamo convertire le condizioni in eventi, possiamo trattarle come tali. Questo ci permette di scrivere \\(\\Pr\\!\\left[\\Theta &gt; \\frac{1}{2} \\, \\big| \\, N, y\\right]\\) per indicare la probabilit√† dell‚Äôevento \\(\\Theta &gt; \\frac{1}{2}\\).\n\n19.11.1 Probabilit√† di Evento tramite Indicatori\nLa funzione indicatrice \\(\\textrm{I}\\) assegna il valore 1 alle proposizioni vere e 0 a quelle false. Ad esempio, \\(\\textrm{I}(\\theta &gt; \\frac{1}{2}) = 1\\) se la proposizione \\(\\theta &gt; \\frac{1}{2}\\) √® vera, cio√® quando \\(\\theta\\) √® maggiore di un mezzo.\nLe probabilit√† di evento sono definite come aspettative condizionali posteriori delle funzioni indicatrici per eventi:\n\\[\n\\begin{align}\n\\Pr[\\Theta &gt; 0.5 \\mid N, y]\n&= \\mathbb{E}\\!\\left[\\textrm{I}[\\Theta &gt; 0.5] \\mid N, y\\right] \\\\\n&= \\int_{\\Theta} \\textrm{I}(\\theta &gt; 0.5) \\cdot p(\\theta \\mid N, y) \\, \\textrm{d}\\theta \\\\\n&\\approx \\frac{1}{M} \\sum_{m=1}^M \\textrm{I}(\\theta^{(m)} &gt; 0.5),\n\\end{align}\n\\]\ndove \\(\\theta^{(m)}\\) rappresenta i campioni dalla distribuzione a posteriori \\(p(\\theta \\mid N, y)\\) per \\(m = 1, 2, \\ldots, M\\).\n\n\n19.11.2 Eventi come Indicatori in Stan\nIn Stan, possiamo codificare direttamente il valore della funzione indicatrice e assegnarlo a una variabile nel blocco delle quantit√† generate.\ngenerated quantities {\n  int&lt;lower=0, upper=1&gt; boys_gt_girls = theta &gt; 0.5;\n}\nLe espressioni condizionali come theta &gt; 0.5 assumono il valore 1 se sono vere e 0 se sono false. In notazione matematica, scriveremmo \\(\\textrm{I}(\\theta &gt; 0.5)\\), che assume valore 1 se \\(\\theta &gt; 0.5\\) e 0 altrimenti. In Stan, come in C++, trattiamo &gt; come un operatore binario che restituisce 0 o 1, quindi scriviamo semplicemente theta &gt; 0.5.\n\n\n19.11.3 La Risposta alla Domanda di Laplace\nLa media a posteriori della variabile boys_gt_girls √® quindi la nostra stima per \\(\\Pr[\\theta &gt; 0.5 \\mid N, y]\\). √à essenzialmente 1. Stampando a 15 cifre decimali, vediamo\n\nPr_boy_gt_girl = np.mean(boys_gt_girls_draws)\nprint(f\"estimated Pr[boy more likely] = {Pr_boy_gt_girl:.15f}\")\n\nestimated Pr[boy more likely] = 1.000000000000000\n\n\nCome possiamo vedere di seguito, tutti i nostri campioni per \\(\\theta\\) sono maggiori di \\(\\frac{1}{2}\\), ovvero boys_gt_girls_draws √® sempre uguale a 1:\n\nnp.unique(boys_gt_girls_draws)\n\narray([1.])\n\n\nIl valore 1 restituito come stima solleva l‚Äôimportante problema della precisione numerica. Laplace calcol√≤ il risultato analiticamente, che √®\n\\[\n\\Pr\\!\\left[\\Theta &gt; \\frac{1}{2} \\ \\bigg| \\ N, y\\right] \\approx 1 - 10^{-27}.\n\\]\nQuindi avremmo bisogno di un numero astronomico di campioni a posteriori prima di generare un valore di \\(\\theta\\) inferiore a \\(\\frac{1}{2}\\). Come detto, la risposta di 1.0 √® molto vicina alla risposta vera e ben entro il nostro errore Monte Carlo atteso.\n\n\n19.11.4 Statistiche di riepilogo MCMC da Stan\nCon Stan, possiamo ottenere un riepilogo completo della variabile \\(\\theta\\) nella distribuzione a posteriori. Per fare ci√≤, basta chiamare la funzione .summary() sul campione. Questo riepilogo include tutte le statistiche rilevanti.\n\nsample.summary(sig_figs = 3)\n\n\n\n\n\n\n\n\n\nMean\nMCSE\nStdDev\n5%\n50%\n95%\nN_Eff\nN_Eff/s\nR_hat\n\n\n\n\nlp__\n-149000.000\n0.004770\n6.720000e-01\n-149000.00\n-149000.000\n-149000.000\n19800.0\n60700.0\n1.0\n\n\ntheta\n0.512\n0.000009\n1.090000e-03\n0.51\n0.512\n0.513\n13700.0\n41900.0\n1.0\n\n\nboys_gt_girls\n1.000\nNaN\n9.380000e-14\n1.00\n1.000\n1.000\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\nL‚Äôistruzione print(sample.diagnose()) in Stan viene utilizzata per eseguire una diagnosi completa del campionamento MCMC. Questa funzione fornisce una serie di statistiche diagnostiche che aiutano a valutare la qualit√† e la convergenza del campionamento.\nQuesti sono alcuni degli aspetti che possono essere diagnosticati:\n\nConvergenza: La diagnosi verifica se le catene di Markov sono convergenti, ad esempio controllando il valore di \\(\\hat{R}\\). Un valore di \\(\\hat{R}\\) vicino a 1 indica che le catene sono ben mescolate e convergenti.\nAutocorrelazione: Fornisce informazioni sull‚Äôautocorrelazione delle catene, che pu√≤ influire sull‚Äôefficienza del campionamento. Bassa autocorrelazione √® desiderabile per ottenere campioni indipendenti.\nEfficienza del campionamento: Viene calcolata la dimensione del campione effettivo (\\(N_{\\text{eff}}\\)), che indica quanti campioni indipendenti equivarrebbero ai campioni correlati ottenuti.\nVarianza e Deviazione Standard: Viene riportata la varianza e la deviazione standard dei campioni, aiutando a comprendere la distribuzione a posteriori del parametro.\n\n\nprint(sample.diagnose())\n\nProcessing csv files: /var/folders/s7/z86r4t9j6yx376cm120nln6w0000gn/T/tmphjrqbjja/sex-ratiov0k3fisb/sex-ratio-20240722114626_1.csv, /var/folders/s7/z86r4t9j6yx376cm120nln6w0000gn/T/tmphjrqbjja/sex-ratiov0k3fisb/sex-ratio-20240722114626_2.csv, /var/folders/s7/z86r4t9j6yx376cm120nln6w0000gn/T/tmphjrqbjja/sex-ratiov0k3fisb/sex-ratio-20240722114626_3.csv, /var/folders/s7/z86r4t9j6yx376cm120nln6w0000gn/T/tmphjrqbjja/sex-ratiov0k3fisb/sex-ratio-20240722114626_4.csv\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nEffective sample size satisfactory.\n\nSplit R-hat values satisfactory all parameters.\n\nProcessing complete, no problems detected.\n\n\n\nUn grafico con le tracce si ottiene nel modo seguente:\n\n_ = az.plot_trace(sample, var_names=(\"theta\"), combined=False)",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#riscaldamento",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#riscaldamento",
    "title": "19¬† Linguaggio Stan",
    "section": "20.1 Riscaldamento",
    "text": "20.1 Riscaldamento\nDurante le fasi iniziali di riscaldamento, Stan cerca di trovare la regione di alta probabilit√† da cui campionare, adattare una buona dimensione del passo e stimare la varianza a posteriori. La varianza stimata viene utilizzata per migliorare l‚Äôefficienza del campionatore, un processo chiamato ‚Äúprecondizionamento‚Äù. Precondizionare significa ridimensionare i parametri per rendere il campionamento pi√π efficiente.\nStan pu√≤ anche stimare una matrice di covarianza completa, che rappresenta le relazioni tra tutti i parametri. Utilizzando questa matrice, Stan pu√≤ effettuare rotazioni e ridimensionamenti dei parametri per campionare in modo pi√π efficace. In questo contesto, ‚Äúrotazione e scalatura‚Äù si riferiscono alla trasformazione dei parametri in una nuova base (rotazione) e alla regolazione delle loro scale (scalatura) per facilitare il campionamento, rendendolo pi√π rapido e affidabile. Per ulteriori dettagli su questi processi, si pu√≤ fare riferimento a Neal (2011).\nIl riscaldamento converge quando la dimensione del passo e le stime della covarianza a posteriori diventano stabili. Con pi√π catene, √® possibile verificare che tutte convergano verso una dimensione del passo e una stima della covarianza simili. A meno che non ci siano problemi, generalmente non misuriamo la convergenza dell‚Äôadattamento, ma piuttosto se otteniamo campioni a posteriori ragionevoli dopo il riscaldamento.\nDurante la fase di riscaldamento, Stan non produce una catena di Markov coerente perch√© utilizza la memoria per adattarsi alle condizioni del modello. Questo adattamento serve a trovare i migliori parametri di campionamento. Tuttavia, una volta terminato il riscaldamento e iniziata la fase di campionamento, Stan inizia a produrre una vera e propria catena di Markov.\nLe nostre analisi a posteriori si baseranno esclusivamente sui campioni generati durante questa fase di campionamento, non sui campioni raccolti durante il riscaldamento. √à comunque possibile salvare ed esaminare i campioni del riscaldamento per comprendere meglio come il processo di adattamento √® avvenuto e se ci sono stati problemi.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#riduzione-potenziale-della-scala-e-widehatr",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#riduzione-potenziale-della-scala-e-widehatr",
    "title": "19¬† Linguaggio Stan",
    "section": "20.2 Riduzione potenziale della scala e \\(\\widehat{R}\\)",
    "text": "20.2 Riduzione potenziale della scala e \\(\\widehat{R}\\)\nStan utilizza la statistica di riduzione potenziale della scala \\(\\widehat{R}\\) (pronunciata ‚ÄúR hat‚Äù). Dato un insieme di catene di Markov, Stan divide ciascuna di esse a met√† per assicurarsi che la prima met√† e la seconda met√† della catena concordino, quindi calcola le varianze all‚Äôinterno di ciascuna catena e tra tutte le catene e le confronta. La statistica \\(\\widehat{R}\\) converge a 1 quando le catene di Markov convergono alla stessa distribuzione.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#quante-catene-per-quanto-tempo",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#quante-catene-per-quanto-tempo",
    "title": "19¬† Linguaggio Stan",
    "section": "20.3 Quante catene per quanto tempo?",
    "text": "20.3 Quante catene per quanto tempo?\nUna semplice regola empirica consiste nell‚Äôeseguire quattro catene finch√© \\(\\widehat{R} \\leq 1.01\\) e la dimensione campionaria effettiva (ESS) √® superiore a 100. La raccomandazione di avere una dimensione campionaria effettiva di ‚Äúsoli‚Äù 100 √® dovuta al fatto che questo valore implica un errore standard pari a \\(\\frac{1}{10}\\) della deviazione standard. Poich√© la deviazione standard a posteriori rappresenta l‚Äôincertezza residua, calcolare le medie con una precisione maggiore √® raramente utile.\nIl modo pi√π semplice per ottenere \\(\\widehat{R} \\leq 1.01\\) e \\(N_{\\text{eff}} &gt; 100\\) √® iniziare con 100 iterazioni di riscaldamento e 100 iterazioni di campionamento. Se i valori di \\(\\widehat{R}\\) sono troppo alti o se la dimensione campionaria effettiva √® troppo bassa, raddoppiare il numero di iterazioni di riscaldamento e di campionamento e riprovare. Eseguire pi√π iterazioni di riscaldamento √® importante perch√© il campionamento non sar√† efficiente se il riscaldamento non √® convergente. Utilizzare lo stesso numero di iterazioni di riscaldamento e di campionamento pu√≤ comportare un costo massimo doppio rispetto alle impostazioni ottimali, che non sono note in anticipo.\nAnche se si utilizzano pi√π di quattro catene, √® necessario assicurarsi che la dimensione campionaria effettiva sia almeno 25 per catena. Non √® tanto per l‚Äôinferenza, quanto per garantire la fiducia nello stimatore della dimensione campionaria effettiva, che non √® affidabile se √® molto inferiore. Un modo per verificare l‚Äôadeguatezza dello stimatore ESS √® raddoppiare il numero di campioni e assicurarsi che anche l‚ÄôESS raddoppi. Se ci√≤ non accade, significa che la prima stima dell‚ÄôESS non √® affidabile.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#esecuzione-delle-catene-contemporaneamente",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#esecuzione-delle-catene-contemporaneamente",
    "title": "19¬† Linguaggio Stan",
    "section": "20.4 Esecuzione delle catene contemporaneamente",
    "text": "20.4 Esecuzione delle catene contemporaneamente\n√à possibile impostare il numero di catene da eseguire utilizzando l‚Äôargomento chains del metodo sample(). Inoltre, √® possibile controllare quante catene possono essere eseguite contemporaneamente con l‚Äôargomento parallel_cores (che per default √® impostato su 1, ovvero esecuzione sequenziale).\nSe il numero massimo di catene parallele √® impostato troppo basso, le risorse della CPU potrebbero non essere sfruttate appieno. Al contrario, se √® impostato troppo alto, la CPU o la memoria potrebbero diventare il collo di bottiglia, rallentando le prestazioni complessive rispetto all‚Äôesecuzione con un numero inferiore di catene parallele.\nIn progetti personali sul nostro hardware, l‚Äôobiettivo √® solitamente ottenere la massima dimensione campionaria effettiva nel minor tempo possibile. Tuttavia, a volte √® necessario lasciare abbastanza potenza di elaborazione per continuare a lavorare su altre attivit√† come documenti, email, ecc.\n\n20.4.1 Matrici, Vettori o Array in Stan\nStan offre vari tipi di dati per gestire operazioni di algebra lineare e per definire strutture di dati pi√π generali come gli array. Capire le differenze tra questi tipi √® fondamentale per sapere cosa possiamo fare con essi e per ottimizzare la velocit√† di esecuzione del nostro modello.\n\nTipi di base per l‚Äôalgebra lineare:\n\nvector: un vettore colonna di dimensione N.\nrow_vector: un vettore riga di dimensione N.\nmatrix: una matrice di dimensioni N1 √ó N2.\n\nArray:\n\nGli array possono essere creati con qualsiasi tipo di elemento e possono avere pi√π dimensioni. Ad esempio:\n\narray[N] real a; definisce un array unidimensionale di numeri reali.\narray[N1, N2] real m; definisce un array bidimensionale di numeri reali.\n\n\nIntercambiabilit√† e limitazioni:\n\nAnche se possiamo usare sia vector che array per contenitori unidimensionali, l‚Äôalgebra matriciale (come la moltiplicazione) √® definita solo per vettori e matrici, non per array.\nAlcune funzioni, come normal_lpdf, accettano sia vettori che array.\n\nEsempi pratici:\n\nQuando definiamo una media (mu) come somma di un parametro (alpha) e il prodotto di un vettore di carichi (c_load) con un coefficiente (beta), dobbiamo usare i vettori:\nvector[N] mu = alpha + c_load * beta;\nPer utilizzare un generatore di numeri casuali (_rng) in modo vettoriale, dobbiamo usare un array:\narray[N] real p_size_pred = normal_rng(alpha + c_load * beta, sigma);\n\n\nIn sintesi, la scelta tra vector, row_vector, matrix e array dipende dalle operazioni che si desidera eseguire e dalle specifiche esigenze del modello. Scegliere il tipo di dato appropriato permette di sfruttare appieno le funzionalit√† di Stan e ottimizzare le prestazioni del modello.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#modello-di-esecuzione-di-stan",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#modello-di-esecuzione-di-stan",
    "title": "19¬† Linguaggio Stan",
    "section": "20.5 Modello di esecuzione di Stan",
    "text": "20.5 Modello di esecuzione di Stan\nI programmi Stan sono composti da diversi blocchi. Ecco una panoramica di ciascun blocco, di quando viene eseguito e di cosa fa. Nessuno di questi blocchi √® obbligatorio, ma se presenti, devono seguire quest‚Äôordine.\n\n\n\n\n\n\n\n\nBlocco\nQuando viene eseguito\nCosa fa\n\n\n\n\nfunctions\nsecondo necessit√†\nDefinizione delle funzioni create dall‚Äôutente\n\n\ndata\nuna volta\nLettura dei dati per costruire il modello\n\n\ntransformed data\nuna volta\nDefinizione dei dati trasformati\n\n\nparameters\nuna volta / densit√† logaritmica\nDefinizione dei parametri con i relativi vincoli\n\n\ntransformed parameters\nuna volta / densit√† logaritmica\nDefinizione dei parametri trasformati\n\n\nmodel\nuna volta / densit√† logaritmica\nValutazione della densit√† logaritmica del modello\n\n\ngenerated quantities\nuna volta / per estrazione\nDefinizione delle quantit√† generate\n\n\n\n\n20.5.1 Dati e dati trasformati\nIl blocco data contiene solo le dichiarazioni delle variabili. Queste variabili vengono lette una volta durante il caricamento dei dati.\nIl blocco transformed data contiene sia dichiarazioni che definizioni delle variabili. Questo blocco serve per calcolare nuove variabili a partire dai dati originali, come predittori standardizzati o costanti per i priori. Pu√≤ anche includere la generazione pseudocasuale di numeri. Viene eseguito una volta, dopo la lettura dei dati, per definire le nuove variabili trasformate.\nIn ogni blocco, tutte le variabili devono avere il loro tipo e dimensione dichiarati (che possono dipendere dai dati). Le variabili locali all‚Äôinterno dei blocchi, invece, sono dichiarate senza specificare la dimensione.\nI vincoli sulle variabili nel blocco data vengono controllati mentre i dati vengono letti, mentre quelli nel blocco transformed data vengono verificati alla fine dell‚Äôesecuzione del blocco. Se ci sono violazioni dei vincoli nei dati o nei dati trasformati, si genera un‚Äôeccezione che interrompe l‚Äôesecuzione del programma.\nLe variabili definite nel blocco transformed data possono essere assegnate una volta, ma non possono essere riassegnate dopo l‚Äôesecuzione del blocco.\n\n\n20.5.2 Parametri e Parametri Trasformati\nIl blocco parameters serve a dichiarare le variabili su cui √® basato il modello. In pratica, si tratta di elencare i parametri che il modello utilizzer√†, specificandone le dimensioni. Quando il blocco viene eseguito, vengono forniti i valori concreti di questi parametri.\nI vincoli sui parametri sono utilizzati per trasformare le variabili vincolate in variabili non vincolate. Ad esempio, se una variabile ha un vincolo lower=0 (cio√® deve essere maggiore o uguale a zero), questa variabile viene trasformata usando il logaritmo per renderla non vincolata. √à essenziale dichiarare tutti i vincoli necessari sui parametri affinch√© il modello funzioni correttamente su tutto lo spazio dei parametri.\nIl blocco transformed parameters permette di definire nuove variabili che sono funzioni dei parametri originali e dei dati. Gli utenti possono creare le loro trasformazioni dei parametri in questo blocco. I vincoli su queste nuove variabili vengono verificati alla fine dell‚Äôesecuzione del blocco. Se questi vincoli non sono rispettati, viene generata un‚Äôeccezione che di solito porta al rifiuto della proposta corrente.\nLe variabili dichiarate nel blocco parameters sono simili agli argomenti di una funzione: la funzione di densit√† logaritmica del programma Stan prende questi parametri come input. Quindi, i valori dei parametri vengono sempre forniti dall‚Äôesterno del programma Stan.\nDopo l‚Äôesecuzione del blocco transformed parameters, le variabili dichiarate in esso non possono essere modificate ulteriormente.\nLa differenza principale tra le variabili dichiarate come locali nel blocco model e quelle nel blocco transformed parameters √® che le variabili trasformate vengono stampate e sono disponibili anche nel blocco generated quantities.\n\n\n20.5.3 Modello\nLo scopo del blocco model √® definire la funzione che calcola la densit√† logaritmica del modello. Una volta caricati i dati, il compito principale di un programma Stan √® fornire questa funzione di densit√† logaritmica non normalizzata sui parametri non vincolati. Algoritmi esterni, come ottimizzatori, campionatori o metodi di inferenza variazionale, forniranno i valori dei parametri non vincolati per la valutazione.\nIl valore della densit√† logaritmica non normalizzata calcolato dal modello viene conservato in una variabile chiamata target. Le densit√† posteriori (che ci interessano) sono calcolate moltiplicando i fattori delle funzioni di densit√† o massa di probabilit√†. In termini logaritmici, questo equivale ad aggiungere i termini delle funzioni di densit√† o massa non normalizzate alla target.\nL‚Äôaccumulatore target parte da zero e viene incrementato durante l‚Äôesecuzione del programma Stan. Come accennato prima, la prima cosa che questa funzione di densit√† logaritmica non normalizzata fa √® trasformare i parametri vincolati in non vincolati e aggiungere un aggiustamento logaritmico per il cambio di variabili alla target. Questo processo √® automatico e fornisce i valori dei parametri trasformati al codice che verr√† eseguito successivamente nel blocco model.\nLa densit√† logaritmica accumulata in target pu√≤ essere incrementata direttamente, come mostrato nell‚Äôesempio seguente:\ntarget += -0.5 * x^2;\nAnche se non √® possibile usare direttamente target come variabile, il suo valore attuale pu√≤ essere recuperato tramite la funzione target(), utile per il debugging.\nLe istruzioni di campionamento sono una scorciatoia per incrementare target. Ad esempio, l‚Äôistruzione\nx ~ normal(0, 1);\n√® equivalente a\ntarget += normal_lupdf(x | 0, 1);\nQui, _lupdf indica che si tratta di una funzione di densit√† di probabilit√† logaritmica non normalizzata.\nLa barra verticale | √® utilizzata per separare le variabili osservate dai parametri. La notazione lpdf denota una funzione di densit√† di probabilit√† logaritmica, mentre lpmf indica una funzione di massa di probabilit√† logaritmica. Le varianti lupdf e lupmf sono le loro controparti non normalizzate, che possono omettere le costanti di normalizzazione che non dipendono dai parametri. A meno che non siano necessarie, ad esempio in un componente di un modello di mescolanza, √® pi√π efficiente usare le forme lupdf e lupmf incrementando direttamente target o tramite istruzioni di campionamento.\n\n\n20.5.4 Quantit√† generate\nIl blocco generated quantities viene eseguito una volta per ogni campione generato, anzich√© ogni volta che viene calcolata la densit√† logaritmica. Con algoritmi come il campionamento Monte Carlo Hamiltoniano, ogni campione pu√≤ richiedere diverse valutazioni della densit√† logaritmica.\nUn vantaggio delle quantit√† generate √® che vengono calcolate utilizzando numeri in virgola mobile a doppia precisione, il che le rende molto efficienti. Questo blocco pu√≤ anche utilizzare numeri pseudocasuali. I vincoli sui dati generati vengono verificati alla fine del blocco, ma eventuali errori non causano il rigetto del campione, solo possibili avvertimenti o valori non definiti (NaN).\nLe quantit√† generate non influenzano il calcolo della densit√† logaritmica, ma sono comunque una parte importante del modello statistico. Sono utilizzate principalmente per fare previsioni su nuovi dati, basandosi sui parametri stimati dal modello. Questo processo √® noto come inferenza predittiva posteriore. In altre parole, ci permette di fare previsioni su nuovi dati utilizzando i valori dei parametri generati dal modello.\nEsempi di utilizzo delle quantit√† generate includono la previsione di nuovi valori o il calcolo di statistiche derivate dai parametri stimati. Le quantit√† generate offrono un modo per esplorare ulteriormente il comportamento del modello e fare inferenze utili dai dati simulati.",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_4/15_stan_beta_binomial.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_4/15_stan_beta_binomial.html#informazioni-sullambiente-di-sviluppo",
    "title": "19¬† Linguaggio Stan",
    "section": "20.6 Informazioni sull‚ÄôAmbiente di Sviluppo",
    "text": "20.6 Informazioni sull‚ÄôAmbiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m -p cmdstanpy\n\nLast updated: Mon Jul 22 2024\n\nPython implementation: CPython\nPython version       : 3.12.4\nIPython version      : 8.26.0\n\ncmdstanpy: 1.2.4\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.5.0\nMachine     : arm64\nProcessor   : arm\nCPU cores   : 8\nArchitecture: 64bit\n\ncmdstanpy : 1.2.4\nnumpy     : 1.26.4\npandas    : 2.2.2\nmatplotlib: 3.9.1\narviz     : 0.18.0\nlogging   : 0.5.1.2\n\nWatermark: 2.4.3",
    "crumbs": [
      "Inferenza Bayesiana",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Linguaggio Stan</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_5/05_03_reglin_bayesian.html",
    "href": "chapters/chapter_5/05_03_reglin_bayesian.html",
    "title": "20¬† Modello di regressione lineare bayesiano",
    "section": "",
    "text": "20.1 Introduzione\nI modelli lineari sono stati utilizzati in varie forme per molto tempo. Stigler (1986) descrive come il metodo dei minimi quadrati, una tecnica per adattare una semplice regressione lineare, fosse associato a problemi fondamentali in astronomia nel 1700, come la determinazione del moto della luna e la riconciliazione del moto non periodico di Giove e Saturno. All‚Äôepoca, gli astronomi erano tra i primi a sentirsi a proprio agio nell‚Äôutilizzare questi metodi, poich√© raccoglievano personalmente le loro osservazioni e sapevano che le condizioni di raccolta dei dati erano simili, anche se i valori delle osservazioni differivano. Questo contrastava con l‚Äôapproccio pi√π cauto delle scienze sociali, dove la riluttanza a combinare dati eterogenei ritardava l‚Äôadozione dei modelli lineari (Stigler 1986).\nCome nota Alexander (2023), quando costruiamo modelli, non stiamo scoprendo ‚Äúla verit√†‚Äù. Un modello non pu√≤ essere una rappresentazione fedele della realt√†. Utilizziamo i modelli per esplorare e comprendere i nostri dati. Non esiste un modello migliore in assoluto, ma solo modelli utili che ci aiutano a imparare qualcosa sui dati che abbiamo e, si spera, qualcosa sul mondo da cui sono stati generati. Quando utilizziamo i modelli, cerchiamo di comprendere il mondo, ma ci sono limiti alla prospettiva che portiamo in questo. Non dovremmo semplicemente inserire dati in un modello sperando che risolva tutto. Non lo far√†.\nI modelli scientifici sono strumenti essenziali per comprendere la realt√† che ci circonda. Il processo di creazione, esplorazione e analisi di questi modelli √® fondamentale per approfondire la nostra conoscenza del mondo. Questo processo si pu√≤ suddividere in diverse fasi:\n√à importante sottolineare che il valore principale di questo processo non risiede nel risultato finale, cio√® nel modello stesso, ma nell‚Äôapprendimento e nella comprensione che otteniamo durante il percorso. Anche se a volte il modello finale pu√≤ effettivamente rappresentare accuratamente la realt√†, √® il processo di sviluppo e analisi che ci fornisce intuizioni preziose.\nQuando lavoriamo con i modelli, dobbiamo considerare due aspetti cruciali:\n√à fondamentale riconoscere che i dati su cui basiamo i nostri modelli spesso non sono perfettamente rappresentativi della realt√†. Questo pu√≤ essere dovuto a limitazioni nella raccolta dei dati, bias nei campioni o semplicemente alla complessit√† del mondo reale. Di conseguenza, i modelli addestrati su questi dati, sebbene utili, non sono infallibili.\nCome gi√† rilevato nel Chapter 10, per utilizzare efficacemente i modelli, dobbiamo porci costantemente due domande chiave:\nMantenere queste domande in primo piano ci aiuta a utilizzare i modelli in modo critico e consapevole, riconoscendone sia il potenziale che i limiti. Questo approccio ci permette di sfruttare al meglio i modelli come strumenti per comprendere il mondo, pur rimanendo consapevoli delle loro imperfezioni e delle sfide nella rappresentazione della realt√† complessa.\nL‚Äôevoluzione e l‚Äôapplicazione dei metodi statistici moderni presentano un interessante caso di studio nell‚Äôadattamento degli strumenti scientifici a contesti in rapida evoluzione. Molti dei metodi statistici attualmente in uso trovano le loro radici in campi come l‚Äôastronomia e l‚Äôagricoltura. Un esempio emblematico √® rappresentato da Ronald Fisher, figura di spicco nello sviluppo della statistica moderna, le cui opere seminali furono concepite durante il suo periodo presso un istituto di ricerca agricola.\nTuttavia, il panorama scientifico e tecnologico ha subito profondi cambiamenti dall‚Äôepoca di Fisher. L‚Äôapplicazione di questi metodi statistici si √® estesa a contesti che i loro ideatori difficilmente avrebbero potuto prevedere. Questa espansione solleva interrogativi cruciali sulla validit√† delle assunzioni fondamentali di questi metodi quando applicati in ambiti cos√¨ diversi da quelli originari.\nIn conclusione, mentre la statistica rimane uno strumento di inestimabile valore, il suo utilizzo efficace richiede un equilibrio tra una solida conoscenza dei principi fondamentali e la flessibilit√† necessaria per adattarsi a scenari di ricerca in continua evoluzione. L‚Äôintegrazione di metodologie diverse √® essenziale per garantire l‚Äôaffidabilit√† e la robustezza dei modelli statistici. Solo attraverso questo approccio olistico e adattativo possiamo sperare di comprendere e interpretare adeguatamente la complessit√† del mondo contemporaneo.\nIn questo capitolo, esploreremo due modelli statistici fondamentali: la regressione lineare bivariata e la regressione lineare multipla. La prima considera una sola variabile esplicativa, mentre la seconda ne include diverse. Per ciascun modello, esamineremo due approcci distinti:\n√à importante sottolineare che i modelli statistici sono generalmente ottimizzati per uno di due scopi: l‚Äôinferenza o la previsione. La focalizzazione sulla previsione √® una caratteristica distintiva del machine learning, un campo tradizionalmente dominato da Python. Tuttavia, in R, il pacchetto tidymodels offre ora funzionalit√† simili.\nIndipendentemente dall‚Äôapproccio scelto, √® fondamentale tenere presente che l‚Äôanalisi di regressione √® essenzialmente una forma di media ponderata. Di conseguenza, i risultati ottenuti riflettono inevitabilmente i bias e le peculiarit√† del dataset utilizzato.\nInfine, una nota sulla terminologia e sulla notazione. Per ragioni storiche e specifiche del contesto, esistono vari termini usati per descrivere la stessa idea nella letteratura. Seguiamo Gelman, Hill, and Vehtari (2020) e utilizziamo i termini ‚Äúoutcome‚Äù e ‚Äúpredictor‚Äù, e la specificazione del modello bayesiano di McElreath (2020).",
    "crumbs": [
      "Modelli lineari",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Modello di regressione lineare bayesiano</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_5/05_03_reglin_bayesian.html#introduzione",
    "href": "chapters/chapter_5/05_03_reglin_bayesian.html#introduzione",
    "title": "20¬† Modello di regressione lineare bayesiano",
    "section": "",
    "text": "La regressione √® in effetti un oracolo, ma un oracolo crudele. Parla per enigmi e si diletta nel punirci per aver posto domande sbagliate.\nMcElreath (2020)\n\n\n\nCostruzione: Creiamo modelli basati sulle nostre attuali conoscenze e ipotesi.\nEsplorazione: Studiamo le caratteristiche e le implicazioni dei modelli creati.\nVerifica: Testiamo i modelli confrontandoli con dati reali e osservazioni.\nValutazione: Apprezziamo l‚Äôeleganza e l‚Äôefficacia dei modelli quando funzionano bene.\nAnalisi critica: Cerchiamo di comprendere i limiti e le debolezze dei nostri modelli.\nRevisione o sostituzione: Quando necessario, modifichiamo o abbandoniamo i modelli inadeguati.\n\n\n\n\nIl ‚Äúmondo del modello‚Äù: le assunzioni, le semplificazioni e le regole interne del modello stesso.\nIl ‚Äúmondo reale‚Äù: la realt√† pi√π ampia e complessa che stiamo cercando di comprendere e descrivere.\n\n\n\n\nIn che misura il modello ci insegna qualcosa sui dati che abbiamo a disposizione?\nQuanto accuratamente i dati che abbiamo riflettono il mondo reale su cui vogliamo trarre conclusioni?\n\n\n\n\n\n\n\nL‚Äôutilizzo di R base, concentrandoci sulle funzioni lm() e glm() (oppure le funzioni di pingouin in Python). Queste sono particolarmente utili per l‚Äôanalisi esplorativa dei dati (EDA) quando si necessita di risultati rapidi.\nL‚Äôapproccio bayesiano, ideale quando l‚Äôobiettivo principale √® l‚Äôinferenza statistica.",
    "crumbs": [
      "Modelli lineari",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Modello di regressione lineare bayesiano</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_5/05_03_reglin_bayesian.html#modellare-lassociazione-statistica-tra-variabili",
    "href": "chapters/chapter_5/05_03_reglin_bayesian.html#modellare-lassociazione-statistica-tra-variabili",
    "title": "20¬† Modello di regressione lineare bayesiano",
    "section": "20.2 Modellare l‚Äôassociazione statistica tra variabili",
    "text": "20.2 Modellare l‚Äôassociazione statistica tra variabili\nPer introdurre l‚Äôapproccio bayesiano al modello di regressione, esamineremo un set di dati che riguarda la relazione tra la temperatura media e gli introiti (in dollari) di un particolare negozio di gelati. Sebbene questa possa non sembrare una questione scientifica particolarmente entusiasmante, √® un esempio semplice e intuitivo da analizzare. La relazione tra consumo di gelati e temperatura pu√≤ apparire banale, ma offre una chiara opportunit√† per comprendere i meccanismi alla base dell‚Äôassociazione tra due variabili. In questo momento, ci concentreremo esclusivamente sui metodi per stimare tale associazione.\nIn precedenza, abbiamo applicato il modello Normale ad una singola variabile. Tuttavia, di solito siamo interessati a modellare come una variabile di esito sia correlata a un‚Äôaltra variabile predittiva. Se la variabile predittiva ha una qualche associazione statistica con la variabile di esito, possiamo utilizzarla per predire il risultato. Quando la variabile predittiva √® integrata nel modello in un modo specifico, otteniamo una regressione lineare.\nI dati dell‚Äôesempio sono forniti di seguito.\n\ndata = {\n    \"temperature\": [\n        14.2,\n        16.4,\n        11.9,\n        15.2,\n        18.5,\n        22.1,\n        19.4,\n        25.1,\n        23.4,\n        18.1,\n        22.6,\n        17.2,\n    ],\n    \"icecream\": [215, 325, 185, 332, 406, 522, 412, 614, 544, 421, 445, 408],\n}\n\n# Create a DataFrame\ndf = pd.DataFrame(data)\ndf.head()\n\n\n\n\n\n\n\n\n\ntemperature\nicecream\n\n\n\n\n0\n14.2\n215\n\n\n1\n16.4\n325\n\n\n2\n11.9\n185\n\n\n3\n15.2\n332\n\n\n4\n18.5\n406\n\n\n\n\n\n\n\n\nL‚Äôassociazione tra le due variabili (gli introiti derivanti dalla vendita di gelati e la temperatura) √® chiaramente visualizzata nel grafico seguente. Il grafico indica che l‚Äôassociazione tra le due variabili pu√≤ essere approssimata da una semplice funzione matematica, ovvero una retta. Tuttavia, √® evidente che una funzione lineare sia troppo semplice per rappresentare accuratamente questi dati: non √® possibile trovare una singola retta che passi per tutti i punti del diagramma di dispersione.\n\nplt.scatter(df[\"temperature\"], df[\"icecream\"], color=\"blue\")\nplt.xlabel(\"Temperature (¬∞C)\")\nplt.ylabel(\"Ice Cream Sales ($)\")\nplt.grid(True)\nplt.show()",
    "crumbs": [
      "Modelli lineari",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Modello di regressione lineare bayesiano</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_5/05_03_reglin_bayesian.html#modello-generativo-dei-dati",
    "href": "chapters/chapter_5/05_03_reglin_bayesian.html#modello-generativo-dei-dati",
    "title": "20¬† Modello di regressione lineare bayesiano",
    "section": "20.3 Modello Generativo dei Dati",
    "text": "20.3 Modello Generativo dei Dati\nPer descrivere la relazione tra introiti e temperatura, utilizzeremo un modello statistico lineare. Assumeremo che la relazione media tra \\(x\\) (temperatura) e \\(y\\) (introiti) possa essere rappresentata da una retta, ma influenzata da un certo grado di errore. Supponiamo che questo errore sia costante ai vari livelli di \\(x\\) e segua una distribuzione Normale. Inoltre, presupponiamo che gli errori attorno alla retta di regressione siano indipendenti tra loro.\nIn questo contesto, le nostre assunzioni delineano un modello statistico lineare, formalizzato come segue:\n\\[ y_i = \\alpha + \\beta x_i + \\epsilon_i \\]\ndove: - \\(\\alpha\\) √® l‚Äôintercetta, - \\(\\beta\\) √® il coefficiente angolare, - \\(\\epsilon_i \\sim \\text{Normale}(0, \\sigma^2)\\) rappresenta l‚Äôerrore, con media zero e varianza costante \\(\\sigma^2\\).\nQueste assunzioni ci permettono di applicare metodi di regressione lineare per stimare i parametri del modello (\\(\\alpha\\) e \\(\\beta\\)) e quantificare l‚Äôincertezza delle predizioni, considerando la variabilit√† nei dati.\nIl modello lineare descritto sopra costituisce il modello generativo dei dati (verosimiglianza):\n\\[ y_n = \\alpha + \\beta x_n + \\epsilon_n \\]\no equivalentemente\n\\[ \\epsilon_n \\sim \\text{Normale}(0, \\sigma) \\]\nQuesto modello descrive come i dati $ y_n $ sono generati. Ogni osservazione $ y_n $ √® una combinazione lineare di una costante \\(\\alpha\\) (intercetta), un coefficiente \\(\\beta\\) che moltiplica il valore della variabile $ x_n $ (temperatura), e un termine di errore \\(\\epsilon_n\\) che cattura la variabilit√† non spiegata dal modello lineare.\nIl termine di errore \\(\\epsilon_n\\) √® distribuito secondo una distribuzione normale con media 0 e deviazione standard \\(\\sigma\\). Questo implica che l‚Äôerrore √® simmetricamente distribuito attorno a zero e ha una variabilit√† definita da \\(\\sigma\\).\nL‚Äôequazione\n\\[ y_n \\sim \\text{Normale}(\\alpha + \\beta x_n, \\sigma) \\]\nmostra come il valore osservato $ y_n $ segue una distribuzione normale con media \\(\\alpha + \\beta x_n\\) e deviazione standard \\(\\sigma\\). Questo significa che, dato $ x_n $, i valori di $ y_n $ sono distribuiti normalmente attorno alla retta di regressione definita da \\(\\alpha + \\beta x_n\\).\nConsideriamo il caso in cui $ y_n $ rappresenta le vendite di gelati e $ x_n $ rappresenta la temperatura media. Secondo il nostro modello:\n\n\\(\\alpha\\) √® l‚Äôintercetta, ovvero le vendite di gelati previste quando la temperatura √® zero.\n\\(\\beta\\) √® il coefficiente che indica quanto aumentano (o diminuiscono) le vendite di gelati per ogni aumento di un grado della temperatura.\n\\(\\sigma\\) √® la deviazione standard che misura la variabilit√† delle vendite di gelati attorno alla media prevista dal modello.\n\nQuesto modello ci consente di stimare l‚Äôeffetto della temperatura sulle vendite di gelati e di quantificare l‚Äôincertezza associata a queste stime.",
    "crumbs": [
      "Modelli lineari",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Modello di regressione lineare bayesiano</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_5/05_03_reglin_bayesian.html#modello-bayesiano-della-regressione-bivariata",
    "href": "chapters/chapter_5/05_03_reglin_bayesian.html#modello-bayesiano-della-regressione-bivariata",
    "title": "20¬† Modello di regressione lineare bayesiano",
    "section": "20.4 Modello Bayesiano della Regressione Bivariata",
    "text": "20.4 Modello Bayesiano della Regressione Bivariata\n\n20.4.1 Verosimiglianza\nAssumiamo la verosimiglianza che abbiamo descritto in precedenza:\n\\[ y \\sim \\text{Normale}(\\alpha + \\beta x, \\sigma) \\]\nQuesto significa che i dati \\(y\\) seguono una distribuzione normale con media \\(\\alpha + \\beta x\\) e deviazione standard \\(\\sigma\\). In altre parole, il valore osservato \\(y\\) √® generato come una combinazione lineare di \\(\\alpha\\) (intercetta), \\(\\beta\\) (coefficiente della variabile \\(x\\)), pi√π un errore che segue una distribuzione normale con deviazione standard \\(\\sigma\\).\n\n\n20.4.2 Distribuzioni a Priori\nIn una prima versione del modello, useremo delle distribuzioni a priori uniformi per i tre parametri.\n\n\n20.4.3 Distribuzioni a Posteriori\nLe distribuzioni a priori vengono combinate con i dati osservati attraverso il teorema di Bayes per aggiornare le nostre credenze sui parametri del modello. Il risultato √® una distribuzione a posteriori per ciascun parametro che riflette sia l‚Äôinformazione contenuta nei dati che le credenze iniziali incorporate nelle distribuzioni a priori. Questo processo permette di fare inferenze pi√π robuste, specialmente quando i dati sono limitati o rumorosi.\n\n\n20.4.4 Codice Stan\nIl codice Stan che implementa il modello precedente √® contenuto nel file icecream_model_1.stan. Compiliamo e stampiamo il modello.\n\nstan_file = os.path.join(project_directory, 'stan', 'icecream_model_1.stan')\nmodel = CmdStanModel(stan_file=stan_file)\nprint(model.code())\n\ndata {\n  int&lt;lower=1&gt; N; // numero totale di osservazioni \n  vector[N] y; // variabile di risposta\n  vector[N] x; // variabile predittore\n}\nparameters {\n  real alpha; // intercetta\n  real beta; // coefficiente angolare\n  real&lt;lower=0&gt; sigma; // deviazione standard residua\n}\nmodel {\n  // verosimiglianza\n  y ~ normal(alpha + beta * x, sigma);\n}\n\n\n\nSi noti che, non avendo specificato le distribuzioni a priori per i parametri \\(\\alpha\\), \\(\\beta\\) e \\(\\sigma\\), Stan assume distribuzioni a priori uniformi.\n\n\n20.4.5 Dizionario con i dati\nSistemiamo i dati in un dizionario come richiesto dal modello Stan.\n\nstan_data = {\n    \"N\": len(df[\"temperature\"]),\n    \"x\": df[\"temperature\"],\n    \"y\": df[\"icecream\"]\n}\nprint(stan_data)\n\n{'N': 12, 'x': 0     14.2\n1     16.4\n2     11.9\n3     15.2\n4     18.5\n5     22.1\n6     19.4\n7     25.1\n8     23.4\n9     18.1\n10    22.6\n11    17.2\nName: temperature, dtype: float64, 'y': 0     215\n1     325\n2     185\n3     332\n4     406\n5     522\n6     412\n7     614\n8     544\n9     421\n10    445\n11    408\nName: icecream, dtype: int64}\n\n\n\n\n20.4.6 Campionamento MCMC\nEseguiamo il campionamento MCMC.\n\nfit = model.sample(\n    data=stan_data,\n    iter_warmup=1_000,\n    iter_sampling=2_000,\n    seed=123,\n    show_progress=False,\n    show_console=False\n)\n\n\n\n20.4.7 Distribuzioni a posteriori\nEsaminiamo le distribuzioni a posteriori dei parametri.\n\n_ = az.plot_trace(fit, var_names=([\"alpha\", \"beta\", \"sigma\"]))\n\n\n\n\n\n\n\n\nL‚Äôoggetto fit generato da cmdstanpy appartiene alla classe cmdstanpy.stanfit.mcmc.CmdStanMCMC. Questo oggetto √® funzionalmente equivalente a un oggetto della classe InferenceData, permettendo quindi la sua manipolazione tramite le funzioni fornite da ArviZ. Esaminiamo dunque un sommario delle distribuzioni a posteriori dei parametri del modello lineare.\n\naz.summary(fit, var_names=([\"alpha\", \"beta\", \"sigma\"]), hdi_prob=0.94)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nalpha\n-158.315\n64.516\n-271.588\n-28.269\n1.429\n1.010\n2051.0\n2311.0\n1.0\n\n\nbeta\n30.032\n3.372\n23.574\n36.268\n0.074\n0.053\n2065.0\n2329.0\n1.0\n\n\nsigma\n44.162\n12.121\n25.540\n66.813\n0.270\n0.195\n2120.0\n2226.0\n1.0\n\n\n\n\n\n\n\n\nAvendo definito nel modello il predittore lineare come \\(x - \\bar{x}\\), l‚Äôintercetta corrisponder√† alla media dei valori dell‚Äôaltezza. La pendenza \\(\\beta\\) ci informa sull‚Äôincremento atteso dell‚Äôaltezza quando il peso aumenta di un‚Äôunit√†. Il parametro \\(\\sigma\\) descrive la deviazione standard della dispersione dei dati attorno alla retta di regressione.\nConfrontiamo i valori ottenuti con l‚Äôapproccio bayesiano con quelli trovati usando la procedura di massima verosimiglianza.\n\nlm = pg.linear_regression(df[\"temperature\"], df[\"icecream\"])\nlm.round(2)\n\n\n\n\n\n\n\n\n\nnames\ncoef\nse\nT\npval\nr2\nadj_r2\nCI[2.5%]\nCI[97.5%]\n\n\n\n\n0\nIntercept\n-159.47\n54.64\n-2.92\n0.02\n0.92\n0.91\n-281.22\n-37.73\n\n\n1\ntemperature\n30.09\n2.87\n10.50\n0.00\n0.92\n0.91\n23.70\n36.47\n\n\n\n\n\n\n\n\nLa somiglianza tra le due soluzioni conferma che, nel caso di modelli semplici come questo, e quando vengono usati dei prior non informativi, i due approcci producono risultati sostanzialmente equivalenti.\n\n\n20.4.8 Interpretazione\nPossiamo interpretare i parametri come segue:\n\nLa media a posteriori di \\(\\alpha = -159.47\\) indica che il valore atteso degli introiti del negozio di gelati √® di -159.47 dollari quando la temperatura √® di 0 gradi centigradi.\nLa media a posteriori di \\(\\beta = 30.09\\) suggerisce che ci aspettiamo un aumento medio di 30.09 dollari negli introiti del negozio di gelati per ogni incremento di 1 grado centigrado nella temperatura.",
    "crumbs": [
      "Modelli lineari",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Modello di regressione lineare bayesiano</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_5/05_03_reglin_bayesian.html#predizione",
    "href": "chapters/chapter_5/05_03_reglin_bayesian.html#predizione",
    "title": "20¬† Modello di regressione lineare bayesiano",
    "section": "20.5 Predizione",
    "text": "20.5 Predizione\nLa distribuzione a posteriori non contiene solo informazioni su ciascun parametro singolarmente, ma anche sulle dipendenze tra i parametri. Queste dipendenze sono riflesse nei campioni a posteriori che possono essere trasformati arbitrariamente.\nAd esempio, supponiamo che alpha e beta siano vettori di campioni a posteriori.\nCon l‚Äôistruzione seguente nel blocco generated quantities, possiamo calcolare la predizione a posteriori per 30 gradi Celsius:\npred = alpha + beta * 30;\nModifichiamo il modello Stan per aggiungere il blocco generated quantities con questa istruzione e compiliamo il modello.\n\nstan_file = os.path.join(project_directory, \"stan\", \"icecream_model_2.stan\")\nmodel = CmdStanModel(stan_file=stan_file)\nprint(model.code())\n\ndata {\n  int&lt;lower=1&gt; N; // numero totale di osservazioni \n  vector[N] y; // variabile di risposta\n  vector[N] x; // variabile predittore\n}\nparameters {\n  real alpha; // intercetta\n  real beta; // coefficiente angolare\n  real&lt;lower=0&gt; sigma; // deviazione standard residua\n}\nmodel {\n  // verosimiglianza\n  y ~ normal(alpha + beta * x, sigma);\n}\ngenerated quantities {\n  real pred; // predizione\n  \n  pred = alpha + beta * 30;\n}\n\n\n\nIn questo modello Stan aggiornato, il blocco generated quantities calcola la predizione a posteriori pred per una variabile predittore con valore 30. Questa aggiunta consente di ottenere la distribuzione a posteriori della predizione per un determinato valore del predittore.\nEseguiamo il campionamento.\n\nfit2 = model.sample(\n    data=stan_data,\n    iter_warmup=1_000,\n    iter_sampling=2_000,\n    seed=123,\n    show_progress=False,\n    show_console=False,\n)\n\nEsaminiamo la stima a posteriori dell‚Äôintroito previsto per il negozio di gelati quando la temperatura raggiunge i 30 gradi Celsius. Questa analisi fornir√† sia una stima puntuale dell‚Äôintroito che una misura dell‚Äôincertezza associata, rappresentata dall‚Äôintervallo di credibilit√† al livello di confidenza prescelto.\n\naz.summary(fit2, var_names=([\"pred\"]), hdi_prob=0.94)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\npred\n742.651\n40.123\n664.016\n816.924\n0.818\n0.579\n2409.0\n3196.0\n1.0\n\n\n\n\n\n\n\n\n\n20.5.1 Quantificazione dell‚Äôincertezza\nPer quantificare l‚Äôincertezza nelle predizioni del modello, possiamo estendere il metodo esaminato in precedenza per calcolare la distribuzione a posteriori delle predizioni per tutti i valori di \\(x\\) considerati. Questo ci permette di ottenere non solo le stime puntuali delle predizioni, ma anche una misura dell‚Äôincertezza associata a ciascuna predizione.\nPer fare ci√≤, modifichiamo il blocco generated quantities nel seguente modo:\ngenerated quantities {\n  vector[N] y_rep; // predizioni a posteriori per ciascun valore di x\n  \n  for (n in 1:N) {\n    y_rep[n] = normal_rng(alpha + beta * x[n], sigma);\n  }\n}\nEsaminiamo nei dettagli la modifica allo script Stan.\n\nDichiarazione del vettore y_rep:\n\nvector[N] y_rep;: Questa linea dichiara un vettore y_rep di lunghezza N che conterr√† le predizioni a posteriori per ciascun valore di x.\n\nCiclo for per generare le predizioni:\n\nfor (n in 1:N): Questo ciclo iterativo va da 1 a N, cio√® copre tutte le osservazioni.\ny_rep[n] = normal_rng(alpha + beta * x[n], sigma);: Per ogni valore di x[n], questa linea genera una predizione dalla distribuzione normale con media alpha + beta * x[n] e deviazione standard sigma. La funzione normal_rng √® utilizzata per generare numeri casuali dalla distribuzione normale specificata, rappresentando cos√¨ l‚Äôincertezza nelle predizioni.\n\n\n\nQuesto approccio consente di ottenere la distribuzione a posteriori delle predizioni, fornendo una visione completa dell‚Äôincertezza associata a ciascuna predizione.\nDalla distribuzione a posteriori di y_rep, possiamo calcolare sia la stima puntuale (ad esempio, la media o la mediana delle predizioni) sia gli intervalli di credibilit√† (ad esempio, l‚Äôintervallo al 95%) per ogni valore di x. Questo offre una misura dell‚Äôincertezza delle predizioni, riflettendo la variabilit√† e l‚Äôaffidabilit√† del modello.\n\nIn sintesi, questa modifica ci permette di valutare l‚Äôincertezza delle predizioni del modello in modo robusto e dettagliato, migliorando la comprensione della performance del modello e della sua capacit√† predittiva.\n\nstan_file = os.path.join(project_directory, \"stan\", \"icecream_model_3.stan\")\nmodel = CmdStanModel(stan_file=stan_file)\nprint(model.code())\n\ndata {\n  int&lt;lower=1&gt; N; // numero totale di osservazioni \n  vector[N] y; // variabile di risposta\n  vector[N] x; // variabile predittore\n}\nparameters {\n  real alpha; // intercetta\n  real beta; // coefficiente angolare\n  real&lt;lower=0&gt; sigma; // deviazione standard residua\n}\nmodel {\n  // verosimiglianza\n  y ~ normal(alpha + beta * x, sigma);\n}\ngenerated quantities {\n  vector[N] y_rep; // variabili predette\n  \n  for (n in 1 : N) {\n    y_rep[n] = normal_rng(alpha + beta * x[n], sigma);\n  }\n}\n\n\n\n\nfit3 = model.sample(\n    data=stan_data,\n    iter_warmup=1_000,\n    iter_sampling=2_000,\n    seed=123,\n    show_progress=False,\n    show_console=False,\n)\n\nCostruiamo ora un grafico che rappresenta i valori osservati insieme alla linea di regressione stimata tramite il modello bayesiano. Al grafico aggiungeremo diverse linee di regressione, ciascuna orientata in base ai valori campionati casualmente dalla distribuzione a posteriori dei parametri \\(\\alpha\\) e \\(\\beta\\).\n\n# Extract posterior samples\nalpha_samples = fit3.stan_variable(\"alpha\")\nbeta_samples = fit3.stan_variable(\"beta\")\n\n\n# Plot y vs x\nx = df[\"temperature\"] \nplt.scatter(x, df[\"icecream\"], color=\"blue\", label=\"Data\", s=10)  # s is the size of the point\n\n# Draw lines from posterior samples\nfor i in range(300):  # assuming you have at least 300 samples\n    plt.plot(\n        x,\n        alpha_samples[i] + beta_samples[i] * x,\n        color=\"gray\",\n        linestyle=\"-\",\n        linewidth=0.5,\n        alpha=0.05,\n    )\n\n# Line using the mean of posterior estimates\nmean_alpha = np.mean(alpha_samples)\nmean_beta = np.mean(beta_samples)\nplt.plot(\n    x,\n    mean_alpha + mean_beta * x,\n    color=\"red\",\n    linewidth=2,\n    label=\"Mean Posterior Prediction\",\n)\n\n# Additional plot formatting\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Regression with Posterior Samples\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nLe numerose linee di regressione presenti nel grafico visualizzano la nostra incertezza riguardo l‚Äôinclinazione esatta della linea di regressione principale. Tuttavia, il grafico mostra chiaramente che questa incertezza √® minima.\nPossiamo procedere in un altro modo per descrivere l‚Äôincertezza della stima. Anzich√© utilizzare le distribuzioni a posteriori di alpha e beta, possiamo utilizzare la distribuzione a posteriori di y_rep. Procedendo in questo modo otteniamo il grafico mostrato qui sotto.\nIn questo plot, la linea rossa rappresenta la media delle predizioni a posteriori, mentre l‚Äôarea grigia rappresenta l‚Äôintervallo di credibilit√† al 95%, mostrando l‚Äôincertezza delle predizioni del modello. Questo approccio fornisce una visione pi√π completa e realistica dell‚Äôincertezza nelle predizioni rispetto all‚Äôapproccio che utilizza solo alpha e beta.\n\n# Estrai i campioni posteriori di y_rep\ny_rep_samples = fit3.stan_variable(\"y_rep\")\n\n# Calcola la media e l'intervallo di credibilit√† (ad esempio, 95%) per y_rep\ny_rep_mean = np.mean(y_rep_samples, axis=0)\ny_rep_lower = np.percentile(y_rep_samples, 2.5, axis=0)\ny_rep_upper = np.percentile(y_rep_samples, 97.5, axis=0)\n\n# Plot y vs x\nx = df[\"temperature\"]\ny = df[\"icecream\"]\nplt.scatter(x, y, color=\"blue\", label=\"Dati\", s=10)\n\n# Plot della media delle predizioni a posteriori\nplt.plot(x, y_rep_mean, color=\"red\", linewidth=2, label=\"Media delle predizioni\")\n\n# Plot dell'intervallo di credibilit√†\nplt.fill_between(\n    x,\n    y_rep_lower,\n    y_rep_upper,\n    color=\"gray\",\n    alpha=0.3,\n    label=\"Intervallo di credibilit√† 95%\",\n)\n\n# Formattazione del plot\nplt.xlabel(\"Temperatura (Celsius)\")\nplt.ylabel(\"Vendite di gelati\")\nplt.title(\"Incertezza delle predizioni del modello\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNel primo approccio, calcoliamo l‚Äôincertezza delle predizioni utilizzando le distribuzioni a posteriori di alpha e beta. Questo metodo consiste nel generare predizioni lineari per ciascun campione a posteriori di alpha e beta, tracciando quindi le linee di regressione risultanti. Questo ci permette di vedere come varia la linea di regressione in base alle incertezze nei parametri alpha e beta. Questo metodo visualizza come l‚Äôincertezza nei parametri del modello si traduce in incertezza nelle predizioni.\nNel secondo approccio, descriviamo l‚Äôincertezza delle predizioni utilizzando direttamente la distribuzione a posteriori di y_rep. In questo caso, generiamo predizioni per ciascun valore osservato di x nel modello Stan, tenendo conto delle distribuzioni a posteriori dei parametri del modello. Questo metodo visualizza direttamente l‚Äôincertezza nelle predizioni, tenendo conto delle variazioni nei dati osservati e delle distribuzioni a posteriori dei parametri.\nLe due descrizioni dell‚Äôincertezza delle predizioni del modello sono diverse perch√© riflettono aspetti differenti della distribuzione a posteriori:\n\nDistribuzione a posteriori di alpha e beta: Questo approccio considera solo l‚Äôincertezza nei parametri del modello (alpha e beta). Le linee di regressione tracciate variano in base a questi parametri, ma non tengono conto dell‚Äôincertezza residua (sigma).\nDistribuzione a posteriori di y_rep: Questo approccio include non solo l‚Äôincertezza nei parametri alpha e beta, ma anche l‚Äôincertezza residua (sigma). La distribuzione di y_rep riflette la variabilit√† totale nel modello, inclusa la variabilit√† nei dati osservati. Pertanto, l‚Äôincertezza nelle predizioni √® maggiore perch√© tiene conto di tutte le fonti di variabilit√†.",
    "crumbs": [
      "Modelli lineari",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Modello di regressione lineare bayesiano</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_5/05_03_reglin_bayesian.html#ricodifica-dei-dati",
    "href": "chapters/chapter_5/05_03_reglin_bayesian.html#ricodifica-dei-dati",
    "title": "20¬† Modello di regressione lineare bayesiano",
    "section": "20.6 Ricodifica dei dati",
    "text": "20.6 Ricodifica dei dati\nL‚Äôintercetta (\\(\\alpha\\)) corrisponde al valore atteso degli introiti quando la temperatura √® di 0 gradi centigradi. Un valore negativo dell‚Äôintercetta indica che, in tali circostanze, ci aspettiamo una perdita. Tuttavia, possiamo ricodificare i dati in modo che l‚Äôintercetta abbia un‚Äôinterpretazione pi√π utile.\n\n20.6.1 Centratura della variabile $ x $\nCentriamo la variabile \\(x\\) (temperatura), sottraendo la media della temperatura (\\(\\bar{x}\\)) da ciascun valore di \\(x\\). In questo modo otteniamo la nuova variabile \\(xc\\) la cui media √® 0. La trasformazione √® definita come:\n\\[ xc_i = x_i - \\bar{x} \\]\nDove \\(\\bar{x}\\) √® la media della temperatura.\n\n\n20.6.2 Interpretazione della nuova intercetta\nIl valore 0 di \\(xc\\) corrisponde al valore medio di \\(x\\). Dato che la retta di regressione passa per il punto \\((\\bar{x}, \\bar{y})\\), se utilizziamo \\(xc\\) al posto di \\(x\\), la nuova intercetta (\\(\\alpha\\)) ci dir√† qual √® il valore atteso degli introiti (\\(y\\)) quando la temperatura assume il suo valore medio (\\(\\bar{x}\\)).\n\n# Calcolo della temperatura media\nmean_temp = df[\"temperature\"].mean()\n\n# Creazione della variabile centrata\ndf[\"temp_c\"] = df[\"temperature\"] - mean_temp\n\n\nstan_data2 = {\"N\": len(df[\"temp_c\"]), \"x\": df[\"temp_c\"], \"y\": df[\"icecream\"]}\nprint(stan_data2)\n\n{'N': 12, 'x': 0    -4.475\n1    -2.275\n2    -6.775\n3    -3.475\n4    -0.175\n5     3.425\n6     0.725\n7     6.425\n8     4.725\n9    -0.575\n10    3.925\n11   -1.475\nName: temp_c, dtype: float64, 'y': 0     215\n1     325\n2     185\n3     332\n4     406\n5     522\n6     412\n7     614\n8     544\n9     421\n10    445\n11    408\nName: icecream, dtype: int64}\n\n\n\nfit4 = model.sample(\n    data=stan_data2,\n    iter_warmup=1_000,\n    iter_sampling=2_000,\n    seed=123,\n    show_progress=False,\n    show_console=False,\n)\n\n\naz.summary(fit4, var_names=([\"alpha\", \"beta\", \"sigma\"]), hdi_prob=0.94)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nalpha\n402.356\n13.079\n377.577\n426.508\n0.166\n0.117\n6454.0\n4831.0\n1.0\n\n\nbeta\n30.093\n3.398\n23.867\n36.725\n0.047\n0.033\n5538.0\n4353.0\n1.0\n\n\nsigma\n43.766\n11.651\n26.153\n66.129\n0.180\n0.128\n4437.0\n5027.0\n1.0\n\n\n\n\n\n\n\n\nNotiamo che la media a posteriori di \\(\\beta\\) √® rimasta invariata. Tuttavia, la media a posteriori di \\(\\alpha\\) √® ora pari a 402,4 dollari. Questo rappresenta il valore atteso degli introiti del negozio di gelati quando la temperatura √® al suo valore medio.",
    "crumbs": [
      "Modelli lineari",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Modello di regressione lineare bayesiano</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_5/05_03_reglin_bayesian.html#distribizioni-a-priori-sui-parametri",
    "href": "chapters/chapter_5/05_03_reglin_bayesian.html#distribizioni-a-priori-sui-parametri",
    "title": "20¬† Modello di regressione lineare bayesiano",
    "section": "20.7 Distribizioni a priori sui parametri",
    "text": "20.7 Distribizioni a priori sui parametri\nSpefichiamo ora le seguenti distribuzioni a priori debolmente informative sui parametri del modello.\n\nIntercetta (\\(\\alpha\\)):\n\n\\(\\alpha \\sim \\text{Normale}(0, 100)\\)\nLa scelta di una deviazione standard ampia (100) riflette l‚Äôincertezza riguardo al valore iniziale dell‚Äôintercetta. Si crede che l‚Äôintercetta possa essere qualsiasi valore vicino a 0, ma con una variazione significativa.\n\nCoefficiente Angolare (\\(\\beta\\)):\n\n\\(\\beta \\sim \\text{Normale}(0, 50)\\)\nUn‚Äôampia deviazione standard (50) per \\(\\beta\\) permette di incorporare l‚Äôincertezza riguardo all‚Äôinfluenza della temperatura sui ricavi del gelato. Questo prior permette che \\(\\beta\\) possa essere sia positivo che negativo con una vasta gamma di valori.\n\nDeviazione Standard Residua (\\(\\sigma\\)):\n\n\\(\\sigma \\sim \\text{Cauchy}^+(0, 5)\\)\nLa distribuzione Half-Cauchy √® scelta perch√© √® debolmente informativa e adatta per i parametri di scala come la deviazione standard residua. La scala di 5 consente a \\(\\sigma\\) di assumere una vasta gamma di valori positivi, riflettendo l‚Äôincertezza riguardo alla variabilit√† residua.\n\n\n\n20.7.1 Interpretazione delle Distribuzioni a Priori\nQueste distribuzioni a priori rappresentano le credenze iniziali riguardanti i parametri del modello prima di osservare i dati. Le distribuzioni normali per \\(\\alpha\\) e \\(\\beta\\) con deviazioni standard ampie permettono una grande flessibilit√†, mentre la distribuzione Half-Cauchy per \\(\\sigma\\) √® scelta per la sua capacit√† di gestire bene i parametri di scala. Queste scelte garantiscono che il modello sia debolmente informativo, permettendo ai dati osservati di avere un‚Äôinfluenza significativa sulle stime posteriori dei parametri.\n\nstan_file = os.path.join(project_directory, \"stan\", \"icecream_model_prior.stan\")\nmodel = CmdStanModel(stan_file=stan_file)\nprint(model.code())\n\ndata {\n  int&lt;lower=1&gt; N; // numero totale di osservazioni \n  vector[N] y; // variabile di risposta\n  vector[N] x; // variabile predittore\n}\nparameters {\n  real alpha; // intercetta\n  real beta; // coefficiente angolare\n  real&lt;lower=0&gt; sigma; // deviazione standard residua\n}\nmodel {\n  // distribuzioni a priori\n  alpha ~ normal(0, 100);\n  beta ~ normal(0, 50);\n  sigma ~ cauchy(0, 5);\n  // verosimiglianza\n  y ~ normal(alpha + beta * x, sigma);\n}\n\n\n\nAdattiamo il nuovo modello ai dati.\n\nfit5 = model.sample(\n    data=stan_data2,\n    iter_warmup=1_000,\n    iter_sampling=2_000,\n    seed=123,\n    show_progress=False,\n    show_console=False,\n)\n\nEsaminiamo le distribuzioni a posteriori dei parametri.\n\naz.summary(fit5, var_names=([\"alpha\", \"beta\", \"sigma\"]), hdi_prob=0.94)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nalpha\n396.790\n12.114\n373.733\n419.045\n0.187\n0.132\n4518.0\n3481.0\n1.0\n\n\nbeta\n30.015\n2.996\n24.402\n35.673\n0.039\n0.028\n6018.0\n4754.0\n1.0\n\n\nsigma\n39.558\n9.771\n24.252\n57.630\n0.164\n0.120\n4130.0\n3957.0\n1.0\n\n\n\n\n\n\n\n\nSi noti che, utilizzando distribuzioni a priori debolmente informative, le distribuzioni a posteriori dei parametri risultano molto simili a quelle ottenute usando distribuzioni uniformi. Tuttavia, le distribuzioni a priori debolmente informative sono preferibili poich√© forniscono una maggiore stabilit√† numerica e sono generalmente pi√π affidabili e robuste, specialmente quando si lavora con dati reali. L‚Äôuso di distribuzioni uniformi √® sconsigliato per via delle possibili instabilit√† numeriche che possono introdurre nei modelli.\n\n\n20.7.2 Posterior-predictive check\nGeneriamo ora un PPC plot per confrontare le predizioni del modello con i dati osservati.\n\nstan_file = os.path.join(project_directory, \"stan\", \"icecream_model_3.stan\")\nmodel = CmdStanModel(stan_file=stan_file)\n\n\nfit6 = model.sample(\n    data=stan_data2,\n    iter_warmup=1_000,\n    iter_sampling=2_000,\n    seed=123,\n    show_progress=False,\n    show_console=False,\n)\n\nPer creare un PPC plot dobbiamo creare un oggetto InferenceData nel quale i dati sono strutturati come si aspetta ArviZ:\n\nidata = az.from_cmdstanpy(\n    posterior=fit6,\n    posterior_predictive=\"y_rep\",\n    observed_data={\"y\": df[\"icecream\"]},\n)\n\nAvendo generato l‚Äôoggetto idata, possiamo ora creare il pp-check plot.\n\n_ = az.plot_ppc(idata, data_pairs={\"y\": \"y_rep\"})\n\n/opt/anaconda3/envs/cmdstan_env/lib/python3.12/site-packages/IPython/core/events.py:82: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n  func(*args, **kwargs)",
    "crumbs": [
      "Modelli lineari",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Modello di regressione lineare bayesiano</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_5/05_03_reglin_bayesian.html#commenti-e-considerazioni-finali",
    "href": "chapters/chapter_5/05_03_reglin_bayesian.html#commenti-e-considerazioni-finali",
    "title": "20¬† Modello di regressione lineare bayesiano",
    "section": "20.8 Commenti e considerazioni finali",
    "text": "20.8 Commenti e considerazioni finali\nIn questo capitolo abbiamo esplorato la stima dei parametri di un modello di regressione bivariato utilizzando l‚Äôapproccio bayesiano.\nPer fornire un confronto con un metodo alternativo, in appendice √® presentata un‚Äôintroduzione all‚Äôapproccio frequentista per il modello di regressione lineare bivariato. Questa sezione aggiuntiva offre una panoramica degli aspetti chiave dell‚Äôapproccio frequentista, consentendo di confrontare e comprendere le differenze tra i due approcci nella stima dei parametri e nell‚Äôinterpretazione dei risultati.",
    "crumbs": [
      "Modelli lineari",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Modello di regressione lineare bayesiano</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_5/05_03_reglin_bayesian.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_5/05_03_reglin_bayesian.html#informazioni-sullambiente-di-sviluppo",
    "title": "20¬† Modello di regressione lineare bayesiano",
    "section": "20.9 Informazioni sull‚ÄôAmbiente di Sviluppo",
    "text": "20.9 Informazioni sull‚ÄôAmbiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -m  \n\nThe watermark extension is already loaded. To reload it, use:\n  %reload_ext watermark\nLast updated: Mon Jul 22 2024\n\nPython implementation: CPython\nPython version       : 3.12.4\nIPython version      : 8.26.0\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.5.0\nMachine     : arm64\nProcessor   : arm\nCPU cores   : 8\nArchitecture: 64bit\n\npandas    : 2.2.2\nlogging   : 0.5.1.2\nnumpy     : 1.26.4\nmatplotlib: 3.9.1\ncmdstanpy : 1.2.4\narviz     : 0.18.0\npingouin  : 0.5.4\n\n\n\n\n\n\n\nAlexander, Rohan. 2023. Telling Stories with Data: With Applications in r. Chapman; Hall/CRC.\n\n\nFox, John. 2015. Applied Regression Analysis and Generalized Linear Models. Sage publications.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press.\n\n\nIoannidis, John PA. 2005. ‚ÄúWhy Most Published Research Findings Are False.‚Äù PLoS Medicine 2 (8): e124.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd Edition. Boca Raton, Florida: CRC Press.\n\n\nStigler, Stephen. 1986. The History of Statistics. Massachusetts: Belknap Harvard.",
    "crumbs": [
      "Modelli lineari",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Modello di regressione lineare bayesiano</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_5/05_04_synt_sugar.html",
    "href": "chapters/chapter_5/05_04_synt_sugar.html",
    "title": "21¬† Zucchero sintattico",
    "section": "",
    "text": "21.1 Introduzione\nI modelli lineari sono cos√¨ ampiamente utilizzati che sono stati sviluppati appositamente una sintassi, dei metodi e delle librerie per la regressione. Una di queste librerie √® bambi (BAyesian Model-Building Interface). bambi √® un pacchetto Python progettato per adattare modelli gerarchici generalizzati lineari (di cui il modello lineare bivariato √® un caso particolare), utilizzando una sintassi simile a quella presente nei pacchetti R, come lme4, nlme, rstanarm o brms. bambi si basa su PyMC, ma offre un‚ÄôAPI di livello superiore.\nIn questo capitolo esploreremo come condurre un‚Äôanalisi di regressione utilizzando bambi invece di cmdstan.",
    "crumbs": [
      "Modelli lineari",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Zucchero sintattico</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_5/05_04_synt_sugar.html#bayesian-model-building-interface",
    "href": "chapters/chapter_5/05_04_synt_sugar.html#bayesian-model-building-interface",
    "title": "21¬† Zucchero sintattico",
    "section": "21.2 BAyesian Model-Building Interface",
    "text": "21.2 BAyesian Model-Building Interface\nLeggiamo i dati utilizzati nel capitolo precedente.\n\ndf = pd.read_csv('../../data/Howell_18.csv')\n\nGeneriamo un diagramma a dispersione:\n\nplt.plot(df[\"weight\"], df[\"height\"], \"x\")\nplt.xlabel(\"Weight\")\nplt.ylabel(\"Height\")\n\nText(0, 0.5, 'Height')\n\n\n\n\n\n\n\n\n\nBambi si concentra sui modelli di regressione e questa restrizione porta a una sintassi pi√π semplice, ovvero la sintassi di Wilkinson {cite:p}wilkinson1973symbolic. Inoltre, bambi implementa delle distribuzioni a priori ottimizzate, eliminando cos√¨ la necessit√† di definirle esplicitamente. Tuttavia, se si preferisce un maggiore controllo sulle distribuzioni a priori, √® possibile specificarle manualmente.\nPer replicare il modello descritto nel capitolo precedente possiamo utilizzare la seguente istruzione.\n\ndf[\"weight_c\"] = df[\"weight\"] - np.mean(df[\"weight\"])\n\nmodel = bmb.Model(\"height ~ weight_c\", df)\n\nSul lato sinistro della tilde (‚àº), abbiamo la variabile dipendente, e sul lato destro, le variabili indipendenti. Con questa sintassi, stiamo semplicemente specificando la media (Œº nel modello lm di PyMC). Per impostazione predefinita, Bambi assume che la verosimiglianza sia gaussiana; √® possibile modificarla con l‚Äôargomento family. La sintassi della formula non specifica la distribuzione delle priors, ma solo come sono associate le variabili dipendenti e indipendenti. Bambi definir√† automaticamente delle priors (molto) debolmente informative per noi. Possiamo ottenere ulteriori informazioni stampando il modello Bambi.\n\nprint(model)\n\n       Formula: height ~ weight_c\n        Family: gaussian\n          Link: mu = identity\n  Observations: 352\n        Priors: \n    target = mu\n        Common-level effects\n            Intercept ~ Normal(mu: 154.5971, sigma: 19.3283)\n            weight_c ~ Normal(mu: 0.0, sigma: 2.9978)\n        \n        Auxiliary parameters\n            sigma ~ HalfStudentT(nu: 4.0, sigma: 7.7313)\n\n\nLa descrizione inizia mostrando la formula utilizzata per definire il modello, y ~ x, che indica come la variabile dipendente y √® predetta dalla variabile indipendente x in una relazione lineare. La seconda riga specifica che si sta utilizzando una distribuzione gaussiana (normale) come funzione di verosimiglianza per il modello, il che implica l‚Äôassunzione che i residui del modello (le differenze tra i valori osservati e i valori predetti) seguano una distribuzione normale.\nLa terza riga menziona la funzione di collegamento, in questo caso l‚Äôidentit√†, che non applica alcuna trasformazione al valore atteso della variabile dipendente. Questo √® caratteristico dei modelli lineari, dove il valore atteso di y √® direttamente modellato come una combinazione lineare delle variabili indipendenti (E(Y) = \\alpha + \\beta x). √à importante notare che, nei modelli lineari generalizzati, la funzione di collegamento gioca un ruolo cruciale nel collegare il valore atteso della variabile risposta alla combinazione lineare delle variabili predittive.\nSegue il numero di osservazioni utilizzate per adattare il modello, indicando la dimensione del dataset su cui il modello √® stato allenato.\nLa parte successiva dell‚Äôoutput dettaglia i priors utilizzati per i parametri del modello. In Bambi, i priors sono assunzioni a priori sui valori dei parametri prima di osservare i dati. Questi priors aiutano a guidare l‚Äôinferenza, soprattutto in presenza di dati limitati o per regolarizzare il modello. L‚Äôintercetta (Intercept) ha un prior normale con media (mu) 2.0759 e deviazione standard (sigma) 3.9401, indicando la posizione iniziale attesa della linea di regressione e quanto ci si aspetta che vari. Il coefficiente della variabile x ha anch‚Äôesso un prior normale, centrato in zero con una deviazione standard ampia (6.8159), riflettendo incertezza su quale possa essere il vero effetto di x su y senza presupporre una direzione specifica dell‚Äôeffetto.\nLa sezione finale riguarda i parametri ausiliari del modello, in questo caso il parametro sigma della distribuzione gaussiana, che rappresenta la deviazione standard dei residui del modello. Questo ha un prior HalfStudentT, che √® una distribuzione che ammette solo valori positivi (essendo la deviazione standard sempre positiva), con un grado di libert√† (nu) 4.0 e una scala (sigma) 0.791. Questo prior √® scelto per la sua flessibilit√† e la capacit√† di gestire dati con potenziali outlier.\n\nmodel.build()\nmodel.graph()\n\n\n\n\n\n\n\n\nEseguiamo il campionamento MCMC.\n\nidata = model.fit(\n    nuts_sampler=\"numpyro\",\n    idata_kwargs={\"log_likelihood\": True}\n)\n\nLe distribuzioni a posteriori dei parametri e i trace plot si ottengono con la seguente istruzione.\n\n_ = az.plot_trace(idata)\n\n\n\n\n\n\n\n\nUn sommario numerico delle distribuzioni a posteriori dei parametri si ottiene con az.summary.\n\naz.summary(idata, round_to=2)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nIntercept\n154.61\n0.27\n154.09\n155.09\n0.0\n0.0\n4300.59\n2955.37\n1.0\n\n\nsigma\n5.09\n0.19\n4.76\n5.45\n0.0\n0.0\n3966.32\n2953.56\n1.0\n\n\nweight_c\n0.90\n0.04\n0.82\n0.98\n0.0\n0.0\n4327.37\n3193.28\n1.0\n\n\n\n\n\n\n\n\nI dati replicano quelli ottenuti in precedenza.\nPossiamo anche generare un grafico che descrive l‚Äôincertezza a posteriori delle predizioni del modello.\nLa funzione plot_predictions del pacchetto Bambi serve per facilitare l‚Äôinterpretazione dei modelli di regressione attraverso la visualizzazione grafica. Il metodo appartiene al sottomodulo interpret di Bambi e si concentra sulla rappresentazione delle previsioni generate dal modello.\nQuando si esegue la funzione plot_predictions con i parametri specificati (model, idata, [\"weight_c\"]), essa produce un grafico che sintetizza le previsioni del modello in relazione a una o pi√π variabili indipendenti. In questo caso, il parametro model indica il modello di regressione Bayesiana costruito con Bambi, idata rappresenta i dati inferenziali (ottenuti tramite il fit del modello), e [\"weight_c\"] specifica la variabile indipendente da considerare per il grafico.\n\nbmb.interpret.plot_predictions(model, idata, [\"weight_c\"]);\n\nDefault computed for conditional variable: weight_c\n\n\n\n\n\n\n\n\n\nIl grafico generato da questa funzione illustra due aspetti principali:\n\nMedia Posteriore di y: Il grafico include una linea che rappresenta la media posteriore della variabile dipendente (height) rispetto alla variabile indipendente specificata (weight_c). La media posteriore √® una stima centrale delle previsioni del modello, che riflette la posizione pi√π probabile dei valori di height data l‚Äôevidenza fornita dai dati.\nIntervallo di Densit√† pi√π Alta del 94%: Attorno alla retta della media posteriore, il grafico mostra anche un‚Äôarea evidenziata che rappresenta l‚Äôintervallo di densit√† pi√π alta (HDI) del 94%. Questo intervallo √® un modo per quantificare l‚Äôincertezza delle previsioni del modello. L‚ÄôHDI del 94% significa che, data la distribuzione posteriore delle previsioni di height, c‚Äô√® il 94% di probabilit√† che il valore vero di height cada all‚Äôinterno di questo intervallo per un dato valore di weight_c. Questo fornisce una misura visiva dell‚Äôincertezza associata alle stime del modello.",
    "crumbs": [
      "Modelli lineari",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Zucchero sintattico</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_5/05_04_synt_sugar.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_5/05_04_synt_sugar.html#informazioni-sullambiente-di-sviluppo",
    "title": "21¬† Zucchero sintattico",
    "section": "21.3 Informazioni sull‚ÄôAmbiente di Sviluppo",
    "text": "21.3 Informazioni sull‚ÄôAmbiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m \n\nLast updated: Tue Jul 23 2024\n\nPython implementation: CPython\nPython version       : 3.12.4\nIPython version      : 8.26.0\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.5.0\nMachine     : arm64\nProcessor   : arm\nCPU cores   : 8\nArchitecture: 64bit\n\nnumpy     : 1.26.4\nmatplotlib: 3.9.1\nbambi     : 0.14.0\ncmdstanpy : 1.2.4\npandas    : 2.2.2\narviz     : 0.18.0\nlogging   : 0.5.1.2\n\nWatermark: 2.4.3",
    "crumbs": [
      "Modelli lineari",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Zucchero sintattico</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_5/05_05_two_means.html",
    "href": "chapters/chapter_5/05_05_two_means.html",
    "title": "22¬† Confronto tra le medie di due gruppi",
    "section": "",
    "text": "22.1 Introduzione\nNel ?sec-two-groups-comparison, abbiamo discusso come eseguire l‚Äôinferenza sulla differenza tra le medie di due campioni indipendenti attraverso un approccio bayesiano. In quella analisi, i due gruppi sono stati trattati come entit√† distinte e abbiamo calcolato la differenza tra le loro medie.\nUn‚Äôalternativa consiste nell‚Äôuso di un modello di regressione. In questo caso, non si calcola direttamente la differenza tra le medie, ma si introduce una variabile ‚Äúdummy‚Äù nel modello di regressione. La variabile ‚Äúdummy‚Äù codifica l‚Äôappartenenza ai gruppi con valori binari: 0 per il gruppo di riferimento e 1 per il gruppo di confronto. Il modello di regressione stima poi un coefficiente per la variabile ‚Äúdummy‚Äù, che rappresenta la differenza tra le medie dei due gruppi. Cos√¨ facendo, la variabile ‚Äúdummy‚Äù agisce come un indicatore del gruppo, permettendoci di stimare in modo efficiente la differenza tra le medie.\nEntrambi i metodi sono validi per analizzare la differenza tra le medie di due gruppi indipendenti, tuttavia il modello di regressione offre maggiore flessibilit√† e potenzialit√† di espansione. Grazie a questo modello, √® possibile includere ulteriori variabili esplicative, ampliando la nostra capacit√† di comprendere i fattori che influenzano il risultato d‚Äôinteresse. Questa versatilit√† √® particolarmente vantaggiosa per esaminare come altre variabili possano influire sulla differenza tra le medie o per analizzare pi√π variabili contemporaneamente.",
    "crumbs": [
      "Modelli lineari",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Confronto tra le medie di due gruppi</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_5/05_05_two_means.html#regressione-bayesiana-per-due-gruppi-indipendenti",
    "href": "chapters/chapter_5/05_05_two_means.html#regressione-bayesiana-per-due-gruppi-indipendenti",
    "title": "22¬† Confronto tra le medie di due gruppi",
    "section": "22.2 Regressione bayesiana per due gruppi indipendenti",
    "text": "22.2 Regressione bayesiana per due gruppi indipendenti\nNel contesto bayesiano, il modello di regressione pu√≤ essere formulato nel modo seguente:\n\\[\n\\begin{align*}\ny_i & \\sim \\mathcal{N}(\\mu_i, \\sigma), \\\\\n\\mu_i & = \\alpha + \\beta x_i.\n\\end{align*}\n\\]\nIn questa rappresentazione:\n\n$ $ agisce come intercetta,\n$ $ √® il coefficiente angolare o la pendenza,\n$ $ √® l‚Äôerrore standard associato alle osservazioni.\n\nNel caso specifico, la variabile $ x $ √® una variabile indicatrice che assume i valori 0 o 1. Per il gruppo identificato da $ x = 0 $, il modello si riduce a:\n\\[\n\\begin{align*}\ny_i & \\sim \\mathcal{N}(\\mu_i, \\sigma), \\\\\n\\mu_i & = \\alpha.\n\\end{align*}\n\\]\nQuesto implica che $ $ rappresenta la media del gruppo codificato come $ x = 0 $.\nPer il gruppo contrassegnato da $ x = 1 $, il modello diventa:\n\\[\n\\begin{align*}\ny_i & \\sim \\mathcal{N}(\\mu_i, \\sigma), \\\\\n\\mu_i & = \\alpha + \\beta.\n\\end{align*}\n\\]\nIn termini dei parametri del modello, la media per il gruppo codificato con $ x = 1 $ √® rappresentata da $ + $. In questa configurazione, $ $ indica la differenza tra la media del gruppo con $ x = 1 $ e quella del gruppo con $ x = 0 $. Di conseguenza, l‚Äôanalisi della differenza tra le medie dei due gruppi pu√≤ essere effettuata attraverso l‚Äôinferenza sul parametro $ $. In sintesi, per confrontare le medie dei due gruppi indipendenti, si pu√≤ esaminare la distribuzione a posteriori di $ $.",
    "crumbs": [
      "Modelli lineari",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Confronto tra le medie di due gruppi</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_5/05_05_two_means.html#un-esempio-illustrativo",
    "href": "chapters/chapter_5/05_05_two_means.html#un-esempio-illustrativo",
    "title": "22¬† Confronto tra le medie di due gruppi",
    "section": "22.3 Un esempio illustrativo",
    "text": "22.3 Un esempio illustrativo\nEsaminiamo nuovamente i dati relativi al quoziente di intelligenza dei bambini le cui madri hanno completato oppure no la scuola superiore. Ci poniamo il problema di replicare i risultati ottenuti in precedenza usando l‚Äôanalisi di regressione.\nLeggiamo i dati:\n\nkidiq = pd.read_stata(\"../../data/kidiq.dta\")\nkidiq.head()\n\n\n\n\n\n\n\n\n\nkid_score\nmom_hs\nmom_iq\nmom_work\nmom_age\n\n\n\n\n0\n65\n1.0\n121.117529\n4\n27\n\n\n1\n98\n1.0\n89.361882\n4\n25\n\n\n2\n85\n1.0\n115.443165\n4\n27\n\n\n3\n83\n1.0\n99.449639\n3\n25\n\n\n4\n115\n1.0\n92.745710\n4\n27\n\n\n\n\n\n\n\n\n\nkidiq.groupby([\"mom_hs\"]).size()\n\nmom_hs\n0.0     93\n1.0    341\ndtype: int64\n\n\nCi sono 93 bambini la cui madre non ha completato le superiori e 341 bambini la cui madre ha ottenuto il diploma di scuola superiore.\n\nsummary_stats = [st.mean, st.stdev]\nkidiq.groupby([\"mom_hs\"]).aggregate(summary_stats)\n\n\n\n\n\n\n\n\n\nkid_score\nmom_iq\nmom_work\nmom_age\n\n\n\nmean\nstdev\nmean\nstdev\nmean\nstdev\nmean\nstdev\n\n\nmom_hs\n\n\n\n\n\n\n\n\n\n\n\n\n0.0\n77.548387\n22.573800\n91.889152\n12.630498\n2.322581\n1.226175\n21.677419\n2.727323\n\n\n1.0\n89.319648\n19.049483\n102.212049\n14.848414\n3.052786\n1.120727\n23.087977\n2.617453\n\n\n\n\n\n\n\n\n\naz.plot_violin(\n    {\n        \"mom_hs=0\": kidiq.loc[kidiq.mom_hs == 0, \"kid_score\"],\n        \"mom_hs=1\": kidiq.loc[kidiq.mom_hs == 1, \"kid_score\"],\n    }\n);\n\n\n\n\n\n\n\n\nIniziamo l‚Äôinferenza statistica sulla differenza tra le medie dei due gruppi utilizzando bambi. Questo pacchetto offre una sintassi semplice per formulare il modello bayesiano di interesse. Un altro vantaggio √® che bambi selezioner√† automaticamente le distribuzioni a priori appropriate per i parametri del modello, rendendo il processo pi√π intuitivo.\nIl modello di regressione sopra descritto si scrive nel modo seguente.\n\nmod = bmb.Model(\"kid_score ~ mom_hs\", kidiq)\n\nEffettuiamo il campionamento.\n\nresults = mod.fit(nuts_sampler=\"numpyro\", idata_kwargs={\"log_likelihood\": True})\n\nPossiamo ispezionare le propriet√† del modello nel modo seguente.\n\nmod\n\n       Formula: kid_score ~ mom_hs\n        Family: gaussian\n          Link: mu = identity\n  Observations: 434\n        Priors: \n    target = mu\n        Common-level effects\n            Intercept ~ Normal(mu: 86.7972, sigma: 110.1032)\n            mom_hs ~ Normal(mu: 0.0, sigma: 124.2132)\n        \n        Auxiliary parameters\n            sigma ~ HalfStudentT(nu: 4.0, sigma: 20.3872)\n------\n* To see a plot of the priors call the .plot_priors() method.\n* To see a summary or plot of the posterior pass the object returned by .fit() to az.summary() or az.plot_trace()\n\n\nLe distribuzioni a priori utilizzate di default dal modello possono essere visualizzate nel modo seguente.\n\n_ = mod.plot_priors()\n\nSampling: [Intercept, kid_score_sigma, mom_hs]\n\n\n\n\n\n\n\n\n\nPer ispezionare il nostro posteriore e il processo di campionamento possiamo utilizzare az.plot_trace(). L‚Äôopzione kind='rank_vlines' ci fornisce una variante del grafico di rango che utilizza linee e punti e ci aiuta a ispezionare la stazionariet√† delle catene. Poich√© non c‚Äô√® un modello chiaro o deviazioni serie dalle linee orizzontali, possiamo concludere che le catene sono stazionarie.\n\n_ = az.plot_trace(results)\n\n\n\n\n\n\n\n\n\naz.summary(results, round_to=2)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nIntercept\n77.52\n2.04\n73.72\n81.48\n0.03\n0.02\n4640.52\n3274.01\n1.0\n\n\nkid_score_sigma\n19.91\n0.67\n18.68\n21.19\n0.01\n0.01\n4231.42\n3052.91\n1.0\n\n\nmom_hs\n11.78\n2.30\n7.23\n15.86\n0.03\n0.02\n4642.76\n3108.48\n1.0\n\n\n\n\n\n\n\n\nIl parametro ‚ÄúIntercept‚Äù rappresenta la stima a posteriori del punteggio del QI per il gruppo codificato con ‚Äúmom_hs‚Äù uguale a 0. La media a posteriori di questo gruppo √® di 77.6, che √® praticamente identica al valore campionario corrispondente.\nIl parametro ‚Äúmom_hs‚Äù corrisponde alla stima a posteriori della differenza nei punteggi del QI tra il gruppo codificato con ‚Äúmom_hs‚Äù uguale a 1 e il gruppo codificato con ‚Äúmom_hs‚Äù uguale a 0. Anche in questo caso, la differenza a posteriori di 11.8 tra le medie dei due gruppi √® molto simile alla differenza campionaria tra le medie dei due gruppi. La parte importante della tabella riguarda l‚Äôintervallo di credibilit√† al 94%, che √® [7.5, 16.2], e che non include lo 0. Ci√≤ significa che, con un livello di certezza soggettiva del 94%, possiamo essere sicuri che il QI dei bambini le cui madri hanno il diploma superiore sar√† maggiore (in media) di almeno 7.5 punti, e tale differenza pu√≤ arrivare fino a 16.2 punti, rispetto al QI dei bambini le cui madri non hanno completato la scuola superiore.\nSe confrontiamo questi risultati con quelli ottenuti nel capitolo {ref}two_groups_comparison_notebook, notiamo che sono quasi identici. Le piccole differenze che si osservano possono essere attribuite sia all‚Äôapprossimazione numerica sia al fatto che nel modello precedente abbiamo consentito deviazioni standard diverse per i due gruppi, mentre nel caso attuale abbiamo assumo la stessa variabilit√† per entrambi i gruppi.\nUsiamo ora l‚Äôapproccio di massima verosimiglianza.\n\nlm = pg.linear_regression(kidiq[\"mom_hs\"], kidiq[\"kid_score\"])\nlm.round(2)\n\n\n\n\n\n\n\n\n\nnames\ncoef\nse\nT\npval\nr2\nadj_r2\nCI[2.5%]\nCI[97.5%]\n\n\n\n\n0\nIntercept\n77.55\n2.06\n37.67\n0.0\n0.06\n0.05\n73.50\n81.59\n\n\n1\nmom_hs\n11.77\n2.32\n5.07\n0.0\n0.06\n0.05\n7.21\n16.34\n\n\n\n\n\n\n\n\nI risultati sono quasi identici a quelli trovati con l‚Äôapproccio bayesiano.\nIl test bayesiano di ipotesi pu√≤ essere svolto, per esempio, calcolando la probabilit√† che \\(\\beta_{mean\\_diff} &gt; 0\\). Questa probabilit√† √® 1, per cui concludiamo che la media del gruppo codificato con ‚Äúmom_hs = 1‚Äù √® maggiore della media del gruppo codificato con ‚Äúmom_hs = 0‚Äù.\n\naz.plot_posterior(results, var_names=\"mom_hs\", ref_val=0, figsize=(6, 3));\n\n\n\n\n\n\n\n\nUn valore numerico si ottiene nel modo seguente.\n\nresults.posterior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 104kB\nDimensions:          (chain: 4, draw: 1000)\nCoordinates:\n  * chain            (chain) int64 32B 0 1 2 3\n  * draw             (draw) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    Intercept        (chain, draw) float64 32kB 78.83 79.84 ... 73.83 79.82\n    kid_score_sigma  (chain, draw) float64 32kB 19.21 20.79 ... 20.87 19.45\n    mom_hs           (chain, draw) float64 32kB 11.97 8.374 ... 15.15 10.02\nAttributes:\n    created_at:                  2024-06-13T05:14:43.527326+00:00\n    arviz_version:               0.18.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (3)Intercept(chain, draw)float6478.83 79.84 73.54 ... 73.83 79.82array([[78.82644605, 79.84168005, 73.54424871, ..., 78.2407767 ,\n        79.65683325, 79.50875636],\n       [80.01627206, 78.71662919, 79.20643166, ..., 77.02806443,\n        79.29123634, 77.23338073],\n       [77.29254392, 77.44678841, 77.42416296, ..., 74.46900909,\n        80.21508608, 78.984981  ],\n       [75.11589965, 75.46194695, 76.33373715, ..., 75.60413192,\n        73.83124842, 79.81667588]])kid_score_sigma(chain, draw)float6419.21 20.79 20.36 ... 20.87 19.45array([[19.20601585, 20.79313289, 20.36108383, ..., 20.97965143,\n        20.94150175, 20.43169363],\n       [20.03731499, 19.08286456, 19.52683884, ..., 19.27515939,\n        19.94139224, 19.46453797],\n       [18.38282855, 19.29950429, 20.1234451 , ..., 19.13765117,\n        19.53346331, 18.6415884 ],\n       [20.69701767, 18.03001439, 19.71130165, ..., 21.17944029,\n        20.86573599, 19.45230071]])mom_hs(chain, draw)float6411.97 8.374 14.71 ... 15.15 10.02array([[11.96777777,  8.37382352, 14.70572866, ..., 12.14492642,\n        12.5707737 , 12.86370927],\n       [ 8.24668315, 12.34781247,  9.87043598, ..., 13.43240059,\n         9.11062374, 11.95853216],\n       [13.72195497, 12.79190398, 11.57487575, ..., 14.54096709,\n         9.29889013, 11.32656956],\n       [13.01259531, 12.49687999, 13.49840034, ..., 13.1175932 ,\n        15.14621113, 10.01597991]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (4)created_at :2024-06-13T05:14:43.527326+00:00arviz_version :0.18.0modeling_interface :bambimodeling_interface_version :0.13.0\n\n\n\n# Probabiliy that posterior is &gt; 0\n(results.posterior[\"mom_hs\"] &gt; 0).mean().item()\n\n1.0",
    "crumbs": [
      "Modelli lineari",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Confronto tra le medie di due gruppi</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_5/05_05_two_means.html#parametrizzazione-alternativa",
    "href": "chapters/chapter_5/05_05_two_means.html#parametrizzazione-alternativa",
    "title": "22¬† Confronto tra le medie di due gruppi",
    "section": "22.4 Parametrizzazione alternativa",
    "text": "22.4 Parametrizzazione alternativa\nConsideriamo adesso il caso in cui, per distinguere i gruppi, anzich√© una variabile dicotomica, con valori 0 e 1, usiamo una variabile qualitativa con i nomi dei due gruppi. Introduciamo questa nuova variabile nel data frame.\n\n# Add a new column 'hs' with the categories based on 'mom_hs'\nkidiq[\"hs\"] = kidiq[\"mom_hs\"].map({0: \"not_completed\", 1: \"completed\"})\nkidiq.tail()\n\n\n\n\n\n\n\n\n\nkid_score\nmom_hs\nmom_iq\nmom_work\nmom_age\nhs\n\n\n\n\n429\n94\n0.0\n84.877412\n4\n21\nnot_completed\n\n\n430\n76\n1.0\n92.990392\n4\n23\ncompleted\n\n\n431\n50\n0.0\n94.859708\n2\n24\nnot_completed\n\n\n432\n88\n1.0\n96.856624\n2\n21\ncompleted\n\n\n433\n70\n1.0\n91.253336\n2\n25\ncompleted\n\n\n\n\n\n\n\n\nAdattiamo il modello ai dati, usando questa nuova variabile e forziamo a zero l‚Äôintercetta che Bambi aggiunge di default al modello.\n\nmod_2 = bmb.Model(\"kid_score ~ 0 + hs\", kidiq)\nresults_2 = mod_2.fit(nuts_sampler=\"numpyro\", idata_kwargs={\"log_likelihood\": True})\n\nIspezionare il modello e le distribuzioni a priori.\n\nmod_2\n\n       Formula: kid_score ~ 0 + hs\n        Family: gaussian\n          Link: mu = identity\n  Observations: 434\n        Priors: \n    target = mu\n        Common-level effects\n            hs ~ Normal(mu: [0. 0.], sigma: [124.2132 124.2132])\n        \n        Auxiliary parameters\n            sigma ~ HalfStudentT(nu: 4.0, sigma: 20.3872)\n------\n* To see a plot of the priors call the .plot_priors() method.\n* To see a summary or plot of the posterior pass the object returned by .fit() to az.summary() or az.plot_trace()\n\n\n\n_ = mod_2.plot_priors()\n\nSampling: [hs, kid_score_sigma]\n\n\n\n\n\n\n\n\n\nEsaminiamo le distribuzioni a posteriori dei parametri del modello.\n\n_ = az.plot_trace(results_2)\n\n\n\n\n\n\n\n\n\naz.summary(results_2)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nhs[completed]\n89.301\n1.059\n87.267\n91.258\n0.016\n0.012\n4147.0\n3064.0\n1.0\n\n\nhs[not_completed]\n77.473\n2.048\n73.722\n81.256\n0.031\n0.022\n4308.0\n3016.0\n1.0\n\n\nkid_score_sigma\n19.881\n0.680\n18.621\n21.181\n0.011\n0.008\n3982.0\n2995.0\n1.0\n\n\n\n\n\n\n\n\nIn questo caso, notiamo che abbiamo ottenuto le distribuzioni a posteriori per i parametri hs[completed] e hs[not_completed] che corrispondono alle medie dei due gruppi. Tali distribuzioni a posteriori illustrano direttamente l‚Äôincertezza sulla media dei due gruppi, alla luce della variabilit√† campionaria e delle nostre credenze a priori.\nPossiamo svolgere il test bayesiano di ipotesi sulla differenza tra le due medie a posteriori nel modo seguente.\n\npost_group = results_2.posterior[\"hs\"]\ndiff = post_group.sel(hs_dim=\"completed\") - post_group.sel(hs_dim=\"not_completed\")\naz.plot_posterior(diff, ref_val=0, figsize=(6, 3));\n\n\n\n\n\n\n\n\n\n# Probabiliy that posterior is &gt; 0\n(post_group &gt; 0).mean().item()\n\n1.0",
    "crumbs": [
      "Modelli lineari",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Confronto tra le medie di due gruppi</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_5/05_05_two_means.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_5/05_05_two_means.html#informazioni-sullambiente-di-sviluppo",
    "title": "22¬† Confronto tra le medie di due gruppi",
    "section": "22.5 Informazioni sull‚ÄôAmbiente di Sviluppo",
    "text": "22.5 Informazioni sull‚ÄôAmbiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m \n\nLast updated: Tue Jul 23 2024\n\nPython implementation: CPython\nPython version       : 3.12.4\nIPython version      : 8.26.0\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.5.0\nMachine     : arm64\nProcessor   : arm\nCPU cores   : 8\nArchitecture: 64bit\n\narviz     : 0.18.0\nnumpy     : 1.26.4\nbambi     : 0.14.0\npandas    : 2.2.2\npingouin  : 0.5.4\nmatplotlib: 3.9.1\n\nWatermark: 2.4.3",
    "crumbs": [
      "Modelli lineari",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Confronto tra le medie di due gruppi</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a00_installation.html",
    "href": "chapters/chapter_7/a00_installation.html",
    "title": "Online Appendix A ‚Äî Ambiente di lavoro",
    "section": "",
    "text": "A.1 Guida all‚ÄôInstallazione Locale dei Jupyter Notebook\nPer facilitare l‚Äôapprendimento e l‚Äôapplicazione delle tecniche di analisi dei dati discusse in questo corso, utilizzeremo i Jupyter Notebook come strumento principale. I Jupyter Notebook sono documenti interattivi che consentono di combinare codice, testo narrativo, visualizzazioni grafiche e altri elementi multimediali, rendendoli ideali per documentare e condividere analisi di dati in modo trasparente e riproducibile.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Ambiente di lavoro</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a00_installation.html#guida-allinstallazione-locale-dei-jupyter-notebook",
    "href": "chapters/chapter_7/a00_installation.html#guida-allinstallazione-locale-dei-jupyter-notebook",
    "title": "Online Appendix A ‚Äî Ambiente di lavoro",
    "section": "",
    "text": "A.1.1 Prerequisiti per l‚ÄôUso dei Jupyter Notebook\nPer utilizzare i Jupyter Notebook, √® necessario soddisfare alcuni prerequisiti:\n\nInstallare Python: √à il linguaggio di programmazione fondamentale per il nostro corso e deve essere installato sul vostro computer.\nGestione degli Ambienti Virtuali con conda: Utilizzeremo conda per creare e gestire ambienti virtuali, che permettono di isolare e gestire le dipendenze del progetto.\nInstallazione dei Pacchetti Python Necessari: Dovrete installare specifici pacchetti Python, inclusi PyMC per l‚Äôanalisi bayesiana, e altri pacchetti utili, all‚Äôinterno dell‚Äôambiente virtuale creato per questo corso.\nInterfaccia per l‚ÄôUso dei Jupyter Notebook: Avrete bisogno di un IDE (Integrated Development Environment) che supporti i Jupyter Notebook, come Visual Studio Code, per scrivere e eseguire i vostri notebook.\n\n\n\nA.1.2 Installazione di Anaconda\nLa maggior parte dei requisiti elencati pu√≤ essere agevolmente soddisfatta tramite l‚Äôinstallazione di Anaconda, una distribuzione di Python che include conda e facilita la gestione degli ambienti virtuali e l‚Äôinstallazione dei pacchetti.\n\n\n\n\n\n\nSe Anaconda √® gi√† stata installata, potrebbero sorgere problemi dopo l‚Äôaggiornamento del sistema operativo. In tal caso, sar√† indispensabile procedere con una nuova installazione di Anaconda.\n\n\n\n\nA.1.2.1 Per Utenti macOS\nSe lavorate su macOS, potreste trovare pi√π pratico utilizzare conda direttamente dal Terminale o da un‚Äôapplicazione terminale moderna come Warp, piuttosto che attraverso Anaconda Navigator. In questo caso, potete optare per installare una versione di Visual Studio Code indipendente da quella fornita con Anaconda, per un maggiore controllo e flessibilit√†.\n\n\nA.1.2.2 Per Utenti Windows\nPer coloro che utilizzano Windows, l‚Äôuso di Jupyter Notebook tramite Anaconda Navigator potrebbe risultare la scelta pi√π semplice e diretta, grazie all‚Äôintegrazione e alla facilit√† d‚Äôuso fornite da Anaconda in ambienti Windows.\n\n\n\nA.1.3 Creazione e Configurazione dell‚ÄôAmbiente Virtuale\nIndipendentemente dal sistema operativo, √® fondamentale installare e configurare conda, che vi permetter√† di creare {ref}appendix-virtual-env dedicati. All‚Äôinterno di questi ambienti, installerete cmdstanpy (o PyMC) e gli altri pacchetti richiesti per il corso. La strada pi√π semplice per soddisfare questi requisiti √® attraverso l‚Äôinstallazione di Anaconda, che semplifica notevolmente il processo di configurazione iniziale e gestione degli ambienti virtuali.\nQuesta guida all‚Äôinstallazione locale mira a fornirvi tutti gli strumenti necessari per iniziare a utilizzare i Jupyter Notebook nel contesto del nostro corso, facilitando un apprendimento efficiente e la condivisione dei risultati delle vostre analisi.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Ambiente di lavoro</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a00_installation.html#guida-allinstallazione-di-anaconda",
    "href": "chapters/chapter_7/a00_installation.html#guida-allinstallazione-di-anaconda",
    "title": "Online Appendix A ‚Äî Ambiente di lavoro",
    "section": "A.2 Guida all‚ÄôInstallazione di Anaconda",
    "text": "A.2 Guida all‚ÄôInstallazione di Anaconda\nAnaconda √® una distribuzione popolare per la programmazione in Python. Ecco una guida passo-passo per l‚Äôinstallazione:\n\nScaricare Anaconda:\n\nVisitate il sito ufficiale di Anaconda: https://www.anaconda.com/.\nScegliete la versione adatta al vostro sistema operativo (Windows, macOS o Linux).\nOptate per il download dell‚Äôultima versione disponibile, che include l‚Äôultima versione di Python.\n\nInstallare Anaconda:\n\nEseguite il file di installazione scaricato.\nSeguite le istruzioni visualizzate, mantenendo le impostazioni predefinite, a meno che non abbiate esigenze specifiche.\n\nAggiungere Anaconda al ‚ÄòPATH‚Äô del Sistema:\n\nDurante l‚Äôinstallazione, vi sar√† chiesto se desiderate aggiungere Anaconda al ‚ÄòPATH‚Äô del sistema. Questo passaggio √® cruciale poich√© consente di utilizzare Python da qualunque parte del computer.\nVi consiglio di selezionare questa opzione (altri metodi sono possibili, ma questo ha dimostrato di funzionare senza problemi).\n\nConfermare l‚ÄôInstallazione:\n\nAl termine dell‚Äôinstallazione, aprite PowerShell all‚Äôinterno di Anaconda Navigagor (Windows) o il terminale (macOS/Linux) e digitate python --version per verificare se l‚Äôinstallazione √® riuscita. Se compare la versione di Python, tutto √® andato a buon fine.\n\n\nAnaconda include il Navigator, un‚Äôinterfaccia utente grafica per gestire ambienti di sviluppo, installare librerie aggiuntive e lanciare strumenti come Jupyter Notebook, che consente (in alternativa a VS Code) di scrivere ed eseguire codice Python.\n\n\n\n\n\n\nIstruzioni Specifiche per Utenti Windows:\n\nScaricare Anaconda:\n\nScaricate la versione ‚Äú64-Bit Graphical Installer‚Äù dal sito di Anaconda.\n\nInstallare Anaconda:\n\nAvviate l‚Äôinstaller scaricato e seguite le istruzioni visualizzate.\nDurante l‚Äôinstallazione, selezionate ‚ÄúJust Me‚Äù (solo per l‚Äôutente corrente).\nMantenete il percorso di installazione predefinito.\n\nIncludere Anaconda nel ‚ÄòPATH‚Äô:\n\nIMPORTANTE: Selezionate l‚Äôopzione per aggiungere Anaconda al PATH e impostarlo come installazione di Python di default. Di default, questa opzione √® deselezionata.\n\nVerifica dell‚ÄôInstallazione:\n\nCercate ‚ÄúAnaconda Navigator‚Äù nel menu Start. Se si apre correttamente, l‚Äôinstallazione √® riuscita.\nAprite ‚ÄúAnaconda Prompt‚Äù (o ‚ÄúPowerShell‚Äù) dal menu Start di Anaconda Navigator e digitate conda --version per confermare l‚Äôinstallazione di conda.\n\n\nSeguite attentamente queste istruzioni per garantire un‚Äôinstallazione senza problemi.\nPer maggiori dettagli, consultate il tutorial su come installare Anaconda su Windows: Tutorial Installazione di Anaconda su Windows. Questo tutorial offre spiegazioni dettagliate e una guida passo-passo.\n\n\n\nUna volta installato Anaconda, potrete utilizzare Anaconda Navigator per gestire progetti Python, installare librerie necessarie e avviare strumenti come Jupyter Notebook.\n\n\n\n\n\n\n√à necessario comprendere la differenza tra applicazione (App) e installer.\nCos‚Äô√® un‚ÄôApplicazione (App)\nUn‚Äôapplicazione, comunemente chiamata ‚Äúapp‚Äù, √® un software che funziona sul vostro computer o dispositivo mobile per uno scopo specifico, come navigare in internet, inviare messaggi, elaborare testi o fare calcoli. Esempi includono browser web come Google Chrome, programmi di elaborazione testi come Microsoft Word, o sistemi come Anaconda Navigator.\nCos‚Äô√® un Installer\nUn installer √® un software che installa un‚Äôapplicazione sul vostro computer. Tipicamente, quando scaricate un‚Äôapplicazione da internet, scaricate in realt√† l‚Äôinstaller. L‚Äôinstaller ha il compito di: - Copiare i file dell‚Äôapp nella corretta cartella del computer. - Creare scorciatoie per l‚Äôapp, come icone sul desktop o voci nel menu Start. - Configurare impostazioni iniziali per il corretto funzionamento dell‚Äôapp.\nDopo l‚ÄôInstallazione\nDopo che l‚Äôinstaller ha completato il suo lavoro, l‚Äôapplicazione sar√† pronta all‚Äôuso e l‚Äôinstaller pu√≤ essere eliminato.\nIn sintesi, l‚Äôapplicazione √® il software che userete per svolgere compiti specifici, mentre l‚Äôinstaller √® lo strumento temporaneo per installare l‚Äôapplicazione sul vostro computer. Capire questa distinzione √® fondamentale nel mondo dell‚Äôinformatica.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Ambiente di lavoro</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a00_installation.html#sec-virtual-environment",
    "href": "chapters/chapter_7/a00_installation.html#sec-virtual-environment",
    "title": "Online Appendix A ‚Äî Ambiente di lavoro",
    "section": "A.3 L‚ÄôAmbiente Virtuale in Python",
    "text": "A.3 L‚ÄôAmbiente Virtuale in Python\nDopo aver installato Python tramite Anaconda, un aspetto fondamentale da considerare √® la creazione di un ambiente virtuale. Un ambiente virtuale rappresenta uno spazio dedicato sul vostro computer, dove √® possibile installare e gestire le librerie Python necessarie per il corso, inclusi quelle per l‚Äôanalisi statistica. La creazione di un ambiente virtuale √® estremamente vantaggiosa poich√© contribuisce all‚Äôorganizzazione del lavoro e previene possibili conflitti tra diverse librerie. Le istruzioni dettagliate per la configurazione di un ambiente virtuale sono disponibili nel Appendix E`.\nL‚Äôesecuzione delle fasi precedentemente delineate, ossia l‚Äôinstallazione di Anaconda, la configurazione di Visual Studio Code e la creazione di un ambiente virtuale, assicurer√† la completa preparazione di un ambiente di sviluppo locale ottimizzato per l‚Äôutilizzo dei Jupyter Notebook nelle vostre attivit√† legate alla data science all‚Äôinterno di questo corso.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Ambiente di lavoro</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a00_installation.html#la-shell",
    "href": "chapters/chapter_7/a00_installation.html#la-shell",
    "title": "Online Appendix A ‚Äî Ambiente di lavoro",
    "section": "A.4 La Shell",
    "text": "A.4 La Shell\nPer la creazione e la gestione dell‚Äôambiente di calcolo, l‚Äôuso di una shell √® indispensabile. Questa pu√≤ essere approfondita nella sezione {ref}appendix-shell. La shell permette di interagire con il sistema operativo attraverso l‚Äôuso di comandi in un terminale. Diverse soluzioni software sono disponibili per facilitare questa interazione.\n\nA.4.1 Unix (MacOS, Linux)\nIn ambienti Unix come MacOS e Linux, ci sono diverse shell tra cui scegliere. Una scelta popolare √® Bash, che √® comunemente preinstallata su molti sistemi Unix. Un‚Äôaltra opzione moderna √® Zsh, nota per la sua facilit√† di personalizzazione e funzionalit√† avanzate. Per un‚Äôesperienza di terminale migliorata, warp √® un‚Äôopzione innovativa che offre un‚Äôinterfaccia utente ricca di funzionalit√† e supporto per i comandi intelligenti.\n\n\nA.4.2 Windows\nSu Windows, la shell predefinita √® il Prompt dei Comandi, ma non √® cos√¨ potente o flessibile come le shell disponibili su Unix. PowerShell √® un‚Äôopzione pi√π avanzata disponibile su Windows, che combina la gestione della configurazione e l‚Äôautomazione delle attivit√† con un linguaggio di scripting.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Ambiente di lavoro</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a00_installation.html#lavorare-con-visual-studio-code",
    "href": "chapters/chapter_7/a00_installation.html#lavorare-con-visual-studio-code",
    "title": "Online Appendix A ‚Äî Ambiente di lavoro",
    "section": "A.5 Lavorare con Visual Studio Code",
    "text": "A.5 Lavorare con Visual Studio Code\nPer utilizzare Visual Studio Code con Python e Jupyter Notebook, √® essenziale installare le relative estensioni. Per fare ci√≤, √® sufficiente seguire alcuni semplici passaggi:\n\nAvvia Visual Studio Code sul tuo computer.\nNella barra laterale sinistra, trova e clicca sull‚Äôicona con quattro quadrati, di cui uno disallineato. Questo √® il menu delle estensioni.\nNella barra di ricerca all‚Äôinterno del menu delle estensioni, digita ‚ÄúPython‚Äù e premi Invio. Troverai diverse estensioni relative a Python.\nTrova l‚Äôestensione ufficiale di Python sviluppata da Microsoft e installala cliccando sul pulsante ‚ÄúInstalla‚Äù.\nSuccessivamente, cerca ‚ÄúJupyter‚Äù nella barra di ricerca delle estensioni e premi Invio.\nTrova l‚Äôestensione ‚ÄúJupyter‚Äù nell‚Äôelenco dei risultati e installala cliccando sul pulsante ‚ÄúInstalla‚Äù.\n\nUna volta completati questi passaggi, avrai installato con successo le componenti aggiuntive necessarie per lavorare con Python e Jupyter Notebook all‚Äôinterno di Visual Studio Code. Potrai quindi iniziare a scrivere, eseguire e testare il tuo codice Python e i tuoi notebook Jupyter direttamente nell‚Äôambiente di sviluppo di Visual Studio Code.\nQuando apri un file con estensione .ipynb in Visual Studio Code ricorda di selezionare l‚Äôambiente virtuale che desiderate utilizzare. Puoi farlo tramite la ‚ÄúCommand Palette‚Äù (‚áß‚åòP), utilizzando l‚Äôistruzione Python: Select Interpreter. In alternativa, puoi fare clic sull‚Äôicona Select kernel di Visual Studio Code, che si trova nell‚Äôangolo in alto a destra, sotto l‚Äôicona degli ingranaggi (‚öôÔ∏è).\nsqdcrmj ../images/select_kernel.png :height: 300px :align: center",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Ambiente di lavoro</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a00_installation.html#google-colab",
    "href": "chapters/chapter_7/a00_installation.html#google-colab",
    "title": "Online Appendix A ‚Äî Ambiente di lavoro",
    "section": "A.6 Google Colab",
    "text": "A.6 Google Colab\nUtilizzando il link √® possibile accedere a Google Colab e iniziare a scrivere codice Python direttamente dal proprio browser, senza dover effettuare alcuna installazione. Basta selezionare l‚Äôopzione ‚ÄúNuovo notebook‚Äù per creare un nuovo ambiente di lavoro. Per avere un‚Äôintroduzione completa sulle funzionalit√† di Colab, si pu√≤ consultare la guida disponibile al seguente link. √à possibile salvare ogni notebook nella propria cartella di Google Drive per una facile gestione e condivisione dei file.\n\nA.6.1 Uso dei Comandi Speciali in Colab\nNell‚Äôambiente Google Colab, √® possibile utilizzare il comando\n!pip list -v\nper visualizzare un elenco dettagliato di tutte le librerie preinstallate. Questo comando fornisce informazioni utili per comprendere quali strumenti sono immediatamente disponibili per l‚Äôuso, comprese le versioni delle librerie e i percorsi di installazione.\nIl prefisso ! indica un comando speciale, noto anche come comando ‚Äúshell‚Äù, che consente di interagire con il sistema sottostante di Colab direttamente dalla cella del notebook, eseguendo operazioni al di fuori dell‚Äôambiente Python standard.\n\n\nA.6.2 Installazione di Librerie Supplementari\nSe necessario aggiungere ulteriori librerie all‚Äôambiente Colab, come pymc, bambi, e arviz, √® possibile farlo facilmente mediante l‚Äôuso dei comandi pip. Ad esempio, per installare queste tre librerie, si possono eseguire i seguenti comandi uno dopo l‚Äôaltro:\n!pip install bambi\n!pip install pymc\n!pip install arviz\nQuesti comandi non solo installeranno le librerie specificate ma gestiranno anche automaticamente l‚Äôinstallazione delle dipendenze necessarie, tra cui numpy, pandas, matplotlib, seaborn, scipy, e statsmodels, assicurando cos√¨ che tutto l‚Äôambiente di lavoro sia pronto per l‚Äôuso.\n\n\nA.6.3 Google Drive\n\nA.6.3.1 Collegare Google Drive a Colab\nPer accedere alla propria cartella di Google Drive durante l‚Äôutilizzo di Colab, √® possibile seguire i seguenti passaggi:\n\nDalla pagina iniziale, fare clic sull‚Äôicona a forma di cartella (Files) situata nel menu in alto a sinistra.\n\nsqdcrmj ../images/colab_1.png :height: 300px :align: center\n\n\nSi aprir√† un menu con diverse opzioni.\n\nsqdcrmj ../images/colab_2.png :height: 300px :align: center\n\n\nSelezionare la terza icona tra le quattro disposte orizzontalmente. Apparir√† l‚Äôistruzione ‚ÄúRun this cell to mount your Google Drive‚Äù. Fare clic sull‚Äôicona del triangolo contenuta in un cerchio grigio.\nA questo punto, fare clic sull‚Äôicona ‚Äúdrive‚Äù e successivamente su ‚ÄúMyDrive‚Äù per accedere alle cartelle e ai file salvati sul proprio Google Drive.\n\n√à importante tenere presente che la versione gratuita del runtime di Google Colaboratory non salva le informazioni in modo permanente, il che significa che tutto il lavoro svolto verr√† eliminato una volta terminata la sessione. Pertanto, √® necessario reinstallare le librerie utilizzate in precedenza ogni volta che ci si connette a Colab. Al contrario, i Jupyter Notebook possono essere salvati nella propria cartella di Google Drive.\nPer salvare un Jupyter Notebook su Google Drive utilizzando Colab, √® possibile seguire i seguenti passaggi:\n\nFare clic su File nella barra del menu di Colab.\nSelezionare Save a copy in Drive. Di default, Colab salver√† il Notebook nella cartella Colab Notebooks/.ipynb_checkpoints con un nome simile a Untitled7.ipynb.\nDopo aver salvato il Notebook, √® consigliabile rinominarlo facendo clic con il pulsante destro del mouse sul file nella cartella di Google Drive e selezionando Rename. In questo modo sar√† possibile assegnare un nome pi√π significativo al Notebook.\nPer organizzare i file, √® possibile trascinare il Notebook nella cartella desiderata all‚Äôinterno di Google Drive.\n\nSeguendo questi passaggi, sar√† possibile salvare e organizzare i Jupyter Notebook nella propria cartella di Google Drive, consentendo di accedervi facilmente e mantenerli in modo permanente anche dopo la sessione di Colab.\n√à possibile accedere a un breve tutorial video su come utilizzare Colab e come leggere i dati da un file esterno in un Notebook di Jupyter in Colab. Il video tutorial pu√≤ essere trovato seguendo il [link](https://drive.google.com/file/d/1s3whYnq_rLXeLT4yKiOyEQjzk6d55Ct4/view?usp=share_link) fornito.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Ambiente di lavoro</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a01_markdown.html",
    "href": "chapters/chapter_7/a01_markdown.html",
    "title": "Online Appendix B ‚Äî Jupyter Notebook",
    "section": "",
    "text": "B.1 Configurazione Locale per Jupyter Notebook\nPer iniziare a lavorare con i Jupyter Notebook nel proprio ambiente di sviluppo locale, √® necessario completare alcuni passaggi preliminari che assicurano una configurazione ottimale:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a01_markdown.html#configurazione-locale-per-jupyter-notebook",
    "href": "chapters/chapter_7/a01_markdown.html#configurazione-locale-per-jupyter-notebook",
    "title": "Online Appendix B ‚Äî Jupyter Notebook",
    "section": "",
    "text": "Installazione di Python tramite Anaconda: Anaconda √® una distribuzione di Python che include gi√† Jupyter e altre librerie utili per la data science e l‚Äôanalisi di dati. Seguendo le istruzioni dettagliate disponibili sul sito di Anaconda (e fornite in precedenza in questa dispensa), si pu√≤ facilmente installare Python e Jupyter sul proprio sistema.\nSelezione di un Ambiente di Sviluppo Integrato (IDE): Visual Studio Code (VS Code) rappresenta una scelta eccellente per chi cerca un IDE versatile e gratuito. Disponibile al download dal sito ufficiale, VS Code supporta Python tramite l‚Äôinstallazione di una specifica estensione. Dopo aver installato VS Code, √® possibile aggiungere il supporto per Python e per i Jupyter Notebook installando l‚Äôestensione ‚ÄúPython‚Äù disponibile nella sezione ‚ÄúExtensions‚Äù, identificabile dall‚Äôicona dei quattro quadrati. Per una completa integrazione dei notebook Jupyter, potrebbe essere necessario installare anche la libreria ipykernel.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a01_markdown.html#celle",
    "href": "chapters/chapter_7/a01_markdown.html#celle",
    "title": "Online Appendix B ‚Äî Jupyter Notebook",
    "section": "B.2 Celle",
    "text": "B.2 Celle\nI Jupyter Notebook sono organizzati in celle, elementi discreti che possono contenere codice o testo (markdown). La possibilit√† di cambiare il tipo di una cella tramite il menu ‚ÄúCell‚Äù o la barra degli strumenti, selezionando ‚ÄúCode‚Äù per codice Python o ‚ÄúMarkdown‚Äù per annotazioni testuali, rende i notebook estremamente versatili.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a01_markdown.html#formattazione-del-testo-con-markdown",
    "href": "chapters/chapter_7/a01_markdown.html#formattazione-del-testo-con-markdown",
    "title": "Online Appendix B ‚Äî Jupyter Notebook",
    "section": "B.3 Formattazione del Testo con Markdown",
    "text": "B.3 Formattazione del Testo con Markdown\nMarkdown permette di arricchire le celle di testo con formattazioni varie, creando un documento strutturato e leggibile. Ecco alcuni esempi:\n\nTitoli: # Titolo per un titolo di primo livello, ## Sottotitolo per un secondo livello, e cos√¨ via.\nElenchi: - Elemento per elenchi puntati, 1. Elemento per elenchi numerati.\nCollegamenti: [Testo del link](URL) per inserire un link.\nEnfasi: **grassetto** per il testo in grassetto, *corsivo* per il corsivo.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a01_markdown.html#comandi-magici-in-jupyter-notebook",
    "href": "chapters/chapter_7/a01_markdown.html#comandi-magici-in-jupyter-notebook",
    "title": "Online Appendix B ‚Äî Jupyter Notebook",
    "section": "B.4 Comandi Magici in Jupyter Notebook",
    "text": "B.4 Comandi Magici in Jupyter Notebook\nI Jupyter Notebook supportano i ‚Äúcomandi magici‚Äù, comandi speciali che iniziano con % (per comandi su una singola riga) o %% (per comandi che occupano un‚Äôintera cella). Questi comandi offrono funzionalit√† avanzate come:\n\n%run: esegue un file Python esterno.\n%timeit: valuta il tempo di esecuzione di una singola riga di codice.\n%matplotlib inline: integra grafici Matplotlib direttamente nel notebook.\n%load: carica il codice da un file esterno in una cella.\n%reset: cancella tutte le variabili definite nel notebook.\n%pwd e %cd: gestiscono il percorso della directory di lavoro.\n\nDigitando %lsmagic in una cella, si pu√≤ accedere all‚Äôelenco completo dei comandi magici disponibili, esplorando cos√¨ ulteriori strumenti e funzionalit√† offerte da Jupyter Notebook.\nIn conclusione, i Jupyter Notebook rappresentano uno strumento indispensabile per chi lavora nel campo della programmazione e dell‚Äôanalisi dati, grazie alla loro capacit√† di unire codice, visualizzazione dei dati, e annotazioni testuali in un unico documento interattivo e facilmente condivisibile.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a02_shell.html",
    "href": "chapters/chapter_7/a02_shell.html",
    "title": "Online Appendix C ‚Äî La Shell",
    "section": "",
    "text": "C.1 Che cos‚Äô√® una Shell?\nUna shell √® un programma che riceve comandi dall‚Äôutente tramite tastiera (o da file) e li passa al sistema operativo per l‚Äôesecuzione. Pu√≤ essere accessibile tramite un terminale (o un emulatore di terminale).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>La Shell</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a02_shell.html#che-cos√®-una-shell",
    "href": "chapters/chapter_7/a02_shell.html#che-cos√®-una-shell",
    "title": "Online Appendix C ‚Äî La Shell",
    "section": "",
    "text": "C.1.1 Breve Storia della Shell\n\n1971: Ken Thompson di Bell Labs sviluppa la shell per UNIX.\n1977: Stephen Bourne introduce la Bourne shell (sh).\nDopo il 1977: Viene sviluppata la C shell (csh) e tcsh.\nBash: Sviluppata da Brian Fox come sostituto migliorato della Bourne shell.\n1990: Paul Falsted sviluppa Zsh, che diventa la shell predefinita per macOS dal 2019.\n\n\n\nC.1.2 Windows vs macOS/Linux\n\nWindows 10: √à possibile utilizzare Bash attivando il Windows Subsystem for Linux. Tuttavia, l‚Äôambiente preferito √® solitamente PowerShell.\nmacOS/Linux: Zsh √® la shell predefinita in entrambi i sistemi. √à consigliabile sfruttare l‚Äôapplicazione warp per un‚Äôesperienza utente moderna e ottimizzata.\n\n\n\nC.1.3 Comandi di Base Unix\n\npwd: Mostra il percorso della directory corrente.\nls: Elenca file e cartelle nella directory corrente.\ncd: Cambia directory. Senza argomenti, ti porta alla directory home.\nmkdir: Crea una nuova directory.\nrmdir: Rimuove una directory vuota.\nImportante: Evitare spazi nei nomi di file e cartelle.\n\n\nC.1.3.1 Gestione File\n\nmv: Rinomina o sposta file (usa \\ o '' per i nomi di file con spazi).\ncp: Copia file o cartelle (usa -r per le cartelle).\nrm: Rimuove file o cartelle (usa -i per confermare prima di eliminare).\n\n\n\nC.1.3.2 Visualizzazione e Manipolazione Contenuti dei File\n\nless/more: Visualizza il contenuto dei file con possibilit√† di navigazione.\ncat: Mostra l‚Äôintero contenuto di un file.\nhead: Mostra le prime righe di un file.\ntail: Mostra le ultime righe di un file.\n\n\n\n\nC.1.4 Comandi di Base PowerShell\nPer adattare i comandi Unix per l‚Äôutilizzo in PowerShell di Windows, molti dei comandi rimangono simili grazie alla natura cross-platform di PowerShell e alla sua flessibilit√† nel gestire sia gli stili di comando Unix che quelli tradizionali di Windows. Ecco come si traducono i comandi:\n\nGet-Location o semplicemente pwd: Mostra il percorso della directory corrente, simile a pwd in Unix.\nGet-ChildItem o semplicemente ls: Elenca file e cartelle nella directory corrente, equivalente a ls in Unix.\nSet-Location o semplicemente cd: Cambia directory. cd senza argomenti ti porta alla directory home in PowerShell con cd ~.\nNew-Item -ItemType Directory -Name 'nomeDirectory': Crea una nuova directory, simile a mkdir in Unix.\nRemove-Item -Path 'nomeDirectory' -Force: Rimuove una directory, anche se non vuota. Equivalente a rmdir in Unix, ma pi√π potente perch√© pu√≤ rimuovere anche directory con contenuti utilizzando il parametro -Force.\n\n\nC.1.4.1 Gestione File\n\nMove-Item -Path 'origine' -Destination 'destinazione': Rinomina o sposta file, equivalente a mv in Unix.\nCopy-Item -Path 'origine' -Destination 'destinazione': Copia file o cartelle, simile a cp in Unix. Usa -Recurse per copiare cartelle.\nRemove-Item -Path 'file' -Force: Rimuove file o cartelle, simile a rm in Unix. Usa -Force per rimuovere senza conferme e -Recurse per rimuovere cartelle con contenuti.\n\n\n\nC.1.4.2 Visualizzazione e Manipolazione Contenuti dei File\n\nGet-Content 'file' | More: Visualizza il contenuto dei file con possibilit√† di navigazione, simile a less/more in Unix.\nGet-Content 'file': Mostra l‚Äôintero contenuto di un file, equivalente a cat in Unix.\nGet-Content 'file' -Head &lt;numero&gt;: Mostra le prime righe di un file, simile a head in Unix.\nGet-Content 'file' -Tail &lt;numero&gt;: Mostra le ultime righe di un file, equivalente a tail in Unix.\n\n```ncbazvfqpprd Suggerimenti per una Gestione Ottimale dei File e delle Cartelle\n√à cruciale familiarizzarsi con l‚Äôutilizzo dei percorsi relativi per semplificare gli spostamenti tra le diverse directory. L‚Äôimpiego dei percorsi relativi rende il processo di navigazione pi√π intuitivo e meno incline agli errori.\n\nNomi Chiari e Concisi: Evitate di includere spazi nei nomi dei file e delle cartelle. Preferite l‚Äôutilizzo di trattini bassi (_) per separare le parole e mantenere una struttura leggibile e facilmente comprensibile.\nEvitare Caratteri Speciali: √à importante evitare l‚Äôinserimento di caratteri speciali come asterischi (*), dollari ($), slash (/, ), punti (.), virgole (,), punti e virgole (;), parentesi (()), parentesi quadre ([]), parentesi graffe ({}), ampersand (&), barre verticali (|), punti esclamativi (!), punti interrogativi (?) nei nomi dei file e delle cartelle. Talvolta, anche l‚Äôuso del trattino (-) pu√≤ causare problemi; quindi √® consigliabile evitarlo. Questi caratteri possono generare problemi di compatibilit√† con alcuni sistemi operativi o applicazioni, rendendo pi√π complessa la gestione dei file.\nDifferenziazione tra Maiuscole e Minuscole: Unix distingue tra maiuscole e minuscole (tratta le lettere come oggetti distinti), mentre Windows non lo fa. Per evitare confusioni, √® consigliabile adottare una politica conservativa: considerate i nomi che differiscono solo per la case delle lettere come distinti.\n\nSeguendo questi consigli, √® possibile ottimizzare notevolmente l‚Äôorganizzazione e la gestione dei propri file, migliorando l‚Äôefficienza del lavoro e riducendo il rischio di errori. ```\nCon la pratica, la riga di comando diventa uno strumento molto efficiente. Questa breve guida fornisce le basi per iniziare a esplorare e gestire i file dal terminale, offrendo una base di partenza per ulteriori apprendimenti. La familiarit√† con la shell √® fondamentale nella data science.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>La Shell</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a03_colab_tutorial.html",
    "href": "chapters/chapter_7/a03_colab_tutorial.html",
    "title": "Online Appendix D ‚Äî Colab: un breve tutorial",
    "section": "",
    "text": "D.1 Preparazione su Google Drive\nPer iniziare, √® necessario effettuare alcune operazioni preliminari su Google Drive:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>¬† <span class='chapter-title'>Colab: un breve tutorial</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a03_colab_tutorial.html#preparazione-su-google-drive",
    "href": "chapters/chapter_7/a03_colab_tutorial.html#preparazione-su-google-drive",
    "title": "Online Appendix D ‚Äî Colab: un breve tutorial",
    "section": "",
    "text": "Salvataggio dei file necessari: Accedi al tuo account Google Drive e salva i file di interesse, come un dataset e un Jupyter Notebook. Per esempio, potresti creare un Jupyter Notebook con Visual Studio Code e salvarlo con l‚Äôestensione .ipynb. In questo esempio, il file STAR.csv √® stato salvato nella cartella drive/MyDrive/teaching/psicometria/2024.\nPosizionamento dei file: Assicurati di conoscere con precisione il percorso della cartella in cui hai salvato i tuoi file. √à possibile salvare il notebook in qualsiasi cartella, ma √® importante ricordare dove si trova. Nel nostro esempio, anche il notebook import_data.ipynb √® stato salvato nella stessa cartella del dataset.\n\n\nD.1.1 Collegamento a Google Colab\nPer collegare il tuo Google Drive a Colab, inserisci il seguente codice nella prima cella del tuo Jupyter Notebook su Colab:\nfrom google.colab import drive\ndrive.mount('/content/drive')\nEsegui questa cella per iniziare il processo di autenticazione. Ti verr√† richiesto di inserire le tue credenziali (si consiglia di utilizzare l‚Äôaccount istituzionale) e di concedere i permessi necessari a Colab per accedere al tuo Drive.\n\n\nD.1.2 Verifica del file\nPrima di procedere, √® utile verificare che il file desiderato si trovi effettivamente nel percorso specificato. Usa un comando simile al seguente per elencare i file presenti nella cartella:\n!ls drive/MyDrive/teaching/psicometria/2024\nSe il comando mostra il file STAR.csv, significa che √® presente nella cartella e pronto per essere utilizzato.\n\n\nD.1.3 Importazione dei pacchetti e del dataset\nPrima di importare i dati, importa i pacchetti Python necessari per la tua analisi:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nSuccessivamente, puoi importare i dati direttamente dal file CSV specificando il percorso completo:\ndf = pd.read_csv(\"drive/MyDrive/teaching/psicometria/2024/STAR.csv\")\n√à fondamentale usare il percorso completo dal punto di montaggio drive fino al nome del file. Il percorso varier√† a seconda dell‚Äôutente e della struttura del suo Drive.\n\n\nD.1.4 Visualizzazione dei dati\nCon i dati ora disponibili in df, puoi procedere con l‚Äôanalisi. Per esempio, per creare un istogramma della variabile reading, puoi usare il seguente codice:\n_ = sns.histplot(data=df, x=\"reading\", stat='density')\nQuesto ti permetter√† di visualizzare la distribuzione dei dati relativi alla lettura nel dataset STAR.csv.\nSeguendo questi passaggi, puoi facilmente lavorare con i file salvati su Google Drive direttamente all‚Äôinterno di un Jupyter Notebook su Google Colab.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>¬† <span class='chapter-title'>Colab: un breve tutorial</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a04_virtual_env.html",
    "href": "chapters/chapter_7/a04_virtual_env.html",
    "title": "Online Appendix E ‚Äî Ambienti virtuali",
    "section": "",
    "text": "E.1 Concetto di Ambiente Virtuale\nUn ambiente virtuale √® uno spazio di lavoro isolato sul vostro computer, dove potete installare e utilizzare librerie Python senza interferire con il sistema principale. Questo isolamento consente di gestire le versioni delle librerie in modo efficiente, mantenendo il sistema organizzato e sicuro.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>¬† <span class='chapter-title'>Ambienti virtuali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a04_virtual_env.html#vantaggi-delluso-degli-ambienti-virtuali",
    "href": "chapters/chapter_7/a04_virtual_env.html#vantaggi-delluso-degli-ambienti-virtuali",
    "title": "Online Appendix E ‚Äî Ambienti virtuali",
    "section": "E.2 Vantaggi dell‚ÄôUso degli Ambienti Virtuali",
    "text": "E.2 Vantaggi dell‚ÄôUso degli Ambienti Virtuali\n\nIsolamento: Permette di selezionare e mantenere versioni specifiche di Python e delle librerie, garantendo la compatibilit√† e la stabilit√† del progetto.\nOrdine e Sicurezza: Mantiene separato l‚Äôambiente virtuale dal sistema principale, evitando conflitti e assicurando che le modifiche non influenzino altri programmi.\nRiproducibilit√† del Codice: Consente di condividere il codice in modo che funzioni correttamente su altri computer, garantendo coerenza e riproducibilit√† del lavoro.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>¬† <span class='chapter-title'>Ambienti virtuali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a04_virtual_env.html#procedura-per-la-creazione-di-un-ambiente-virtuale-con-conda",
    "href": "chapters/chapter_7/a04_virtual_env.html#procedura-per-la-creazione-di-un-ambiente-virtuale-con-conda",
    "title": "Online Appendix E ‚Äî Ambienti virtuali",
    "section": "E.3 Procedura per la Creazione di un Ambiente Virtuale con Conda",
    "text": "E.3 Procedura per la Creazione di un Ambiente Virtuale con Conda\nPer creare e gestire ambienti virtuali, √® possibile utilizzare conda, uno strumento incluso in Anaconda. Seguire i seguenti passaggi:\n\nAssicurarsi di avere Anaconda correttamente installato sul sistema.\nUtilizzare il terminale su macOS/Linux o PowerShell su Windows.\nEvitare di installare pacchetti direttamente nell‚Äôambiente base di Conda.\nCreare un nuovo ambiente virtuale usando il comando conda create.\nAttivare l‚Äôambiente virtuale appena creato utilizzando conda activate.\nInstallare i pacchetti necessari all‚Äôinterno dell‚Äôambiente virtuale.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>¬† <span class='chapter-title'>Ambienti virtuali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a04_virtual_env.html#gestione-dellambiente-virtuale",
    "href": "chapters/chapter_7/a04_virtual_env.html#gestione-dellambiente-virtuale",
    "title": "Online Appendix E ‚Äî Ambienti virtuali",
    "section": "E.4 Gestione dell‚ÄôAmbiente Virtuale",
    "text": "E.4 Gestione dell‚ÄôAmbiente Virtuale\nPer una gestione pi√π efficiente degli ambienti virtuali, √® consigliabile utilizzare la linea di comando anzich√© l‚Äôinterfaccia grafica di Anaconda. Questo offre maggiore controllo e flessibilit√† nel processo di creazione e gestione degli ambienti.\nSeguendo correttamente questi passaggi, √® possibile sfruttare appieno i vantaggi degli ambienti virtuali, garantendo un ambiente di sviluppo Python pulito e ben organizzato.\n√à fondamentale **evitare l'installazione diretta di pacchetti nell'ambiente `base` di Conda**. Assicuratevi sempre di seguire attentamente i seguenti passaggi:\n\n1. Disattivate l'ambiente `base`.\n2. Create un nuovo ambiente virtuale.\n3. Attivate il nuovo ambiente appena creato.\n\nSolo dopo aver completato questi passaggi, √® sicuro procedere con l'installazione dei pacchetti necessari. √à possibile verificare l'ambiente attivo osservando il nome visualizzato all'inizio del prompt nel terminale.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>¬† <span class='chapter-title'>Ambienti virtuali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a04_virtual_env.html#gestione-degli-ambienti-virtuali-passaggi-essenziali",
    "href": "chapters/chapter_7/a04_virtual_env.html#gestione-degli-ambienti-virtuali-passaggi-essenziali",
    "title": "Online Appendix E ‚Äî Ambienti virtuali",
    "section": "E.5 Gestione degli Ambienti Virtuali: Passaggi Essenziali",
    "text": "E.5 Gestione degli Ambienti Virtuali: Passaggi Essenziali\n\nDisattivare l‚ÄôAmbiente Virtuale Corrente: Se siete in un ambiente virtuale e desiderate uscirne, utilizzate il comando:\nconda deactivate\nSe non siete in un ambiente virtuale, potete procedere al passaggio successivo.\nCreare un Nuovo Ambiente Virtuale: Per utilizzare il campionatore CmdStan e il linguaggio di programmazione probabilistica Stan, adottate l‚Äôambiente virtuale cmdstan_env. Se si utilizza conda, √® possibile installare CmdStanPy e i componenti sottostanti di CmdStan dal repository conda-forge tramite la seguente procedura:\nconda create -n cmdstan_env -c conda-forge cmdstanpy\nConda richieder√† la conferma digitando y. Questo passaggio crea l‚Äôambiente e installa cmdstanpy insieme alle dipendenze necessarie.\nAttivare il Nuovo Ambiente: Per utilizzare l‚Äôambiente appena creato, attivatelo tramite:\nconda activate cmdstan_env\nInstallare le Librerie Richieste: All‚Äôinterno dell‚Äôambiente, installate altre librerie necessarie. Ecco come installare le librerie che utilizzeremo:\nconda install -c conda-forge jax numpyro bambi arviz seaborn jupyter-book ipywidgets watermark pingouin networkx -y \nNota: Gli utenti Windows potrebbero dover utilizzare nutpie come alternativa a jax.\nComandi Utili per Gestire Ambienti e Librerie:\n\nElencare gli ambienti virtuali disponibili e verificare quello attivo:\nconda env list\nRimuovere un ambiente virtuale specifico, ad esempio my_env:\nconda env remove -n my_env\nRimuovere una libreria da un ambiente specifico, ad esempio package_name:\nconda remove -n nome_ambiente package_name\n\n\nSeguendo attentamente questi passaggi e utilizzando i comandi di gestione, sarete in grado di creare e gestire efficacemente gli ambienti virtuali con Conda, garantendo una gestione pulita e ordinata delle dipendenze dei vostri progetti.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>¬† <span class='chapter-title'>Ambienti virtuali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a04_virtual_env.html#conda-forge",
    "href": "chapters/chapter_7/a04_virtual_env.html#conda-forge",
    "title": "Online Appendix E ‚Äî Ambienti virtuali",
    "section": "E.6 Conda Forge",
    "text": "E.6 Conda Forge\n√à consigliato aggiungere Conda Forge come canale aggiuntivo da cui Conda pu√≤ cercare e installare i pacchetti. Conda Forge √® una collezione di pacchetti gestita dalla community.\n\nE.6.1 Aggiungere Conda Forge\nPer aggiungere Conda Forge come canale, eseguire i seguenti comandi:\nconda config --add channels conda-forge\nconda config --set channel_priority strict\n\nconda config --add channels conda-forge: Aggiunge Conda Forge come canale aggiuntivo per la ricerca e l‚Äôinstallazione dei pacchetti.\nconda config --set channel_priority strict: Imposta la priorit√† dei canali su ‚Äústrict‚Äù, dando priorit√† ai pacchetti trovati nei canali elencati per primi nel file di configurazione .condarc.\n\n\n\nE.6.2 Vantaggi dell‚ÄôUso di Conda Forge\n\nAmpia Disponibilit√† di Pacchetti: Conda Forge offre un numero maggiore di pacchetti rispetto al canale predefinito di Conda, aumentando le possibilit√† di trovare il pacchetto necessario senza ricorrere ad altre soluzioni.\nAggiornamenti Frequenti: I pacchetti su Conda Forge vengono aggiornati pi√π frequentemente, rendendo pi√π probabile trovare le versioni pi√π recenti.\nCoerenza e Compatibilit√†: Utilizzando la priorit√† ‚Äústrict‚Äù e Conda Forge, si aumenta la coerenza e la compatibilit√† tra i pacchetti, riducendo il rischio di conflitti tra dipendenze.\n\nAggiungere Conda Forge come canale e impostare la priorit√† dei canali su ‚Äústrict‚Äù sono pratiche consigliate per migliorare la gestione dei pacchetti con Conda. Questo approccio aiuta a mantenere l‚Äôambiente stabile, aggiornato e compatibile con le ultime versioni dei pacchetti disponibili.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>¬† <span class='chapter-title'>Ambienti virtuali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a04_virtual_env.html#utilizzo-di-graphviz-opzionale",
    "href": "chapters/chapter_7/a04_virtual_env.html#utilizzo-di-graphviz-opzionale",
    "title": "Online Appendix E ‚Äî Ambienti virtuali",
    "section": "E.7 Utilizzo di Graphviz (Opzionale)",
    "text": "E.7 Utilizzo di Graphviz (Opzionale)\nPer utilizzare il pacchetto Python graphviz, √® necessario installare prima Graphviz sul vostro computer. Le istruzioni specifiche per l‚Äôinstallazione variano a seconda del sistema operativo e sono disponibili sul sito ufficiale di Graphviz.\n\nE.7.1 Installazione di Graphviz\n\nInstallazione di Graphviz: Seguire le istruzioni sul sito di Graphviz per installare il software sul vostro sistema operativo (Windows, macOS, Linux).\nVerifica dell‚ÄôInstallazione: Dopo l‚Äôinstallazione, assicuratevi che Graphviz sia correttamente installato eseguendo il seguente comando nel terminale:\ndot -V\nQuesto comando dovrebbe restituire la versione di Graphviz installata.\n\n\n\nE.7.2 Installazione del Pacchetto Python graphviz\nDopo aver installato Graphviz, potete procedere con l‚Äôinstallazione del pacchetto Python graphviz nel vostro ambiente virtuale. Seguite questi passaggi:\n\nAttivare l‚ÄôAmbiente Virtuale: Attivate l‚Äôambiente virtuale in cui avete installato pymc (ad esempio, pymc_env) nella vostra console:\nconda activate pymc_env\nInstallare il Pacchetto Python graphviz: Eseguite il seguente comando per installare il pacchetto graphviz tramite Conda Forge:\nconda install -c conda-forge graphviz\n\n\n\nE.7.3 Vantaggi dell‚ÄôUtilizzo di Graphviz\nL‚Äôinstallazione di Graphviz e del relativo pacchetto Python consente di creare e visualizzare grafici e diagrammi all‚Äôinterno del vostro ambiente Python. Questo pu√≤ essere particolarmente utile per visualizzare strutture di dati complesse, flussi di lavoro o grafi probabilistici.\nSeguendo questi passaggi, sarete in grado di utilizzare le funzionalit√† di Graphviz nel vostro ambiente Python, migliorando le capacit√† di visualizzazione e analisi dei vostri progetti.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>¬† <span class='chapter-title'>Ambienti virtuali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a10_math_symbols.html",
    "href": "chapters/chapter_7/a10_math_symbols.html",
    "title": "Online Appendix F ‚Äî Simbologia di base",
    "section": "",
    "text": "Per una scrittura pi√π sintetica possono essere utilizzati alcuni simboli matematici.\n\n\\(\\log(x)\\): il logaritmo naturale di \\(x\\).\nL‚Äôoperatore logico booleano \\(\\land\\) significa ‚Äúe‚Äù (congiunzione forte) mentre il connettivo di disgiunzione \\(\\lor\\) significa ‚Äúo‚Äù (oppure) (congiunzione debole).\nIl quantificatore esistenziale \\(\\exists\\) vuol dire ‚Äúesiste almeno un‚Äù e indica l‚Äôesistenza di almeno una istanza del concetto/oggetto indicato. Il quantificatore esistenziale di unicit√† \\(\\exists!\\) (‚Äúesiste soltanto un‚Äù) indica l‚Äôesistenza di esattamente una istanza del concetto/oggetto indicato. Il quantificatore esistenziale \\(\\nexists\\) nega l‚Äôesistenza del concetto/oggetto indicato.\nIl quantificatore universale \\(\\forall\\) vuol dire ‚Äúper ogni.‚Äù\n\\(\\mathcal{A, S}\\): insiemi.\n\\(x \\in A\\): \\(x\\) √® un elemento dell‚Äôinsieme \\(A\\).\nL‚Äôimplicazione logica ‚Äú\\(\\Rightarrow\\)‚Äù significa ‚Äúimplica‚Äù (se ‚Ä¶allora). \\(P \\Rightarrow Q\\) vuol dire che \\(P\\) √® condizione sufficiente per la verit√† di \\(Q\\) e che \\(Q\\) √® condizione necessaria per la verit√† di \\(P\\).\nL‚Äôequivalenza matematica ‚Äú\\(\\iff\\)‚Äù significa ‚Äúse e solo se‚Äù e indica una condizione necessaria e sufficiente, o corrispondenza biunivoca.\nIl simbolo \\(\\vert\\) si legge ‚Äútale che.‚Äù\nIl simbolo \\(\\triangleq\\) (o \\(:=\\)) si legge ‚Äúuguale per definizione.‚Äù\nIl simbolo \\(\\Delta\\) indica la differenza fra due valori della variabile scritta a destra del simbolo.\nIl simbolo \\(\\propto\\) si legge ‚Äúproporzionale a.‚Äù\nIl simbolo \\(\\approx\\) si legge ‚Äúcirca.‚Äù\nIl simbolo \\(\\in\\) della teoria degli insiemi vuol dire ‚Äúappartiene‚Äù e indica l‚Äôappartenenza di un elemento ad un insieme. Il simbolo \\(\\notin\\) vuol dire ‚Äúnon appartiene.‚Äù\nIl simbolo \\(\\subseteq\\) si legge ‚Äú√® un sottoinsieme di‚Äù (pu√≤ coincidere con l‚Äôinsieme stesso). Il simbolo \\(\\subset\\) si legge ‚Äú√® un sottoinsieme proprio di.‚Äù\nIl simbolo \\(\\#\\) indica la cardinalit√† di un insieme.\nIl simbolo \\(\\cap\\) indica l‚Äôintersezione di due insiemi. Il simbolo \\(\\cup\\) indica l‚Äôunione di due insiemi.\nIl simbolo \\(\\emptyset\\) indica l‚Äôinsieme vuoto o evento impossibile.\nIn matematica, \\(\\mbox{argmax}\\) identifica l‚Äôinsieme dei punti per i quali una data funzione raggiunge il suo massimo. In altre parole, \\(\\mbox{argmax}_x f(x)\\) √® l‚Äôinsieme dei valori di \\(x\\) per i quali \\(f(x)\\) raggiunge il valore pi√π alto.\n\\(a, c, \\alpha, \\gamma\\): scalari.\n\\(\\boldsymbol{x}, \\boldsymbol{y}\\): vettori.\n\\(\\boldsymbol{X}, \\boldsymbol{Y}\\): matrici.\n\\(X \\sim p\\): la variabile casuale \\(X\\) si distribuisce come \\(p\\).\n\\(p(\\cdot)\\): distribuzione di massa o di densit√† di probabilit√†.\n\\(p(y \\mid \\boldsymbol{x})\\): la probabilit√† o densit√† di \\(y\\) dato \\(\\boldsymbol{x}\\), ovvero \\(p(y = \\boldsymbol{Y} \\mid x = \\boldsymbol{X})\\).\n\\(f(x)\\): una funzione arbitraria di \\(x\\).\n\\(f(\\boldsymbol{X}; \\theta, \\gamma)\\): \\(f\\) √® una funzione di \\(\\boldsymbol{X}\\) con parametri \\(\\theta, \\gamma\\). Questa notazione indica che \\(\\boldsymbol{X}\\) sono i dati che vengono passati ad un modello di parametri \\(\\theta, \\gamma\\).\n\\(\\mathcal{N}(\\mu, \\sigma^2)\\): distribuzione gaussiana di media \\(\\mu\\) e varianza \\(sigma^2\\).\n\\(\\mbox{Beta}(\\alpha, \\beta)\\): distribuzione Beta di parametri \\(\\alpha\\) e \\(\\beta\\).\n\\(\\mathcal{U}(a, b)\\): distribuzione uniforme con limite inferiore \\(a\\) e limite superiore \\(b\\).\n\\(\\mbox{Cauchy}(\\alpha, \\beta)\\): distribuzione di Cauchy di parametri \\(\\alpha\\) (posizione: media) e \\(\\beta\\) (scala: radice quadrata della varianza).\n\\(\\mathcal{B}(p)\\): distribuzione di Bernoulli di parametro \\(p\\) (probabilit√† di successo).\n\\(\\mbox{Bin}(n, p)\\): distribuzione binomiale di parametri \\(n\\) (numero di prove) e \\(p\\) (probabilit√† di successo).\n\\(\\mathbb{KL} (p \\mid\\mid q)\\): la divergenza di Kullback-Leibler da \\(p\\) a \\(q\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>¬† <span class='chapter-title'>Simbologia di base</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a11_numbers.html",
    "href": "chapters/chapter_7/a11_numbers.html",
    "title": "Online Appendix G ‚Äî Numeri binari, interi, razionali, irrazionali e reali",
    "section": "",
    "text": "G.1 Numeri binari\nI numeri binari costituiscono la forma pi√π fondamentale di sistema numerico in informatica, essendo composto esclusivamente da due simboli, ovvero 0 e 1. Questo sistema √® frequentemente impiegato per rappresentare dualit√† logiche, come vero/falso o presenza/assenza, in virt√π della sua innata semplicit√† binaria. La sua applicazione √® particolarmente efficace nell‚Äôelaborazione di dati per generare statistiche sintetiche in modo efficiente e rapido.\nImmaginiamo di porre la seguente domanda a 10 studenti: ‚ÄúTi piacciono i mirtilli?‚Äù Le risposte potrebbero essere le seguenti:\nopinion = (True, False, True, True, True, False, True, True, True, False)\nopinion\n\n(True, False, True, True, True, False, True, True, True, False)\nIn questo caso, abbiamo utilizzato i numeri binari 0 e 1 per rappresentare risposte diverse, dove False indica ‚ÄúNo‚Äù e True indica ‚ÄúSi‚Äù. Questa rappresentazione binaria ci consente di ottenere facilmente una panoramica delle preferenze degli studenti riguardo i mirtilli.\nIn Python True equivale a 1 e False a zero. Possiamo dunque calcolare la proporzione di risposte positive nel modo seguente:\nsum(opinion) / len(opinion)\n\n0.7",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>¬† <span class='chapter-title'>Numeri binari, interi, razionali, irrazionali e reali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a11_numbers.html#numeri-interi",
    "href": "chapters/chapter_7/a11_numbers.html#numeri-interi",
    "title": "Online Appendix G ‚Äî Numeri binari, interi, razionali, irrazionali e reali",
    "section": "G.2 Numeri interi",
    "text": "G.2 Numeri interi\nI numeri interi sono numeri privi di decimali e comprendono sia i numeri naturali utilizzati per il conteggio, come 1, 2, ‚Ä¶, sia i numeri con il segno, necessari per rappresentare grandezze negative. L‚Äôinsieme dei numeri naturali √® indicato con il simbolo \\(\\mathbb{N}\\). L‚Äôinsieme numerico dei numeri interi relativi si rappresenta come \\(\\mathbb{Z} = \\{0, \\pm 1, \\pm 2, \\dots \\}\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>¬† <span class='chapter-title'>Numeri binari, interi, razionali, irrazionali e reali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a11_numbers.html#numeri-razionali",
    "href": "chapters/chapter_7/a11_numbers.html#numeri-razionali",
    "title": "Online Appendix G ‚Äî Numeri binari, interi, razionali, irrazionali e reali",
    "section": "G.3 Numeri razionali",
    "text": "G.3 Numeri razionali\nI numeri razionali sono numeri frazionari rappresentabili come \\(m/n\\), dove \\(m\\) e \\(n\\) sono numeri interi e \\(n\\) √® diverso da zero. Gli elementi dell‚Äôinsieme dei numeri razionali sono quindi dati da \\(\\mathbb{Q} = \\{\\frac{m}{n} \\,\\vert\\, m, n \\in \\mathbb{Z}, n \\neq 0\\}\\). √à importante notare che l‚Äôinsieme dei numeri naturali √® incluso in quello dei numeri interi, che a sua volta √® incluso in quello dei numeri razionali, ovvero \\(\\mathbb{N} \\subseteq \\mathbb{Z} \\subseteq \\mathbb{Q}\\). Per rappresentare solo i numeri razionali non negativi, utilizziamo il simbolo \\(\\mathbb{Q^+} = \\{q \\in \\mathbb{Q} \\,\\vert\\, q \\geq 0\\}\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>¬† <span class='chapter-title'>Numeri binari, interi, razionali, irrazionali e reali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a11_numbers.html#numeri-irrazionali",
    "href": "chapters/chapter_7/a11_numbers.html#numeri-irrazionali",
    "title": "Online Appendix G ‚Äî Numeri binari, interi, razionali, irrazionali e reali",
    "section": "G.4 Numeri irrazionali",
    "text": "G.4 Numeri irrazionali\nTuttavia, alcune grandezze non possono essere esprimibili come numeri interi o razionali. Questi numeri sono noti come numeri irrazionali e sono rappresentati dall‚Äôinsieme \\(\\mathbb{R}\\). Essi comprendono numeri illimitati e non periodici, che non possono essere scritti come frazioni. Ad esempio, \\(\\sqrt{2}\\), \\(\\sqrt{3}\\) e \\(\\pi = 3.141592\\ldots\\) sono esempi di numeri irrazionali.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>¬† <span class='chapter-title'>Numeri binari, interi, razionali, irrazionali e reali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a11_numbers.html#numeri-reali",
    "href": "chapters/chapter_7/a11_numbers.html#numeri-reali",
    "title": "Online Appendix G ‚Äî Numeri binari, interi, razionali, irrazionali e reali",
    "section": "G.5 Numeri reali",
    "text": "G.5 Numeri reali\nI numeri razionali rappresentano solo una parte dei punti sulla retta \\(r\\). Per rappresentare ogni possibile punto sulla retta, √® necessario introdurre i numeri reali. L‚Äôinsieme dei numeri reali comprende numeri positivi, negativi e nulli, e contiene come casi particolari i numeri interi, razionali e irrazionali. In statistica, il numero di decimali spesso indica il grado di precisione della misurazione.\n\nG.5.1 Intervalli\nQuesto ci porta a esplorare gli intervalli, una struttura matematica che ci aiuta a definire sottoinsiemi specifici sulla retta numerica. Gli intervalli aperti, che escludono i punti di inizio e fine. D‚Äôaltro canto, gli intervalli chiusi includono sia il punto di inizio che quello di fine, fornendo una copertura di valori senza tralasciare i confini. Le caratteristiche degli intervalli sono riportate nella tabella seguente.\n\n\n\nIntervallo\n\n\n\n\n\n\nchiuso\n\\([a, b]\\)\n\\(a \\leq x \\leq b\\)\n\n\naperto\n\\((a, b)\\)\n\\(a &lt; x &lt; b\\)\n\n\nchiuso a sinistra e aperto a destra\n\\([a, b)\\)\n\\(a \\leq x &lt; b\\)\n\n\naperto a sinistra e chiuso a destra\n\\((a, b]\\)\n\\(a &lt; x \\leq b\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>¬† <span class='chapter-title'>Numeri binari, interi, razionali, irrazionali e reali</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a13_sets.html",
    "href": "chapters/chapter_7/a13_sets.html",
    "title": "Online Appendix H ‚Äî Insiemi",
    "section": "",
    "text": "H.1 Diagrammi di Eulero-Venn\nI diagrammi di Venn sono uno strumento grafico molto utile per rappresentare gli insiemi e per verificare le propriet√† delle operazioni tra di essi. Questi diagrammi prendono il nome dal matematico inglese del 19¬∞ secolo John Venn, anche se rappresentazioni simili erano gi√† state utilizzate in precedenza da Leibniz e Eulero.\nI diagrammi di Venn rappresentano gli insiemi come regioni del piano delimitate da una curva chiusa. Nel caso di insiemi finiti, √® possibile evidenziare alcuni elementi di un insieme tramite punti, e in alcuni casi possono essere evidenziati tutti gli elementi degli insiemi considerati.\nIn sostanza, questi diagrammi sono un modo visuale per rappresentare le propriet√† degli insiemi e delle operazioni tra di essi. Sono uno strumento molto utile per visualizzare la relazione tra gli insiemi e per capire meglio come si combinano gli elementi all‚Äôinterno di essi.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>¬† <span class='chapter-title'>Insiemi</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a13_sets.html#appartenenza-ad-un-insieme",
    "href": "chapters/chapter_7/a13_sets.html#appartenenza-ad-un-insieme",
    "title": "Online Appendix H ‚Äî Insiemi",
    "section": "H.2 Appartenenza ad un insieme",
    "text": "H.2 Appartenenza ad un insieme\nUsiamo ora Python.\n\nSet1 = {1, 2}\nprint(Set1)\nprint(type(Set1))\n\n{1, 2}\n&lt;class 'set'&gt;\n\n\n\nmy_list = [1, 2, 3, 4]\nmy_set_from_list = set(my_list)\nprint(my_set_from_list)\n\n{1, 2, 3, 4}\n\n\nL‚Äôappartenenza ad un insieme si verifica con in e not in.\n\nmy_set = set([1, 3, 5])\nprint(\"Ecco il mio insieme:\", my_set)\nprint(\"1 appartiene all'insieme:\", 1 in my_set)\nprint(\"2 non appartiene all'insieme:\", 2 in my_set)\nprint(\"4 NON appartiene all'insieme:\", 4 not in my_set)\n\nEcco il mio insieme: {1, 3, 5}\n1 appartiene all'insieme: True\n2 non appartiene all'insieme: False\n4 NON appartiene all'insieme: True",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>¬† <span class='chapter-title'>Insiemi</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a13_sets.html#relazioni-tra-insiemi",
    "href": "chapters/chapter_7/a13_sets.html#relazioni-tra-insiemi",
    "title": "Online Appendix H ‚Äî Insiemi",
    "section": "H.3 Relazioni tra insiemi",
    "text": "H.3 Relazioni tra insiemi\nEsaminiamo le funzioni Python per descrivere le relazioni tra insiemi. In particolare, dopo avere definito l‚Äôinsieme universo e l‚Äôinsieme vuoto, considereremo la relazione di inclusione che conduce al concetto di sottoinsieme. Analogamente si definisce il concetto di sovrainsieme. Mostreremo anche come valutare se due insiemi sono disgiunti (si dicono disgiunti gli insiemi con intersezione vuota).\n\nUniv = set([x for x in range(11)])\nSuper = set([x for x in range(11) if x % 2 == 0])\nDisj = set([x for x in range(11) if x % 2 == 1])\nSub = set([4, 6])\nNull = set([x for x in range(11) if x &gt; 10])\n\n\nprint(\"Insieme Universo (tutti gli interi positivi fino a 10):\", Univ)\nprint(\"Tutti gli interi positivi pari fino a 10:\", Super)\nprint(\"Tutti gli interi positivi dispari fino a 10:\", Disj)\nprint(\"Insieme di due elementi, 4 e 6:\", Sub)\nprint(\"Un isieme vuoto:\", Null)\n\nInsieme Universo (tutti gli interi positivi fino a 10): {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\nTutti gli interi positivi pari fino a 10: {0, 2, 4, 6, 8, 10}\nTutti gli interi positivi dispari fino a 10: {1, 3, 5, 7, 9}\nInsieme di due elementi, 4 e 6: {4, 6}\nUn isieme vuoto: set()\n\n\n\nprint('√à \"Super\" un sovrainsieme di \"Sub\"?', Super.issuperset(Sub))\nprint('√à \"Super\" un sottoinsieme di \"Univ\"?', Super.issubset(Univ))\nprint('√à \"Sub\" un sovrainsieme di \"Super\"?', Sub.issuperset(Super))\nprint('Sono \"Super\" e \"Disj\" insiemi disgiunti?', Sub.isdisjoint(Disj))\n\n√à \"Super\" un sovrainsieme di \"Sub\"? True\n√à \"Super\" un sottoinsieme di \"Univ\"? True\n√à \"Sub\" un sovrainsieme di \"Super\"? False\nSono \"Super\" e \"Disj\" insiemi disgiunti? True",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>¬† <span class='chapter-title'>Insiemi</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a13_sets.html#operazioni-tra-insiemi",
    "href": "chapters/chapter_7/a13_sets.html#operazioni-tra-insiemi",
    "title": "Online Appendix H ‚Äî Insiemi",
    "section": "H.4 Operazioni tra insiemi",
    "text": "H.4 Operazioni tra insiemi\nSi definisce intersezione di \\(A\\) e \\(B\\) l‚Äôinsieme \\(A \\cap B\\) di tutti gli elementi \\(x\\) che appartengono ad \\(A\\) e contemporaneamente a \\(B\\):\n\\[\nA \\cap B = \\{x ~\\vert~ x \\in A \\land x \\in B\\}.\n\\]\nSi definisce unione di \\(A\\) e \\(B\\) l‚Äôinsieme \\(A \\cup B\\) di tutti gli elementi \\(x\\) che appartengono ad \\(A\\) o a \\(B\\), cio√®\n\\[\nA \\cup B = \\{x ~\\vert~ x \\in A \\lor x \\in B\\}.\n\\]\nDifferenza. Si indica con \\(A \\setminus B\\) l‚Äôinsieme degli elementi di \\(A\\) che non appartengono a \\(B\\):\n\\[\nA \\setminus B = \\{x ~\\vert~ x \\in A \\land x \\notin B\\}.\n\\]\nInsieme complementare. Nel caso che sia \\(B \\subseteq A\\), l‚Äôinsieme differenza \\(A \\setminus B\\) √® detto insieme complementare di \\(B\\) in \\(A\\) e si indica con \\(B^C\\).\nDato un insieme \\(S\\), una partizione di \\(S\\) √® una collezione di sottoinsiemi di \\(S\\), \\(S_1, \\dots, S_k\\), tali che\n\\[\nS = S_1 \\cup S_2 \\cup \\dots S_k\n\\]\ne\n\\[\nS_i \\cap S_j, \\quad \\text{con } i \\neq j.\n\\]\nLa relazione tra unione, intersezione e insieme complementare √® data dalle leggi di DeMorgan:\n\\[\n(A \\cup B)^c = A^c \\cap B^c,\n\\]\n\\[\n(A \\cap B)^c = A^c \\cup B^c.\n\\]\nIn tutte le seguenti figure, \\(S\\) √® la regione delimitata dal rettangolo, \\(L\\) √® la regione all‚Äôinterno del cerchio di sinistra e \\(R\\) √® la regione all‚Äôinterno del cerchio di destra. La regione evidenziata mostra l‚Äôinsieme indicato sotto ciascuna figura.\n\n\n\nDiagrammi di Venn\n\n\nI diagrammi di Eulero-Venn che illustrano le leggi di DeMorgan sono forniti nella figura seguente.\n\n\n\nLeggi di DeMorgan\n\n\nVediamo ora come si eseguono le operazioni tra insiemi con Python.\nEguaglianza e differenza.\n\nS1 = {1, 2}\nS2 = {2, 2, 1, 1, 2}\nprint(\n    \"S1 e S2 sono uguali perch√© l'ordine o la ripetizione di elementi non importano per gli insiemi\\nS1==S2:\",\n    S1 == S2,\n)\n\nS1 e S2 sono uguali perch√© l'ordine o la ripetizione di elementi non importano per gli insiemi\nS1==S2: True\n\n\n\nS1 = {1, 2, 3, 4, 5, 6}\nS2 = {1, 2, 3, 4, 0, 6}\nprint(\n    \"S1 e S2 NON sono uguali perch√© si differenziano per almeno uno dei loro elementi\\nS1==S2:\",\n    S1 == S2,\n)\n\nS1 e S2 NON sono uguali perch√© si differenziano per almeno uno dei loro elementi\nS1==S2: False\n\n\nIntersezione. Si noti che il connettivo logico & corrisponde all‚Äôintersezione.\n\nS1 = set([x for x in range(1, 11) if x % 3 == 0])\nprint(\"S1:\", S1)\n\nS1: {9, 3, 6}\n\n\n\nS2 = set([x for x in range(1, 7)])\nprint(\"S2:\", S2)\n\nS2: {1, 2, 3, 4, 5, 6}\n\n\n\nS_intersection = S1.intersection(S2)\nprint(\"Intersezione di S1 e S2:\", S_intersection)\n\nS_intersection = S1 & S2\nprint(\"Intersezione di S1 e S2:\", S_intersection)\n\nIntersezione di S1 e S2: {1, 2, 3, 4, 6}\nIntersezione di S1 e S2: {1, 2, 3, 4, 6}\n\n\n\nS3 = set([x for x in range(6, 10)])\nprint(\"S3:\", S3)\nS1_S2_S3 = S1.intersection(S2).intersection(S3)\nprint(\"Intersection of S1, S2, and S3:\", S1_S2_S3)\n\nS3: {8, 9, 6, 7}\nIntersection of S1, S2, and S3: {6}\n\n\nUnione. Si noti che il connettivo logico | corrisponde all‚Äôunione.\n\nS1 = set([x for x in range(1, 11) if x % 3 == 0])\nprint(\"S1:\", S1)\nS2 = set([x for x in range(1, 5)])\nprint(\"S2:\", S2)\n\nS_union = S1.union(S2)\nprint(\"Unione di S1 e S2:\", S_union)\nS_union = S1 | S2\nprint(\"Unione di S1 e S2:\", S_union)\n\nS1: {9, 3, 6}\nS2: {1, 2, 3, 4}\nUnione di S1 e S2: {1, 2, 3, 4, 6, 9}\nUnione di S1 e S2: {1, 2, 3, 4, 6, 9}\n\n\nInsieme complementare.\n\nS = set([x for x in range(21) if x % 2 == 0])\nprint(\"S √® l'insieme dei numeri interi pari tra 0 e 20:\", S)\n\nS √® l'insieme dei numeri interi pari tra 0 e 20: {0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20}\n\n\n\nS_complement = set([x for x in range(21) if x % 2 != 0])\nprint(\"S_complement √® l'insieme dei numeri interi dispari tra 0 e 20:\", S_complement)\n\nS_complement √® l'insieme dei numeri interi dispari tra 0 e 20: {1, 3, 5, 7, 9, 11, 13, 15, 17, 19}\n\n\n\nprint(\n    \"√à l'unione di S e S_complement uguale a tutti i numeri interi tra 0 e 20?\",\n    S.union(S_complement) == set([x for x in range(21)]),\n)\n\n√à l'unione di S e S_complement uguale a tutti i numeri interi tra 0 e 20? True\n\n\nDifferenza tra insiemi.\n\nS1 = set([x for x in range(31) if x % 3 == 0])\nprint(\"Set S1:\", S1)\n\nSet S1: {0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30}\n\n\n\nS2 = set([x for x in range(31) if x % 5 == 0])\nprint(\"Set S2:\", S2)\n\nSet S2: {0, 5, 10, 15, 20, 25, 30}\n\n\n\nS_difference = S2 - S1\nprint(\"Differenza tra S2 e S1, i.e. S2\\S1:\", S_difference)\n\nS_difference = S1.difference(S2)\nprint(\"Differenza tra S1 e S2, i.e. S1\\S2:\", S_difference)\n\nDifferenza tra S1 e S2 i.e. S2\\S1: {25, 10, 20, 5}\nDifferenza tra S2 e S1 i.e. S1\\S2: {3, 6, 9, 12, 18, 21, 24, 27}\n\n\nDifferenza simmetrica. La differenza simmetrica, indicata con il simbolo Œî, √® un‚Äôoperazione insiemistica definita come unione tra la differenza tra il primo e il secondo insieme e la differenza tra il secondo e il primo insieme. In modo equivalente, la differenza simmetrica equivale all‚Äôunione tra i due insiemi meno la loro intersezione.\n\nprint(\"S1\", S1)\nprint(\"S2\", S2)\nprint(\"Differenza simmetrica\", S1 ^ S2)\nprint(\"Differenza simmetrica\", S2.symmetric_difference(S1))\n\nS1 {0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30}\nS2 {0, 5, 10, 15, 20, 25, 30}\nDifferenza simmetrica {3, 5, 6, 9, 10, 12, 18, 20, 21, 24, 25, 27}\nDifferenza simmetrica {3, 5, 6, 9, 10, 12, 18, 20, 21, 24, 25, 27}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>¬† <span class='chapter-title'>Insiemi</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a13_sets.html#coppie-ordinate-e-prodotto-cartesiano",
    "href": "chapters/chapter_7/a13_sets.html#coppie-ordinate-e-prodotto-cartesiano",
    "title": "Online Appendix H ‚Äî Insiemi",
    "section": "H.5 Coppie ordinate e prodotto cartesiano",
    "text": "H.5 Coppie ordinate e prodotto cartesiano\nUna coppia ordinata \\((x,y)\\) √® l‚Äôinsieme i cui elementi sono \\(x \\in A\\) e \\(y \\in B\\) e nella quale \\(x\\) √® la prima componente (o prima coordinata) e \\(y\\) la seconda. L‚Äôinsieme di tutte le coppie ordinate costruite a partire dagli insiemi \\(A\\) e \\(B\\) viene detto prodotto cartesiano:\n\\[\nA \\times B = \\{(x, y) ~\\vert~ x \\in A \\land y \\in B\\}.\n\\]\nAd esempio, sia \\(A = \\{1, 2, 3\\}\\) e \\(B = \\{a, b\\}\\). Allora,\n\\[\n\\{1, 2\\} \\times \\{a, b, c\\} = \\{(1, a), (1, b), (1, c), (2, a), (2, b), (2, c)\\}.\n\\]\nPi√π in generale, un prodotto cartesiano di \\(n\\) insiemi pu√≤ essere rappresentato da un array di \\(n\\) dimensioni, dove ogni elemento √® una ennupla o tupla ordinata (ovvero, una collezione o un elenco ordinato di \\(n\\) oggetti). Una n-pla ordinata si distingue da un insieme di \\(n\\) elementi in quanto fra gli elementi di un insieme non √® dato alcun ordine. Inoltre gli elementi di una ennupla possono anche essere ripetuti. Essendo la n-pla un elenco ordinato, in generale di ogni suo elemento √® possibile dire se sia il primo, il secondo, il terzo, eccetera, fino all‚Äôn-esimo. Il prodotto cartesiano prende il nome da Ren√© Descartes la cui formulazione della geometria analitica ha dato origine al concetto.\n\nA = set([\"a\", \"b\", \"c\"])\nS = {1, 2, 3}\n\n\ndef cartesian_product(S1, S2):\n    result = set()\n    for i in S1:\n        for j in S2:\n            result.add(tuple([i, j]))\n    return result\n\n\nC = cartesian_product(A, S)\nprint(f\"Prodotto cartesiano di A e S\\n{A} x {S} = {C}\")\n\nProdotto cartesiano di A e S\n{'Head', 'Tail'} x {1, 2, 3} = {('Tail', 1), ('Head', 2), ('Head', 1), ('Tail', 3), ('Tail', 2), ('Head', 3)}\n\n\nSi definisce cardinalit√† (o potenza) di un insieme finito il numero degli elementi dell‚Äôinsieme. Viene indicata con \\(\\vert A\\vert, \\#(A)\\) o \\(\\text{c}(A)\\).\n\nprint(\"La cardinalit√† dell'insieme prodotto cartesiano √®:\", len(C))\n\nLa cardinalit√† dell'insieme prodotto cartesiano √®: 9\n\n\nInvece di scrivere funzioni noi stessi, √® possibile usare la libreria itertools di Python. Si ricordi di trasformare l‚Äôoggetto risultante in una lista per la visualizzazione e la successiva elaborazione.\n\nfrom itertools import product as prod\n\n\nA = set([x for x in range(1, 7)])\nB = set([x for x in range(1, 7)])\np = list(prod(A, B))\n\nprint(\"A √® l'insieme di tutti i possibili lanci di un dado:\", A)\nprint(\"B √® l'insieme di tutti i possibili lanci di un dado:\", B)\nprint(\n    \"\\nIl prodotto di A e B √® l'insieme dei risultati che si possono ottenere lanciando due dadi:\\n\",\n    p,\n)\n\nA √® l'insieme di tutti i possibili lanci di un dado: {1, 2, 3, 4, 5, 6}\nB √® l'insieme di tutti i possibili lanci di un dado: {1, 2, 3, 4, 5, 6}\n\nIl prodotto di A e B √® l'insieme dei risultati che si possono ottenere lanciando due dadi:\n [(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6), (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6)]\n\n\nLa cardinalit√† (cio√® il numero di elementi) del prodotto cartesiano tra due o pi√π insiemi √® uguale al prodotto delle cardinalit√† degli insiemi considerati: card(A √ó B) = card(A) ¬∑ card(B).\nUsando itertools √® facile calcolare la cardinalit√† del prodotto cartesiano di un insieme per se stesso. Consideriamo il quadrato dell‚Äôinsieme costituito dai risultati del lancio di una moneta. L‚Äôinsieme risultante avr√† cardinalit√† \\(2 \\cdot 2 = 4\\).\n\nA = {\"Head\", \"Tail\"} \np2 = list(prod(A, repeat=2))  \nprint(f\"Il quadrato dell'insieme A √® un insieme che contiene {len(p2)} elementi: {p2}\")\n\nIl quadrato dell'insieme A √® un insieme che contiene 4 elementi: [('Head', 'Head'), ('Head', 'Tail'), ('Tail', 'Head'), ('Tail', 'Tail')]\n\n\nL‚Äôinsieme \\(A\\) elevato alla terza potenza produce un insieme la cui cardinalit√† √®\n\np3 = list(prod(A, repeat=3))  \nprint(f\"L'insieme A elevato alla terza potenza √® costituito da {len(p3)} elementi: {p3}\")\n\nL'insieme A elevato alla terza potenza √® costituito da 8 elementi: [('Head', 'Head', 'Head'), ('Head', 'Head', 'Tail'), ('Head', 'Tail', 'Head'), ('Head', 'Tail', 'Tail'), ('Tail', 'Head', 'Head'), ('Tail', 'Head', 'Tail'), ('Tail', 'Tail', 'Head'), ('Tail', 'Tail', 'Tail')]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>¬† <span class='chapter-title'>Insiemi</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a13_sets.html#watermark",
    "href": "chapters/chapter_7/a13_sets.html#watermark",
    "title": "Online Appendix H ‚Äî Insiemi",
    "section": "H.6 Watermark",
    "text": "H.6 Watermark\n\n%load_ext watermark\n%watermark -n -u -v -iv -w",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>¬† <span class='chapter-title'>Insiemi</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a13a_probability.html",
    "href": "chapters/chapter_7/a13a_probability.html",
    "title": "Online Appendix I ‚Äî Sigma algebra",
    "section": "",
    "text": "I.1 Spiegazione\nUna \\(\\sigma\\)-algebra (o \\(\\sigma\\)-campo) √® un insieme di sottoinsiemi di uno spazio campionario \\(\\Omega\\) che soddisfa specifiche propriet√†. Queste propriet√† permettono di definire una struttura matematica per assegnare e calcolare le probabilit√† in modo consistente. Le propriet√† fondamentali di una \\(\\sigma\\)-algebra sono:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>¬† <span class='chapter-title'>Sigma algebra</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a13a_probability.html#spiegazione",
    "href": "chapters/chapter_7/a13a_probability.html#spiegazione",
    "title": "Online Appendix I ‚Äî Sigma algebra",
    "section": "",
    "text": "Insieme Vuoto: L‚Äôinsieme vuoto \\(\\varnothing\\) deve appartenere alla \\(\\sigma\\)-algebra \\(\\mathcal{A}\\). Questo garantisce che anche l‚Äôassenza di qualsiasi evento sia considerata.\nChiusura rispetto all‚ÄôUnione: Se una sequenza infinita di insiemi \\(A_1, A_2, \\dots\\) appartiene a \\(\\mathcal{A}\\), allora anche la loro unione \\(\\cup_{i=1}^\\infty A_i\\) deve appartenere a \\(\\mathcal{A}\\). Questa propriet√† permette di costruire nuovi eventi a partire da unione di eventi gi√† considerati misurabili.\nChiusura rispetto al Complemento: Se un insieme \\(A\\) appartiene a \\(\\mathcal{A}\\), allora anche il suo complemento \\(A^c\\) deve appartenere a \\(\\mathcal{A}\\). Questa propriet√† assicura che, se possiamo misurare un evento, possiamo anche misurare l‚Äôevento opposto.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>¬† <span class='chapter-title'>Sigma algebra</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a13a_probability.html#spazi-misurabili-e-di-probabilit√†",
    "href": "chapters/chapter_7/a13a_probability.html#spazi-misurabili-e-di-probabilit√†",
    "title": "Online Appendix I ‚Äî Sigma algebra",
    "section": "I.2 Spazi Misurabili e di Probabilit√†",
    "text": "I.2 Spazi Misurabili e di Probabilit√†\n\nSpazio Misurabile: Una coppia \\((\\Omega, \\mathcal{A})\\), dove \\(\\Omega\\) √® lo spazio campionario e \\(\\mathcal{A}\\) √® una \\(\\sigma\\)-algebra su \\(\\Omega\\), √® chiamata spazio misurabile. In questo contesto, gli insiemi misurabili sono quelli a cui possiamo assegnare una misura, come la probabilit√†.\nSpazio di Probabilit√†: Se aggiungiamo una misura di probabilit√† \\(\\mathbb{P}\\) a uno spazio misurabile \\((\\Omega, \\mathcal{A})\\), otteniamo uno spazio di probabilit√† \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\). La misura di probabilit√† \\(\\mathbb{P}\\) assegna valori di probabilit√† a ciascuno degli insiemi in \\(\\mathcal{A}\\) in modo che le propriet√† assiomatiche della probabilit√† siano soddisfatte.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>¬† <span class='chapter-title'>Sigma algebra</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a13a_probability.html#sigma-campo-di-borel",
    "href": "chapters/chapter_7/a13a_probability.html#sigma-campo-di-borel",
    "title": "Online Appendix I ‚Äî Sigma algebra",
    "section": "I.3 \\(\\sigma\\)-campo di Borel",
    "text": "I.3 \\(\\sigma\\)-campo di Borel\nQuando lo spazio campionario \\(\\Omega\\) √® la retta reale, si usa spesso il \\(\\sigma\\)-campo di Borel. Questo √® il pi√π piccolo \\(\\sigma\\)-campo che contiene tutti gli insiemi aperti della retta reale. √à fondamentale per definire la misurabilit√† e le probabilit√† su \\(\\mathbb{R}\\), poich√© contiene tutti gli insiemi che possiamo considerare ‚Äúmisurabili‚Äù nella pratica.\n\nI.3.1 Conclusione\nLe \\(\\sigma\\)-algebre permettono di gestire insiemi e misure in modo strutturato e coerente, formando la base matematica per la teoria della probabilit√† e altre aree dell‚Äôanalisi matematica. Utilizzando \\(\\sigma\\)-algebre, possiamo definire spazi di probabilit√† che sono essenziali per modellare e analizzare fenomeni aleatori in maniera rigorosa.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>¬† <span class='chapter-title'>Sigma algebra</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a14_combinatorics.html",
    "href": "chapters/chapter_7/a14_combinatorics.html",
    "title": "Online Appendix J ‚Äî Calcolo combinatorio",
    "section": "",
    "text": "J.1 Principio del prodotto\nI metodi di base del calcolo combinatorio applicano due principi: la regola del prodotto e la regola della somma. Consideriamo il principio del prodotto.\nIn generale, una scelta pu√≤ essere effettuata in pi√π fasi, ad esempio \\(k\\). Supponiamo che per ogni \\(i = 1, \\dots, k\\) la scelta da compiere al \\(i\\)-esimo stadio possa essere effettuata in \\(n_i\\) modi. Secondo il principio del prodotto, il numero totale di possibili scelte √® dato dal prodotto dei singoli numeri, ovvero:\n\\[\nn_{\\text{tot}} = n_1 \\cdot  n_2 \\cdots n_{k-1} \\cdot n_k.\n\\]\nEsempio 1. Ho a disposizione 2 paia di scarpe, 3 paia di pantaloni e 5 magliette. In quanti modi diversi mi posso vestire?\n\\[\n2 \\cdot 3 \\cdot 5 = 30\n\\]\nEsempio 2. In Minnesota le targhe delle automobili sono costituite da tre lettere (da A a Z) seguite da tre numeri (da 0 a 9). Qual √® la proporzione di targhe che iniziano con GZN?\nLa soluzione √® data dal numero di targhe che iniziano con GZN diviso per il numero totale di targhe possibili.\nIl numero totale di targe √® \\(26 \\cdot 26 \\cdot 26 \\cdot 10 \\cdot 10 \\cdot 10 = 17,576,000\\). Per calcolare il numero di targhe che iniziano con GZN, consideriamo le targhe che hanno la forma GZN _ _ _. Per i tre simboli mancanti ci sono \\(10 \\cdot 10 \\cdot 10\\) possibilit√†. Dunque la proporzione cercata √®\n\\[\n10^3/(26^3 \\cdot 10^3) = 1/26^3 = 0.0000569.\n\\]\n10**3 / (26**3 * 10**3)\n\n5.689576695493855e-05",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>¬† <span class='chapter-title'>Calcolo combinatorio</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a14_combinatorics.html#principio-della-somma",
    "href": "chapters/chapter_7/a14_combinatorics.html#principio-della-somma",
    "title": "Online Appendix J ‚Äî Calcolo combinatorio",
    "section": "J.2 Principio della somma",
    "text": "J.2 Principio della somma\nIl principio della somma afferma che se un insieme pu√≤ essere suddiviso in due o pi√π sottoinsiemi disgiunti, allora il numero totale di elementi nell‚Äôinsieme √® dato dalla somma dei numeri di elementi in ciascun sottoinsieme.\nIn altre parole, se si vuole determinare il numero totale di modi in cui √® possibile realizzare un certo evento, e questo evento pu√≤ essere realizzato in modo esclusivo in modo A oppure B, allora il numero totale di modi in cui √® possibile realizzare l‚Äôevento √® dato dalla somma dei modi in cui pu√≤ essere realizzato in modo A e dei modi in cui pu√≤ essere realizzato in modo B.\nAd esempio, se si vuole determinare il numero totale di modi in cui √® possibile scegliere un dolce da una tavola con due tipi di dolci (ad esempio torta e biscotti), il principio della somma afferma che il numero totale di modi √® dato dalla somma del numero di modi in cui √® possibile scegliere la torta e del numero di modi in cui √® possibile scegliere i biscotti.\nEsempio 3. L‚Äôurna \\(A\\) contiene \\(5\\) palline numerate da \\(1\\) a \\(5\\), l‚Äôurna \\(B\\) contiene \\(6\\) palline numerate da \\(6\\) a \\(11\\), l‚Äôurna \\(C\\) contiene \\(3\\) palline numerate da \\(12\\) a \\(14\\) e l‚Äôurna \\(D\\) contiene \\(2\\) palline numerate \\(15\\) e \\(16\\). Quanti insiemi composti da due palline, ciascuna estratta da un‚Äôurna differente, si possono formare?\nIl numero di insiemi di tipo \\(AB\\) √® dato dal prodotto delle palline che possono essere estratte dall‚Äôurna \\(A\\) (5) e da quelle che possono essere estratte dall‚Äôurna \\(B\\) (6), ovvero \\(5 \\cdot 6 = 30\\). In modo analogo, si ottengono 15 insiemi di tipo \\(AC\\), 10 di tipo \\(AD\\), 18 di tipo \\(BC\\), 12 di tipo \\(BD\\), 6 di tipo \\(CD\\). Quindi, per la regola della somma, il numero totale di insiemi distinti che si possono formare con due palline provenienti dalle quattro urne √® dato dalla somma di questi valori, ovvero \\(30 + 15 + 10 + 18 + 12 + 6 = 91\\). Pertanto, ci sono 91 insiemi composti da due palline, ciascuna estratta da un‚Äôurna differente, che si possono formare.\nIn conclusione, il principio del prodotto e il principio della somma sono due concetti fondamentali del calcolo combinatorio. In generale, il principio del prodotto si applica quando si tratta di eventi indipendenti che si verificano in successione, mentre il principio della somma si applica quando si tratta di eventi mutuamente esclusivi (cio√® non possono accadere contemporaneamente) e si cerca di calcolare il numero totale di possibili risultati.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>¬† <span class='chapter-title'>Calcolo combinatorio</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a14_combinatorics.html#il-modello-dellurna",
    "href": "chapters/chapter_7/a14_combinatorics.html#il-modello-dellurna",
    "title": "Online Appendix J ‚Äî Calcolo combinatorio",
    "section": "J.3 Il modello dell‚Äôurna",
    "text": "J.3 Il modello dell‚Äôurna\nI problemi di combinatoria spesso coinvolgono l‚Äôestrazione di palline da urne, le quali rappresentano dei modelli delle corrispondenti situazioni considerate. Una procedura comune per rappresentare queste situazioni √® il modello dell‚Äôurna, che consiste nell‚Äôestrazione di \\(k\\) palline da un‚Äôurna contenente \\(n\\) palline. Le palline possono essere tutte diverse, oppure alcune palline possono essere indistinguibili tra loro. Tra le possibili modalit√† di estrazione, sono particolarmente importanti:\n\nL‚Äôestrazione Bernoulliana di \\(k\\) palline, che si ottiene estraendo una pallina alla volta e rimettendola nell‚Äôurna dopo ogni estrazione;\nL‚Äôestrazione senza ripetizione di \\(k\\) palline, che si ottiene estraendo una pallina alla volta senza rimetterla nell‚Äôurna dopo l‚Äôestrazione;\nL‚Äôestrazione in blocco di \\(k\\) palline, che si ottiene estraendo \\(k\\) palline contemporaneamente.\n\nPer esempio, nel caso di campioni di ampiezza 2 estratti da un‚Äôurna con tre elementi \\(\\{1, 2, 3\\}\\), abbiamo i seguenti quattro casi:\n\ncampionamento con reimmissione tenendo conto dell‚Äôordine di estrazione: \\(\\{1,  1\\}, \\{2,  1\\}, \\{3,  1\\}, \\{1,  2\\}, \\{2,  2\\}, \\{3,  2\\}, \\{1,  3\\}, \\{2,  3\\}, \\{3,  3\\}\\);\ncampionamento con reimmissione senza tenere conto dell‚Äôordine di estrazione: \\(\\{1,  1\\}, \\{1,  2\\}, \\{1,  3\\}, \\{2,  2\\}, \\{2,  3\\}, \\{3,  3\\}\\);\ncampionamento senza reimmissione tenendo conto dell‚Äôordine di estrazione: \\(\\{1,  2\\}, \\{2,  1\\}, \\{1,  3\\}, \\{3,  1\\}, \\{2,  3\\}, \\{3,  2\\}\\);\ncampionamento senza reimmissione e senza tenere conto dell‚Äôordine di estrazione: \\(\\{1 , 2\\}, \\{1,  3\\}, \\{2, 3\\}\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>¬† <span class='chapter-title'>Calcolo combinatorio</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a14_combinatorics.html#permutazioni-semplici",
    "href": "chapters/chapter_7/a14_combinatorics.html#permutazioni-semplici",
    "title": "Online Appendix J ‚Äî Calcolo combinatorio",
    "section": "J.4 Permutazioni semplici",
    "text": "J.4 Permutazioni semplici\nLe permutazioni semplici sono il risultato di uno scambio dell‚Äôordine degli elementi di un insieme che contiene elementi distinti tra loro. Queste permutazioni sono indicate con il simbolo \\(P_n\\), e il numero di permutazioni semplici di \\(n\\) elementi distinti √® pari al fattoriale di \\(n\\), cio√® \\(n!\\), come espresso dall‚Äôeq. {eq}eq-permsem:\n\\[\nP_n = n!\n\\] (eq-permsem)\ndove il simbolo \\(n!\\) si legge \\(n\\) fattoriale ed √® uguale al prodotto di \\(n\\) numeri interi decrescenti da \\(n\\) fino a 1. Per definizione, il fattoriale di 0 √® 1.\nIl numero di permutazioni di \\(n\\) elementi distinti pu√≤ essere visto come l‚Äôestrazione senza rimessa di \\(n\\) elementi diversi da un‚Äôurna contenente gli \\(n\\) oggetti. Questo ci consente di applicare il principio del prodotto, il quale afferma che il numero di modi in cui √® possibile combinare o disporre un insieme di oggetti √® dato dal prodotto del numero di scelte possibili per ciascuna categoria di oggetti. Nel caso delle permutazioni, il principio del prodotto si applica nel seguente modo: se abbiamo \\(n\\) oggetti distinti da disporre in un ordine particolare, il numero di permutazioni possibili √® dato dal prodotto del numero di scelte possibili per la prima posizione, per la seconda posizione, per la terza posizione, e cos√¨ via, fino alla \\(n\\)-esima posizione.\nEsempio 4. Consideriamo l‚Äôinsieme: \\(A = \\{a, b, c\\}\\). Calcoliamo il numero di permutazioni semplici.\nLe permutazioni semplici di \\(A\\) sono: \\(\\{a, b, c\\}\\), \\(\\{a, c, b\\}\\), \\(\\{b, c, a\\}\\), \\(\\{b, a, c\\}\\), \\(\\{c, a, b\\}\\), \\(\\{c, b, a\\}\\), ovvero 6. Applichiamo l‚Äôeq. {ref}eq-permsem:\n\\[\nP_n = P_3 = 3! = 3 \\cdot 2 \\cdot 1 = 6.\n\\]\nLo strumento principale che usiamo in Python per trovare le permutazioni di un insieme √® una libreria specificamente progettata per iterare sugli oggetti in modi diversi, ovvero itertools. Con itertools.permutations() generiamo le permutazioni.\n\nA = {\"A\", \"B\", \"C\"}\nprint(A)\n\n{'A', 'B', 'C'}\n\n\n\npermutations = it.permutations(A)\n\nPer visualizzare il risultato dobbiamo trasformarlo in una tupla:\n\ntuple(permutations)\n\n(('A', 'B', 'C'),\n ('A', 'C', 'B'),\n ('B', 'A', 'C'),\n ('B', 'C', 'A'),\n ('C', 'A', 'B'),\n ('C', 'B', 'A'))\n\n\nLo stesso risultato si ottiene con\n\npermutations = it.permutations(\"ABC\")\npermutations = tuple(permutations)\npermutations\n\n(('A', 'B', 'C'),\n ('A', 'C', 'B'),\n ('B', 'A', 'C'),\n ('B', 'C', 'A'),\n ('C', 'A', 'B'),\n ('C', 'B', 'A'))\n\n\nPossiamo ora contare quanti elementi ci sono nella tupla usando la funzione len():\n\nlen(permutations)\n\n6\n\n\nOppure, possiamo appliare la formula {eq}eq-permsem mediante la funzione factorial() contenuta nella libreria math di Numpy:\n\nmath.factorial(3)\n\n6\n\n\nEsempio 5. Gli anagrammi sono le permutazioni che si ottengono da una parola variando l‚Äôordine delle lettere. Le permutazioni semplici si applicano al caso di parole costituite da lettere tutte diverse tra loro. Ad esempio, con la parola NUMERO si ottengono \\(P_6 = 6! = 6\\cdot5\\cdot4\\cdot3\\cdot2\\cdot1 = 720\\) anagrammi.\n\npermutations = it.permutations(\"NUMERO\")\npermutations = tuple(permutations)\npermutations[1:10]\n\n(('N', 'U', 'M', 'E', 'O', 'R'),\n ('N', 'U', 'M', 'R', 'E', 'O'),\n ('N', 'U', 'M', 'R', 'O', 'E'),\n ('N', 'U', 'M', 'O', 'E', 'R'),\n ('N', 'U', 'M', 'O', 'R', 'E'),\n ('N', 'U', 'E', 'M', 'R', 'O'),\n ('N', 'U', 'E', 'M', 'O', 'R'),\n ('N', 'U', 'E', 'R', 'M', 'O'),\n ('N', 'U', 'E', 'R', 'O', 'M'))\n\n\n\nlen(permutations)\n\n720\n\n\n\nmath.factorial(6)\n\n720\n\n\nEsempio 6. Un altro esempio riguarda i giochi di carte. Ci sono 52! \\(\\approx 8 \\times 10^{67}\\) modi di ordinare un mazzo di carte da poker; questo numero √® ‚Äúquasi‚Äù grande come il numero di atomi dell‚Äôuniverso che si stima essere uguale a circa \\(10^{80}\\).\n\nmath.factorial(52)\n\n80658175170943878571660636856403766975289505440883277824000000000000\n\n\n\nprint(\"{:.2e}\".format(math.factorial(52)))\n\n8.07e+67\n\n\nEsempio 7. Le cifre 1, 2, 3, 4 e 5 sono disposte in ordine casuale per formare un numero di cinque cifre.\n\nQuanti diversi numeri di cinque cifre possono essere formati?\nQuanti diversi numeri di cinque cifre sono dispari?\n\nIniziamo a creare una tupla con le cinque cifre:\n\ntuple(range(1, 6))\n\n(1, 2, 3, 4, 5)\n\n\nCome in precedenza, possiamo usare it.permutations():\n\npermutations = it.permutations(range(1, 6))\npermutations = tuple(permutations)\npermutations[1:10]\n\n((1, 2, 3, 5, 4),\n (1, 2, 4, 3, 5),\n (1, 2, 4, 5, 3),\n (1, 2, 5, 3, 4),\n (1, 2, 5, 4, 3),\n (1, 3, 2, 4, 5),\n (1, 3, 2, 5, 4),\n (1, 3, 4, 2, 5),\n (1, 3, 4, 5, 2))\n\n\nCi sono 120 permutazioni.\n\nlen(permutations)\n\n120\n\n\nPer trovare i numeri dispari tra queste 120 permutazioni utilizziamo la funzione sum() in Python abbinato alle espressioni for e in. Accediamo al quinto elemento di una permutazione utilizzando la notazione [4] (il primo elemento √® indicato con 0, quindi il quinto √® 4):\n\nsum(permutation[4] % 2 for permutation in permutations)\n\n72\n\n\nPossiamo controllare questo teoricamente: nel caso presente, ci sono tre possibili cifre dispari per l‚Äôultima posizione di un numero di cinque cifre: 1, 3 e 5. Dopo aver scelto una di queste, le cifre rimanenti nelle prime quattro posizioni possono essere formate in 4! modi. Pertanto:\n\nmath.factorial(4) * 3\n\n72",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>¬† <span class='chapter-title'>Calcolo combinatorio</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a14_combinatorics.html#disposizioni-semplici",
    "href": "chapters/chapter_7/a14_combinatorics.html#disposizioni-semplici",
    "title": "Online Appendix J ‚Äî Calcolo combinatorio",
    "section": "J.5 Disposizioni semplici",
    "text": "J.5 Disposizioni semplici\nLe disposizioni semplici rappresentano tutti i modi in cui un insieme di oggetti pu√≤ essere disposto in sequenza, tenendo conto dell‚Äôordine in cui gli oggetti vengono scelti e senza permettere la scelta di un oggetto pi√π di una volta.\nQuindi, se abbiamo un insieme di \\(n\\) oggetti distinti e vogliamo selezionarne \\(k\\) per formare una sequenza, le disposizioni semplici rappresentano tutti i sottoinsiemi di \\(k\\) oggetti distinti che possono essere selezionati dall‚Äôinsieme di \\(n\\) oggetti distinti in modo tale che l‚Äôordine in cui vengono selezionati sia importante.\nAd esempio, se abbiamo l‚Äôinsieme di oggetti \\({a,b,c}\\) e vogliamo selezionare due oggetti per formare una sequenza, le disposizioni semplici sarebbero: \\(ab\\), \\(ba\\), \\(ac\\), \\(ca\\), \\(bc\\), \\(cb\\). Nota che, in questo caso, l‚Äôordine in cui gli oggetti vengono scelti √® importante e ogni oggetto viene scelto una sola volta.\nIl numero di disposizioni semplici di \\(n\\) elementi distinti della classe \\(k\\) √® indicato con \\(D_{n,k}\\) e pu√≤ essere calcolato dividendo il numero di permutazioni di \\(n\\) oggetti distinti per il numero di permutazioni dei restanti \\(n-k\\) oggetti distinti, poich√© ogni disposizione semplice pu√≤ essere ottenuta come una permutazione di un sottoinsieme di \\(k\\) oggetti distinti.\nQuindi, il numero di disposizioni semplici di \\(n\\) elementi distinti della classe \\(k\\) √® dato da\n\\[\nD_{n,k} = \\frac{n!}{(n-k)!},\n\\] (eq_disp_simple)\ndove \\(n!\\) rappresenta il numero di permutazioni di \\(n\\) oggetti distinti e \\((n-k)!\\) rappresenta il numero di permutazioni dei restanti \\(n-k\\) oggetti distinti.\nEsempio 8. Consideriamo l‚Äôinsieme: \\(A = \\{a, b, c\\}\\). Qual √® il numero di disposizioni semplici di classe 2? Come abbiamo visto sopra, le disposizioni semplici di classe 2 sono \\(\\{a, b\\}\\), \\(\\{b, a\\}\\), \\(\\{a, c\\}\\), \\(\\{c, a\\}\\), \\(\\{b, c\\}\\), \\(\\{c, b\\}\\), ovvero 6.\nApplichiamo l‚Äôeq. {eq}eq_disp_simple:\n\\[\nD_{n,k} = \\frac{n!}{(n-k)!} = 3 \\cdot 2 = 6.\n\\]\nIn maniera equivalente possiamo trovare il risultato usando itertools.permutations(iterable, k). Tale istruzione ci consente di trovare il numero di permutazioni possibili di tutti i sottoinsiemi di \\(k\\) elementi distinti, ovvero il numero di diverse sequenze ordinate che possiamo ottenere scegliendo \\(k\\) oggetti dall‚Äôinsieme.\n\ntuple(it.permutations(\"ABC\", 2))\n\n(('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C', 'B'))\n\n\n\nres = tuple(it.permutations(\"ABC\", 2))\nlen(res)\n\n6\n\n\nOppure possiamo implementare l‚Äôeq. {eq}eq_disp_simple:\n\ndef simple_disp(n, k):\n    return math.factorial(n) / math.factorial(n - k)\n\n\nsimple_disp(3, 2)\n\n6.0\n\n\n(combinazione-semplice-section)= ## Combinazioni semplici\nLe combinazioni semplici rappresentano il numero di modi in cui \\(k\\) oggetti diversi possono essere scelti tra \\(n\\) oggetti distinti, ma a differenza delle disposizioni semplici, non tiene conto dell‚Äôordine in cui vengono scelti. In altre parole, le combinazioni semplici rappresentano tutti i possibili sottoinsiemi di \\(k\\) elementi distinti scelti tra \\(n\\) elementi distinti, senza considerare l‚Äôordine di estrazione.\nQuesto concetto pu√≤ essere modellato attraverso l‚Äôestrazione senza reimmissione di \\(k\\) oggetti da un‚Äôurna contenente \\(n\\) oggetti differenti. Tuttavia, a differenza delle disposizioni semplici, le combinazioni semplici considerano distinti solo i raggruppamenti che differiscono almeno per un elemento.\nGli elementi di ciascuna combinazione di \\(k\\) oggetti possono essere ordinati tra loro in \\(k!\\) modi diversi. Pertanto, il numero di combinazioni semplici √® dato dal numero di disposizioni semplici \\(D_{n,k}\\) diviso per il numero di permutazioni \\(P_k\\) dei \\(k\\) elementi.\nIl numero di combinazioni semplici \\(C_{n,k}\\) √® espresso dall‚Äôequazione\n\\[\nC_{n,k} = \\frac{D_{n,k}}{P_k} = \\frac{n!}{k!(n-k)!},\n\\] (eq_combsemp)\nche √® spesso indicata con il simbolo \\(\\binom{n}{k}\\) e viene chiamato ‚Äúcoefficiente binomiale‚Äù. In sintesi, le combinazioni semplici rappresentano il numero di sottoinsiemi di \\(k\\) elementi distinti scelti da un insieme di \\(n\\) elementi distinti senza considerare l‚Äôordine di estrazione, e il numero di combinazioni semplici √® dato dalla formula \\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\).\nEsempio 9. Per l‚Äôinsieme \\(A = \\{a, b, c\\}\\) si trovino le combinazioni semplici di classe 2.\nLe combinazioni semplici dell‚Äôinsieme \\(A\\) sono \\(\\{a, b\\}\\), \\(\\{a, c\\}\\), \\(\\{b, c\\}\\), ovvero 3. Applichiamo l‚Äôeq. {eq}eq_combsemp:\n\\[\nC_{n,k} = \\binom{n}{k} = \\binom{3}{2} = 3.\n\\]\nUsiamo itertools:\n\nc_nk = tuple(it.combinations(\"ABC\", 2))\nc_nk\n\n(('A', 'B'), ('A', 'C'), ('B', 'C'))\n\n\n\nlen(c_nk)\n\n3\n\n\nLa soluzione si trova anche usando la funzione comb() della libreria math.\n\nmath.comb(3, 2)\n\n3\n\n\nOppure usando la funzione comb() della libreria scipy.special.\n\nimport scipy.special as sp\n\nsp.comb(3, 2)\n\n3.0\n\n\nEsempio 10. Quanti gruppi di 2 si possono formare con 5 individui?\n\nc_nk = tuple(it.combinations(range(5), 2))\nc_nk\n\n((0, 1),\n (0, 2),\n (0, 3),\n (0, 4),\n (1, 2),\n (1, 3),\n (1, 4),\n (2, 3),\n (2, 4),\n (3, 4))\n\n\n\nlen(c_nk)\n\n10\n\n\novvero\n\nmath.comb(5, 2)\n\n10\n\n\nEsempio 11. Ho un‚Äôassociazione con 50 soci. Devo scegliere 5 membri che compongano il comitato direttivo. Quante possibili scelte?\n\nmath.comb(50, 5)\n\n2118760\n\n\nEsempio 12. Una gelateria offre 15 gusti di gelato differenti. Quante coppe diverse posso formare se ognuna contiene 3 gusti di gelato differenti tra loro?\n\nmath.comb(15, 3)\n\n455\n\n\nEsempio 13. Uno studente deve rispondere a 5 domande su 10. Solo 5 su 10. Quante possibili scelte ha?\n\nmath.comb(10, 5)\n\n252",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>¬† <span class='chapter-title'>Calcolo combinatorio</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a14_combinatorics.html#watermark",
    "href": "chapters/chapter_7/a14_combinatorics.html#watermark",
    "title": "Online Appendix J ‚Äî Calcolo combinatorio",
    "section": "J.6 Watermark",
    "text": "J.6 Watermark\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m\n\nLast updated: Tue Feb 06 2024\n\nPython implementation: CPython\nPython version       : 3.11.7\nIPython version      : 8.19.0\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.3.0\nMachine     : x86_64\nProcessor   : i386\nCPU cores   : 8\nArchitecture: 64bit\n\npandas    : 2.1.4\nmatplotlib: 3.8.2\nscipy     : 1.11.4\nnumpy     : 1.26.2\narviz     : 0.17.0\nseaborn   : 0.13.0\n\nWatermark: 2.4.3",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>¬† <span class='chapter-title'>Calcolo combinatorio</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a15_calculus.html",
    "href": "chapters/chapter_7/a15_calculus.html",
    "title": "Online Appendix K ‚Äî Per liberarvi dai terrori preliminari",
    "section": "",
    "text": "K.1 Introduzione ai logaritmi\nIl logaritmo √® una funzione matematica che risponde alla domanda: ‚Äúquante volte devo moltiplicare un certo numero (chiamato‚Äùbase‚Äù) per ottenere un altro numero?‚Äù Matematicamente, questo √® espresso come:\n\\[\n\\log_b(a) = x \\iff b^x = a\n\\]\nAd esempio, \\(\\log_2(8) = 3\\) perch√© \\(2^3 = 8\\).\nNel contesto dei logaritmi, i valori molto piccoli (compresi tra 0 e 1) diventano pi√π grandi (in termini assoluti) e negativi quando applichiamo una funzione logaritmica. Questo √® utile per stabilizzare i calcoli, specialmente quando lavoriamo con prodotti di numeri molto piccoli che potrebbero portare a problemi di underflow.\nPer esempio: - \\(\\log(1) = 0\\) - \\(\\log(0.1) = -1\\) - \\(\\log(0.01) = -2\\) - \\(\\log(0.001) = -3\\)\nCome si pu√≤ vedere, i valori assoluti dei logaritmi crescono man mano che il numero originale si avvicina a zero.\nUna delle propriet√† pi√π utili dei logaritmi √® che consentono di trasformare un prodotto in una somma:\n\\[\n\\log_b(a \\times c) = \\log_b(a) + \\log_b(c)\n\\]\nQuesta propriet√† √® estremamente utile in calcoli complessi, come nella statistica bayesiana, dove il prodotto di molte probabilit√† potrebbe diventare un numero molto piccolo e causare problemi numerici.\nUn‚Äôaltra propriet√† utile dei logaritmi √® che un rapporto tra due numeri diventa la differenza dei loro logaritmi:\n\\[\n\\log_b\\left(\\frac{a}{c}\\right) = \\log_b(a) - \\log_b(c)\n\\]\nAnche questa propriet√† √® molto utilizzata in matematica, specialmente in situazioni in cui √® necessario normalizzare i dati.\nIn sintesi, i logaritmi sono strumenti potenti per semplificare e stabilizzare i calcoli matematici. Essi consentono di lavorare pi√π agevolmente con numeri molto grandi o molto piccoli e di trasformare operazioni complesse come prodotti e divisioni in somme e differenze, rendendo i calcoli pi√π gestibili e meno inclini a errori numerici.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>¬† <span class='chapter-title'>Per liberarvi dai terrori preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_7/a15_calculus.html#watermark",
    "href": "chapters/chapter_7/a15_calculus.html#watermark",
    "title": "Online Appendix K ‚Äî Per liberarvi dai terrori preliminari",
    "section": "K.2 Watermark",
    "text": "K.2 Watermark\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m\n\nLast updated: Thu Feb 29 2024\n\nPython implementation: CPython\nPython version       : 3.11.8\nIPython version      : 8.22.1\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.3.0\nMachine     : x86_64\nProcessor   : i386\nCPU cores   : 8\nArchitecture: 64bit\n\nmatplotlib: 3.8.3\nseaborn   : 0.13.2\nnumpy     : 1.26.4\narviz     : 0.17.0\nscipy     : 1.12.0\n\nWatermark: 2.4.3",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>¬† <span class='chapter-title'>Per liberarvi dai terrori preliminari</span>"
    ]
  },
  {
    "objectID": "99-references.html",
    "href": "99-references.html",
    "title": "References",
    "section": "",
    "text": "Alexander, Rohan. 2023. Telling Stories with Data: With Applications\nin r. Chapman; Hall/CRC.\n\n\nClayton, Aubrey. 2021. Bernoulli‚Äôs Fallacy: Statistical Illogic and\nthe Crisis of Modern Science. Columbia University Press.\n\n\nDagan, Noa, Noam Barda, Eldad Kepten, Oren Miron, Shay Perchik, Mark\nKatz, Miguel Hern√°n, Marc Lipsitch, Ben Reis, and Ran Balicer. 2021.\n‚ÄúBNT162b2 mRNA Covid-19 Vaccine in a Nationwide Mass Vaccination\nSetting.‚Äù New England Journal of Medicine 384 (15):\n1412‚Äì23. https://doi.org/10.1056/NEJMoa2101765.\n\n\nDe Finetti, Bruno. 2017. Theory of Probability: A Critical\nIntroductory Treatment. Vol. 6. John Wiley & Sons.\n\n\nFishburn, Peter C. 1986. ‚ÄúThe Axioms of Subjective\nProbability.‚Äù Statistical Science 1 (3): 335‚Äì45.\n\n\nFox, John. 2015. Applied Regression Analysis and Generalized Linear\nModels. Sage publications.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and\nOther Stories. Cambridge University Press.\n\n\nHowson, Colin, and Peter Urbach. 2006. Scientific Reasoning: The\nBayesian Approach. Open Court Publishing.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to\nResearch Design and Causality. 1st ed. Chapman & Hall. https://theeffectbook.net.\n\n\nIoannidis, John PA. 2005. ‚ÄúWhy Most Published Research Findings\nAre False.‚Äù PLoS Medicine 2 (8): e124.\n\n\nJohnson, Alicia A., Miles Ott, and Mine Dogucu. 2022. Bayes Rules! An Introduction to Bayesian Modeling with\nR. CRC Press.\n\n\nKaplan, David. 2023. Bayesian Statistics for the Social\nSciences. Guilford Publications.\n\n\nLindley, Dennis V. 2013. Understanding Uncertainty. John Wiley\n& Sons.\n\n\nMatter, Ulrich. 2025. Data Analysis with AI and\nR. 1st Edition. New York, NY: Manning Publications.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A\nBayesian Course with Examples in R and\nStan. 2nd Edition. Boca Raton, Florida: CRC Press.\n\n\nPearl, Judea. 2009. Causality. Cambridge University Press.\n\n\nPress, S James. 2009. Subjective and Objective Bayesian Statistics:\nPrinciples, Models, and Applications. John Wiley & Sons.\n\n\nRamsey, Frank P. 1926. ‚ÄúTruth and Probability.‚Äù In\nReadings in Formal Epistemology: Sourcebook, 21‚Äì45. Springer.\n\n\nRiederer, Emily. 2021. ‚ÄúCausal Design Patterns for Data\nAnalysts,‚Äù January. https://emilyriederer.netlify.app/post/causal-design-patterns/.\n\n\nRohrer, Julia M. 2018. ‚ÄúThinking Clearly about Correlations and\nCausation: Graphical Causal Models for Observational Data.‚Äù\nAdvances in Methods and Practices in Psychological Science 1\n(1): 27‚Äì42.\n\n\nSpeelman, Craig P, Laura Parker, Benjamin J Rapley, and Marek McGann.\n2024. ‚ÄúMost Psychological Researchers Assume Their Samples Are\nErgodic: Evidence from a Year of Articles in Three Major\nJournals.‚Äù Collabra: Psychology 10 (1).\n\n\nStevens, Stanley Smith. 1946. ‚ÄúOn the Theory of Scales of\nMeasurement.‚Äù Science 103 (2684): 677‚Äì80.\n\n\nStigler, Stephen. 1986. The History of Statistics.\nMassachusetts: Belknap Harvard.\n\n\nVasishth, Shravan, and Andrew Gelman. 2021. ‚ÄúHow to Embrace\nVariation and Accept Uncertainty in Linguistic and Psycholinguistic Data\nAnalysis.‚Äù Linguistics 59 (5): 1311‚Äì42.\n\n\nZetsche, Ulrike, Paul-Christian Buerkner, and Babette Renneberg. 2019.\n‚ÄúFuture Expectations in Clinical Depression: Biased or\nRealistic?‚Äù Journal of Abnormal Psychology 128 (7): 678.\n\n\nZwet, Erik van, Andrew Gelman, Sander Greenland, Guido Imbens, Simon\nSchwab, and Steven N Goodman. 2023. ‚ÄúA New Look at p Values for\nRandomized Clinical Trials.‚Äù NEJM Evidence 3 (1):\nEVIDoa2300003.",
    "crumbs": [
      "Appendices",
      "References"
    ]
  },
  {
    "objectID": "chapters/chapter_3/02_conditional_prob.html#sec-v",
    "href": "chapters/chapter_3/02_conditional_prob.html#sec-v",
    "title": "18¬† Probabilit√† condizionata",
    "section": "18.3 Probabilit√† condizionata su altri eventi",
    "text": "18.3 Probabilit√† condizionata su altri eventi\nLa probabilit√† di un evento √® intrinsecamente condizionata dal nostro stato di informazione. In presenza di un determinato insieme di informazioni, attribuiamo a un evento una probabilit√† specifica di occorrenza. Tuttavia, qualora il nostro stato informativo subisca una modifica, anche la probabilit√† associata all‚Äôevento verr√† corrispondentemente aggiornata.\nIn realt√†, tutte le probabilit√† possono essere intese come probabilit√† condizionate, anche quando la variabile o l‚Äôevento condizionante non √® esplicitamente specificato. Ci√≤ implica che le probabilit√† sono sempre contestualizzate e dipendono dal set informativo disponibile in un dato scenario.\nQuesto quadro concettuale ci induce a considerare le probabilit√† come una ‚Äòmisura di plausibilit√†‚Äô che riflette la nostra conoscenza corrente del sistema o del fenomeno sotto indagine. A seguito dell‚Äôacquisizione di nuove informazioni o di cambiamenti nel contesto, la nostra misura di plausibilit√†, e quindi la probabilit√† attribuita agli eventi, pu√≤ essere rivista.\n\nTheorem 18.1 Siano \\(A\\) e \\(B\\) due eventi definiti su uno spazio campionario \\(S\\). Supponendo che l‚Äôevento \\(B\\) si verifichi, la probabilit√† condizionata di \\(A\\) dato \\(B\\) √® data da\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}, \\quad \\text{per}\\, P(B) &gt; 0,\n\\tag{18.1}\\]\ndove \\(P(A \\cap B)\\) rappresenta la probabilit√† congiunta dei due eventi, ovvero la probabilit√† che entrambi si verifichino.\n\nNell‚ÄôEquation¬†18.1, \\(P(A \\cap B)\\) √® la probabilit√† congiunta che entrambi gli eventi si verifichino, mentre \\(P(B)\\) √® la probabilit√† marginale dell‚Äôevento \\(B\\). Riorganizzando i termini, otteniamo la regola della moltiplicazione:\n\\[\nP(A \\cap B) = P(A \\mid B)P(B) = P(B \\mid A)P(A).\n\\]\nUtilizzando questa regola, possiamo derivare una forma alternativa della legge della probabilit√† totale:\n\\[\nP(A) = P(A \\mid B)P(B) + P(A \\mid B^c)P(B^c).\n\\]\nDove \\(B^c\\) rappresenta il complemento dell‚Äôevento \\(B\\).\n√à importante notare che \\(P(A \\mid B)\\) non √® definita se \\(P(B) = 0\\).\nLa probabilit√† condizionata pu√≤ essere interpretata come una ricalibrazione dello spazio campionario da \\(S\\) a \\(B\\). Per spazi campionari discreti, la probabilit√† condizionata √® espressa come\n\\[\nP(A \\mid B) = \\frac{| A \\cap B |}{| B |}.\n\\]\n\nExample 18.1 Lanciamo due dadi equilibrati e vogliamo calcolare la probabilit√† che la somma dei punteggi ottenuti sia minore di 8.\nInizialmente, quando non abbiamo ulteriori informazioni, possiamo calcolare la probabilit√† in modo tradizionale. Ci sono 21 risultati possibili con somma minore di 8. Poich√© ci sono 36 possibili combinazioni di lancio dei due dadi, la probabilit√† di ottenere una somma minore di 8 √® 21/36, che equivale a circa 0.58.\nSupponiamo ora di sapere che la somma del lancio di due dadi ha prodotto un risultato dispari. In questo caso, ci sono solo 18 possibili combinazioni di lancio dei due dadi (dato che abbiamo escluso i risultati pari). Tra essi, vi sono 12 risultati che soddisfano la condizione per cui la somma √® minore di 8. Quindi, la probabilit√† di ottenere una somma minore di 8 cambia da circa 0.58 a 12/18, ovvero 0.67 quando consideriamo l‚Äôinformazione aggiuntiva del risultato dispari.\nSvolgiamo il problema in Python.\n\nr = range(1, 7)\nsample = [(i, j) for i in r for j in r]\nsample\n\n[(1, 1),\n (1, 2),\n (1, 3),\n (1, 4),\n (1, 5),\n (1, 6),\n (2, 1),\n (2, 2),\n (2, 3),\n (2, 4),\n (2, 5),\n (2, 6),\n (3, 1),\n (3, 2),\n (3, 3),\n (3, 4),\n (3, 5),\n (3, 6),\n (4, 1),\n (4, 2),\n (4, 3),\n (4, 4),\n (4, 5),\n (4, 6),\n (5, 1),\n (5, 2),\n (5, 3),\n (5, 4),\n (5, 5),\n (5, 6),\n (6, 1),\n (6, 2),\n (6, 3),\n (6, 4),\n (6, 5),\n (6, 6)]\n\n\n\nevent = [roll for roll in sample if sum(roll) &lt; 8]\nprint(f\"{len(event)} / {len(sample)}\")\n\n21 / 36\n\n\n\nsample_odd = [roll for roll in sample if (sum(roll) % 2) != 0]\nsample_odd\n\n[(1, 2),\n (1, 4),\n (1, 6),\n (2, 1),\n (2, 3),\n (2, 5),\n (3, 2),\n (3, 4),\n (3, 6),\n (4, 1),\n (4, 3),\n (4, 5),\n (5, 2),\n (5, 4),\n (5, 6),\n (6, 1),\n (6, 3),\n (6, 5)]\n\n\n\nevent = [roll for roll in sample_odd if sum(roll) &lt; 8]\nprint(f\"{len(event)} / {len(sample_odd)}\")\n\n12 / 18\n\n\nSe applichiamo l‚ÄôEquation¬†18.1, abbiamo: \\(P(A \\cap B)\\) = 12/36, \\(P(B)\\) = 18/36 e\n\\[\nP(A \\mid B) = \\frac{12}{18}.\n\\]\nQuesto esempio illustra come la probabilit√† di un evento possa variare in base alle informazioni aggiuntive di cui disponiamo. Nel secondo caso, avendo l‚Äôinformazione che la somma √® dispari, la probabilit√† di ottenere una somma minore di 8 aumenta notevolmente rispetto al caso iniziale in cui non avevamo questa informazione.\n\n\nExample 18.2 Consideriamo uno screening per la diagnosi precoce del tumore mammario utilizzando un test con determinate caratteristiche:\n\nSensibilit√† del test: 90%. Questo significa che il test classifica correttamente come positivo il 90% delle donne colpite dal cancro al seno.\nSpecificit√† del test: 90%. Ci√≤ indica che il test classifica correttamente come negativo il 90% delle donne che non hanno il cancro al seno.\nPrevalenza del cancro al seno nella popolazione sottoposta allo screening: 1% (0.01). Questo √® il 1% delle donne che ha effettivamente il cancro al seno, mentre il restante 99% (0.99) non ne √® affetto.\n\nOra cerchiamo di rispondere alle seguenti domande:\n\nQual √® la probabilit√† che una donna scelta a caso ottenga una mammografia positiva? Poich√© il 1% delle donne ha il cancro al seno, la probabilit√† di ottenere una mammografia positiva (test positivo) √® pari alla sensibilit√† del test, ovvero 0.90 (cio√® 90%).\nSe la mammografia √® positiva, qual √® la probabilit√† che vi sia effettivamente un tumore al seno?\n\nPer risolvere questo problema, consideriamo un campione di 1000 donne sottoposte al test di screening per il tumore al seno. Di queste 1000 donne:\n\n10 donne (1% del campione) hanno effettivamente il cancro al seno. Per queste 10 donne con il cancro, il test dar√† un risultato positivo (vera positivit√†) in 9 casi (90%).\nPer le restanti 990 donne (99% del campione) che non hanno il cancro al seno, il test dar√† un risultato positivo (falsa positivit√†) in 99 casi (10%).\n\nQuesta situazione pu√≤ essere rappresentata graficamente nel seguente modo:\n\n\n\n\n\n\nFigure¬†18.1: Esiti della mammografia per 1000 donne.\n\n\n\nCombinando i due risultati precedenti, vediamo che il test d√† un risultato positivo per 9 donne che hanno effettivamente il cancro al seno e per 99 donne che non lo hanno, per un totale di 108 risultati positivi su 1000. Pertanto, la probabilit√† di ottenere un risultato positivo al test √® \\(\\frac{108}{1000}\\) = 0.108.\nTuttavia, tra le 108 donne che hanno ottenuto un risultato positivo al test, solo 9 hanno effettivamente il cancro al seno. Quindi, la probabilit√† di avere il cancro al seno, dato un risultato positivo al test, √® pari a \\(\\frac{9}{108}\\) = 0.083, corrispondente all‚Äô8.3%.\nIn questo esempio, la probabilit√† dell‚Äôevento ‚Äúottenere un risultato positivo al test‚Äù √® una probabilit√† non condizionata, poich√© calcoliamo semplicemente la proporzione di risultati positivi nel campione totale. D‚Äôaltra parte, la probabilit√† dell‚Äôevento ‚Äúavere il cancro al seno, dato che il test ha prodotto un risultato positivo‚Äù √® una probabilit√† condizionata, poich√© calcoliamo la proporzione delle donne con il cancro al seno tra quelle che hanno ottenuto un risultato positivo al test.\nQuesto esempio illustra come la conoscenza di ulteriori informazioni (il risultato positivo al test) pu√≤ influenzare la probabilit√† di un evento (avere il cancro al seno), mostrando chiaramente la differenza tra probabilit√† condizionate e non condizionate.\n\n\nExample 18.3 Il paradosso di Monty Hall rappresenta un curioso esempio di come l‚Äôintroduzione di nuove informazioni possa influenzare l‚Äôesito di una situazione probabilistica. Questo famoso problema trae origine dal popolare programma televisivo americano ‚ÄúLet‚Äôs Make a Deal‚Äù e deve la sua notoriet√† al conduttore Monty Hall.\nNel gioco ci sono tre porte chiuse: dietro una si nasconde un‚Äôautomobile, mentre dietro le altre due ci sono delle capre. Inizialmente, il concorrente sceglie una delle tre porte senza aprirla. Successivamente, Monty Hall apre una delle due porte rimaste, rivelando una capra. A questo punto, offre al concorrente la possibilit√† di cambiare la sua scelta iniziale e optare per l‚Äôaltra porta ancora chiusa. Il paradosso si presenta quando si scopre che cambiando la scelta in questa fase, il concorrente aumenta le sue probabilit√† di vincere l‚Äôautomobile, passando da 1/3 a 2/3.\n\n\n\nLo show televisivo Monty Hall.\n\n\nPer confermare questo risultato inaspettato, √® possibile eseguire una simulazione in Python. In questa simulazione, consideriamo due scenari: uno in cui il concorrente mantiene la sua scelta iniziale e un altro in cui cambia la sua scelta dopo che Monty Hall ha svelato una capra. Ripetendo questa simulazione migliaia di volte, possiamo confrontare i risultati empirici e confermare come effettivamente il cambiamento di scelta aumenti le probabilit√† del concorrente di vincere l‚Äôautomobile.\nDi seguito √® riportato lo script di una simulazione progettata per illustrare il paradosso di Monty Hall.\n\nporte = [\n    \"capra1\",\n    \"capra2\",\n    \"macchina\",\n]  # definisco il gioco, scelgo una porta a caso per n volte\ncounter = 0\ncontatore_cambio = 0\nn = 10000\nporta_vincente = \"macchina\"\nfor i in range(n):\n    scelta_casuale = random.choice(porte)\n    porte_rimaste = [x for x in porte if x != scelta_casuale]\n    porta_rivelata = random.choice([x for x in porte_rimaste if x != porta_vincente])\n    porta_alternativa = [\n        x for x in porte if x != scelta_casuale and x != porta_rivelata\n    ]\n    if \"macchina\" in porta_alternativa:\n        contatore_cambio += 1\n    if scelta_casuale == \"macchina\":\n        counter += 1\n\nprint(counter / n)  # quante volte vinco non cambiando porta\nprint(contatore_cambio / n)  # quante volte vinco cambiando porta\n\nQuesto script Python √® stato creato da un gruppo di studenti di Psicometria nell‚ÄôAA 2023-2023. La simulazione mostra che, effettivamente, la probabilit√† di vincere la macchina aumenta quando il concorrente sceglie di cambiare porta.\nEcco una spiegazione del paradosso:\n\nFase Iniziale: Nel gioco, il concorrente deve scegliere una delle tre porte (A, B, C), dietro una delle quali si trova una macchina. Inizialmente, la probabilit√† che la macchina si trovi dietro la porta scelta √® \\(1/3\\), dato che esistono tre possibilit√† ugualmente probabili e solo una contiene la macchina.\nAggiunta di Informazioni: Dopo la scelta iniziale, Monty Hall, che conosce il contenuto dietro ogni porta, apre una delle due porte non scelte, rivelando sempre una capra. Questo passaggio √® fondamentale: non cambia la probabilit√† \\(1/3\\) che la macchina sia dietro la porta originariamente scelta dal concorrente, ma la probabilit√† che la macchina si trovi dietro l‚Äôaltra porta non scelta aumenta ora a \\(2/3\\). Questo aumento di probabilit√† deriva dal fatto che Monty ha scelto deliberatamente una porta con una capra, basando la sua scelta sulla posizione della macchina.\n\nConsideriamo i tre possibili scenari dopo che il concorrente ha scelto la porta A:\n\nLa macchina √® dietro la porta A: La probabilit√† di questo scenario √® \\(1/3\\). Monty pu√≤ aprire sia la porta B che la porta C, poich√© entrambe nascondono una capra. Se il concorrente cambia la sua scelta, perder√†.\nLa macchina √® dietro la porta B: La probabilit√† di questo scenario √® \\(1/3\\). Monty aprir√† la porta C, perch√© sa che la macchina √® dietro la porta B e non pu√≤ rivelarla. Se il concorrente cambia la sua scelta da A a B, vincer√†.\nLa macchina √® dietro la porta C: La probabilit√† di questo scenario √® \\(1/3\\). Monty aprir√† la porta B. Se il concorrente cambia la sua scelta da A a C, vincer√†.\n\nIn conclusione, cambiare la scelta originale porta alla vittoria in due dei tre scenari possibili. Pertanto, la probabilit√† complessiva di vincere cambiando la scelta √® \\(2/3\\).\nQuesto paradosso evidenzia come, in presenza di informazioni aggiuntive, le probabilit√† iniziali possano essere riviste significativamente. √à un classico esempio di come l‚Äôintuizione umana spesso si scontri con i principi della teoria delle probabilit√†, sottolineando l‚Äôimportanza della revisione bayesiana delle probabilit√† alla luce di nuove informazioni.\n\n\n18.3.1 Il paradosso di Simpson\nNel campo della probabilit√† condizionata, uno dei fenomeni pi√π interessanti e, nel contempo, pi√π controintuitivi, √® rappresentato dal paradosso di Simpson. Il paradosso di Simpson √® un fenomeno statistico in cui una tendenza che appare in diversi gruppi separati di dati scompare o si inverte quando i dati vengono combinati. Questo paradosso mette in luce l‚Äôimportanza di considerare le variabili confondenti e di analizzare i dati con attenzione per evitare conclusioni errate.\n\nExample 18.4 Due psicoterapeuti, Rossi e Bianchi, praticano due tipi di terapie: terapia per disturbi d‚Äôansia e coaching per migliorare le prestazioni lavorative. Ogni terapia pu√≤ avere un esito positivo o negativo.\nI rispettivi bilanci dei due terapeuti sono riportati nelle seguenti tabelle.\nRossi\n\n\n\nTipo di terapia\nSuccesso\nFallimento\n\n\n\n\nDisturbi d‚Äôansia\n70\n20\n\n\nCoaching lavorativo\n10\n0\n\n\nTotale\n80\n20\n\n\n\nBianchi\n\n\n\nTipo di terapia\nSuccesso\nFallimento\n\n\n\n\nDisturbi d‚Äôansia\n2\n8\n\n\nCoaching lavorativo\n81\n9\n\n\nTotale\n83\n17\n\n\n\nRossi ha un tasso di successo superiore a Bianchi nella terapia per i disturbi d‚Äôansia: 70 su 90 rispetto a 2 su 10. Anche nel coaching lavorativo, Rossi ha un tasso di successo superiore: 10 su 10 rispetto a 81 su 90. Tuttavia, se aggregiamo i dati dei due tipi di terapia per confrontare i tassi di successo globali, Rossi √® efficace in 80 su 100 terapie, mentre Bianchi in 83 su 100: il tasso di successo globale di Bianchi risulta superiore!\nQuesto fenomeno √® un esempio del paradosso di Simpson, dove una tendenza osservata in diversi gruppi si inverte quando i gruppi sono combinati.\nPer essere pi√π precisi, possiamo calcolare i tassi di successo per ciascun terapeuta e per ciascun tipo di terapia, oltre al tasso di successo globale.\n\nRossi\n\nTasso di successo in terapia per disturbi d‚Äôansia: \\(\\frac{70}{70+20} = \\frac{70}{90} \\approx 0.778\\)\nTasso di successo in coaching lavorativo: \\(\\frac{10}{10+0} = \\frac{10}{10} = 1\\)\nTasso di successo globale: \\(\\frac{70+10}{70+20+10+0} = \\frac{80}{100} = 0.8\\)\n\nBianchi\n\nTasso di successo in terapia per disturbi d‚Äôansia: \\(\\frac{2}{2+8} = \\frac{2}{10} = 0.2\\)\nTasso di successo in coaching lavorativo: \\(\\frac{81}{81+9} = \\frac{81}{90} \\approx 0.9\\)\nTasso di successo globale: \\(\\frac{2+81}{2+8+81+9} = \\frac{83}{100} = 0.83\\)\n\n\nQuello che sta succedendo √® che Rossi, presumibilmente a causa della sua reputazione come terapeuta pi√π esperto, sta effettuando un numero maggiore di terapie per disturbi d‚Äôansia, che sono intrinsecamente pi√π complesse e con una probabilit√† di successo variabile rispetto al coaching lavorativo. Il suo tasso di successo globale √® inferiore non a causa di una minore abilit√† in un particolare tipo di terapia, ma perch√© una frazione maggiore delle sue terapie riguarda casi pi√π complessi.\nL‚Äôaggregazione dei dati tra diversi tipi di terapia presenta un quadro fuorviante delle abilit√† dei terapeuti perch√© perdiamo l‚Äôinformazione su quale terapeuta tende a effettuare quale tipo di terapia. Quando sospettiamo la presenza di variabili di confondimento, come ad esempio il tipo di terapia in questo contesto, √® fondamentale analizzare i dati in modo disaggregato per comprendere con precisione la dinamica in atto.",
    "crumbs": [
      "Probabilit√†",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Probabilit√† condizionata</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_3/03_bayes_theorem.html",
    "href": "chapters/chapter_3/03_bayes_theorem.html",
    "title": "19¬† Il teorema di Bayes",
    "section": "",
    "text": "19.1 Introduzione\nIn questo capitolo esploreremo il teorema di Bayes, un fondamentale risultato della teoria delle probabilit√† che ci permette di calcolare le probabilit√† a posteriori di eventi ipotetici, dati i loro valori a priori e nuove informazioni. In altre parole, ci consente di aggiornare razionalmente le nostre conoscenze alla luce di nuove evidenze.",
    "crumbs": [
      "Probabilit√†",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Il teorema di Bayes</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_3/03_bayes_theorem.html#introduzione",
    "href": "chapters/chapter_3/03_bayes_theorem.html#introduzione",
    "title": "19¬† Il teorema di Bayes",
    "section": "19.2 Introduzione",
    "text": "19.2 Introduzione\nIn questo capitolo esploreremo il teorema di Bayes, un fondamentale risultato della teoria delle probabilit√† che ci permette di calcolare le probabilit√† a posteriori di eventi ipotetici, dati i loro valori a priori e nuove informazioni. In altre parole, ci consente di aggiornare razionalmente le nostre conoscenze alla luce di nuove evidenze.",
    "crumbs": [
      "Probabilit√†",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Il teorema di Bayes</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_3/03_bayes_theorem.html#storia",
    "href": "chapters/chapter_3/03_bayes_theorem.html#storia",
    "title": "19¬† Il teorema di Bayes",
    "section": "19.2 Storia",
    "text": "19.2 Storia\nIl teorema di Bayes, cos√¨ denominato in onore del Reverendo Thomas Bayes, un matematico e filosofo del XVIII secolo, rappresenta uno dei concetti fondamentali nel campo della statistica e del calcolo delle probabilit√†. Sebbene Bayes non abbia pubblicato il suo lavoro durante la sua vita, ritenendolo non abbastanza significativo, fu il suo amico Richard Price a riconoscere il valore delle sue scoperte. Price non solo edit√≤ il manoscritto inedito di Bayes ma lo arricch√¨ significativamente prima di sottometterlo per la pubblicazione nelle ‚ÄúPhilosophical Transactions‚Äù nel 1763. Questa pubblicazione non solo introdusse il teorema di Bayes al mondo scientifico ma forn√¨ anche una base filosofica per quello che sarebbe poi diventato noto come l‚Äôapproccio bayesiano alla statistica.\nParallelamente e indipendentemente dai lavori di Bayes, Pierre-Simon Laplace, un eminente matematico e astronomo francese, formul√≤ concetti simili. Nel 1774, e pi√π dettagliatamente nella sua opera ‚ÄúTh√©orie analytique des probabilit√©s‚Äù del 1812, Laplace esplor√≤ l‚Äôuso della probabilit√† condizionale per aggiornare le conoscenze precedenti (probabilit√† a priori) sulla base di nuove evidenze (probabilit√† a posteriori). Sebbene Laplace abbia sviluppato questi principi senza conoscere il lavoro di Bayes, i suoi contributi hanno notevolmente esteso l‚Äôapplicazione e l‚Äôinterpretazione delle statistiche bayesiane.\nIl teorema di Bayes, nella sua essenza, fornisce un meccanismo matematico per aggiornare le probabilit√† iniziali di un‚Äôipotesi in base all‚Äôosservazione di nuove evidenze. Questo processo di revisione continua della probabilit√† si basa sulla combinazione di conoscenze preesistenti (il prior) con informazioni appena acquisite (la likelihood), per produrre una nuova comprensione (il posterior). Tale meccanismo riflette un approccio olistico alla conoscenza, che considera la comprensione come un processo dinamico e iterativo.\nNel contesto contemporaneo, l‚Äôimportanza e l‚Äôapplicabilit√† del teorema di Bayes vanno ben oltre la teoria della probabilit√† e la statistica, influenzando vari campi come l‚Äôintelligenza artificiale, l‚Äôapprendimento automatico, la psicologia, la medicina e persino la presa di decisioni nella vita quotidiana. La capacit√† di aggiornare costantemente le nostre convinzioni in base a nuove informazioni √® fondamentale in un‚Äôera caratterizzata da un flusso incessante di dati. In modo emblematico, potremmo affermare che rilevanza universale del teorema di Bayes nella comprensione dei fenomeni e nella previsione degli eventi √® enfatizzata nel titolo del libro di Tom Chivers pubblicato nel 2024: ‚ÄúEverything is predictable: how bayesian statistics explain our world‚Äù.",
    "crumbs": [
      "Probabilit√†",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Il teorema di Bayes</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_3/03_bayes_theorem.html#teorema",
    "href": "chapters/chapter_3/03_bayes_theorem.html#teorema",
    "title": "19¬† Il teorema di Bayes",
    "section": "19.3 Teorema",
    "text": "19.3 Teorema\nConsideriamo una situazione in cui lo spazio degli eventi possibili, \\(\\Omega\\), √® diviso in due eventi distinti e mutualmente esclusivi, denominati ipotesi \\(H_1\\) e \\(H_2\\). Supponiamo di avere gi√† una certa comprensione di questi eventi, espressa attraverso le loro probabilit√† a priori \\(P(H_1)\\) e \\(P(H_2)\\). A questo punto, introduciamo un nuovo evento \\(E\\), la cui occorrenza √® accompagnata da una probabilit√† non nulla e per il quale conosciamo le probabilit√† condizionate \\(P(E \\mid H_1)\\) e \\(P(E \\mid H_2)\\), che indicano quanto sia probabile osservare \\(E\\) assumendo che una delle due ipotesi sia vera. Se \\(E\\) si verifica, siamo interessati a determinare le probabilit√† a posteriori \\(P(H_1 \\mid E)\\) e \\(P(H_2 \\mid E)\\) delle nostre ipotesi alla luce di questa nuova evidenza.\nLa seguente illustrazione rappresenta come lo spazio totale degli eventi si suddivide tra le ipotesi \\(H_1\\) e \\(H_2\\), con l‚Äôevidenza \\(E\\) posizionata all‚Äôinterno di questo contesto.\n\n\n\n\n\n\nFigure¬†19.1: Partizione dello spazio campionario per il teorema di Bayes.\n\n\n\nPer calcolare la probabilit√† a posteriori dell‚Äôipotesi 1 data l‚Äôosservazione di \\(E\\), utilizziamo la formula:\n\\[\nP(H_1 \\mid E) = \\frac{P(E \\cap H_1)}{P(E)}.\n\\]\nQuesto calcolo pu√≤ essere semplificato sfruttando la definizione di probabilit√† condizionata, che ci permette di sostituire \\(P(E \\cap H_1)\\) con \\(P(E \\mid H_1)P(H_1)\\). Applicando questa sostituzione, otteniamo:\n\\[\nP(H_1 \\mid E) = \\frac{P(E \\mid H_1) P(H_1)}{P(E)}.\n\\]\nDato che \\(H_1\\) e \\(H_2\\) si escludono a vicenda, la probabilit√† totale di \\(E\\) pu√≤ essere espressa come la somma delle probabilit√† di \\(E\\) occorrente in concomitanza con ciascuna ipotesi, utilizzando il teorema della probabilit√† totale:\n\\[\nP(E) = P(E \\mid H_1)P(H_1) + P(E \\mid H_2)P(H_2).\n\\]\nIncorporando questi valori nella formula di Bayes, giungiamo a:\n\\[\nP(H_1 \\mid E) = \\frac{P(E \\mid H_1)P(H_1)}{P(E \\mid H_1)P(H_1) + P(E \\mid H_2)P(H_2)}.\n\\tag{19.1}\\]\nQuesta espressione costituisce l‚Äôessenza della formula di Bayes per il caso semplificato in cui le ipotesi si limitano a due eventi mutualmente esclusivi, \\(H_1\\) e \\(H_2\\).\nNel quadro delle probabilit√† discrete, questa formula pu√≤ essere generalizzata per accogliere un insieme pi√π ampio di ipotesi che formano una partizione completa dello spazio degli eventi \\(\\Omega\\), dove ogni \\(E\\) rappresenta un evento con probabilit√† maggiore di zero. Per ogni ipotesi \\(H_i\\) all‚Äôinterno di un insieme numerabile, la formula di Bayes si estende come segue:\n\\[\nP(H_i \\mid E) = \\frac{P(E \\mid H_i)P(H_i)}{\\sum_{j=1}^{\\infty}P(E \\mid H_j)P(H_j)}.\n\\tag{19.2}\\]\nQui, il denominatore agisce come un fattore di normalizzazione che integra i prodotti delle probabilit√† a priori e delle verosimiglianze associate a ogni ipotesi considerata.\nPer variabili continue, la formula di Bayes assume una forma integrale, adattandosi a situazioni in cui le ipotesi \\(H_i\\) rappresentano valori in un continuum. In questo contesto, la formula diventa:\n\\[\nP(H_i \\mid E) = \\frac{P(E \\mid H_i) \\cdot P(H_i)}{\\int P(E \\mid H) \\cdot P(H) \\, dH},\n\\tag{19.3}\\]\noffrendo un framework potente per aggiornare le probabilit√† a posteriori di ipotesi continue basate su nuove evidenze, sottolineando l‚Äôimportanza della formula di Bayes non solo come strumento matematico, ma anche come filosofia di apprendimento continuo e adattamento alle nuove informazioni.\n\n19.3.1 Interpretazione della Formula di Bayes\nLa formula di Bayes si articola in tre elementi fondamentali che ne facilitano la comprensione e l‚Äôapplicazione in diversi campi di studio:\n\nProbabilit√† a Priori, \\(P(H)\\): Questa componente riflette la nostra valutazione preliminare riguardo la verosimiglianza dell‚Äôipotesi \\(H\\) prima di prendere in esame nuove evidenze \\(E\\). Essa incarna il livello di credibilit√† o fiducia attribuita all‚Äôipotesi, basandosi su conoscenze preesistenti o su deduzioni logiche. In sostanza, la probabilit√† a priori quantifica le nostre convinzioni pregresse o le aspettative iniziali su quanto sia probabile che l‚Äôipotesi sia vera.\nProbabilit√† a Posteriori, \\(P(H \\mid E)\\): Questo valore aggiorna la nostra fiducia nell‚Äôipotesi \\(H\\) in seguito all‚Äôosservazione dell‚Äôevidenza \\(E\\). In termini pi√π intuitivi, rappresenta il livello di convinzione ricalibrato in \\(H\\) dopo aver considerato l‚Äôevidenza. La formula di Bayes ci offre un meccanismo matematicamente rigoroso per modulare questa probabilit√† alla luce delle nuove informazioni ricevute.\nVerosimiglianza, \\(P(E \\mid H)\\): La verosimiglianza esprime la probabilit√† di rilevare l‚Äôevidenza \\(E\\) dato che l‚Äôipotesi \\(H\\) sia vera. √à un indice di quanto l‚Äôevidenza supporti o confermi l‚Äôipotesi. Un valore elevato di verosimiglianza indica che l‚Äôevidenza √® fortemente in linea o prevista dalla veridicit√† dell‚Äôipotesi.\n\nGrazie alla formula di Bayes, possiamo adottare un processo di aggiornamento continuo delle nostre credenze in base a nuove informazioni, promuovendo un metodo dinamico per navigare tra conoscenza e incertezza. Questa metodologia ci fornisce un approccio strutturato per rivedere e adattare le nostre convinzioni riguardo l‚Äôipotesi \\(H\\) di fronte a nuovi dati o evidenze \\(E\\). La capacit√† di rielaborare costantemente le nostre aspettative in funzione di informazioni aggiuntive si rivela essenziale in una vasta gamma di ambiti, inclusi l‚Äôintelligenza artificiale, la ricerca statistica, le discipline scientifiche e umanistiche. Tale prassi ci consente di prendere decisioni pi√π informate, di interpretare con maggiore precisione i dati disponibili e di affinare significativamente le nostre previsioni e comprensioni del mondo circostante.",
    "crumbs": [
      "Probabilit√†",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Il teorema di Bayes</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_3/03_bayes_theorem.html#alcuni-esempi",
    "href": "chapters/chapter_3/03_bayes_theorem.html#alcuni-esempi",
    "title": "19¬† Il teorema di Bayes",
    "section": "19.4 Alcuni esempi",
    "text": "19.4 Alcuni esempi\nEsamineremo alcuni esempi basilari per illustrare in maniera pi√π chiara il modo in cui il teorema di Bayes viene applicato e funziona.\n\nExample 19.1 Consideriamo il caso in cui osserviamo una persona con i capelli lunghi e desideriamo valutare la probabilit√† che questa persona sia femminile. In termini formali, definiamo \\(H = \\text{\"donna\"}\\) e \\(E = \\text{\"capelli lunghi\"}\\), e miriamo a stimare \\(P(H \\mid E)\\).\nLe nostre conoscenze preliminari includono:\n\nLa probabilit√† a priori che la persona osservata sia una donna, \\(P(H) = 0.5\\),\nLa probabilit√† generale di osservare qualcuno con i capelli lunghi, \\(P(E) = 0.4\\),\nLa probabilit√† di avere i capelli lunghi dato che la persona √® una donna, \\(P(E \\mid H) = 0.7\\).\n\nApplicando la formula di Bayes, possiamo determinare:\n\\[ P(H \\mid E) = \\frac{P(H) \\cdot P(E \\mid H)}{P(E)} = \\frac{0.5 \\cdot 0.7}{0.4} = 0.875. \\]\nQuesto risultato ci mostra che, alla luce delle informazioni a nostra disposizione, la probabilit√† che una persona con i capelli lunghi sia una donna √® dell‚Äô0.875. In altre parole, prima di osservare l‚Äôevidenza (i capelli lunghi), partivamo da una conoscenza di base secondo cui c‚Äôera una chance su due (50%) che la persona fosse una donna. Dopo aver considerato l‚Äôevidenza dei capelli lunghi, abbiamo aggiornato la nostra stima alla probabilit√† ‚Äúa posteriori‚Äù di 0.875, riflettendo cos√¨ un incremento della nostra convinzione che la persona sia una donna basandoci su quest‚Äôultima osservazione.\n\n\nExample 19.2 Un esempio pratico che illustra efficacemente l‚Äôuso del teorema di Bayes √® l‚Äôanalisi del rapporto tra la mammografia e la diagnosi del cancro al seno, gi√† considerato in precedenza. In questo contesto, si considerano due ipotesi: la presenza della malattia, indicata con \\(M^+\\), e l‚Äôassenza della malattia, denotata con \\(M^-\\). L‚Äôevidenza in questo caso √® rappresentata dal risultato positivo di un test di mammografia, che indicheremo con \\(T^+\\). Applicando la formula di Bayes, possiamo esprimere la probabilit√† di avere il cancro al seno dato un risultato positivo al test come segue:\n\\[\nP(M^+ \\mid T^+) = \\frac{P(T^+ \\mid M^+) \\cdot P(M^+)}{P(T^+ \\mid M^+) \\cdot P(M^+) + P(T^+ \\mid M^-) \\cdot P(M^-)},\n\\]\ndove:\n\n\\(P(M^+ \\mid T^+)\\) √® la probabilit√† di avere il cancro (\\(M^+\\)) dato un risultato positivo al test (\\(T^+\\)),\n\\(P(T^+ \\mid M^+)\\) rappresenta la probabilit√† che il test di mammografia risulti positivo (\\(T^+\\)) in presenza effettiva del cancro (\\(M^+\\)),\n\\(P(M^+)\\) denota la probabilit√† a priori che una persona abbia il cancro prima di sottoporsi al test,\n\\(P(T^+ \\mid M^-)\\) indica la probabilit√† che il test risulti positivo (\\(T^+\\)) nonostante l‚Äôassenza del cancro (\\(M^-\\)); dal momento che la specificit√† √® data e uguale a 0.9, possiamo calcolare la probabilit√† di un falso positivo come segue:\n\n\\[ P(T^+ \\mid M^-) = 1 - \\text{Specificit√†} = 1 - 0.9 = 0.1. \\]\nQuindi, in questo contesto, \\(P(T^+ \\mid M^-) = 0.1\\) significa che c‚Äô√® un 10% di probabilit√† che il test diagnostichi erroneamente la presenza del cancro in una persona sana.\n\n\\(P(M^-)\\) rappresenta la probabilit√† a priori che una persona non sia affetta da cancro prima di effettuare il test.\n\nInserendo i valori specifici del contesto analizzato otteniamo:\n\\[\n\\begin{align}\nP(M^+ \\mid T^+) &= \\frac{0.9 \\cdot 0.01}{0.9 \\cdot 0.01 + 0.1 \\cdot 0.99} \\\\\n&= \\frac{9}{108} \\\\\n&\\approx 0.083.\n\\end{align}\n\\]\nQuesto calcolo dimostra che, considerando una mammografia con risultato positivo ottenuta tramite un test con una sensibilit√† del 90% e una specificit√† del 90%, la probabilit√† che il paziente sia effettivamente affetto da cancro al seno √® approssimativamente dell‚Äô8.3%.\n\n\n19.4.1 Il Valore Predittivo di un Test di Laboratorio\nL‚Äôapplicazione del teorema di Bayes nell‚Äôinterpretazione dei risultati dei test di laboratorio √® cruciale nella pratica clinica. Questo teorema consente di calcolare la probabilit√† che un individuo sia affetto da una specifica malattia dopo un risultato positivo al test, nonch√© la probabilit√† di non essere malati in caso di esito negativo. La comprensione di tre elementi √® fondamentale per questo calcolo: la prevalenza della malattia, la sensibilit√† e la specificit√† del test.\n\nPrevalenza: Si riferisce alla percentuale di individui in una popolazione che sono affetti da una certa malattia in un determinato momento. Viene espressa come percentuale o frazione della popolazione. Per esempio, una prevalenza del 0.5% indica che su mille persone, cinque sono affette dalla malattia.\nSensibilit√†: Indica la capacit√† del test di identificare correttamente la malattia negli individui malati. Viene calcolata come la frazione di veri positivi (individui malati correttamente identificati) sul totale degli individui malati. La formula della sensibilit√† (\\(Sens\\)) √® la seguente:\n\n\\[ Sens = \\frac{TP}{TP + FN}, \\]\ndove \\(TP\\) rappresenta i veri positivi e \\(FN\\) i falsi negativi. Pertanto, la sensibilit√† misura la probabilit√† che il test risulti positivo se la malattia √® effettivamente presente.\n\nSpecificit√†: Misura la capacit√† del test di riconoscere gli individui sani, producendo un risultato negativo per chi non √® affetto dalla malattia. Si calcola come la frazione di veri negativi (individui sani correttamente identificati) sul totale degli individui sani. La specificit√† (\\(Spec\\)) si definisce come:\n\n\\[ Spec = \\frac{TN}{TN + FP}, \\]\ndove \\(TN\\) sono i veri negativi e \\(FP\\) i falsi positivi. Cos√¨, la specificit√† rappresenta la probabilit√† che il test risulti negativo in assenza della malattia.\nQuesta tabella riassume la terminologia:\n\n\n\n\n\n\n\n\n\n\n\\(T^+\\)\n\\(T^-\\)\nTotale\n\n\n\n\n\\(M^+\\)\n\\(P(T^+ \\cap M^+)\\)  (Sensibilit√†)\n\\(P(T^- \\cap M^+)\\)  (1 - Sensibilit√†)\n\\(P(M^+)\\)\n\n\n\\(M^-\\)\n\\(P(T^+ \\cap M^-)\\)  (1 - Specificit√†)\n\\(P(T^- \\cap M^-)\\)  (Specificit√†)\n\\(P(M^-)\\)\n\n\nTotale\n\\(P(T^+)\\)\n\\(P(T^-)\\)\n1\n\n\n\ndove \\(T^+\\) e \\(T^-\\) indicano rispettivamente un risultato positivo o negativo del test, mentre \\(M^+\\) e \\(M^-\\) la presenza o assenza effettiva della malattia. In questa tabella, i totali marginali rappresentano:\n\nTotale per \\(M^+\\) e \\(M^-\\) (ultima colonna): La probabilit√† totale di avere la malattia (\\(P(M^+)\\)) e la probabilit√† totale di non avere la malattia (\\(P(M^-)\\)), rispettivamente. Questi valori sono calcolati sommando le probabilit√† all‚Äôinterno di ciascuna riga.\nTotale per \\(T^+\\) e \\(T^-\\) (ultima riga): La probabilit√† totale di un risultato positivo al test (\\(P(T^+)\\)) e la probabilit√† totale di un risultato negativo al test (\\(P(T^-)\\)), rispettivamente. Questi valori sono calcolati sommando le probabilit√† all‚Äôinterno di ciascuna colonna.\nTotale generale (angolo in basso a destra): La somma di tutte le probabilit√†, che per definizione √® 1, rappresentando l‚Äôintera popolazione o il set di casi considerati.\n\nMediante il teorema di Bayes, possiamo usare queste informazioni per stimare la probabilit√† post-test di avere o non avere la malattia basandoci sul risultato del test, fornendo cos√¨ una base razionale per decisioni diagnostiche e terapeutiche.\nLa probabilit√† post-test che un individuo sia malato dato un risultato positivo del test √® calcolata come:\n\\[\nP(M^+ \\mid T^+) = \\frac{P(T^+ \\mid M^+) \\cdot P(M^+)}{P(T^+ \\mid M^+) \\cdot P(M^+) + P(T^+ \\mid M^-) \\cdot P(M^-)}.\n\\]\nQuesta formula rappresenta il valore predittivo positivo, indicando la probabilit√† che un individuo sia realmente malato se il test √® positivo. Analogamente, il valore predittivo negativo, che √® la probabilit√† che un individuo non sia malato dato un risultato negativo del test, si calcola come:\n\\[\nP(M^- \\mid T^-) = \\frac{P(T^- \\mid M^-) \\cdot (1 - P(M^+))}{P(T^- \\mid M^-) \\cdot (1 - P(M^+)) + P(T^- \\mid M^+) \\cdot P(M^+)}.\n\\]\nQuesti valori predittivi sono essenziali per valutare l‚Äôefficacia di un test diagnostico, aiutando a determinare quanto sia affidabile un risultato positivo o negativo nel contesto di una specifica popolazione e malattia.\n\n\n\n\n\n\nFigure¬†19.2\n\n\n\n\nExample 19.3 Calcoliamo ora il valore predittivo del test positivo e il valore predittivo del test negativo per i dati dell‚Äôesempio sulla mammografia e cancro al seno.\n\ndef positive_predictive_value_of_diagnostic_test(sens, spec, prev):\n    return (sens * prev) / (sens * prev + (1 - spec) * (1 - prev))\n\nValore Predittivo Positivo (Positive Predictive Value, PPV): Questa misura indica la probabilit√† che un individuo sia effettivamente malato se ha ricevuto un risultato positivo al test. La formula che hai fornito per il PPV √®:\n\\[ PPV = \\frac{Sens \\cdot Prev}{Sens \\cdot Prev + (1 - Spec) \\cdot (1 - Prev)} \\]\nQui, sens rappresenta la sensibilit√† del test, spec la specificit√†, e prev la prevalenza della malattia nella popolazione. La formula riflette il calcolo del PPV basato sul teorema di Bayes.\n\ndef negative_predictive_value_of_diagnostic_test(sens, spec, prev):\n    return (spec * (1 - prev)) / (spec * (1 - prev) + (1 - sens) * prev)\n\nValore Predittivo Negativo (Negative Predictive Value, NPV): Questa misura indica la probabilit√† che un individuo non sia malato se ha ricevuto un risultato negativo al test. La formula per il NPV √®:\n\\[ NPV = \\frac{Spec \\cdot (1 - Prev)}{Spec \\cdot (1 - Prev) + (1 - Sens) \\cdot Prev} \\]\nAnche in questo caso, sens, spec, e prev hanno gli stessi significati menzionati sopra. Questa formula calcola il NPV basandosi sul teorema di Bayes.\nInseriamo i dati del problema.\n\nsens = 0.9  # sensibilit√†\nspec = 0.9  # specificit√†\nprev = 0.01  # prevalenza\n\nIl valore predittivo del test positivo √®:\n\nres_pos = positive_predictive_value_of_diagnostic_test(sens, spec, prev)\nprint(f\"P(M+ | T+) = {round(res_pos, 3)}\")\n\nP(M+ | T+) = 0.083\n\n\nIl valore predittivo del test negativo √®:\n\nres_neg = negative_predictive_value_of_diagnostic_test(sens, spec, prev)\nprint(f\"P(M- | T-) = {round(res_neg, 3)}\")\n\nP(M- | T-) = 0.999\n\n\n\n\nExample 19.4 Consideriamo ora un altro esempio relativo al test antigenico rapido per il virus SARS-CoV-2, che pu√≤ essere eseguito mediante tampone nasale, tampone naso-orofaringeo o campione di saliva. L‚ÄôIstituto Superiore di Sanit√† ha pubblicato un documento il 5 novembre 2020, nel quale viene sottolineato che, fino a quel momento, i dati disponibili sui vari test sono quelli forniti dai produttori: la sensibilit√† varia tra il 70% e l‚Äô86%, mentre la specificit√† si attesta tra il 95% e il 97%.\nPer fare un esempio, consideriamo i dati di un certo momento temporale. Nella settimana tra il 17 e il 23 marzo 2023, in Italia, il numero di individui positivi al virus √® stato stimato essere di 138.599 (fonte: Il Sole 24 Ore). Questo dato corrisponde a una prevalenza di circa 0.2% su una popolazione totale di circa 59 milioni di persone.\n\n prev = 138599 / 59000000\n prev\n\n0.002349135593220339\n\n\nL‚Äôobiettivo √® determinare la probabilit√† di essere effettivamente affetti da Covid-19, dato un risultato positivo al test antigenico rapido, ossia \\(P(M^+ \\mid T^+)\\). Per raggiungere questo scopo, useremo la formula relativa al valore predittivo positivo del test.\n\nsens = (0.7 + 0.86) / 2  # sensibilit√†\nspec = (0.95 + 0.97) / 2 # specificit√†\n\nres_pos = positive_predictive_value_of_diagnostic_test(sens, spec, prev)\nprint(f\"P(M+ | T+) = {round(res_pos, 3)}\")\n\nP(M+ | T+) = 0.044\n\n\nPertanto, se il risultato del tampone √® positivo, la probabilit√† di essere effettivamente affetti da Covid-19 √® solo del 4.4%, approssimativamente.\nSe la prevalenza fosse 100 volte superiore (cio√®, pari al 23.5%), la probabilit√† di avere il Covid-19, dato un risultato positivo del tampone, aumenterebbe notevolmente e sarebbe pari a circa l‚Äô86%.\n\nprev = 138599 / 59000000 * 100\n\nres_pos = positive_predictive_value_of_diagnostic_test(sens, spec, prev)\nprint(f\"P(M+ | T+) = {round(res_pos, 3)}\")\n\nP(M+ | T+) = 0.857\n\n\nSe il risultato del test fosse negativo, considerando la prevalenza stimata del Covid-19 nella settimana dal 17 al 23 marzo 2023, la probabilit√† di non essere infetto sarebbe del 99.9%, approssimativamente.\n\nsens = (0.7 + 0.86) / 2  # sensibilit√†\nspec = (0.95 + 0.97) / 2  # specificit√†\nprev = 138599 / 59000000  # prevalenza\n\nres_neg = negative_predictive_value_of_diagnostic_test(sens, spec, prev)\nprint(f\"P(M- | T-) = {round(res_neg, 3)}\")\n\nP(M- | T-) = 0.999\n\n\nTuttavia, un‚Äôesito del genere non dovrebbe sorprenderci, considerando che la prevalenza della malattia √® molto bassa; in altre parole, il risultato negativo conferma una situazione gi√† presunta prima di sottoporsi al test. Il vero ostacolo, specialmente nel caso di malattie rare come il Covid-19 in quel periodo specifico, non risiede tanto nell‚Äôasserire l‚Äôassenza della malattia quanto piuttosto nel confermarne la presenza.",
    "crumbs": [
      "Probabilit√†",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Il teorema di Bayes</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_3/03_bayes_theorem.html#commenti-e-considerazioni-finali",
    "href": "chapters/chapter_3/03_bayes_theorem.html#commenti-e-considerazioni-finali",
    "title": "19¬† Il teorema di Bayes",
    "section": "19.5 Commenti e considerazioni finali",
    "text": "19.5 Commenti e considerazioni finali\nLa riflessione epistemologica contemporanea ha ribadito che la conoscenza non pu√≤ essere vista come una certezza inconfutabile o una garanzia razionale di verit√†. Invece, essa emerge come una serie di decisioni prese all‚Äôinterno di un contesto di incertezza. Questa comprensione √® particolarmente pertinente nel campo della ricerca scientifica, dove n√© la logica deduttiva n√© le rigorose dimostrazioni matematiche sono sufficienti. Di conseguenza, la scienza necessita di una ‚Äúlogica dell‚Äôincertezza,‚Äù che √® efficacemente fornita dalla teoria delle probabilit√†, e pi√π specificamente, dal teorema di Bayes.\nIl teorema di Bayes ci offre un framework per interpretare la probabilit√† come una valutazione soggettiva influenzata da diversi fattori condizionanti. In pratica, il teorema esprime la probabilit√† a posteriori \\(P(H_i \\mid E)\\) come un risultato derivato dalla combinazione della probabilit√† a priori \\(P(H_i)\\) e della verosimiglianza \\(P(E \\mid H_i)\\). Questa formulazione sottolinea che la nostra valutazione probabilistica di una data ipotesi \\(H_i\\) √® modulata sia dall‚Äôevidenza empirica \\(E\\) che dalle nostre credenze a priori \\(P(H_i)\\).\nPoich√© la probabilit√† √® una valutazione intrinsecamente soggettiva, √® possibile che diversi osservatori arrivino a conclusioni differenti. Tuttavia, il teorema di Bayes fornisce un meccanismo razionale‚Äîconosciuto come ‚Äúaggiornamento bayesiano‚Äù‚Äîper ricalibrare queste credenze in risposta a nuove informazioni o evidenze.\nL‚Äôapproccio bayesiano offre strumenti per valutare l‚Äôefficacia di diverse strategie di trattamento o interventi in psicologia clinica. Per esempio, se si afferma che la meditazione mattutina √® efficace nel trattamento della depressione, √® essenziale valutare questa affermazione nel contesto di tutte le evidenze disponibili, compresi i casi in cui la meditazione non ha portato a miglioramenti.\nIn sintesi, la metodologia bayesiana fornisce una cornice robusta per l‚Äôaggiornamento delle probabilit√† in presenza di nuove informazioni, offrendo spunti importanti sia per la ricerca che per la pratica clinica in psicologia.\nNel contesto di questo capitolo, abbiamo focalizzato la nostra discussione sul teorema di Bayes nel caso delle variabili casuali discrete. Tuttavia, nel prossimo capitolo, esploreremo come il teorema pu√≤ essere applicato in modo pi√π intuitivo alle variabili casuali continue.",
    "crumbs": [
      "Probabilit√†",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Il teorema di Bayes</span>"
    ]
  },
  {
    "objectID": "chapters/chapter_3/03_bayes_theorem.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/chapter_3/03_bayes_theorem.html#informazioni-sullambiente-di-sviluppo",
    "title": "19¬† Il teorema di Bayes",
    "section": "19.6 Informazioni sull‚ÄôAmbiente di Sviluppo",
    "text": "19.6 Informazioni sull‚ÄôAmbiente di Sviluppo\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -m\n\nLast updated: Sat Mar 16 2024\n\nPython implementation: CPython\nPython version       : 3.11.8\nIPython version      : 8.22.2\n\nCompiler    : Clang 16.0.6 \nOS          : Darwin\nRelease     : 23.4.0\nMachine     : arm64\nProcessor   : arm\nCPU cores   : 8\nArchitecture: 64bit\n\nseaborn   : 0.13.2\npandas    : 2.2.1\nmatplotlib: 3.8.3\nnumpy     : 1.26.4\narviz     : 0.17.0\n\nWatermark: 2.4.3",
    "crumbs": [
      "Probabilit√†",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Il teorema di Bayes</span>"
    ]
  }
]