<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Corrado Caudek">

<title>38&nbsp; La verosimiglianza – Psicometria</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/probability/15_simulation.html" rel="next">
<link href="../../chapters/probability/13_qq_plot.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/probability/introduction_probability.html">Probabilità</a></li><li class="breadcrumb-item"><a href="../../chapters/probability/14_likelihood.html"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Psicometria</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ccaudek/psicometria/" title="Eseguire il codice" class="quarto-navigation-tool px-1" aria-label="Eseguire il codice"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Attiva/disattiva la modalità oscura"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benvenuti</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../prefazione.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Programmazione</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../programmazione2024.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Calendario delle lezioni</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Python</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/introduction_python.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/00_prelims.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Preliminari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/01_python_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Python (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/02_python_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Python (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/03_numpy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">NumPy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/04_pandas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Pandas (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/05_pandas_aggregate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Pandas (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/06_pandas_functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pandas (3)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/07_matplotlib.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Matplotlib</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/08_seaborn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Seaborn</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Fondamenti</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/introduction_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/00_uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Abbracciare l’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/01_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Concetti chiave</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/02_measurement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">La misurazione in psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/03_data_analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">L’analisi dei dati psicologici</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">EDA</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/introduction_eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/01_project_structure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Le fasi del progetto di analisi dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/02_data_cleaning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Flusso di lavoro per la pulizia dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/03_dplyr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Data tidying</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/04_exploring_qualitative_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Esplorare i dati qualitativi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/05_exploring_numeric_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Esplorare i dati numerici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/06_data_visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Principi della visualizzazione dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/07_loc_scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Indicatori di tendenza centrale e variabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/08_correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Relazioni tra variabili: correlazione e covarianza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/09_causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Causalità dai dati osservazionali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/10_estimand.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Estimandi teorici e estimandi empirici</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Probabilità</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/introduction_probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/01_intro_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Interpretazione della probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/02_prob_spaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Misura di Probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/03_prob_on_general_spaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Fondamenti della probabilità: assiomi di Kolmogorov e sigma-algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/04_conditional_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Probabilità condizionata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/05_random_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/06_expval_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Proprietà delle variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/07_bayes_theorem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/08_sampling_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/09_joint_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Probabilità congiunta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/10_density_func.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">La funzione di densità di probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11_discr_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. discrete</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/12_cont_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. continue</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/13_qq_plot.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Diagramma quantile-quantile</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/14_likelihood.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/15_simulation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Simulazioni</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">Inferenza</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/introduction_bayes_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/01_intro_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">La quantificazione dell’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/02_bayesian_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Inferenza bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/03_subj_prop.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Pensare ad una proporzione in termini soggettivi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/04_grid_gauss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Calcolo della Distribuzione a Posteriori Gaussiana tramite Metodo a Griglia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/05_conjugate_families_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/06_conjugate_families_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/07_summary_posterior.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Sintesi a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/08_balance_prior_post.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">L’influenza della distribuzione a priori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/09_gamma_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Modello coniugato Gamma-Poisson</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/10_gamma_exponential_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Modello gamma-esponenziale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/11_post_pred_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Distribuzione predittiva a posteriori</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">MCMC</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/introduction_mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/01_metropolis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Monte Carlo a Catena di Markov</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/02_ppl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Linguaggi di programmazione probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/03_stan_language.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">Linguaggio Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/04_stan_summary_posterior.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Metodi di sintesi della distribuzione a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/05_stan_diagnostics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">Diagnostica delle catene markoviane</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/06_stan_prediction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">La predizione bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/07_bayesian_workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">Flusso di lavoro bayesiano</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/08_stan_odds_ratio.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Analisi bayesiana dell’odds-ratio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/09_stan_normal_normal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">59</span>&nbsp; <span class="chapter-title">Inferenza bayesiana su una media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/10_stan_two_groups.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">60</span>&nbsp; <span class="chapter-title">Confronto tra due gruppi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/11_stan_poisson_model_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">61</span>&nbsp; <span class="chapter-title">Modello di Poisson (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/12_stan_poisson_model_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">62</span>&nbsp; <span class="chapter-title">Modello di Poisson (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/13_stan_exponential_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">63</span>&nbsp; <span class="chapter-title">Modello esponenziale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/14_stan_gaussian_mixture.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">64</span>&nbsp; <span class="chapter-title">Modelli Mistura Gaussiani</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/15_stan_nuisance_parameters.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">65</span>&nbsp; <span class="chapter-title">Modelli con più di un parametro</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/16_stan_hier_beta_binom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">66</span>&nbsp; <span class="chapter-title">Modello gerarchico beta-binomiale con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/17_stan_categorical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">67</span>&nbsp; <span class="chapter-title">Modello categoriale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/18_cmdstanr_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">68</span>&nbsp; <span class="chapter-title">Introduzione a CmdStanR</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/19_approximations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">69</span>&nbsp; <span class="chapter-title">Metodi approssimativi nell’inferenza Bayesiana</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">Regressione</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/introduction_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/01_reglin_bayesian.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">70</span>&nbsp; <span class="chapter-title">Modello bayesiano di regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/02_non_centered_param.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">71</span>&nbsp; <span class="chapter-title">Non-Centered Parameterization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/03_beauty_sex_power.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">Bellezza, sesso e potere</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/04_synt_sugar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">Zucchero sintattico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/05_two_means.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Confronto tra le medie di due gruppi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/06_prediction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Predizione e inferenza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/07_design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">Disegno della ricerca e potere statistico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/08_linear_algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Elementi di algebra lineare</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/09_stan_multreg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">78</span>&nbsp; <span class="chapter-title">Il modello di regressione multipla</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/10_hier_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">79</span>&nbsp; <span class="chapter-title">Il modello lineare gerarchico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/11_stan_mixed_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">80</span>&nbsp; <span class="chapter-title">Modelli misti con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/12_specification_error.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">81</span>&nbsp; <span class="chapter-title">Errore di specificazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/14_interactions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">82</span>&nbsp; <span class="chapter-title">Interazioni statistiche</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/16_missing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">Dati mancanti</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Causalità</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/causal_inference/01_rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">84</span>&nbsp; <span class="chapter-title">Trial controllati randomizzati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/causal_inference/02_ate_att_atu.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">85</span>&nbsp; <span class="chapter-title">Concetti di ATE, ATT e ATU</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/causal_inference/03_endogenous_treatments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">86</span>&nbsp; <span class="chapter-title">Inferenza causale con trattamenti endogeni</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/causal_inference/04_mult_reg_causal_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">87</span>&nbsp; <span class="chapter-title">Inferenza causale nella regressione multipla</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/causal_inference/05_collider_bias.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">88</span>&nbsp; <span class="chapter-title">Collider bias</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">GLM</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/introduction_glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/01_robust_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">89</span>&nbsp; <span class="chapter-title">Regressione robusta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/02_stan_binomial_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">90</span>&nbsp; <span class="chapter-title">Modelli lineari generalizzati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/03_stan_logistic_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">91</span>&nbsp; <span class="chapter-title">Regressione logistica con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/04_stan_poisson_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">92</span>&nbsp; <span class="chapter-title">Regressione di Poisson con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/05_simchon_2023.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">93</span>&nbsp; <span class="chapter-title">Tweets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/06_stan_rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">94</span>&nbsp; <span class="chapter-title">Incorporare dati storici di controllo in una RCT</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/07_stan_mediation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">95</span>&nbsp; <span class="chapter-title">Modello di mediazione con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/08_uses_and_abuses.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">96</span>&nbsp; <span class="chapter-title">Usi e abusi dei modelli lineari</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false">
 <span class="menu-text">Entropia</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/introduction_entropy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/01_entropy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">97</span>&nbsp; <span class="chapter-title">Entropia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/02_kl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">98</span>&nbsp; <span class="chapter-title">La divergenza di Kullback-Leibler</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/03_model_comparison.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">99</span>&nbsp; <span class="chapter-title">Divergenza KL, LPPD, ELPD e LOO-CV</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/04_loo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">100</span>&nbsp; <span class="chapter-title">Validazione Incrociata Leave-One-Out</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/05_regularization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">101</span>&nbsp; <span class="chapter-title">Regolarizzazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/06_cognitive_modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">102</span>&nbsp; <span class="chapter-title">Dall’analisi descrittiva alla modellazione cognitiva</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/07_inductive_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">103</span>&nbsp; <span class="chapter-title">Limiti dell’inferenza induttiva</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false">
 <span class="menu-text">Dinamiche</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/introduction_dynamic_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/01_canoeing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">104</span>&nbsp; <span class="chapter-title">Dinamiche post-errore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/02_change_across_time.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">105</span>&nbsp; <span class="chapter-title">Modellare il cambiamento nel tempo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/03_rescorla_wagner.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">106</span>&nbsp; <span class="chapter-title">Apprendimento per rinforzo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/04_affect.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">107</span>&nbsp; <span class="chapter-title">Le emozioni influenzano le emozioni</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/05_sequential_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">108</span>&nbsp; <span class="chapter-title">Analisi dinamica delle sequenze di apprendimento</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/07_bipolar_disorder.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">109</span>&nbsp; <span class="chapter-title">Modello di Markov nascosto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/08_haslbeck.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">110</span>&nbsp; <span class="chapter-title">Recuperare le dinamiche intra-personali dalle serie temporali psicologiche</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false">
 <span class="menu-text">Modelli cognitivi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/cognitive_models/introduction_cognitive_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/cognitive_models/01_ddm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">111</span>&nbsp; <span class="chapter-title">Drift Diffusion Model</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="false">
 <span class="menu-text">Frequentismo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/introduction_frequentist_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/01_intro_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">112</span>&nbsp; <span class="chapter-title">Introduzione all’inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/02_conf_interv.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">113</span>&nbsp; <span class="chapter-title">Intervallo di confidenza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/03_test_ipotesi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">114</span>&nbsp; <span class="chapter-title">Significatività statistica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/04_two_ind_samples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">115</span>&nbsp; <span class="chapter-title">Test t di Student per campioni indipendenti</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="false">
 <span class="menu-text">Crisi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/introduction_replication_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/01_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">116</span>&nbsp; <span class="chapter-title">La Crisi della Replicazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/02_limits_stat_freq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">117</span>&nbsp; <span class="chapter-title">Limiti dell’inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/03_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">118</span>&nbsp; <span class="chapter-title">La grandezza dell’effetto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/04_s_m_errors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">119</span>&nbsp; <span class="chapter-title">Errori di segno e errori di grandezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/05_p_values.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">120</span>&nbsp; <span class="chapter-title">La fragilità del <em>p</em>-valore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/06_changes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">121</span>&nbsp; <span class="chapter-title">Proposte di cambiamento</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/07_integrity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">122</span>&nbsp; <span class="chapter-title">Integrità della ricerca</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" role="navigation" aria-expanded="false">
 <span class="menu-text">Epilogo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-16" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/epiloque/epiloque.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Considerazioni Conclusive</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../99-references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliografia</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-17" role="navigation" aria-expanded="false">
 <span class="menu-text">Appendici</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-17" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-17" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a00_installation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Ambiente di lavoro</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01_markdown.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Jupyter Notebook</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a02_shell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">La Shell</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a03_colab_tutorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Colab: un breve tutorial</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a04_virtual_env.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Ambienti virtuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a10_math_symbols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Simbologia di base</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a11_numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Numeri e intervalli</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a12_sum_notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">H</span>&nbsp; <span class="chapter-title">Sommatorie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a13_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">I</span>&nbsp; <span class="chapter-title">Insiemi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a14_combinatorics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">J</span>&nbsp; <span class="chapter-title">Calcolo combinatorio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a15_calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">K</span>&nbsp; <span class="chapter-title">Per liberarvi dai terrori preliminari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a20_kde_plot.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">L</span>&nbsp; <span class="chapter-title">Kernel Density Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a30_prob_tutorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">M</span>&nbsp; <span class="chapter-title">Esercizi di probabilità discreta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a40_rng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">N</span>&nbsp; <span class="chapter-title">Generazione di numeri casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a44_montecarlo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">O</span>&nbsp; <span class="chapter-title">Simulazione Monte Carlo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a46_stan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">P</span>&nbsp; <span class="chapter-title">Linguaggio Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a47_first_order_markov.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">Q</span>&nbsp; <span class="chapter-title">Catene di Markov</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a48_hidden_markov_processes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">R</span>&nbsp; <span class="chapter-title">Processi di Markov Nascosti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a50_lin_fun.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">S</span>&nbsp; <span class="chapter-title">La funzione lineare</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a51_r_squared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">T</span>&nbsp; <span class="chapter-title">Teorema della scomposizione della devianza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a53_entropy_rv_cont.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">U</span>&nbsp; <span class="chapter-title">Entropia di una variabile casuale continua</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a55_rescorla_wagner.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">V</span>&nbsp; <span class="chapter-title">Apprendimento per rinforzo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a60_ttest_exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">W</span>&nbsp; <span class="chapter-title">Esercizi sull’inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a70_predict_counts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">X</span>&nbsp; <span class="chapter-title">La predizione delle frequenze</span></span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-18" role="navigation" aria-expanded="false">
 <span class="menu-text">Soluzioni degli esercizi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-18" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-18" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/solutions_probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">Y</span>&nbsp; <span class="chapter-title">Probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/solutions_bayesian_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">Z</span>&nbsp; <span class="chapter-title">Inferenza bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/solutions_mult_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/solutions_rescorla_wagner.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">\</span>&nbsp; <span class="chapter-title">Modello Rescorla-Wagner</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/solutions_entropia.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">]</span>&nbsp; <span class="chapter-title">Entropia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/solutions_replication_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">^</span>&nbsp; <span class="chapter-title">Crisi della replicazione</span></span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indice</h2>
   
  <ul>
  <li><a href="#introduzione" id="toc-introduzione" class="nav-link active" data-scroll-target="#introduzione">Introduzione</a></li>
  <li><a href="#il-principio-della-verosimiglianza-e-la-sua-formalizzazione" id="toc-il-principio-della-verosimiglianza-e-la-sua-formalizzazione" class="nav-link" data-scroll-target="#il-principio-della-verosimiglianza-e-la-sua-formalizzazione"><span class="header-section-number">38.1</span> Il Principio della Verosimiglianza e la sua Formalizzazione</a></li>
  <li><a href="#verosimiglianza-binomiale" id="toc-verosimiglianza-binomiale" class="nav-link" data-scroll-target="#verosimiglianza-binomiale"><span class="header-section-number">38.2</span> Verosimiglianza Binomiale</a>
  <ul>
  <li><a href="#interpretazione-della-funzione-di-verosimiglianza" id="toc-interpretazione-della-funzione-di-verosimiglianza" class="nav-link" data-scroll-target="#interpretazione-della-funzione-di-verosimiglianza"><span class="header-section-number">38.2.1</span> Interpretazione della Funzione di Verosimiglianza</a></li>
  </ul></li>
  <li><a href="#massima-verosimiglianza" id="toc-massima-verosimiglianza" class="nav-link" data-scroll-target="#massima-verosimiglianza"><span class="header-section-number">38.3</span> Massima Verosimiglianza</a>
  <ul>
  <li><a href="#la-strategia-di-base" id="toc-la-strategia-di-base" class="nav-link" data-scroll-target="#la-strategia-di-base"><span class="header-section-number">38.3.1</span> La Strategia di Base</a></li>
  <li><a href="#metodi-di-ottimizzazione" id="toc-metodi-di-ottimizzazione" class="nav-link" data-scroll-target="#metodi-di-ottimizzazione"><span class="header-section-number">38.3.2</span> Metodi di Ottimizzazione</a></li>
  <li><a href="#interpretazione-intuitiva" id="toc-interpretazione-intuitiva" class="nav-link" data-scroll-target="#interpretazione-intuitiva"><span class="header-section-number">38.3.3</span> Interpretazione Intuitiva</a></li>
  <li><a href="#ottimizzazione-della-verosimiglianza" id="toc-ottimizzazione-della-verosimiglianza" class="nav-link" data-scroll-target="#ottimizzazione-della-verosimiglianza"><span class="header-section-number">38.3.4</span> Ottimizzazione della Verosimiglianza</a></li>
  </ul></li>
  <li><a href="#la-funzione-di-log-verosimiglianza" id="toc-la-funzione-di-log-verosimiglianza" class="nav-link" data-scroll-target="#la-funzione-di-log-verosimiglianza"><span class="header-section-number">38.4</span> La Funzione di Log-Verosimiglianza</a>
  <ul>
  <li><a href="#esempio-applicazione-della-log-verosimiglianza" id="toc-esempio-applicazione-della-log-verosimiglianza" class="nav-link" data-scroll-target="#esempio-applicazione-della-log-verosimiglianza"><span class="header-section-number">38.4.1</span> Esempio: Applicazione della Log-Verosimiglianza</a></li>
  <li><a href="#vantaggi-della-log-verosimiglianza" id="toc-vantaggi-della-log-verosimiglianza" class="nav-link" data-scroll-target="#vantaggi-della-log-verosimiglianza"><span class="header-section-number">38.4.2</span> Vantaggi della Log-Verosimiglianza</a></li>
  <li><a href="#rappresentazione-grafica" id="toc-rappresentazione-grafica" class="nav-link" data-scroll-target="#rappresentazione-grafica"><span class="header-section-number">38.4.3</span> Rappresentazione Grafica</a></li>
  </ul></li>
  <li><a href="#verosimiglianza-congiunta" id="toc-verosimiglianza-congiunta" class="nav-link" data-scroll-target="#verosimiglianza-congiunta"><span class="header-section-number">38.5</span> Verosimiglianza Congiunta</a></li>
  <li><a href="#la-verosimiglianza-marginale" id="toc-la-verosimiglianza-marginale" class="nav-link" data-scroll-target="#la-verosimiglianza-marginale"><span class="header-section-number">38.6</span> La Verosimiglianza Marginale</a>
  <ul>
  <li><a href="#esempio-con-parametri-discreti" id="toc-esempio-con-parametri-discreti" class="nav-link" data-scroll-target="#esempio-con-parametri-discreti"><span class="header-section-number">38.6.1</span> Esempio con Parametri Discreti</a></li>
  <li><a href="#esempio-con-parametri-continui" id="toc-esempio-con-parametri-continui" class="nav-link" data-scroll-target="#esempio-con-parametri-continui"><span class="header-section-number">38.6.2</span> Esempio con Parametri Continui</a></li>
  <li><a href="#calcolo-della-verosimiglianza-marginale" id="toc-calcolo-della-verosimiglianza-marginale" class="nav-link" data-scroll-target="#calcolo-della-verosimiglianza-marginale"><span class="header-section-number">38.6.3</span> Calcolo della Verosimiglianza Marginale</a></li>
  </ul></li>
  <li><a href="#modello-gaussiano-e-verosimiglianza" id="toc-modello-gaussiano-e-verosimiglianza" class="nav-link" data-scroll-target="#modello-gaussiano-e-verosimiglianza"><span class="header-section-number">38.7</span> Modello Gaussiano e Verosimiglianza</a>
  <ul>
  <li><a href="#caso-di-una-singola-osservazione" id="toc-caso-di-una-singola-osservazione" class="nav-link" data-scroll-target="#caso-di-una-singola-osservazione"><span class="header-section-number">38.7.1</span> Caso di una Singola Osservazione</a></li>
  <li><a href="#campione-indipendente-di-osservazioni-da-una-distribuzione-gaussiana" id="toc-campione-indipendente-di-osservazioni-da-una-distribuzione-gaussiana" class="nav-link" data-scroll-target="#campione-indipendente-di-osservazioni-da-una-distribuzione-gaussiana"><span class="header-section-number">38.7.2</span> Campione indipendente di osservazioni da una distribuzione gaussiana</a></li>
  <li><a href="#la-stima-di-massima-verosimiglianza-per-mu" id="toc-la-stima-di-massima-verosimiglianza-per-mu" class="nav-link" data-scroll-target="#la-stima-di-massima-verosimiglianza-per-mu"><span class="header-section-number">38.7.3</span> La Stima di Massima Verosimiglianza per <span class="math inline">\(\mu\)</span></a></li>
  </ul></li>
  <li><a href="#conclusione-e-riflessioni-finali" id="toc-conclusione-e-riflessioni-finali" class="nav-link" data-scroll-target="#conclusione-e-riflessioni-finali"><span class="header-section-number">38.8</span> Conclusione e Riflessioni Finali</a></li>
  <li><a href="#esercizi" id="toc-esercizi" class="nav-link" data-scroll-target="#esercizi"><span class="header-section-number">38.9</span> Esercizi</a></li>
  <li><a href="#informazioni-sullambiente-di-sviluppo" id="toc-informazioni-sullambiente-di-sviluppo" class="nav-link" data-scroll-target="#informazioni-sullambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.dev/ccaudek/psicometria/blob/main/chapters/probability/14_likelihood.ipynb" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/probability/introduction_probability.html">Probabilità</a></li><li class="breadcrumb-item"><a href="../../chapters/probability/14_likelihood.html"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-likelihood" class="quarto-section-identifier"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><strong>Prerequisiti</strong></p>
<p><strong>Concetti e Competenze Chiave</strong></p>
<ul>
<li>Comprendere il concetto di verosimiglianza e il suo ruolo nella dei parametri.</li>
<li>Generare grafici della funzione di verosimiglianza binomiale.</li>
<li>Generare grafici della funzione di verosimiglianza del modello gaussiano.</li>
<li>Interpretare i grafici della funzione di verosimiglianza.</li>
<li>Comprendere il concetto di stima di massima verosimiglianza.</li>
</ul>
<p><strong>Preparazione del Notebook</strong></p>
<div id="271de755-b218-48e3-a1c5-6bd1d0a55266" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats <span class="im">as</span> stats</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.integrate <span class="im">import</span> quad</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> arviz <span class="im">as</span> az</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="bb772cc1-f190-4e46-9fcd-2eb1bcc59ac2" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>seed: <span class="bu">int</span> <span class="op">=</span> <span class="bu">sum</span>(<span class="bu">map</span>(<span class="bu">ord</span>, <span class="st">"likelihood"</span>))</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>rng: np.random.Generator <span class="op">=</span> np.random.default_rng(seed<span class="op">=</span>seed)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>sns.set_theme(palette<span class="op">=</span><span class="st">"colorblind"</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>az.style.use(<span class="st">"arviz-darkgrid"</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>config InlineBackend.figure_format <span class="op">=</span> <span class="st">"retina"</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="introduzione" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="introduzione">Introduzione</h2>
<p>I ricercatori sviluppano modelli con forme funzionali diverse, espressi in termini di parametri, per fare previsioni su come si comporteranno i dati. Il confronto tra i dati osservati e queste previsioni è fondamentale per scegliere tra modelli alternativi. Si seleziona il modello che produce previsioni più vicine ai dati osservati, dimostrandosi il più adatto a descrivere il fenomeno in esame. La funzione di verosimiglianza gioca un ruolo centrale in questo processo, poiché quantifica la probabilità che i dati osservati siano compatibili con un modello e i suoi parametri.</p>
<p>La verosimiglianza rappresenta il processo generativo dei dati: stabilisce una relazione tra i parametri del modello e le osservazioni empiriche, descrivendo come i dati potrebbero essere generati se il modello fosse corretto. Tuttavia, la verosimiglianza da sola non è sufficiente a rappresentare un modello scientifico completo. Un modello aggiunge ulteriore struttura e assunzioni che non sono contenute solo nella verosimiglianza.</p>
<p>Ad esempio, in un approccio bayesiano, il modello scientifico include anche i <em>priori</em>, ossia le ipotesi iniziali che formuliamo sui parametri del modello prima di osservare i dati. I <em>priori</em> rappresentano la nostra conoscenza o incertezza preesistente sui parametri e si combinano con la verosimiglianza per generare la distribuzione a posteriori, che descrive i parametri alla luce dei dati osservati. Questo passaggio è fondamentale nel confronto tra modelli perché i <em>priori</em> possono influenzare le conclusioni finali.</p>
<p>Inoltre, un modello scientifico potrebbe includere altri aspetti, come l’errore di misurazione, che tiene conto del fatto che i dati osservati spesso non riflettono esattamente il processo reale, ma sono affetti da rumore o errori. La modellazione dell’errore di misurazione è un altro elemento cruciale che differenzia un modello scientifico completo dalla sola verosimiglianza, poiché consente di catturare le imperfezioni nei dati.</p>
<p>In sintesi, la verosimiglianza descrive come i dati potrebbero essere generati da un modello dato un insieme di parametri, ma un modello scientifico aggiunge ulteriori componenti, come i <em>priori</em> (in un approccio bayesiano) o l’errore di misurazione, che aiutano a rendere la rappresentazione più accurata e realistica. Il confronto tra i modelli, quindi, si basa non solo sulla verosimiglianza, ma sull’intera struttura del modello, che include le ipotesi sui parametri e l’incertezza che li circonda.</p>
<p>Lo scopo di questo capitolo è di approfondire il concetto di verosimiglianza.</p>
</section>
<section id="il-principio-della-verosimiglianza-e-la-sua-formalizzazione" class="level2" data-number="38.1">
<h2 data-number="38.1" class="anchored" data-anchor-id="il-principio-della-verosimiglianza-e-la-sua-formalizzazione"><span class="header-section-number">38.1</span> Il Principio della Verosimiglianza e la sua Formalizzazione</h2>
<p>La funzione di verosimiglianza e la funzione di densità (o massa) di probabilità sono concetti fondamentali in statistica. Sebbene condividano la stessa espressione matematica, essi hanno interpretazioni e ruoli distinti a seconda del contesto. La chiave per distinguerli sta nel modo in cui vengono trattati i dati e i parametri del modello.</p>
<p>Nella funzione di densità (o massa) di probabilità, i parametri del modello sono fissati, mentre i dati sono considerati variabili. L’obiettivo è calcolare la probabilità di osservare un determinato insieme di dati, dati i parametri noti. Ad esempio, se stiamo lanciando una moneta più volte, potremmo utilizzare la distribuzione binomiale per calcolare la probabilità di ottenere un certo numero di teste, assumendo che la probabilità di ottenere testa in ogni lancio sia un valore fisso e conosciuto.</p>
<p>La funzione di verosimiglianza, invece, inverte questa prospettiva: i dati osservati sono considerati fissi, mentre i parametri del modello sono variabili. Lo scopo è valutare quanto bene diversi valori dei parametri si adattino ai dati osservati. In questo contesto, la funzione di verosimiglianza permette di esplorare la plausibilità di differenti set di parametri, dati i dati raccolti, e di identificare il set che meglio spiega le osservazioni.</p>
<p>Formalmente, la relazione tra la funzione di verosimiglianza e la funzione di densità di probabilità è espressa come:</p>
<p><span class="math display">\[
L(\theta \mid y) \propto p(y \mid \theta),
\]</span></p>
<p>dove <span class="math inline">\(L(\theta \mid y)\)</span> rappresenta la funzione di verosimiglianza per i parametri <span class="math inline">\(\theta\)</span> dato l’insieme di osservazioni <span class="math inline">\(y\)</span>, e <span class="math inline">\(p(y \mid \theta)\)</span> è la probabilità (o densità) di osservare i dati <span class="math inline">\(y\)</span> dato un certo set di parametri <span class="math inline">\(\theta\)</span>.</p>
<p>Consideriamo l’esempio del lancio di una moneta. Se osserviamo 7 teste su 10 lanci, la funzione di massa di probabilità della distribuzione binomiale ci permette di calcolare la probabilità di ottenere esattamente questo risultato, dato un valore fissato di <span class="math inline">\(p\)</span> (la probabilità di ottenere testa in un singolo lancio). In questo caso, <span class="math inline">\(p\)</span> è un parametro fisso, mentre i dati (7 teste su 10 lanci) sono le variabili.</p>
<p>Invece, nella funzione di verosimiglianza, manteniamo fissi i dati (7 teste su 10 lanci) e variamo <span class="math inline">\(p\)</span> per valutare quanto ciascun valore di <span class="math inline">\(p\)</span> si adatti a questo risultato osservato. La funzione di verosimiglianza, dunque, ci dice quali valori di <span class="math inline">\(p\)</span> sono più plausibili alla luce dei dati osservati.</p>
<p>Sebbene le due funzioni condividano la stessa forma matematica, il loro utilizzo è profondamente diverso. La funzione di densità di probabilità si concentra sulla probabilità di osservare certi esiti dato un set di parametri fissi, mentre la funzione di verosimiglianza ci permette di valutare la plausibilità dei parametri dati i risultati osservati. Questa distinzione è cruciale per l’inferenza statistica, poiché ci consente di stimare i parametri del modello che meglio si adattano ai dati e di approfondire la comprensione del fenomeno sotto studio.</p>
</section>
<section id="verosimiglianza-binomiale" class="level2" data-number="38.2">
<h2 data-number="38.2" class="anchored" data-anchor-id="verosimiglianza-binomiale"><span class="header-section-number">38.2</span> Verosimiglianza Binomiale</h2>
<p>Riprendendo l’esempio della distribuzione binomiale, possiamo approfondire il ruolo della funzione di verosimiglianza nell’analisi statistica con uno scenario pratico. Immaginiamo di condurre un esperimento con un numero definito di prove <span class="math inline">\(n\)</span>, dove ogni prova può concludersi con un successo o un fallimento, come nel caso dei lanci di una moneta. Se registriamo <span class="math inline">\(y\)</span> successi e <span class="math inline">\(n - y\)</span> fallimenti, la probabilità di osservare esattamente <span class="math inline">\(y\)</span> successi è descritta dalla funzione di massa di probabilità (FMP) binomiale, espressa come:</p>
<p><span class="math display">\[
P(Y = y) = \binom{n}{y} \theta^y (1 - \theta)^{n - y},
\]</span></p>
<p>dove <span class="math inline">\(\theta\)</span> rappresenta la probabilità di successo in una singola prova di Bernoulli.</p>
<p>Quando utilizziamo la funzione di verosimiglianza, ci concentriamo su come differenti valori di <span class="math inline">\(\theta\)</span> spieghino i dati osservati <span class="math inline">\(y\)</span>. La funzione di verosimiglianza è espressa come:</p>
<p><span class="math display">\[
\mathcal{L}(\theta \mid y) = \theta^y (1 - \theta)^{n - y},
\]</span></p>
<p>in quanto il coefficiente binomiale <span class="math inline">\(\binom{n}{y}\)</span>, non dipendendo da <span class="math inline">\(\theta\)</span>, può essere omesso per semplificare la formulazione.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 38.1</strong></span> Per illustrare meglio questo concetto, consideriamo uno studio che indaga sulle aspettative negative tra individui clinicamente depressi. Supponiamo di esaminare un gruppo di 30 pazienti, di cui 23 esprimono un atteggiamento negativo verso il futuro <span class="citation" data-cites="zetsche_2019future">(<a href="../../99-references.html#ref-zetsche_2019future" role="doc-biblioref">Zetsche, Buerkner, e Renneberg 2019</a>)</span>. In questo caso, i dati osservati sono <span class="math inline">\(y = 23\)</span> e <span class="math inline">\(n = 30\)</span>, e la funzione di verosimiglianza per <span class="math inline">\(\theta\)</span>, la probabilità sconosciuta di avere aspettative negative, diventa:</p>
<p><span class="math display">\[
\mathcal{L}(\theta \mid y) = \theta^{23} (1 - \theta)^7.
\]</span></p>
<p>Notiamo che il coefficiente binomiale è stato omesso poiché non influisce sulla stima di <span class="math inline">\(\theta\)</span>.</p>
<p>Valutando questa funzione per una serie di valori di <span class="math inline">\(\theta\)</span>, possiamo determinare quale valore di <span class="math inline">\(\theta\)</span> rende i dati osservati più verosimili. Un approccio comune è quello di simulare 100 valori equidistanti di <span class="math inline">\(\theta\)</span> nell’intervallo <span class="math inline">\([0, 1]\)</span> e calcolare la verosimiglianza per ciascuno di essi. In questo modo, possiamo individuare il valore di <span class="math inline">\(\theta\)</span> che massimizza la verosimiglianza, ovvero il valore che meglio spiega i dati osservati.</p>
<div id="cb433da7-f10f-472c-9a5c-cf312d42ff68" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">23</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Creiamo i possibili valori del parametro <span class="math inline">\(\theta\)</span> per i quali calcoleremo la verosimiglianza.</p>
<div id="06e02b06-0fb8-4843-be54-dfadd07c6315" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.linspace(<span class="fl">0.0</span>, <span class="fl">1.0</span>, num<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(theta)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.         0.01010101 0.02020202 0.03030303 0.04040404 0.05050505
 0.06060606 0.07070707 0.08080808 0.09090909 0.1010101  0.11111111
 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717
 0.18181818 0.19191919 0.2020202  0.21212121 0.22222222 0.23232323
 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929
 0.3030303  0.31313131 0.32323232 0.33333333 0.34343434 0.35353535
 0.36363636 0.37373737 0.38383838 0.39393939 0.4040404  0.41414141
 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747
 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354
 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.5959596
 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566
 0.66666667 0.67676768 0.68686869 0.6969697  0.70707071 0.71717172
 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778
 0.78787879 0.7979798  0.80808081 0.81818182 0.82828283 0.83838384
 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.8989899
 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596
 0.96969697 0.97979798 0.98989899 1.        ]</code></pre>
</div>
</div>
<p>Per esempio, ponendo <span class="math inline">\(\theta = 0.1\)</span> otteniamo il seguente valore dell’ordinata della funzione di verosimiglianza:</p>
<p><span class="math display">\[
\mathcal{L}(\theta \mid y) = \frac{(23 + 7)!}{23!7!} 0.1^{23} + (1-0.1)^7.
\]</span></p>
<div id="3747aefb-2424-4fef-8ad5-724738c9c093" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>stats.binom.pmf(y, n, <span class="fl">0.1</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>9.737168290200003e-18</code></pre>
</div>
</div>
<p>Ponendo <span class="math inline">\(\theta = 0.2\)</span> otteniamo il seguente valore dell’ordinata della funzione di verosimiglianza:</p>
<p><span class="math display">\[
\mathcal{L}(\theta \mid y) = \frac{(23 + 7)!}{23!7!} 0.2^{23} + (1-0.2)^7.
\]</span></p>
<div id="fe975edb-365a-4471-a305-ec16385f24bc" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>stats.binom.pmf(y, n, <span class="fl">0.2</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>3.581417234922211e-11</code></pre>
</div>
</div>
<p>Se ripetiamo questo processo 100 volte, una volta per ciascuno dei valori <span class="math inline">\(\theta\)</span> che abbiamo elencato sopra, otteniamo 100 coppie di punti <span class="math inline">\(\theta\)</span> e <span class="math inline">\(f(\theta)\)</span>. A tale fine, definiamo la seguente funzione.</p>
<div id="7a114165-85fa-4a71-a379-4e38efae1a81" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> like(r, n, theta):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> math.comb(n, r) <span class="op">*</span> theta<span class="op">**</span>r <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> theta)<span class="op">**</span>(n <span class="op">-</span> r)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La curva che interpola i punti ottenuti è la funzione di verosimiglianza, come indicato dalla figura seguente.</p>
<div id="5e75d37a-ae54-41e0-ae27-56cf7e3b7e95" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>plt.plot(theta, like(r<span class="op">=</span>y, n<span class="op">=</span>n, theta<span class="op">=</span>theta), <span class="st">"-"</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Funzione di verosimiglianza"</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Valore della variabile casuale theta [0, 1]"</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Verosimiglianza"</span>)<span class="op">;</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="14_likelihood_files/figure-html/cell-9-output-1.png" width="731" height="491" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>È importante notare che, invece di utilizzare la funzione <code>like()</code> che abbiamo definito precedentemente per motivi didattici, è possibile ottenere lo stesso risultato utilizzando in modo equivalente la funzione <code>binom.pmf()</code>.</p>
<div id="cell-19" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>plt.plot(theta, stats.binom.pmf(y, n, theta), <span class="st">"-"</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Funzione di verosimiglianza"</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Valore della variabile casuale theta [0, 1]"</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Verosimiglianza"</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>Text(0, 0.5, 'Verosimiglianza')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="14_likelihood_files/figure-html/cell-10-output-2.png" width="731" height="491" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
<section id="interpretazione-della-funzione-di-verosimiglianza" class="level3" data-number="38.2.1">
<h3 data-number="38.2.1" class="anchored" data-anchor-id="interpretazione-della-funzione-di-verosimiglianza"><span class="header-section-number">38.2.1</span> Interpretazione della Funzione di Verosimiglianza</h3>
<p>La funzione di verosimiglianza ci permette di valutare quanto bene i diversi valori di <span class="math inline">\(\theta\)</span> si adattano ai dati osservati. Il valore che massimizza la funzione di verosimiglianza rappresenta la stima più plausibile di <span class="math inline">\(\theta\)</span> dato l’insieme di dati. Ad esempio, se il valore che massimizza la verosimiglianza è <span class="math inline">\(\theta = 0.767\)</span>, ciò suggerisce che la probabilità più plausibile di successo (o, nel nostro caso, di atteggiamento negativo) nella popolazione studiata è del 76.7%.</p>
<p>La determinazione numerica di questo valore ottimale può essere effettuata mediante tecniche di ottimizzazione. Metodi computazionali, come quelli disponibili in linguaggi di programmazione come Python, permettono di identificare il punto in cui la funzione di verosimiglianza raggiunge il massimo. L’uso di librerie statistiche e matematiche, come NumPy o SciPy, consente di calcolare la stima di <span class="math inline">\(\theta\)</span> in modo preciso ed efficiente, fornendo un parametro che meglio si adatta ai dati osservati.</p>
<p>Questa metodologia, centrata sull’uso della funzione di verosimiglianza, è fondamentale nell’inferenza statistica. Essa consente ai ricercatori di stimare i parametri di un modello basandosi sui dati empirici, e di valutare l’adeguatezza del modello rispetto ai dati reali.</p>
<p>In pratica, per identificare numericamente il valore ottimale di <span class="math inline">\(\theta\)</span>, si può utilizzare un approccio computazionale che identifica il massimo della verosimiglianza. Ad esempio, la funzione <code>argmax</code> di NumPy può essere impiegata per localizzare l’indice del vettore dei valori di verosimiglianza in cui si raggiunge il picco. Una volta identificato questo indice, si risale al corrispondente valore di <span class="math inline">\(\theta\)</span>, ottenendo così la stima del parametro che rende i dati osservati più plausibili.</p>
<div id="cell-22" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> stats.binom.pmf(y, n, theta)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>l.argmax()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>76</code></pre>
</div>
</div>
<div id="b0342493-69a4-499d-b5c0-a99e057e20e9" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>theta[<span class="dv">76</span>]</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>0.7676767676767677</code></pre>
</div>
</div>
</section>
</section>
<section id="massima-verosimiglianza" class="level2" data-number="38.3">
<h2 data-number="38.3" class="anchored" data-anchor-id="massima-verosimiglianza"><span class="header-section-number">38.3</span> Massima Verosimiglianza</h2>
<p>Quando cerchiamo di stimare i parametri di un modello, come <span class="math inline">\(\theta\)</span>, il nostro obiettivo è trovare il valore che massimizza la probabilità dei dati osservati, ovvero quello che corrisponde al massimo della funzione di verosimiglianza.</p>
<p>In termini di ottimizzazione, possiamo parlare di “minimizzazione” o “massimizzazione” a seconda della funzione. La minimizzazione consiste nel trovare il punto più basso di una “valle”, mentre la massimizzazione implica trovare il punto più alto su una “collina”. Nel caso della funzione di verosimiglianza, vogliamo trovare il punto in cui questa funzione raggiunge il suo valore massimo. Tuttavia, poiché molti algoritmi sono progettati per minimizzare le funzioni, possiamo convertire il problema cercando di minimizzare il negativo della funzione di verosimiglianza, il cui minimo corrisponderà al massimo della funzione originale.</p>
<section id="la-strategia-di-base" class="level3" data-number="38.3.1">
<h3 data-number="38.3.1" class="anchored" data-anchor-id="la-strategia-di-base"><span class="header-section-number">38.3.1</span> La Strategia di Base</h3>
<p>L’ottimizzazione segue alcune fasi generali:</p>
<ul>
<li><strong>Punto di Partenza</strong>: L’algoritmo inizia da un punto iniziale, scelto casualmente o basato su un’ipotesi ragionevole.</li>
<li><strong>Esplorazione</strong>: L’algoritmo “esplora” la superficie della funzione, spostandosi nelle direzioni che sembrano portare verso il minimo (o massimo, se stiamo massimizzando). È simile a sentirsi il terreno intorno a noi e camminare nella direzione della discesa.</li>
<li><strong>Aggiustamento</strong>: Durante l’esplorazione, l’algoritmo regola la traiettoria in base a ciò che rileva. Se trova una discesa, continua in quella direzione; se incontra una salita, prova a cambiare rotta.</li>
<li><strong>Convergenza</strong>: L’algoritmo procede fino a quando non trova un punto in cui non ci sono più discese significative, suggerendo di aver raggiunto il minimo (o massimo) raggiungibile con quel percorso.</li>
</ul>
</section>
<section id="metodi-di-ottimizzazione" class="level3" data-number="38.3.2">
<h3 data-number="38.3.2" class="anchored" data-anchor-id="metodi-di-ottimizzazione"><span class="header-section-number">38.3.2</span> Metodi di Ottimizzazione</h3>
<p>Esistono diversi metodi che l’algoritmo può utilizzare per determinare come muoversi:</p>
<ul>
<li><strong>Discesa del Gradiente (Gradient Descent)</strong>: Utilizza il gradiente della funzione, che indica la direzione e la pendenza, per decidere in che direzione spostarsi.</li>
<li><strong>Metodo di Newton-Raphson</strong>: Impiega sia il gradiente che la curvatura della funzione (ossia la derivata seconda) per compiere passi più informati verso il minimo.</li>
<li><strong>Algoritmi Genetici</strong>: Ispirati alla selezione naturale, questi algoritmi generano soluzioni “evolutive” attraverso iterazioni che simulano l’evoluzione biologica.</li>
</ul>
</section>
<section id="interpretazione-intuitiva" class="level3" data-number="38.3.3">
<h3 data-number="38.3.3" class="anchored" data-anchor-id="interpretazione-intuitiva"><span class="header-section-number">38.3.3</span> Interpretazione Intuitiva</h3>
<p>L’ottimizzazione può essere vista come un processo di esplorazione metodica, in cui l’algoritmo riceve continuamente feedback dalla funzione che sta cercando di ottimizzare. Questo feedback guida il percorso verso il punto ottimale, che può essere il massimo o il minimo della funzione, a seconda del problema.</p>
</section>
<section id="ottimizzazione-della-verosimiglianza" class="level3" data-number="38.3.4">
<h3 data-number="38.3.4" class="anchored" data-anchor-id="ottimizzazione-della-verosimiglianza"><span class="header-section-number">38.3.4</span> Ottimizzazione della Verosimiglianza</h3>
<p>Per trovare il massimo della funzione di verosimiglianza, possiamo quindi definire e minimizzare il negativo della funzione stessa. Questo ci permette di utilizzare algoritmi di minimizzazione per raggiungere il massimo, ottenendo così la stima del parametro <span class="math inline">\(\theta\)</span> che meglio si adatta ai dati osservati.</p>
<div id="bc09df21-31de-43d3-99b4-8cbdfc55c8f1" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> negative_likelihood(theta, n, y):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calcolo del negativo della funzione di verosimiglianza</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>stats.binom.pmf(y, n, theta)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Utilizziamo ora <code>scipy.optimize.minimize</code> per trovare il valore di theta che massimizza la verosimiglianza. Bisogna specificare un valore iniziale per theta, qui assumiamo 0.5 come punto di partenza. I vincoli su theta sono che deve essere compreso tra 0 e 1.</p>
<div id="0eab72ba-76f4-41c3-a8cc-49409180d35d" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> minimize(negative_likelihood, x0<span class="op">=</span><span class="fl">0.5</span>, args<span class="op">=</span>(n, y), bounds<span class="op">=</span>[(<span class="dv">0</span>, <span class="dv">1</span>)])</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>result.x</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>array([0.76666666])</code></pre>
</div>
</div>
</section>
</section>
<section id="la-funzione-di-log-verosimiglianza" class="level2" data-number="38.4">
<h2 data-number="38.4" class="anchored" data-anchor-id="la-funzione-di-log-verosimiglianza"><span class="header-section-number">38.4</span> La Funzione di Log-Verosimiglianza</h2>
<p>Proseguendo con l’analisi della funzione di verosimiglianza, ci avviciniamo a una sua trasformazione matematica frequentemente utilizzata dagli statistici: la funzione di log-verosimiglianza. Questa è definita come il logaritmo naturale della funzione di verosimiglianza:</p>
<p><span class="math display">\[
\ell(\theta) = \log \mathcal{L}(\theta \mid y),
\]</span></p>
<p>La log-verosimiglianza non altera la posizione del massimo della funzione di verosimiglianza, grazie alla proprietà di monotonicità del logaritmo. In termini pratici, il valore di <span class="math inline">\(\theta\)</span> che massimizza la log-verosimiglianza, denotato come <span class="math inline">\(\hat{\theta}\)</span>, è lo stesso che massimizza la verosimiglianza originale:</p>
<p><span class="math display">\[
\hat{\theta} = \arg \max_{\theta \in \Theta} \ell(\theta) = \arg \max_{\theta \in \Theta} \mathcal{L}(\theta).
\]</span></p>
<p>L’uso della log-verosimiglianza semplifica il processo di ottimizzazione, specialmente quando si lavora con numeri molto piccoli o con dataset di grandi dimensioni. La log-verosimiglianza trasforma infatti il prodotto delle probabilità in una somma di logaritmi, facilitando i calcoli e migliorando la stabilità numerica. Questo è particolarmente utile nei casi in cui si gestiscono molte osservazioni, evitando problemi di underflow (quando i numeri diventano troppo piccoli per essere rappresentati accuratamente).</p>
<p>Ad esempio, per un modello binomiale, la log-verosimiglianza è data da:</p>
<p><span class="math display">\[
\ell(\theta \mid y) = \log(\theta^y (1 - \theta)^{n - y}) = y \log(\theta) + (n - y) \log(1 - \theta).
\]</span></p>
<p>Questa trasformazione rende i calcoli più semplici perché converte il prodotto di probabilità in una somma, semplificando l’ottimizzazione, soprattutto in presenza di osservazioni indipendenti. Inoltre, la funzione logaritmica consente un’esecuzione più efficiente delle operazioni numeriche, rendendo l’analisi più stabile.</p>
<section id="esempio-applicazione-della-log-verosimiglianza" class="level3" data-number="38.4.1">
<h3 data-number="38.4.1" class="anchored" data-anchor-id="esempio-applicazione-della-log-verosimiglianza"><span class="header-section-number">38.4.1</span> Esempio: Applicazione della Log-Verosimiglianza</h3>
<p>Riprendendo l’esempio della distribuzione binomiale, possiamo applicare la funzione di log-verosimiglianza per stimare il parametro <span class="math inline">\(\theta\)</span> che meglio si adatta ai dati osservati. Metodi computazionali, come l’uso di librerie statistiche in Python, ad esempio tramite <code>binom.logpmf()</code>, permettono di calcolare direttamente la log-verosimiglianza per un insieme di osservazioni e diversi valori di <span class="math inline">\(\theta\)</span>. In questo modo, è possibile identificare il valore di <span class="math inline">\(\theta\)</span> che massimizza la log-verosimiglianza e ottenere una stima accurata e computazionalmente efficiente del parametro.</p>
</section>
<section id="vantaggi-della-log-verosimiglianza" class="level3" data-number="38.4.2">
<h3 data-number="38.4.2" class="anchored" data-anchor-id="vantaggi-della-log-verosimiglianza"><span class="header-section-number">38.4.2</span> Vantaggi della Log-Verosimiglianza</h3>
<p>L’adozione della funzione di log-verosimiglianza non solo risolve problemi pratici legati alla manipolazione di probabilità molto piccole, ma offre anche una struttura concettuale più chiara per interpretare la plausibilità dei parametri del modello dati gli osservati. Questa trasformazione è fondamentale nell’analisi inferenziale, poiché migliora la precisione delle stime e la robustezza numerica durante il processo di ottimizzazione.</p>
</section>
<section id="rappresentazione-grafica" class="level3" data-number="38.4.3">
<h3 data-number="38.4.3" class="anchored" data-anchor-id="rappresentazione-grafica"><span class="header-section-number">38.4.3</span> Rappresentazione Grafica</h3>
<p>Infine, una rappresentazione grafica della funzione di log-verosimiglianza fornisce ulteriori intuizioni sul comportamento della funzione stessa e aiuta a visualizzare chiaramente il punto in cui viene massimizzata, facilitando l’interpretazione delle stime dei parametri.</p>
<div id="e59a4a11-fdcd-41e4-9f8f-c526fe5f1c47" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> <span class="dv">23</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>plt.plot(theta, stats.binom.logpmf(y, n, theta), <span class="st">"-"</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Funzione di log-verosimiglianza"</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Valore della variabile casuale theta [0, 1]"</span>)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Log-verosimiglianza"</span>)<span class="op">;</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="14_likelihood_files/figure-html/cell-15-output-1.png" width="731" height="491" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Il risultato replica quello trovato in precedenza con la funzione di verosimiglianza.</p>
<div id="51fe9970-46c4-4fbd-a554-97d20dcbeac5" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>ll <span class="op">=</span> stats.binom.logpmf(y, n, theta)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>ll.argmax()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>76</code></pre>
</div>
</div>
<div id="bd008466-3f1d-4bb7-9705-6f97c6d4fc28" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>theta[<span class="dv">76</span>]</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>0.7676767676767677</code></pre>
</div>
</div>
<p>Definizione della funzione del negativo della log-verosimiglianza con correzioni per evitare errori di dominio:</p>
<div id="078c29c3-3d45-4b26-a774-0154caa16ec9" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> corrected_negative_log_likelihood(theta, n, y):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Assicurarsi che theta sia all'interno di un intervallo valido per evitare errori di logaritmo</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.clip(theta, <span class="fl">1e-10</span>, <span class="dv">1</span><span class="op">-</span><span class="fl">1e-10</span>)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span> (y <span class="op">*</span> np.log(theta) <span class="op">+</span> (n <span class="op">-</span> y) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">-</span> theta))</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Utilizzo di <code>scipy.optimize.minimize</code> per trovare il valore di theta che massimizza la log-verosimiglianza:</p>
<div id="1823ee04-f058-424c-9188-f50d01867cfb" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>result_log_likelihood_corrected <span class="op">=</span> minimize(</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    corrected_negative_log_likelihood, x0<span class="op">=</span>[<span class="fl">0.5</span>], args<span class="op">=</span>(n, y), bounds<span class="op">=</span>[(<span class="dv">0</span>, <span class="dv">1</span>)]</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="a424f60a-c8a9-43ce-973c-f9dafbf9f2f4" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Il risultato ottimizzato per theta utilizzando la log-verosimiglianza corretta</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>optimized_theta <span class="op">=</span> result_log_likelihood_corrected.x</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>optimized_theta</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>array([0.76666666])</code></pre>
</div>
</div>
</section>
</section>
<section id="verosimiglianza-congiunta" class="level2" data-number="38.5">
<h2 data-number="38.5" class="anchored" data-anchor-id="verosimiglianza-congiunta"><span class="header-section-number">38.5</span> Verosimiglianza Congiunta</h2>
<p>Proseguendo nell’esplorazione dell’inferenza statistica basata sulla verosimiglianza, ci soffermiamo ora sul caso in cui disponiamo di più osservazioni indipendenti, tutte provenienti dalla stessa distribuzione binomiale. Questo scenario è comune nelle applicazioni pratiche, dove raccogliamo un insieme di <span class="math inline">\(n\)</span> osservazioni <span class="math inline">\(Y = [y_1, y_2, \ldots, y_n]\)</span>, ciascuna ottenuta nelle stesse condizioni sperimentali e indipendentemente dalle altre (condizione di indipendenza e identica distribuzione, IID).</p>
<p>Per analizzare queste osservazioni congiuntamente, dobbiamo calcolare la probabilità congiunta di osservare <span class="math inline">\(y_1, y_2, \ldots, y_n\)</span>, data una comune probabilità di successo <span class="math inline">\(\theta\)</span> per tutte le prove. L’indipendenza delle osservazioni ci permette di esprimere questa probabilità congiunta come il prodotto delle probabilità individuali di ciascuna osservazione:</p>
<p><span class="math display">\[
p(y_1, y_2, \ldots, y_n \mid \theta) = \prod_{i=1}^{n} p(y_i \mid \theta) = \prod_{i=1}^{n} \text{Binomiale}(y_i \mid \theta).
\]</span></p>
<p>In questo contesto, la verosimiglianza congiunta rappresenta la plausibilità complessiva di <span class="math inline">\(\theta\)</span>, dati tutti i dati osservati <span class="math inline">\(Y\)</span>. È semplicemente il prodotto delle verosimiglianze individuali di ciascuna osservazione <span class="math inline">\(y_i\)</span> rispetto a <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
\mathcal{L}(\theta \mid Y) = \prod_{i=1}^{n} \mathcal{L}(\theta \mid y_i) = \prod_{i=1}^{n} p(y_i \mid \theta).
\]</span></p>
<p>Questa formulazione della verosimiglianza congiunta evidenzia quanto bene il parametro <span class="math inline">\(\theta\)</span> si adatta all’intero insieme di dati <span class="math inline">\(Y\)</span>, offrendo un quadro coerente per l’inferenza statistica. Il parametro <span class="math inline">\(\theta\)</span> che massimizza la verosimiglianza congiunta è chiamato <strong>stimatore di massima verosimiglianza (MLE)</strong>. Questo stimatore rappresenta il valore di <span class="math inline">\(\theta\)</span> che rende l’intero set di dati <span class="math inline">\(Y\)</span> il più plausibile possibile, secondo il modello scelto.</p>
<p>L’approccio basato sulla verosimiglianza congiunta è potente, poiché ci permette di considerare tutte le osservazioni contemporaneamente, sfruttando l’informazione fornita da ciascun dato per ottenere una stima più precisa e affidabile del parametro <span class="math inline">\(\theta\)</span>.</p>
<p>Facciamo un esempio concreto. Quando disponiamo di più gruppi di osservazioni Bernoulliane indipendenti e identicamente distribuite (iid), la funzione di log-verosimiglianza congiunta per tutti i gruppi può essere espressa come la somma delle log-verosimiglianze di ciascun gruppo. Questo deriva dalla proprietà secondo cui il logaritmo di un prodotto è uguale alla somma dei logaritmi.</p>
<p>Supponiamo di avere i seguenti dati per quattro gruppi di osservazioni:</p>
<ul>
<li><strong>Gruppo 1</strong>: 30 prove con 23 successi</li>
<li><strong>Gruppo 2</strong>: 28 prove con 21 successi</li>
<li><strong>Gruppo 3</strong>: 40 prove con 31 successi</li>
<li><strong>Gruppo 4</strong>: 36 prove con 29 successi</li>
</ul>
<p>La funzione di log-verosimiglianza congiunta per questi dati, assumendo una singola probabilità di successo <span class="math inline">\(\theta\)</span> comune a tutti i gruppi, è data dalla seguente espressione:</p>
<p><span class="math display">\[
\log L(\theta) = \sum_{i=1}^{4} \left[ y_i \log(\theta) + (n_i - y_i) \log(1 - \theta) \right],
\]</span></p>
<p>dove <span class="math inline">\(n_i\)</span> e <span class="math inline">\(y_i\)</span> rappresentano rispettivamente il numero di prove e il numero di successi nel gruppo <span class="math inline">\(i\)</span>-esimo.</p>
<p>Per trovare il valore di <span class="math inline">\(\theta\)</span> che massimizza questa funzione di log-verosimiglianza, possiamo utilizzare metodi di ottimizzazione numerica come <code>scipy.optimize.minimize</code> in Python. Questo metodo ci permette di minimizzare il negativo della funzione di log-verosimiglianza, il che equivale a massimizzarla. Inoltre, per evitare problemi numerici (ad esempio, logaritmi di zero), possiamo utilizzare <code>np.clip</code> per limitare i valori di <span class="math inline">\(\theta\)</span> all’interno di un intervallo ragionevole (ad esempio, tra <span class="math inline">\(10^{-10}\)</span> e <span class="math inline">\(1 - 10^{-10}\)</span>).</p>
<div id="aaf8a603-f96a-481b-aac7-91129e725d99" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_verosimiglianza_congiunta(theta, dati):</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.clip(theta, <span class="fl">1e-10</span>, <span class="dv">1</span><span class="op">-</span><span class="fl">1e-10</span>)  <span class="co"># Evita valori esattamente 0 o 1</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    log_likelihood <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n, y <span class="kw">in</span> dati:</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>        log_likelihood <span class="op">+=</span> y <span class="op">*</span> np.log(theta) <span class="op">+</span> (n <span class="op">-</span> y) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">-</span> theta)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>log_likelihood  <span class="co"># Restituisce il negativo per l'ottimizzazione</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="f35f2fd5-2c3b-484a-8aea-f44e7f72b55d" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Dati dei gruppi: (prove, successi)</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>dati_gruppi <span class="op">=</span> [(<span class="dv">30</span>, <span class="dv">23</span>), (<span class="dv">28</span>, <span class="dv">20</span>), (<span class="dv">40</span>, <span class="dv">29</span>), (<span class="dv">36</span>, <span class="dv">29</span>)]</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dati_gruppi)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[(30, 23), (28, 20), (40, 29), (36, 29)]</code></pre>
</div>
</div>
<p>Ottimizzazione con la funzione <code>log_verosimiglianza_congiunta</code></p>
<div id="7c569f4a-d7f4-4ea3-94e7-86d300a46a9c" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> minimize(</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    log_verosimiglianza_congiunta, x0<span class="op">=</span>[<span class="fl">0.5</span>], args<span class="op">=</span>(dati_gruppi,), bounds<span class="op">=</span>[(<span class="dv">0</span>, <span class="dv">1</span>)]</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Il risultato ottimizzato per theta con la funzione corretta</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>result.x</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>array([0.75373134])</code></pre>
</div>
</div>
<div id="300c7b56-61a2-4b78-b23a-8e659c06b2b9" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Intervallo di valori di theta da esplorare</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>theta_values <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="dv">100</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcolo dei valori di log-verosimiglianza per ogni theta</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>log_likelihood_values <span class="op">=</span> [log_verosimiglianza_congiunta(theta, dati_gruppi) <span class="cf">for</span> theta <span class="kw">in</span> theta_values]</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Creazione del grafico</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>plt.plot(theta_values, log_likelihood_values, label<span class="op">=</span><span class="st">'Log-verosimiglianza'</span>)</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Theta'</span>)</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Log-verosimiglianza negativa'</span>)</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Funzione di Log-verosimiglianza'</span>)</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="14_likelihood_files/figure-html/cell-24-output-1.png" width="1011" height="611" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In questo esempio, utilizziamo <code>minimize</code> per ottimizzare la funzione di log-verosimiglianza congiunta e ottenere una stima di <span class="math inline">\(\theta\)</span>. Il valore risultante di <span class="math inline">\(\theta\)</span> rappresenta la stima di massima verosimiglianza (MLE) basata sui dati forniti.</p>
<p>In conclusione, questo esempio illustra come sia possibile stimare il parametro <span class="math inline">\(\theta\)</span> utilizzando la log-verosimiglianza congiunta in presenza di più gruppi di dati. La combinazione della funzione di log-verosimiglianza con tecniche di ottimizzazione numerica rende possibile risolvere problemi di inferenza anche con dataset più complessi o con più gruppi di osservazioni.</p>
</section>
<section id="la-verosimiglianza-marginale" class="level2" data-number="38.6">
<h2 data-number="38.6" class="anchored" data-anchor-id="la-verosimiglianza-marginale"><span class="header-section-number">38.6</span> La Verosimiglianza Marginale</h2>
<p>Proseguendo con la nostra discussione sulla verosimiglianza, approfondiamo un concetto chiave nell’inferenza bayesiana: la verosimiglianza marginale. Questo concetto è cruciale quando il parametro di interesse, <span class="math inline">\(\theta\)</span>, non è un valore fisso, ma è descritto da una distribuzione di probabilità che riflette la nostra incertezza o variabilità.</p>
<p>In molte applicazioni pratiche, <span class="math inline">\(\theta\)</span> può assumere una gamma di valori possibili, ciascuno con una certa probabilità associata, anziché essere un valore deterministico. La verosimiglianza marginale ci consente di calcolare la probabilità complessiva di osservare un determinato risultato tenendo conto di tutti i possibili valori di <span class="math inline">\(\theta\)</span>, integrando o sommando le probabilità associate a ciascun valore di <span class="math inline">\(\theta\)</span>.</p>
<section id="esempio-con-parametri-discreti" class="level3" data-number="38.6.1">
<h3 data-number="38.6.1" class="anchored" data-anchor-id="esempio-con-parametri-discreti"><span class="header-section-number">38.6.1</span> Esempio con Parametri Discreti</h3>
<p>Consideriamo una sequenza di prove binomiali, in cui stiamo osservando un risultato specifico, come <span class="math inline">\(k = 7\)</span> successi su <span class="math inline">\(n = 10\)</span> prove. Se <span class="math inline">\(\theta\)</span> rappresenta la probabilità di successo in ciascuna prova e può assumere un insieme discreto di valori (ad esempio, 0.1, 0.5 e 0.9), ciascuno con probabilità uguale, la verosimiglianza marginale può essere calcolata come:</p>
<p><span class="math display">\[
p(k = 7, n = 10) = \sum_{\theta \in \{0.1, 0.5, 0.9\}} \binom{10}{7} \theta^7 (1 - \theta)^3 p(\theta),
\]</span></p>
<p>dove <span class="math inline">\(p(\theta)\)</span> è la probabilità associata a ciascun valore di <span class="math inline">\(\theta\)</span>. In questo caso, poiché <span class="math inline">\(\theta\)</span> assume valori discreti, la somma calcola la verosimiglianza complessiva pesando i diversi valori di <span class="math inline">\(\theta\)</span> in base alla loro probabilità.</p>
</section>
<section id="esempio-con-parametri-continui" class="level3" data-number="38.6.2">
<h3 data-number="38.6.2" class="anchored" data-anchor-id="esempio-con-parametri-continui"><span class="header-section-number">38.6.2</span> Esempio con Parametri Continui</h3>
<p>Nella maggior parte delle applicazioni reali, <span class="math inline">\(\theta\)</span> può variare continuamente all’interno di un intervallo, ad esempio tra 0 e 1 in una distribuzione binomiale. In questi casi, la verosimiglianza marginale richiede l’integrazione su tutto lo spazio dei valori di <span class="math inline">\(\theta\)</span>, per tener conto della gamma continua di possibili probabilità di successo. La formula si estende a:</p>
<p><span class="math display">\[
p(k = 7, n = 10) = \int_{0}^{1} \binom{10}{7} \theta^7 (1 - \theta)^3 p(\theta) \, d\theta,
\]</span></p>
<p>dove <span class="math inline">\(p(\theta) \, d\theta\)</span> rappresenta la densità di probabilità di <span class="math inline">\(\theta\)</span> in un intervallo infinitesimale, e l’integrale copre tutti i possibili valori di <span class="math inline">\(\theta\)</span> nell’intervallo da 0 a 1.</p>
</section>
<section id="calcolo-della-verosimiglianza-marginale" class="level3" data-number="38.6.3">
<h3 data-number="38.6.3" class="anchored" data-anchor-id="calcolo-della-verosimiglianza-marginale"><span class="header-section-number">38.6.3</span> Calcolo della Verosimiglianza Marginale</h3>
<p>Per calcolare la verosimiglianza marginale in presenza di una distribuzione continua di <span class="math inline">\(\theta\)</span>, possiamo utilizzare tecniche di integrazione numerica. In Python, la libreria <code>scipy</code> offre strumenti efficienti per eseguire questo tipo di calcoli. Ad esempio, possiamo utilizzare la funzione <code>quad</code> di <code>scipy.integrate</code> per integrare la funzione su uno spazio continuo:</p>
<div id="ba1aaa69-7d39-4461-9a78-e5dad7b098d2" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Definire la funzione di verosimiglianza</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> likelihood(theta):</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> stats.binom.pmf(k<span class="op">=</span><span class="dv">7</span>, n<span class="op">=</span><span class="dv">10</span>, p<span class="op">=</span>theta)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcolare la verosimiglianza marginale integrando su θ</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>marginal_likelihood, _ <span class="op">=</span> quad(<span class="kw">lambda</span> theta: likelihood(theta), <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"La verosimiglianza marginale è:"</span>, marginal_likelihood)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>La verosimiglianza marginale è: 0.09090909090909094</code></pre>
</div>
</div>
<p>In conclusione, l’esempio presentato illustra come integrare la probabilità su un insieme continuo di valori di <span class="math inline">\(\theta\)</span> per ottenere la probabilità complessiva di osservare un determinato risultato. Il codice precedente esegue l’integrazione della funzione di verosimiglianza binomiale su tutti i possibili valori di <span class="math inline">\(\theta\)</span> (da 0 a 1), fornendo così la verosimiglianza marginale per il nostro esempio. Questo approccio consente di tenere conto dell’incertezza su <span class="math inline">\(\theta\)</span>, offrendo una visione più completa della verosimiglianza dell’evento osservato senza fissare <span class="math inline">\(\theta\)</span> a un singolo valore.</p>
<p>Dal punto di vista numerico, nel caso della verosimiglianza basata su una distribuzione binomiale, la verosimiglianza marginale può essere interpretata come l’area sottesa dalla funzione di verosimiglianza, calcolata integrandola su tutto l’intervallo dei possibili valori di <span class="math inline">\(\theta\)</span> (da 0 a 1). Questa integrazione fornisce un valore che quantifica quanto bene il modello complessivo, considerando tutti i possibili valori di <span class="math inline">\(\theta\)</span>, spiega i dati osservati. È importante notare che questo valore non rappresenta la probabilità dei dati dati i parametri, poiché la verosimiglianza non è una densità di probabilità sui parametri. Piuttosto, esso misura la capacità complessiva del modello di spiegare i dati, tenendo conto dell’incertezza sui parametri.</p>
<p>La vera rilevanza della verosimiglianza marginale emerge nel contesto dell’inferenza bayesiana: essa funge da fattore di normalizzazione nella formula di Bayes. Più precisamente, la verosimiglianza marginale normalizza il prodotto tra la verosimiglianza e la distribuzione a priori dei parametri (il numeratore nella formula di Bayes), garantendo che il risultato sia una distribuzione di probabilità valida sui parametri. In altre parole, la verosimiglianza marginale assicura che l’area sotto la curva della distribuzione posteriore sia pari a 1, rendendola una vera distribuzione di probabilità.</p>
</section>
</section>
<section id="modello-gaussiano-e-verosimiglianza" class="level2" data-number="38.7">
<h2 data-number="38.7" class="anchored" data-anchor-id="modello-gaussiano-e-verosimiglianza"><span class="header-section-number">38.7</span> Modello Gaussiano e Verosimiglianza</h2>
<p>Ampliamo ora la nostra analisi al caso della distribuzione gaussiana. Inizieremo con la verosimiglianza associata a una singola osservazione $ Y $, per poi estendere la discussione a un insieme di osservazioni gaussiane indipendenti e identicamente distribuite (IID).</p>
<section id="caso-di-una-singola-osservazione" class="level3" data-number="38.7.1">
<h3 data-number="38.7.1" class="anchored" data-anchor-id="caso-di-una-singola-osservazione"><span class="header-section-number">38.7.1</span> Caso di una Singola Osservazione</h3>
<p>Iniziamo esaminiamo il caso di una singola osservazione. Quale esempio, prendiamo in considerazione la situazione in cui una variabile casuale rappresenta il Quoziente d’Intelligenza (QI) di un individuo. Se consideriamo la distribuzione del QI come gaussiana, possiamo esprimere la funzione di verosimiglianza per un singolo valore osservato di QI tramite la formula della distribuzione gaussiana, che misura la probabilità di osservare quel particolare valore di QI dato un insieme di parametri specifici, <span class="math inline">\(\mu\)</span> (la media) e <span class="math inline">\(\sigma\)</span> (la deviazione standard). La verosimiglianza offre quindi un modo per quantificare quanto bene i parametri <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma\)</span> si accordano con il valore osservato di QI.</p>
<p>Supponiamo che il QI osservato sia 114 e, per semplicità, assumiamo che la deviazione standard <span class="math inline">\(\sigma\)</span> sia conosciuta e pari a 15. Vogliamo esaminare un’ampia gamma di possibili valori per la media <span class="math inline">\(\mu\)</span>, diciamo tra 70 e 160, e valutare quale di questi valori rende più plausibile l’osservazione fatta Definiamo quindi un insieme di 1000 valori per <span class="math inline">\(\mu\)</span> da esplorare:</p>
<div id="1e7bf1a3-c601-42fb-8312-db4033571208" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> np.linspace(<span class="fl">70.0</span>, <span class="fl">160.0</span>, num<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">114</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La nostra analisi consiste nell’applicare la funzione di densità di probabilità gaussiana a ciascuno di questi 1000 valori di <span class="math inline">\(\mu\)</span>, mantenendo fisso il valore osservato di QI, <span class="math inline">\(y=114\)</span>, e la deviazione standard, <span class="math inline">\(\sigma=15\)</span>. In questo modo, possiamo costruire la funzione di verosimiglianza che esprime la plausibilità di ciascun valore di <span class="math inline">\(\mu\)</span> alla luce del QI osservato.</p>
<p>Il calcolo specifico della densità di probabilità per ogni valore di <span class="math inline">\(\mu\)</span> può essere eseguito con la funzione <code>norm.pdf</code> di <code>scipy.stats</code>, che accetta il valore osservato <span class="math inline">\(y\)</span>, un array di medie (i nostri valori di <span class="math inline">\(\mu\)</span>) e la deviazione standard <span class="math inline">\(\sigma\)</span>. Per un singolo valore <code>mu</code> = 70, otteniamo</p>
<div id="5979501b-ef77-429a-85e0-538901b4d4f8" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>stats.norm.pdf(y, loc<span class="op">=</span><span class="dv">70</span>, scale<span class="op">=</span><span class="dv">15</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>0.00036007041207962535</code></pre>
</div>
</div>
<p>Per il valore <code>mu</code> = 70.05 otteniamo</p>
<div id="53682fa2-40c9-4ce6-b8c8-600b08543ba1" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>stats.norm.pdf(y, loc<span class="op">=</span><span class="fl">70.05</span>, scale<span class="op">=</span><span class="dv">15</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>0.00036360634900376967</code></pre>
</div>
</div>
<p>e così via. Se usiamo utti i 1000 valori possibili di <code>mu</code>, otteniamo un vettore di 1000 risultati:</p>
<div id="726e752b-6324-4b3f-b1d2-f2b0556cccde" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>f_mu <span class="op">=</span> stats.norm.pdf(y, loc<span class="op">=</span>mu, scale<span class="op">=</span><span class="dv">15</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Questo passaggio ci fornisce un array di valori che rappresentano la verosimiglianza di ciascun valore di <span class="math inline">\(\mu\)</span> data l’osservazione <span class="math inline">\(y\)</span>. Tracciando questi valori <code>f_mu</code> in funzione di <span class="math inline">\(\mu\)</span>, otteniamo una curva di verosimiglianza che illustra visivamente quanto bene ciascun valore di <span class="math inline">\(\mu\)</span> si adatta al dato osservato <code>y</code> = 114:</p>
<div id="fd4c49fa-4144-4874-a9db-8fe0b3d1910a" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>plt.plot(mu, f_mu, <span class="st">"-"</span>)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Funzione di verosimiglianza per QI = 114"</span>)</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Valore di mu [70, 160]"</span>)</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Verosimiglianza"</span>)</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>plt.xlim([<span class="dv">70</span>, <span class="dv">160</span>])</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="14_likelihood_files/figure-html/cell-30-output-1.png" width="731" height="491" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Abbiamo dunque proceduto come nel caso della distribuzione binomiale esaminata in precedenza. Abbiamo utilizzato la formula</p>
<p><span class="math display">\[
f(x | \mu, \sigma) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right),
\]</span></p>
<p>tenendo costante il valore <span class="math inline">\(x\)</span> = 114 e considerando noto <span class="math inline">\(\sigma\)</span> = 15, e abbiamo applicato la formula 1000 volte facendo variare <code>mu</code> ogni volta utilizziando ciascuno dei valori definiti con <code>np.linspace(70.0, 160.0, num=1000)</code>.</p>
<p>La moda della distribuzione, si trova con</p>
<div id="47eabf93-c1d8-4cc9-841f-be3f70daf2b9" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>optimal_mu <span class="op">=</span> mu[f_mu.argmax()]</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(optimal_mu)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>113.96396396396396</code></pre>
</div>
</div>
<p>In questo esempio, otteniamo il valore <span class="math inline">\(\mu\)</span> = 113.96 che massimizza la verosimiglianza.</p>
<p>Per calcolare il massimo della log-verosimiglianza per una distribuzione Gaussiana usando la funzione <code>optimize()</code> di SciPy, possiamo seguire questi passi. Partiamo dalla formula della densità di probabilità della distribuzione gaussiana per una singola osservazione <span class="math inline">\(y\)</span>, con media <span class="math inline">\(\mu\)</span> e deviazione standard <span class="math inline">\(\sigma\)</span>. La formula è:</p>
<p><span class="math display">\[
f(y \mid \mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi}} \exp \left( -\frac{(y - \mu)^2}{2\sigma^2} \right)
\]</span></p>
<p>Poiché abbiamo una singola osservazione <span class="math inline">\(y\)</span>, la funzione di verosimiglianza coincide con la funzione di densità di probabilità. Quindi, prendiamo il logaritmo naturale di entrambi i lati della equazione della densità di probabilità gaussiana per ottenere la log-verosimiglianza:</p>
<p><span class="math display">\[
\log f(y \mid \mu, \sigma) = \log \left( \frac{1}{\sigma \sqrt{2\pi}} \exp \left( -\frac{(y - \mu)^2}{2\sigma^2} \right) \right)
\]</span></p>
<p>Applichiamo le proprietà dei logaritmi. Ricordiamo che:</p>
<ul>
<li><span class="math inline">\(\log(ab) = \log(a) + \log(b)\)</span></li>
<li><span class="math inline">\(\log\left(\frac{1}{a}\right) = -\log(a)\)</span></li>
<li><span class="math inline">\(\log(e^x) = x\)</span></li>
</ul>
<p>Quindi, possiamo scrivere:</p>
<p><span class="math display">\[
\log f(y \mid \mu, \sigma) = \log\left(\frac{1}{\sigma \sqrt{2\pi}}\right) + \log\left(\exp \left( -\frac{(y - \mu)^2}{2\sigma^2} \right)\right)
\]</span></p>
<p><span class="math display">\[
= -\log(\sigma \sqrt{2\pi}) -\frac{(y - \mu)^2}{2\sigma^2}.
\]</span></p>
<p>Ricordando che <span class="math inline">\(\log(ab) = \log(a) + \log(b)\)</span>, possiamo scrivere <span class="math inline">\(\log(\sigma \sqrt{2\pi})\)</span> come la somma di due logaritmi:</p>
<p><span class="math display">\[
-\log(\sigma \sqrt{2\pi}) = -\log(\sigma) - \log(\sqrt{2\pi}).
\]</span></p>
<p>E dato che <span class="math inline">\(\log(\sqrt{2\pi}) = \frac{1}{2}\log(2\pi)\)</span>, possiamo sostituire per ottenere:</p>
<p><span class="math display">\[
-\log(\sigma) - \frac{1}{2}\log(2\pi).
\]</span></p>
<p>Combinando tutto, otteniamo:</p>
<p><span class="math display">\[
\log L(\mu; y, \sigma) = -\frac{1}{2} \log(2 \pi) - \log(\sigma) - \frac{(y - \mu)^2}{2 \sigma^2}.
\]</span></p>
<p>Questa è la trasformata logaritmica della funzione di densità di probabilità gaussiana per una singola osservazione, che rappresenta la log-verosimiglianza di osservare <span class="math inline">\(y\)</span> dato <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma\)</span>.</p>
<p>Vogliamo trovare il valore di <span class="math inline">\(\mu\)</span> che massimizza questa funzione di log-verosimiglianza. Siccome <code>optimize()</code> di SciPy minimizza una funzione, possiamo passare il negativo della log-verosimiglianza per trovare il massimo.</p>
<div id="fd8d1548-fee9-4d37-bb54-269803b2c908" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Dati osservati</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>y_obs <span class="op">=</span> <span class="dv">114</span></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Definizione della funzione negativa della log-verosimiglianza</span></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> negative_log_likelihood(mu, y, sigma):</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> np.log(<span class="dv">2</span> <span class="op">*</span> np.pi) <span class="op">+</span> np.log(sigma) <span class="op">+</span> ((y <span class="op">-</span> mu)<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> sigma<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Ottimizzazione per trovare il valore di mu che massimizza la log-verosimiglianza</span></span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> minimize(negative_log_likelihood, x0<span class="op">=</span><span class="dv">0</span>, args<span class="op">=</span>(y_obs, sigma))</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Il risultato ottimizzato per mu</span></span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>result.x</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>array([113.99997648])</code></pre>
</div>
</div>
<p>Il valore di <span class="math inline">\(\mu\)</span> che massimizza la log-verosimiglianza per una distribuzione Gaussiana con <span class="math inline">\(y = 114\)</span> e <span class="math inline">\(\sigma = 15\)</span> è circa <span class="math inline">\(114\)</span>. Questo risultato dimostra che, nel caso di una distribuzione Gaussiana con una singola osservazione e deviazione standard nota, il massimo della log-verosimiglianza si ottiene quando la media stimata <span class="math inline">\(\mu\)</span> è molto vicina al valore osservato <span class="math inline">\(y\)</span>.</p>
</section>
<section id="campione-indipendente-di-osservazioni-da-una-distribuzione-gaussiana" class="level3" data-number="38.7.2">
<h3 data-number="38.7.2" class="anchored" data-anchor-id="campione-indipendente-di-osservazioni-da-una-distribuzione-gaussiana"><span class="header-section-number">38.7.2</span> Campione indipendente di osservazioni da una distribuzione gaussiana</h3>
<p>Passiamo ora all’esame di un contesto più complesso: quello di un campione composto da <span class="math inline">\(n\)</span> osservazioni indipendenti, tutte provenienti da una distribuzione gaussiana. Consideriamo questo insieme di osservazioni come realizzazioni indipendenti ed identicamente distribuite (i.i.d.) di una variabile casuale <span class="math inline">\(X\)</span>, che segue una distribuzione normale con media $ $ e deviazione standard <span class="math inline">\(\sigma\)</span>, entrambi parametri sconosciuti. Denotiamo questa situazione con la notazione <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>.</p>
<p>In presenza di osservazioni i.i.d., la densità di probabilità congiunta del campione è il prodotto delle funzioni di densità per ogni singola osservazione. Matematicamente, ciò si esprime attraverso l’equazione:</p>
<p><span class="math display">\[ p(y_1, y_2, \ldots, y_n | \mu, \sigma) = \prod_{i=1}^{n} p(y_i | \mu, \sigma), \]</span></p>
<p>dove <span class="math inline">\(p(y_i | \mu, \sigma)\)</span> indica la funzione di densità gaussiana per l’osservazione <span class="math inline">\(y_i\)</span>, parametrizzata da <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma\)</span>.</p>
<p>Se manteniamo i dati osservati come costanti, ciò che cambia in questa equazione quando variamo $ $ e <span class="math inline">\(\sigma\)</span> sono le probabilità associate ad ogni configurazione dei parametri, portandoci così alla funzione di verosimiglianza congiunta per il campione.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 38.2</strong></span> Consideriamo, per illustrare questa dinamica, il caso di uno studio clinico che misura i punteggi del Beck Depression Inventory II (BDI-II) su trenta partecipanti. Supponiamo che questi punteggi seguano una distribuzione normale. Dati i punteggi BDI-II per i trenta partecipanti, il nostro obiettivo è costruire una funzione di verosimiglianza per questi dati, assumendo che la deviazione standard <span class="math inline">\(\sigma\)</span> sia nota e pari alla deviazione standard campionaria di 6.50.</p>
<p>Per la totalità del campione, la densità di probabilità congiunta diventa quindi il prodotto delle densità per ogni osservazione. Di conseguenza, la funzione di verosimiglianza per il campione intero è rappresentata dal prodotto delle densità di probabilità di tutte le osservazioni.</p>
<p>In questo contesto, ogni possibile valore di <span class="math inline">\(\mu\)</span> viene valutato in termini di verosimiglianza. Per esemplificare, consideriamo un range di 1000 valori per <span class="math inline">\(\mu\)</span> e calcoliamo la funzione di verosimiglianza per ognuno di questi. Per rendere più gestibili i calcoli, utilizziamo il logaritmo della funzione di verosimiglianza.</p>
<p>Definendo una funzione <code>log_likelihood</code> in Python che accetta i punteggi BDI-II <span class="math inline">\(y\)</span>, un valore medio <span class="math inline">\(\mu\)</span>, e imposta <span class="math inline">\(\sigma\)</span> al valore noto, possiamo calcolare la log-verosimiglianza per un’ampia gamma di valori di <span class="math inline">\(\mu\)</span> entro un intervallo specifico. Ciò ci permette di visualizzare la credibilità relativa di ciascun valore di <span class="math inline">\(\mu\)</span> alla luce dei dati osservati.</p>
<p>Infine, il valore di <span class="math inline">\(\mu\)</span> che massimizza la funzione di log-verosimiglianza corrisponde alla stima di massima verosimiglianza di <span class="math inline">\(\mu\)</span> data la distribuzione dei punteggi BDI-II nel campione. Questo valore, nel nostro esempio, coincide con la media campionaria dei punteggi BDI-II, offrendo una stima concorde con l’intuizione che la media del campione sia un buon rappresentante del parametro <span class="math inline">\(\mu\)</span> in una distribuzione normale.</p>
<p>I dati sono:</p>
<div id="4a117251-93a9-4dfa-960e-09bf6f9019ff" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> [</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    <span class="dv">26</span>, <span class="dv">35</span>, <span class="dv">30</span>, <span class="dv">25</span>, <span class="dv">44</span>, <span class="dv">30</span>, <span class="dv">33</span>, <span class="dv">43</span>, <span class="dv">22</span>, <span class="dv">43</span>, <span class="dv">24</span>, <span class="dv">19</span>, <span class="dv">39</span>, <span class="dv">31</span>, <span class="dv">25</span>,</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>    <span class="dv">28</span>, <span class="dv">35</span>, <span class="dv">30</span>, <span class="dv">26</span>, <span class="dv">31</span>, <span class="dv">41</span>, <span class="dv">36</span>, <span class="dv">26</span>, <span class="dv">35</span>, <span class="dv">33</span>, <span class="dv">28</span>, <span class="dv">27</span>, <span class="dv">34</span>, <span class="dv">27</span>, <span class="dv">22</span>,</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Il nostro scopo è sviluppare una funzione di verosimiglianza utilizzando le 30 osservazioni indicate sopra. Basandoci su studi precedenti, ipotizziamo che questi punteggi seguano una distribuzione normale. Assumiamo inoltre che la deviazione standard <span class="math inline">\(\sigma\)</span> sia nota e corrisponda a quella osservata nel campione, ossia 6.50.</p>
<p>Per la prima osservazione del campione, dove <span class="math inline">\(y_1 = 26\)</span>, la funzione di densità di probabilità si esprime come:</p>
<p><span class="math display">\[
f(26 \,|\, \mu, \sigma = 6.50) = \frac{1}{6.50\sqrt{2\pi}} \exp \left( -\frac{(26 - \mu)^2}{2 \cdot 6.50^2} \right).
\]</span></p>
<p>Estendendo questo calcolo all’intero campione, la funzione di densità di probabilità congiunta si ottiene come il prodotto delle densità di tutte le osservazioni individuali:</p>
<p><span class="math display">\[
f(y \,|\, \mu, \sigma = 6.50) = \prod_{i=1}^{n} f(y_i \,|\, \mu, \sigma = 6.50).
\]</span></p>
<p>Di conseguenza, la funzione di verosimiglianza, indicata con <span class="math inline">\(\mathcal{L}(\mu, \sigma = 6.50 \,|\, y)\)</span>, si determina moltiplicando insieme le densità di probabilità di tutte le osservazioni nel campione:</p>
<p><span class="math display">\[
\begin{aligned}
\mathcal{L}(\mu, \sigma=6.50 \,|\, y) &amp;= \prod_{i=1}^{30} \frac{1}{6.50\sqrt{2\pi}} \exp \left( -\frac{(y_i - \mu)^2}{2 \cdot 6.50^2} \right) \\
&amp;= \left( \frac{1}{6.50\sqrt{2\pi}} \right)^{30} \exp\left( -\sum_{i=1}^{30} \frac{(y_i - \mu)^2}{2 \cdot 6.50^2} \right).
\end{aligned}
\]</span></p>
<p>In questa formula, <span class="math inline">\(\mu\)</span> rappresenta il parametro di interesse, la media della distribuzione, la cui stima massimizza la funzione di verosimiglianza. Se si considerano 1000 valori differenti per <span class="math inline">\(\mu\)</span>, dovremmo calcolare la funzione di verosimiglianza per ciascuno di questi valori.</p>
<p>Per rendere i calcoli più gestibili, è consigliabile utilizzare il logaritmo della funzione di verosimiglianza. In Python, possiamo definire una funzione <code>log_likelihood()</code> che accetta come argomenti <code>y</code>, <code>mu</code> e <code>sigma = true_sigma</code>. Per semplificare, impostiamo <code>true_sigma</code> uguale alla deviazione standard osservata nel campione.</p>
<div id="f29f8230-961c-4469-82bd-9d4d034b5999" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>true_sigma <span class="op">=</span> np.std(y)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(true_sigma)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>6.495810615739622</code></pre>
</div>
</div>
<div id="52306120-fefd-4548-bfef-98a3c68097d0" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_likelihood(y, mu, sigma<span class="op">=</span>true_sigma):</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">sum</span>(stats.norm.logpdf(y, loc<span class="op">=</span>mu, scale<span class="op">=</span>true_sigma))</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Consideriamo, ad esempio, il valore <span class="math inline">\(\mu_0 = \bar{y}\)</span>, ovvero</p>
<div id="c366b5cd-cff0-427f-bd55-8eb45374a171" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>bar_y <span class="op">=</span> np.mean(y)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(bar_y)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>30.933333333333334</code></pre>
</div>
</div>
<p>L’ordinata della funzione di log-verosimiglianza in corrispondenza di <span class="math inline">\(\mu = 30.93\)</span> è</p>
<div id="3a11b51f-e578-4b55-a83d-2a0c48555c35" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>log_likelihood(y, <span class="fl">30.93</span>, sigma<span class="op">=</span>true_sigma)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>-98.70288339960591</code></pre>
</div>
</div>
<p>Troviamo ora i valori della log-verosimiglianza per ciascuno dei 1000 valori <span class="math inline">\(\mu\)</span> nell’intervallo <span class="math inline">\([\bar{y} - 2 \sigma, \bar{y} + 2 \sigma]\)</span>. Iniziamo a definire il vettore <code>mu</code>.</p>
<div id="41cb06e4-e7e5-4dbb-b9f4-939ae49cc0bc" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> np.linspace(np.mean(y) <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> np.std(y), np.mean(y) <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.std(y), num<span class="op">=</span><span class="dv">1000</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Troviamo il valore dell’ordinata della funzione di log-verosimiglianza in corrispondenza di ciascuno dei 1000 valori <code>mu</code> che abbiamo definito.</p>
<div id="3bbcf36a-d7c4-4ba0-b559-789de63f21e8" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>ll <span class="op">=</span> [log_likelihood(y, mu_val, true_sigma) <span class="cf">for</span> mu_val <span class="kw">in</span> mu]</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Nel caso di un solo parametro sconosciuto (nel caso presente, <span class="math inline">\(\mu\)</span>) è possibile rappresentare la log-verosimiglianza con una curva che interpola i punti (<code>mu</code>, <code>ll</code>). Tale funzione descrive la <em>credibilità relativa</em> che può essere attribuita ai valori del parametro <span class="math inline">\(\mu\)</span> alla luce dei dati osservati.</p>
<div id="492abac6-60a8-412c-9f85-0f7fcfba36d0" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>plt.plot(mu, ll, <span class="st">"-"</span>)</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Funzione di log-verosimiglianza"</span>)</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Valore della variabile casuale mu"</span>)</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Log-verosimiglianza"</span>)</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span>np.mean(y), alpha<span class="op">=</span><span class="fl">0.4</span>, ls<span class="op">=</span><span class="st">"--"</span>)<span class="op">;</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="14_likelihood_files/figure-html/cell-40-output-1.png" width="731" height="491" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Il valore <span class="math inline">\(\mu\)</span> più credibile corrisponde al massimo della funzione di log-verosimiglinza e viene detto <em>stima di massima verosimiglianza</em>.</p>
<p>Il massimo della funzione di log-verosimiglianza, ovvero 30.93 per l’esempio in discussione, è identico alla media dei dati campionari.</p>
<p>Per applicare lo stesso approccio usato precedentemente con <code>optimize</code> ad un campione di dati, anziché a una singola osservazione, possiamo modificare la funzione di log-verosimiglianza per prendere in considerazione tutte le osservazioni nel campione. La log-verosimiglianza per un campione da una distribuzione Gaussiana, dove ogni osservazione <span class="math inline">\(y_i\)</span> ha la stessa media <span class="math inline">\(\mu\)</span> e deviazione standard <span class="math inline">\(\sigma\)</span>, è la somma delle log-verosimiglianze di ogni osservazione individuale.</p>
<p>La formula modificata per il campione sarà:</p>
<p><span class="math display">\[
\log L(\mu; y, \sigma) = \sum_{i=1}^{n} \left[ -\frac{1}{2} \log(2 \pi) - \log(\sigma) - \frac{(y_i - \mu)^2}{2 \sigma^2} \right],
\]</span></p>
<p>dove <span class="math inline">\(y\)</span> è l’array delle osservazioni e <span class="math inline">\(n\)</span> è il numero di osservazioni nel campione.</p>
<p>Poiché, per semplicità, assumiamo <span class="math inline">\(\sigma\)</span> come la deviazione standard del campione, prima calcoleremo <span class="math inline">\(\sigma\)</span> dal campione fornito e poi useremo quel valore per l’ottimizzazione della log-verosimiglianza, cercando il valore di <span class="math inline">\(\mu\)</span> che la massimizza.</p>
<div id="4dd4af98-0adc-48c3-9295-0a5e400fea06" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcolo della deviazione standard del campione</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>sigma_sample <span class="op">=</span> np.std(y, ddof<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Definizione della funzione negativa della log-verosimiglianza per il campione</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> negative_log_likelihood_sample(mu, y, sigma):</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(y)</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> n <span class="op">*</span> <span class="fl">0.5</span> <span class="op">*</span> np.log(<span class="dv">2</span> <span class="op">*</span> np.pi) <span class="op">+</span> n <span class="op">*</span> np.log(sigma) <span class="op">+</span> np.<span class="bu">sum</span>((y <span class="op">-</span> mu)<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> sigma<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Ottimizzazione per trovare il valore di mu che massimizza la log-verosimiglianza per il campione</span></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>result_sample <span class="op">=</span> minimize(negative_log_likelihood_sample, x0<span class="op">=</span>np.mean(y), args<span class="op">=</span>(y, sigma_sample))</span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Il risultato ottimizzato per mu</span></span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>result_sample.x</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>array([30.93333333])</code></pre>
</div>
</div>
<p>Il valore di <span class="math inline">\(\mu\)</span> che massimizza la log-verosimiglianza per il campione di dati fornito, assumendo noto il valore di <span class="math inline">\(\sigma\)</span> (la deviazione standard del campione), è circa <span class="math inline">\(30.93\)</span>. Questo rappresenta la stima ottimale per la media della distribuzione Gaussiana che meglio si adatta al campione di dati dato.</p>
</div>
</section>
<section id="la-stima-di-massima-verosimiglianza-per-mu" class="level3" data-number="38.7.3">
<h3 data-number="38.7.3" class="anchored" data-anchor-id="la-stima-di-massima-verosimiglianza-per-mu"><span class="header-section-number">38.7.3</span> La Stima di Massima Verosimiglianza per <span class="math inline">\(\mu\)</span></h3>
<p>Per determinare il valore di <span class="math inline">\(\mu\)</span> che massimizza la funzione di log-verosimiglianza, procediamo calcolando la sua derivata parziale rispetto a <span class="math inline">\(\mu\)</span> e impostando il risultato uguale a zero:</p>
<ol type="1">
<li><p>Partiamo dalla funzione di log-verosimiglianza, che è data da:</p>
<p><span class="math display">\[
\ell = -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \mu)^2.
\]</span></p></li>
<li><p>Calcoliamo la derivata parziale di $ $ rispetto a $ $:</p>
<p><span class="math display">\[
\frac{\partial \ell}{\partial \mu} = \sum_{i=1}^n \frac{(y_i - \mu)}{\sigma^2}.
\]</span></p></li>
<li><p>Impostiamo la derivata uguale a zero per trovare il punto di massimo:</p>
<p><span class="math display">\[
\frac{1}{\sigma^2} \sum_{i=1}^n (y_i - \mu) = 0.
\]</span></p></li>
</ol>
<p>Risolvendo questa equazione per $ $, otteniamo la stima di massima verosimiglianza:</p>
<p><span class="math display">\[
\hat{\mu}_{MLE} = \frac{1}{n} \sum_{i=1}^n y_i = \bar{y}.
\]</span></p>
<p>Questa formula ci mostra che la stima di massima verosimiglianza per <span class="math inline">\(\mu\)</span> corrisponde semplicemente alla media aritmetica delle osservazioni.</p>
<p>Questo processo può essere analogamente applicato per stimare <span class="math inline">\(\sigma^2\)</span>, la varianza, e si trova che la stima di massima verosimiglianza per <span class="math inline">\(\sigma^2\)</span> è pari alla varianza campionaria.</p>
<p>In conclusione, all’interno di una distribuzione gaussiana, le stime di massima verosimiglianza per <span class="math inline">\(\mu\)</span> (la media) e <span class="math inline">\(\sigma^2\)</span> (la varianza) coincidono con la media campionaria e la varianza campionaria, rispettivamente.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 38.3</strong></span> Consideriamo un esempio relativo all’apprendimento per rinforzo. Lo scopo degli studi sull’apprendimento per rinforzo è quello di comprendere come le persone imparano a massimizzare le loro ricompense in situazioni in cui la scelta migliore è inizialmente sconosciuta. In modo più specifico, consideriamo il seguente problema di apprendimento. Un partecipante deve effettuare ripetutamente delle scelte tra diverse opzioni o azioni, e dopo ogni scelta riceve una ricompensa numerica estratta da una distribuzione di probabilità che dipende dall’azione selezionata. L’obiettivo del partecipante è massimizzare la ricompensa totale attesa durante un certo periodo di tempo, ad esempio, durante 100 scelte. Per descrivere questa situazione, viene spesso utilizzata la metafora di un giocatore che deve fare una serie di <span class="math inline">\(T\)</span> scelte tra <span class="math inline">\(K\)</span> slot machine (conosciute anche come “multi-armed bandits”) al fine di massimizzare le sue vincite. Se nella scelta <span class="math inline">\(t\)</span> viene selezionata la slot machine <span class="math inline">\(k\)</span>, viene ottenuta una ricompensa <span class="math inline">\(r_t\)</span> che ha valore <code>1</code> con una probabilità di successo <span class="math inline">\(\mu^k_t\)</span>, altrimenti ha valore <code>0</code>. Le probabilità di successo sono diverse per ogni slot machine e inizialmente sono sconosciute al partecipante. Nella versione più semplice di questo compito, le probabilità di successo rimangono costanti nel tempo.</p>
<p>Il modello di Rescorla-Wagner è un modello di apprendimento associativo che descrive come gli animali o gli umani aggiornano le loro aspettative di rinforzo in risposta a stimoli. Il modello può essere descritto con la seguente formula di aggiornamento:</p>
<p><span class="math display">\[ V_{t+1} = V_t + \alpha (\lambda - V_t), \]</span></p>
<p>dove:</p>
<ul>
<li><span class="math inline">\(V_t\)</span> è il valore predetto del rinforzo al tempo <span class="math inline">\(t\)</span>,</li>
<li><span class="math inline">\(\alpha\)</span> è il tasso di apprendimento, un parametro che vogliamo stimare,</li>
<li><span class="math inline">\(\lambda\)</span> è l’intensità del rinforzo,</li>
<li><span class="math inline">\(V_{t+1}\)</span> è il valore aggiornato dopo aver sperimentato il rinforzo.</li>
</ul>
<p>Per semplificare, consideriamo un caso in cui gli stimoli si presentano in maniera binaria (rinforzo presente o assente), e <span class="math inline">\(\lambda\)</span> è noto. L’obiettivo è stimare il valore di <span class="math inline">\(\alpha\)</span> che massimizza la verosimiglianza dei dati osservati sotto il modello.</p>
<p>La funzione di verosimiglianza per questo modello dipende dalla differenza tra i valori predetti e gli effettivi rinforzi ricevuti. Tuttavia, la formulazione esatta della funzione di verosimiglianza può variare a seconda della specifica formulazione del problema e dei dati disponibili. Per mantenere le cose semplici, consideriamo una versione semplificata in cui la “verosimiglianza” è basata sulla somma dei quadrati degli errori (SSE) tra i rinforzi previsti e quelli osservati (anche se tecnicamente questo non è un approccio basato sulla verosimiglianza nel senso statistico classico).</p>
<p>Per questo esempio, assumiamo di avere un semplice set di dati di rinforzi osservati e vogliamo trovare il valore di <span class="math inline">\(\alpha\)</span> che minimizza l’SSE:</p>
<p><span class="math display">\[ SSE = \sum_{t=1}^{n} (\lambda - V_t)^2. \]</span></p>
<p>Ecco un esempio di implementazione in Python che utilizza <code>scipy.optimize.minimize</code> per stimare <span class="math inline">\(\alpha\)</span>:</p>
<div id="d5659652-f986-49ff-80ec-710c1ab85abc" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Dati di esempio: rinforzi osservati (lambda)</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="co"># In questo esempio, assumiamo lambda = 1 per rinforzo presente e lambda = 0 per rinforzo assente</span></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="co"># per semplicità. In pratica, lambda potrebbe essere diverso a seconda degli esperimenti.</span></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>rinforzi_osservati <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>]  <span class="co"># Esempio di sequenza di rinforzi</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="396ece00-1b4d-4f76-905b-b24df6f31fdb" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Funzione che calcola l'SSE per un dato valore di alpha</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sse(alpha, rinforzi, V0<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> V0</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    sse <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> lambda_ <span class="kw">in</span> rinforzi:</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>        sse <span class="op">+=</span> (lambda_ <span class="op">-</span> V)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>        V <span class="op">+=</span> alpha <span class="op">*</span> (lambda_ <span class="op">-</span> V)  <span class="co"># Aggiornamento del valore secondo il modello Rescorla-Wagner</span></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sse</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Ottimizzazione per trovare il valore di alpha che minimizza l'SSE</span></span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>result_alpha <span class="op">=</span> minimize(sse, x0<span class="op">=</span><span class="fl">0.5</span>, args<span class="op">=</span>(rinforzi_osservati,))</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Il risultato ottimizzato per alpha</span></span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>result_alpha.x</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="43">
<pre><code>array([0.29739989])</code></pre>
</div>
</div>
<p>Il valore di <span class="math inline">\(\alpha\)</span> (tasso di apprendimento) che minimizza la somma dei quadrati degli errori (SSE) per il modello di Rescorla-Wagner, dato il campione di rinforzi osservati, è circa <span class="math inline">\(0.297\)</span>. Questo suggerisce che il tasso di apprendimento ottimale per adattare il modello ai dati osservati in questo esempio semplificato è di circa 0.297, secondo l’approccio di minimizzazione dell’errore utilizzato qui.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 38.4</strong></span> Consideriamo ora un esempio relativo alla distribuzione esponenziale. Supponiamo che i seguenti siano i tempi di attesa per un certo evento:</p>
<div id="d453296c-aa5c-4f4d-9f4b-e022a1f7c343" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array([<span class="dv">27</span>, <span class="dv">64</span>, <span class="dv">3</span>, <span class="dv">18</span>, <span class="dv">8</span>])</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Definiamo la funzione di log-verosimiglianza negativa. Per iniziare, ricordiamo che la funzione di densità di probabilità (PDF) per una distribuzione esponenziale, dato un tasso <span class="math inline">\(\lambda\)</span>, è definita come:</p>
<p><span class="math display">\[
f(x; \lambda) = \lambda e^{-\lambda x} \quad \text{per } x \geq 0.
\]</span></p>
<p>La verosimiglianza (<span class="math inline">\(L\)</span>) di osservare un insieme di dati <span class="math inline">\(\{x_1, x_2, ..., x_n\}\)</span> dato un parametro <span class="math inline">\(\lambda\)</span> è il prodotto delle funzioni di densità di probabilità per ogni punto dati, assumendo che ciascun dato sia indipendente dagli altri. Quindi, per <span class="math inline">\(n\)</span> dati osservati, la funzione di verosimiglianza è:</p>
<p><span class="math display">\[
L(\lambda) = \prod_{i=1}^{n} f(x_i; \lambda) = \prod_{i=1}^{n} \lambda e^{-\lambda x_i}
\]</span></p>
<p>La log-verosimiglianza (<span class="math inline">\(\log(L(\lambda))\)</span>) è il logaritmo naturale di <span class="math inline">\(L(\lambda)\)</span>. Utilizziamo il logaritmo per semplificare la moltiplicazione in una somma, il che rende più semplici sia il calcolo che la differenziazione. Pertanto, la log-verosimiglianza diventa:</p>
<p><span class="math display">\[
\log(L(\lambda)) = \log\left(\prod_{i=1}^{n} \lambda e^{-\lambda x_i}\right) = \sum_{i=1}^{n} \log(\lambda e^{-\lambda x_i}) = \sum_{i=1}^{n} (\log(\lambda) - \lambda x_i)
\]</span></p>
<p>Il motivo per utilizzare il negativo della log-verosimiglianza, cioè <span class="math inline">\(-\log(L(\lambda))\)</span>, nelle tecniche di ottimizzazione, è perché molte librerie e funzioni di ottimizzazione sono progettate per minimizzare una funzione obiettivo piuttosto che massimizzarla. Dato che vogliamo trovare il valore di <span class="math inline">\(\lambda\)</span> che massimizza la log-verosimiglianza (e quindi la verosimiglianza), possiamo invece minimizzare il suo negativo. Di conseguenza, la funzione obiettivo che passiamo all’algoritmo di minimizzazione è:</p>
<p><span class="math display">\[
-\log(L(\lambda)) = -\sum_{i=1}^{n} (\log(\lambda) - \lambda x_i)
\]</span></p>
<p>Scriviamo la funzione in Python:</p>
<div id="740022bc-bf1c-4306-84e5-fd81d7b796ba" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> neg_log_likelihood(lambda_, data, eps<span class="op">=</span><span class="fl">1e-8</span>):</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>    lambda_ <span class="op">=</span> np.clip(lambda_, eps, <span class="va">None</span>)  <span class="co"># Assicura che lambda_ sia almeno eps</span></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.<span class="bu">sum</span>(np.log(lambda_) <span class="op">-</span> lambda_ <span class="op">*</span> data)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Minimizzaziamo la funzione di log-verosimiglianza negativa:</p>
<div id="e8674b5c-60ff-4ed4-9adf-bc8865b93e53" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> minimize(neg_log_likelihood, x0<span class="op">=</span><span class="fl">0.1</span>, args<span class="op">=</span>(data,), bounds<span class="op">=</span>[(<span class="dv">0</span>, <span class="va">None</span>)])</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Il valore di lambda che massimizza la log-verosimiglianza è: </span><span class="sc">{</span>result<span class="sc">.</span>x[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Il valore di lambda che massimizza la log-verosimiglianza è: 0.04166666292998713</code></pre>
</div>
</div>
<p>Avendo trovato il tasso <span class="math inline">\(\lambda\)</span>, la stima di massima verosimiglianza del tempo di attesa medio diventa:</p>
<div id="4d4fcbb8-20c2-4ad8-a73b-90a2a8183f35" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="op">/</span> result.x</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>array([24.00000215])</code></pre>
</div>
</div>
<p>Visualizzazione.</p>
<div id="eeb2b512-3bb6-4d93-b74b-96aef7c23082" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>lambda_opt <span class="op">=</span> result.x[<span class="dv">0</span>]</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>lambda_array <span class="op">=</span> np.geomspace(<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">100</span>)</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>LL <span class="op">=</span> [<span class="op">-</span>neg_log_likelihood(L, data) <span class="cf">for</span> L <span class="kw">in</span> lambda_array]</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>plt.plot(lambda_array, LL, label<span class="op">=</span><span class="st">'Log-likelihood'</span>)</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>plt.axvline(lambda_opt, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="ss">f'Optimal $\lambda$ = </span><span class="sc">{</span>lambda_opt<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'$\lambda$'</span>)</span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Log-likelihood'</span>)</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Log-likelihood over a range of $\lambda$ values'</span>)</span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>&lt;&gt;:6: SyntaxWarning: invalid escape sequence '\l'
&lt;&gt;:7: SyntaxWarning: invalid escape sequence '\l'
&lt;&gt;:9: SyntaxWarning: invalid escape sequence '\l'
&lt;&gt;:6: SyntaxWarning: invalid escape sequence '\l'
&lt;&gt;:7: SyntaxWarning: invalid escape sequence '\l'
&lt;&gt;:9: SyntaxWarning: invalid escape sequence '\l'
/var/folders/s7/z86r4t9j6yx376cm120nln6w0000gn/T/ipykernel_53240/73438383.py:6: SyntaxWarning: invalid escape sequence '\l'
  plt.axvline(lambda_opt, color='r', linestyle='--', label=f'Optimal $\lambda$ = {lambda_opt:.4f}')
/var/folders/s7/z86r4t9j6yx376cm120nln6w0000gn/T/ipykernel_53240/73438383.py:7: SyntaxWarning: invalid escape sequence '\l'
  plt.xlabel('$\lambda$')
/var/folders/s7/z86r4t9j6yx376cm120nln6w0000gn/T/ipykernel_53240/73438383.py:9: SyntaxWarning: invalid escape sequence '\l'
  plt.title('Log-likelihood over a range of $\lambda$ values')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="14_likelihood_files/figure-html/cell-48-output-2.png" width="731" height="491" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="conclusione-e-riflessioni-finali" class="level2" data-number="38.8">
<h2 data-number="38.8" class="anchored" data-anchor-id="conclusione-e-riflessioni-finali"><span class="header-section-number">38.8</span> Conclusione e Riflessioni Finali</h2>
<p>La funzione di verosimiglianza rappresenta un elemento cruciale che collega i dati osservati ai parametri di un modello statistico. Essa fornisce una misura della plausibilità dei dati in relazione a diversi valori possibili dei parametri del modello. La strutturazione di una funzione di verosimiglianza richiede la considerazione di tre componenti fondamentali: il modello statistico che si presume abbia generato i dati, l’insieme di valori possibili per i parametri di tale modello e le osservazioni empiriche che effettivamente abbiamo a disposizione.</p>
<p>La funzione di verosimiglianza è centrale nella pratica dell’inferenza statistica. Essa ci permette di quantificare quanto bene differenti set di parametri potrebbero aver generato i dati osservati. Questo è fondamentale sia per la selezione del modello che per la stima dei parametri, e pertanto è indispensabile per un’analisi dati rigorosa e per un’interpretazione accurata dei risultati.</p>
<p>Un’applicazione pratica e illustrativa dei principi esposti in questo capitolo è fornita nella sezione sul modello Rescorla-Wagner, che è un esempio di come la teoria della verosimiglianza possa essere applicata per affrontare questioni empiriche in psicologia.</p>
<p>In sintesi, la comprensione e l’applicazione appropriata della funzione di verosimiglianza sono passaggi essenziali nel processo di analisi dati. Essa costituisce uno strumento indispensabile per chi è impegnato nella ricerca empirica e nell’interpretazione di dati complessi.</p>
</section>
<section id="esercizi" class="level2" data-number="38.9">
<h2 data-number="38.9" class="anchored" data-anchor-id="esercizi"><span class="header-section-number">38.9</span> Esercizi</h2>
<div id="exr-entropy-1" class="theorem exercise">
<p><span class="theorem-title"><strong>Esercizio 38.1</strong></span> Spiega ciascuno dei concetti seguenti con una frase:</p>
<ul>
<li>probabilità.</li>
<li>funzione di massa di probabilità.</li>
<li>funzione di densità di probabilità.</li>
<li>distribuzione di probabilità.</li>
<li>distribuzione di probabilità discreta.</li>
<li>distribuzione di probabilità continua.</li>
<li>funzione di distribuzione cumulativa (cdf).</li>
<li>verosimiglianza</li>
</ul>
</div>
<div class="callout callout-style-simple callout-warning">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>All’esame ti verrà chiesto di:</p>
<ul>
<li>Calcolare la funzione di verosimiglianza binomiale e riportare il valore della funzione in corrispondenza di specifici valori <span class="math inline">\(\theta\)</span>.</li>
<li>Calcolare la funzione di verosimiglianza del modello gaussiano, per <span class="math inline">\(\sigma\)</span> noto, e riportare il valore della funzione in corrispondenza di specifici valori <span class="math inline">\(\mu\)</span>.</li>
<li>Calcolare la stima di massima verosimiglianza.</li>
<li>Rispondere a domande che implicano una adeguata comprensione del concetto di funzione di verosimiglianza.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="informazioni-sullambiente-di-sviluppo" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="informazioni-sullambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</h2>
<div id="b32126bb-3cd1-4041-8e88-7575a7532963" class="cell" data-tags="[]" data-execution_count="3">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext watermark</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>watermark <span class="op">-</span>n <span class="op">-</span>u <span class="op">-</span>v <span class="op">-</span>iv <span class="op">-</span>w <span class="op">-</span>m</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Last updated: Sun Oct 13 2024

Python implementation: CPython
Python version       : 3.12.4
IPython version      : 8.26.0

Compiler    : Clang 16.0.6 
OS          : Darwin
Release     : 24.0.0
Machine     : arm64
Processor   : arm
CPU cores   : 8
Architecture: 64bit

seaborn   : 0.13.2
scipy     : 1.14.0
arviz     : 0.18.0
matplotlib: 3.9.1
pandas    : 2.2.2
numpy     : 1.26.4

Watermark: 2.4.3
</code></pre>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-zetsche_2019future" class="csl-entry" role="listitem">
Zetsche, Ulrike, Paul-Christian Buerkner, e Babette Renneberg. 2019. <span>«Future expectations in clinical depression: biased or realistic?»</span> <em>Journal of Abnormal Psychology</em> 128 (7): 678.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiato!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiato!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/ccaudek\.github\.io\/psicometria\/intro\.html");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../chapters/probability/13_qq_plot.html" class="pagination-link" aria-label="Diagramma quantile-quantile">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Diagramma quantile-quantile</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/probability/15_simulation.html" class="pagination-link" aria-label="Simulazioni">
        <span class="nav-page-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Simulazioni</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><strong>Psicometria</strong> è una risorsa didattica creata per il corso di Scienze e Tecniche Psicologiche dell’Università di Firenze.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.dev/ccaudek/psicometria/blob/main/chapters/probability/14_likelihood.ipynb" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Realizzato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>