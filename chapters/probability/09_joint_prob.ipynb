{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c514e75d-4b5e-4ae0-a60d-1aa4a9b808ad",
   "metadata": {},
   "source": [
    "# Probabilità congiunta {#sec-join-probability}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisiti**\n",
    "\n",
    "**Concetti e Competenze Chiave**\n",
    "\n",
    "- Comprendere la probabilità che due o più eventi si verifichino contemporaneamente.\n",
    "- Definire e calcolare la funzione di probabilità congiunta per variabili casuali discrete.\n",
    "- Verificare le proprietà fondamentali delle distribuzioni di probabilità congiunta.\n",
    "- Determinare la probabilità di eventi definiti in termini di variabili aleatorie.\n",
    "- Derivare e interpretare le distribuzioni marginali da una distribuzione congiunta.\n",
    "- Formalizzare l'indipendenza tra variabili casuali e calcolare la loro distribuzione congiunta.\n",
    "- Definire e calcolare la covarianza per quantificare la relazione lineare tra due variabili casuali.\n",
    "- Utilizzare la correlazione per misurare l'intensità della relazione lineare tra variabili casuali, indipendentemente dalle loro unità di misura.\n",
    "- Comprendere le proprietà chiave della covarianza e della correlazione, inclusa l'incorrelazione.\n",
    "- Estendere i concetti di probabilità congiunta, marginale e condizionale alle variabili continue, utilizzando gli integrali.\n",
    "\n",
    "**Preparazione del Notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b540416-90ad-479d-b656-aa0479ed8b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c453fc0e-5716-449b-a832-ece3d5dfd5ca",
   "metadata": {},
   "source": [
    "## Introduzione {.unnumbered}\n",
    "\n",
    "In questo capitolo ci proponiamo di analizzare in dettaglio il concetto di probabilità congiunta, focalizzando l'attenzione sul caso di variabili aleatorie discrete. La probabilità congiunta rappresenta la misura della probabilità che due o più eventi si verifichino simultaneamente.\n",
    "\n",
    "## Funzione di Probabilità Congiunta {#sec-fun-join-prob}\n",
    "\n",
    "Finora abbiamo analizzato la probabilità associata a un singolo evento, o più precisamente, a un singolo valore assunto da una variabile aleatoria. Tuttavia, spesso siamo interessati a studiare la relazione tra due o più eventi. La funzione di probabilità congiunta ci permette di estendere il concetto di probabilità al caso di più variabili aleatorie, descrivendo la probabilità che queste assumano specifici valori contemporaneamente.\n",
    "\n",
    "### Esempio: Lancio di Tre Monete Equilibrate\n",
    "\n",
    "Per comprendere meglio il concetto di *probabilità congiunta*, immaginiamo di lanciare tre monete. Tutti i possibili risultati di questo esperimento (ad esempio, tre teste, due teste e una croce, ecc.) costituiscono quello che chiamiamo *spazio campionario*. In questo caso, lo spazio campionario $\\Omega$ è dato da:\n",
    "\n",
    "$$\n",
    "\\Omega = \\{TTT, TTC, TCT, CTT, CCT, CTC, TCC, CCC\\},\n",
    "$$\n",
    "\n",
    "dove $T$ indica \"testa\" e $C$ indica \"croce\". Assumendo che ogni lancio sia indipendente dagli altri, ogni risultato nello spazio campionario $\\Omega$ ha la stessa probabilità di verificarsi, ovvero $1/8$.\n",
    "\n",
    "Definiamo ora le seguenti variabili casuali sullo spazio campionario $\\Omega$:\n",
    "\n",
    "- $X \\in \\{0, 1, 2, 3\\}$ rappresenta il numero totale di teste ottenute nei tre lanci.\n",
    "- $Y \\in \\{0, 1\\}$ indica se il primo lancio ha dato testa ($1$) oppure croce ($0$).\n",
    "\n",
    "La tabella seguente mostra lo spazio campionario con i valori di $X$ e $Y$ associati a ciascun esito, insieme alla probabilità di ciascun evento $\\omega$:\n",
    "\n",
    "|     $\\omega$     | $X$ | $Y$ | $P(\\omega)$ |\n",
    "|:----------------:|:---:|:---:|:-----------:|\n",
    "| $\\omega_1$ = TTT |  3  |  1  |     1/8     |\n",
    "| $\\omega_2$ = TTC |  2  |  1  |     1/8     |\n",
    "| $\\omega_3$ = TCT |  2  |  1  |     1/8     |\n",
    "| $\\omega_4$ = CTT |  2  |  0  |     1/8     |\n",
    "| $\\omega_5$ = CCT |  1  |  0  |     1/8     |\n",
    "| $\\omega_6$ = CTC |  1  |  0  |     1/8     |\n",
    "| $\\omega_7$ = TCC |  1  |  1  |     1/8     |\n",
    "| $\\omega_8$ = CCC |  0  |  0  |     1/8     |\n",
    "\n",
    "Ora possiamo determinare la probabilità congiunta per ogni coppia $(X, Y)$, che rappresenta la probabilità di ottenere un determinato numero di teste $X$ e un determinato risultato per il primo lancio $Y$. Ad esempio:\n",
    "\n",
    "$$P(X=0, Y=0) = P(\\text{CCC}) = 1/8,$$\n",
    "\n",
    "e così via per le altre coppie.\n",
    "\n",
    "Le probabilità congiunte per tutte le possibili combinazioni $(X, Y)$ sono calcolate come segue:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(X = 0, Y = 0) &= 1/8, \\\\\n",
    "P(X = 1, Y = 0) &= P(\\text{CCT}) + P(\\text{CTC}) = 1/4, \\\\\n",
    "P(X = 1, Y = 1) &= P(\\text{TCC}) = 1/8, \\\\\n",
    "P(X = 2, Y = 0) &= P(\\text{CTT}) = 1/8, \\\\\n",
    "P(X = 2, Y = 1) &= P(\\text{TTC}) + P(\\text{TCT}) = 1/4, \\\\\n",
    "P(X = 3, Y = 1) &= P(\\text{TTT}) = 1/8.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Queste probabilità costituiscono la *distribuzione di probabilità congiunta* delle variabili casuali $X$ (numero di teste) e $Y$ (testa al primo lancio). Questa distribuzione fornisce un quadro completo delle probabilità per tutte le combinazioni di risultati di queste due variabili.\n",
    "\n",
    "### Definizione: Funzione di Probabilità Congiunta\n",
    "\n",
    "La *funzione di probabilità congiunta* di due variabili casuali $X$ e $Y$ associa a ogni coppia $(x, y)$ una probabilità $P(X = x, Y = y)$.\n",
    "\n",
    "### Proprietà\n",
    "\n",
    "Una distribuzione di probabilità congiunta deve soddisfare:\n",
    "\n",
    "1. $0 \\leq P(x_i, y_j) \\leq 1$ per ogni coppia $(x_i, y_j)$,\n",
    "2. $\\sum_{i} \\sum_{j} P(x_i, y_j) = 1$, ovvero la somma delle probabilità su tutte le coppie deve essere 1.\n",
    "\n",
    "### Calcolo della Probabilità di Eventi Specifici\n",
    "\n",
    "Data la distribuzione di probabilità congiunta, possiamo determinare la probabilità di eventi definiti in termini delle variabili aleatorie $X$ e $Y$. Ad esempio, per trovare la probabilità che $X + Y \\leq 1$, sommiamo le probabilità di tutte le coppie $(x, y)$ che soddisfano questa condizione, ottenendo $P(X+Y \\leq 1) = 3/8$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6d59285-1d7b-4a62-bdd2-c8202b708a7d",
   "metadata": {},
   "source": [
    "### Funzioni di Probabilità Marginali {#sec-marg-distr-discr}\n",
    "\n",
    "La distribuzione marginale di un insieme di variabili casuali descrive la distribuzione di probabilità di queste variabili considerate singolarmente, indipendentemente dalle altre. La \"marginalizzazione\" è un processo che permette di ottenere la distribuzione di probabilità di una o più variabili casuali marginali sommando o integrando la distribuzione congiunta su tutte le possibili realizzazioni delle altre variabili casuali, ovvero quelle non considerate (e quindi \"marginalizzate\").\n",
    "\n",
    "Per esempio, data la distribuzione congiunta di due variabili casuali discrete $X$ e $Y$, la distribuzione marginale di $X$, indicata come $P(X=x)$, si calcola come:\n",
    "\n",
    "$$\n",
    "P(X = x) = \\sum_y P(X = x, Y = y),\n",
    "$$\n",
    "\n",
    "dove $P(X = x, Y = y)$ rappresenta la probabilità congiunta di $X$ e $Y$. Le distribuzioni marginali e congiunte di variabili casuali discrete sono frequentemente rappresentate in tabelle di contingenza. Si garantisce che le distribuzioni marginali siano normalizzate:\n",
    "\n",
    "$$\n",
    "\\sum_x P(X=x) = 1, \\quad \\sum_y P(Y=y) = 1.\n",
    "$$\n",
    "\n",
    "Per variabili casuali continue, la somma è sostituita dall'integrazione.\n",
    "\n",
    "::: {#exm-}\n",
    "Prendiamo come riferimento l'esperimento del lancio di tre monete equilibrate descritto precedentemente. Per calcolare le probabilità marginali di $X$ e $Y$, sommiamo le probabilità congiunte su una dimensione. La probabilità marginale di $X$, $P_X$, si ottiene sommando le probabilità lungo le colonne per ciascun valore fisso di $X$; analogamente, la probabilità marginale di $Y$, $P_Y$, si calcola sommando le probabilità lungo le righe per ciascun valore fisso di $Y$.\n",
    "\n",
    "La tabella seguente mostra la distribuzione di probabilità congiunta $P(X, Y)$ e le probabilità marginali $P(X)$ e $P(Y)$:\n",
    "\n",
    "| $x \\setminus y$ |  0  |  1  | $P(x)$ |\n",
    "|:---------------:|:---:|:---:|:------:|\n",
    "|         0       | 1/8 |  0  |  1/8   |\n",
    "|         1       | 2/8 | 1/8 |  3/8   |\n",
    "|         2       | 1/8 | 2/8 |  3/8   |\n",
    "|         3       |  0  | 1/8 |  1/8   |\n",
    "|      $P(y)$     | 4/8 | 4/8 |  1.0   |\n",
    "\n",
    ":::\n",
    "\n",
    "### Marginalizzazione per Variabili Casuali Continue\n",
    "\n",
    "Nell'ambito della statistica bayesiana, il concetto di marginalizzazione gioca un ruolo cruciale. Un esempio di equazione che emerge da questo processo è:\n",
    "\n",
    "$$\n",
    "p(y) = \\int_{\\theta} p(y, \\theta) \\, d\\theta = \\int_{\\theta} p(y \\mid \\theta) p(\\theta) \\, d\\theta,\n",
    "$$\n",
    "\n",
    "dove $y$ e $\\theta$ sono variabili casuali continue, con $y$ che rappresenta i dati osservati e $\\theta$ i parametri di un modello statistico. Questa equazione illustra come, in un contesto continuo, la marginalizzazione possa essere vista come l'estensione dell'approccio discreto a un continuum di valori per le variabili in esame."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23ba463a-462e-4476-a2ac-6884806a1f1e",
   "metadata": {},
   "source": [
    "## Indipendenza tra Variabili Casuali\n",
    "\n",
    "L'indipendenza tra variabili casuali è un concetto fondamentale in statistica e probabilità, parallelo all'idea di indipendenza tra eventi. Due variabili casuali si considerano indipendenti quando l'informazione su una non altera in alcun modo la distribuzione di probabilità dell'altra. Questa sezione offre una formalizzazione dell'indipendenza tra due variabili casuali discrete, basata sulla loro distribuzione di probabilità congiunta.\n",
    "\n",
    "### Definizione di Indipendenza\n",
    "\n",
    "Due variabili casuali $X$ e $Y$, con una distribuzione congiunta, sono definite indipendenti se, e solo se, per ogni coppia di valori $(x, y)$ si verifica che:\n",
    "\n",
    "$$\n",
    "P_{X, Y}(x, y) = P_X(x) \\cdot P_Y(y).\n",
    "$$\n",
    "\n",
    "In termini pratici, ciò significa che se $X$ e $Y$ sono variabili casuali discrete indipendenti, la loro distribuzione di probabilità congiunta è il prodotto delle rispettive distribuzioni di probabilità marginali. Se invece $P_{X, Y}(x, y) \\neq P_X(x) \\cdot P_Y(y)$, le variabili non sono indipendenti e si dicono associate o dipendenti.\n",
    "\n",
    "Questo concetto si applica anche alle variabili casuali continue, mantenendo la stessa logica: l'indipendenza si verifica quando la funzione di densità congiunta è il prodotto delle funzioni di densità marginali.\n",
    "\n",
    "### Associazione tra Variabili Casuali\n",
    "\n",
    "Quando due variabili casuali non sono indipendenti, si descrivono come associate o dipendenti. In questo contesto, è utile introdurre il concetto di covarianza (e correlazione) come misura del grado di associazione lineare tra due variabili casuali. La covarianza e la correlazione quantificano in che modo la variazione di una variabile è associata alla variazione dell'altra, fornendo un indice della loro interdipendenza lineare.\n",
    "\n",
    "Riepilogando, l'indipendenza tra variabili casuali è un concetto chiave per comprendere le relazioni tra fenomeni aleatori. Riconoscere se due variabili sono indipendenti o associate è fondamentale per l'analisi statistica e per la modellazione di relazioni causali o di correlazione tra variabili."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3041bd6e-0e79-4983-9d95-5ff34efa0fdd",
   "metadata": {},
   "source": [
    "## Covarianza\n",
    "\n",
    "La covarianza è un parametro statistico che quantifica il grado e la direzione della relazione lineare tra due variabili casuali, $X$ e $Y$. In termini semplici, misura come le variazioni di una variabile si accompagnano a quelle dell'altra. Per esempio, considerando l'altezza e il peso di giraffe, scopriremmo che queste due misure tendono ad aumentare insieme, evidenziando così una covarianza positiva. La covarianza è denotata come $Cov(X, Y) = \\sigma_{xy}$.\n",
    "\n",
    "### Definizione di Covarianza\n",
    "\n",
    "La covarianza tra due variabili casuali $X$ e $Y$ è definita come:\n",
    "\n",
    "$$\n",
    "Cov(X, Y) = \\mathbb{E}\\left[\\left(X - \\mathbb{E}[X]\\right) \\left(Y - \\mathbb{E}[Y]\\right)\\right],\n",
    "$$\n",
    "\n",
    "dove $\\mathbb{E}[X]$ e $\\mathbb{E}[Y]$ rappresentano i valori attesi (o medie) di $X$ ed $Y$, rispettivamente.\n",
    "\n",
    "In termini più espliciti, la covarianza può essere espressa come:\n",
    "\n",
    "$$\n",
    "Cov(X, Y) = \\sum_{(x, y) \\in \\Omega} (x - \\mu_X) (y - \\mu_Y) f(x, y),\n",
    "$$\n",
    "\n",
    "dove $\\mu_X$ e $\\mu_Y$ sono le medie di $X$ ed $Y$, e $f(x, y)$ è la funzione di probabilità congiunta delle variabili.\n",
    "\n",
    "Questa definizione mostra una stretta analogia con la varianza, che è la covarianza di una variabile con se stessa:\n",
    "\n",
    "$$\n",
    "\\mathbb{V}(X) = Cov(X, X).\n",
    "$$\n",
    "\n",
    "Inoltre, la covarianza può essere calcolata attraverso la relazione:\n",
    "\n",
    "$$\n",
    "Cov(X, Y) = \\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y).\n",
    "$$\n",
    "\n",
    "### Dimostrazione\n",
    "\n",
    "La formula alternativa per la covarianza si dimostra come segue:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Cov(X, Y) &= \\mathbb{E}\\left[\\left(X - \\mathbb{E}[X]\\right) \\left(Y - \\mathbb{E}[Y]\\right)\\right]\\\\\n",
    "&= \\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Esempio di Calcolo della Covarianza\n",
    "\n",
    "Consideriamo le variabili casuali $X$ e $Y$ con medie $\\mu_X = 1.5$ e $\\mu_Y = 0.5$. La covarianza di $X$ e $Y$ si calcola come:\n",
    "\n",
    "$$\n",
    "Cov(X, Y) = \\sum_{(x, y) \\in \\Omega} (x - \\mu_X) (y - \\mu_Y) f(x, y) = \\frac{1}{4}.\n",
    "$$\n",
    "\n",
    "Questo risultato si può ottenere anche dalla formula alternativa, calcolando prima $\\mathbb{E}(XY)$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(XY) = 1.0.\n",
    "$$\n",
    "\n",
    "Allora, la covarianza tra $X$ e $Y$ è:\n",
    "\n",
    "$$\n",
    "Cov(X, Y) = 1 - 1.5 \\cdot 0.5 = 0.25.\n",
    "$$\n",
    "\n",
    "::: {#exm-}\n",
    "Per fare un esempio con Python, consideriamo l'esempio precedente nel quale $X$ è il numero che si ottiene dal lancio di tre monete equilibrate e $Y$ è il numero di teste al primo lancio. Troviamo $Cov(X, Y)$.\n",
    "\n",
    "Creiamo il prodotto cartesiano che si ottiene per tutti i possibili valori $X$ e i possibili valori $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e332108-80f2-4451-87d0-1c1298078e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "c3 = np.arange(0, 4)\n",
    "c1 = np.arange(0, 2)\n",
    "sample = [(i, j) for i in c1 for j in c3]\n",
    "sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a1749e7-6fc3-49d0-a1e8-9f96d7ab0c4c",
   "metadata": {},
   "source": [
    "Il primo numero di ogni coppia rappresenta il valore di $Y$, mentre il secondo numero è il valore di $X$. Come abbiamo visto in precedenza, però, quete coppie di valori $X, Y$ non hanno tutte la stessa probabilità di verificarsi. Infatti, la probabilità che ciascuna coppia $X, Y$ si osservi è data, in sequenza, dai valori 1/8, 2/8, 1/8, 0, 0, 1/8, 2/8, 1/8. Questi valori rappresentano la distribuzione di massa di probabilità congiunta delle variabili casuali $X$ e $Y$. Possiamo quindi applicare l'eq. {eq}`eq-cov-def-rv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df08f279-5ebe-456b-beae-c958008f534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "\n",
    "pmf = np.array([1 / 8, 2 / 8, 1 / 8, 0, 0, 1 / 8, 2 / 8, 1 / 8])\n",
    "\n",
    "for i in range(8):\n",
    "    res.append((sample[i][0] - 0.5) * (sample[i][1] - 1.5) * pmf[i])\n",
    "\n",
    "sum(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e39c54d-10f4-43d5-8e6e-db538f0ea335",
   "metadata": {},
   "source": [
    "La covarianza tra $X$ e $Y$ è dunque uguale a 0.25.\n",
    ":::\n",
    "\n",
    "## Correlazione\n",
    "\n",
    "Mentre la covarianza fornisce un'indicazione della tendenza di due variabili casuali a variare insieme, essa è influenzata dalle unità di misura delle variabili, rendendo difficile valutare l'intensità della loro relazione lineare. Per ovviare a questo, si utilizza la correlazione, che normalizza la covarianza attraverso le deviazioni standard delle variabili, offrendo così una misura standardizzata dell'associazione lineare tra di esse.\n",
    "\n",
    "::: {#def-}\n",
    "Il coefficiente di correlazione tra due variabili casuali $X$ e $Y$, denotato come $\\rho(X,Y)$ o $\\rho_{X,Y}$, è definito come:\n",
    "\n",
    "$$\n",
    "\\rho(X,Y) = \\frac{Cov(X,Y)}{\\sqrt{\\mathbb{V}(X)\\mathbb{V}(Y)}},\n",
    "$$\n",
    "\n",
    "dove $\\mathbb{V}(X)$ e $\\mathbb{V}(Y)$ rappresentano le varianze di $X$ e $Y$, rispettivamente.\n",
    ":::\n",
    "\n",
    "Il coefficiente di correlazione $\\rho_{xy}$ è un valore adimensionale, ovvero non dipende dalle unità di misura delle variabili, e varia nell'intervallo $-1 \\leq \\rho \\leq 1$.\n",
    "\n",
    "## Proprietà\n",
    "\n",
    "- **Covarianza con una Costante:** La covarianza tra una variabile aleatoria $X$ e una costante $c$ è sempre nulla: $Cov(c, X) = 0$.\n",
    "- **Simmetria:** La covarianza è simmetrica: $Cov(X,Y) = Cov(Y,X)$.\n",
    "- **Intervallo di Correlazione:** Il coefficiente di correlazione $\\rho$ varia tra -1 e 1: $-1 \\leq \\rho(X,Y) \\leq 1$.\n",
    "- **Indipendenza dalle Unità di Misura:** La correlazione è indipendente dalle unità di misura: $\\rho(aX, bY) = \\rho(X,Y)$ per ogni $a, b > 0$.\n",
    "- **Relazione Lineare Perfetta:** Se $Y = a + bX$ è una funzione lineare di $X$, allora $\\rho(X,Y) = \\pm 1$, a seconda del segno di $b$.\n",
    "- **Covarianza e Costanti:** La covarianza tra $X$ e $Y$, ciascuna moltiplicata per una costante, è $Cov(aX, bY) = ab \\, Cov(X,Y)$.\n",
    "- **Varianza della Somma/Differenza:** $\\mathbb{V}(X \\pm Y) = \\mathbb{V}(X) + \\mathbb{V}(Y) \\pm 2Cov(X,Y)$.\n",
    "- **Covarianza e Somma di Variabili:** $Cov(X + Y, Z) = Cov(X,Z) + Cov(Y,Z)$.\n",
    "- **Varianza di una Somma di Variabili Aleatorie:** Per variabili aleatorie $X_1, \\dots, X_n$, si ha $\\mathbb{V}(\\sum_{i=1}^n X_i) = \\sum_{i=1}^n \\mathbb{V}(X_i) + 2\\sum_{i<j} Cov(X_i, X_j)$.\n",
    "- **Covarianza e Somme di Prodotti:** $Cov(\\sum_{i=1}^n a_i X_i, \\sum_{j=1}^m b_j Y_j) = \\sum_{i=1}^n \\sum_{j=1}^m a_i b_j Cov(X_i, Y_j)$.\n",
    "- **Indipendenza e Covarianza di Somme:** Se $X_1, X_2, \\dots, X_n$ sono indipendenti, allora $Cov(\\sum_{i=1}^n a_i X_i, \\sum_{j=1}^n b_j X_j) = \\sum_{i=1}^n a_i b_i \\mathbb{V}(X_i)$.\n",
    "\n",
    "### Incorrelazione\n",
    "\n",
    "Due variabili casuali $X$ ed $Y$ si dicono incorrelate, o linearmente indipendenti, se la loro covarianza è nulla:\n",
    "\n",
    "$$\n",
    "Cov(X,Y) = \\mathbb{E}[(X - \\mu_X)(Y - \\mu_Y)] = 0,\n",
    "$$\n",
    "\n",
    "equivalente a dire che $\\rho_{XY} = 0$ e $\\mathbb{E}(XY) = \\mathbb{E}(X)\\mathbb{E}(Y)$.\n",
    "\n",
    "Questa condizione indica una forma di indipendenza più debole rispetto all'indipendenza stocastica. Tuttavia, $Cov(X, Y) = 0$ non implica necessariamente che $X$ ed $Y$ siano stocasticamente indipendenti.\n",
    "\n",
    "::: {#exm-}\n",
    "Consideriamo una distribuzione di probabilità congiunta di due variabili aleatorie, $X$ e $Y$, definita come:\n",
    "\n",
    "$$\n",
    "f_{XY}(x,y) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\frac{1}{4} & \\text{per } (x,y) \\in \\{(0,0), (1,1), (1, -1), (2,0) \\}, \\\\\n",
    "0 & \\text{altrimenti.}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Questo implica che le variabili aleatorie $X$ e $Y$ assumono valori specifici con probabilità uniforme solo per determinate coppie $(x, y)$ e zero in tutti gli altri casi.\n",
    "\n",
    "**Distribuzioni Marginali**\n",
    "\n",
    "La distribuzione marginale di $X$ si ottiene sommando le probabilità congiunte su tutti i possibili valori di $Y$, e viceversa per $Y$. Le distribuzioni marginali risultano essere:\n",
    "\n",
    "- Per $X$:\n",
    "\n",
    "  $$\n",
    "  f_X(x) = \\left\\{\n",
    "  \\begin{array}{ll}\n",
    "  \\frac{1}{4} & \\text{per } x=0, \\\\\n",
    "  \\frac{1}{2} & \\text{per } x=1, \\\\\n",
    "  \\frac{1}{4} & \\text{per } x=2.\n",
    "  \\end{array}\n",
    "  \\right.\n",
    "  $$\n",
    "\n",
    "- Per $Y$:\n",
    "\n",
    "  $$\n",
    "  f_Y(y) = \\left\\{\n",
    "  \\begin{array}{ll}\n",
    "  \\frac{1}{4} & \\text{per } y=-1, \\\\\n",
    "  \\frac{1}{2} & \\text{per } y=0, \\\\\n",
    "  \\frac{1}{4} & \\text{per } y=1.\n",
    "  \\end{array}\n",
    "  \\right.\n",
    "  $$\n",
    "\n",
    "**Medie e Varianze**\n",
    "\n",
    "Calcoliamo ora le medie e le varianze di $X$ e $Y$:\n",
    "\n",
    "- Media di $X$:\n",
    "\n",
    "  $$\n",
    "  \\mathbb{E}(X) = 0 \\cdot \\frac{1}{4} + 1 \\cdot \\frac{1}{2} + 2 \\cdot \\frac{1}{4} = 1.\n",
    "  $$\n",
    "\n",
    "- Varianza di $X$:\n",
    "\n",
    "  $$\n",
    "  \\mathbb{V}(X) = \\left(0^2 \\cdot \\frac{1}{4} + 1^2 \\cdot \\frac{1}{2} + 2^2 \\cdot \\frac{1}{4}\\right) - \\mathbb{E}(X)^2 = \\frac{3}{2} - 1 = \\frac{1}{2}.\n",
    "  $$\n",
    "\n",
    "- Media di $Y$:\n",
    "\n",
    "  $$\n",
    "  \\mathbb{E}(Y) = (-1) \\cdot \\frac{1}{4} + 0 \\cdot \\frac{1}{2} + 1 \\cdot \\frac{1}{4} = 0.\n",
    "  $$\n",
    "\n",
    "- Varianza di $Y$:\n",
    "\n",
    "  $$\n",
    "  \\mathbb{V}(Y) = \\left((-1)^2 \\cdot \\frac{1}{4} + 0^2 \\cdot \\frac{1}{2} + 1^2 \\cdot \\frac{1}{4}\\right) - \\mathbb{E}(Y)^2 = \\frac{1}{2}.\n",
    "  $$\n",
    "\n",
    "**Covarianza tra X e Y**\n",
    "\n",
    "La covarianza si calcola come:\n",
    "\n",
    "$$\n",
    "Cov(X,Y) = \\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y),\n",
    "$$\n",
    "\n",
    "dove $\\mathbb{E}(XY)$ si trova sommando il prodotto delle coppie di valori $(x, y)$ per la loro probabilità congiunta:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(XY) = 0.\n",
    "$$\n",
    "\n",
    "Di conseguenza, la covarianza tra $X$ e $Y$ è zero:\n",
    "\n",
    "$$\n",
    "Cov(X,Y) = 0 - 1 \\cdot 0 = 0.\n",
    "$$\n",
    "\n",
    "**Conclusioni**\n",
    "\n",
    "Sebbene $X$ e $Y$ siano incorrelate (covarianza nulla), ciò non implica la loro indipendenza. L'indipendenza richiede che la funzione di probabilità congiunta si possa esprimere come il prodotto delle funzioni di probabilità marginali per ogni $x$ e $y$, condizione che non si verifica in questo caso. Quindi, nonostante l'assenza di correlazione, $X$ e $Y$ non sono indipendenti, dimostrando che l'incorrelazione non garantisce l'indipendenza.\n",
    ":::\n",
    "\n",
    "## Variabili continue\n",
    "\n",
    "Consideriamo ora le distribuzioni di densità. Nella figura successiva, tratta da @martin2024bayesian, vediamo una rappresentazione della relazione tra la probabilità congiunta $p(A,B)$, le probabilità marginali $p(A)$ e $p(B)$, e le probabilità condizionali $p(A \\mid B)$.\n",
    "\n",
    "![Distribuzioni di densità (figura tratta da @martin2024bayesian).](../../figures/cond_prob_cont.png){ width=60% }\n",
    "\n",
    "- **Probabilità congiunta $p(A,B)$**: rappresenta la probabilità che A e B assumano certi valori contemporaneamente. Per le variabili continue, questa è data dall'integrazione della funzione di densità congiunta su un'area o volume di interesse.\n",
    "\n",
    "- **Probabilità marginale $p(A)$ e $p(B)$**: è la probabilità di osservare un particolare valore di A (o B) indipendentemente dal valore di B (o A). Si ottiene integrando la funzione di densità congiunta sull'intero intervallo di valori dell'altra variabile.\n",
    "\n",
    "- **Probabilità condizionale $p(A \\mid B)$**: esprime la probabilità di A dato B. Si calcola dividendo la probabilità congiunta per la probabilità marginale di B, applicando la definizione di probabilità condizionale anche nel contesto continuo.\n",
    "\n",
    "La transizione dal trattamento delle variabili discrete a quello delle variabili continue richiede un cambiamento di strumenti matematici da somme ad integrali, ma i concetti fondamentali di probabilità congiunta, marginale e condizionale rimangono applicabili. \n",
    "\n",
    "## Commenti e considerazioni finali \n",
    "\n",
    "In alcune situazioni, ogni singolo elemento di una popolazione può essere associato a diverse variabili casuali. Ad esempio, consideriamo l'elenco di tutti gli studenti iscritti a un'università e immaginiamo di selezionare uno studente a caso per misurare la sua altezza e il suo peso. In questo caso, ogni individuo della popolazione è associato a due variabili casuali, l'altezza e il peso. Quando si hanno due o più variabili casuali associate ad ogni elemento di una popolazione, è possibile studiare la distribuzione congiunta di tali variabili casuali. In questo capitolo abbiamo esaminato come rappresentare la distribuzione di massa di probabilità congiunta di due variabili casuali discrete e come ottenere le distribuzioni marginali delle due variabili. Inoltre, abbiamo discusso i concetti di incorrelazione e indipendenza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5875216-70f1-4e5d-94e5-c29a5f536725",
   "metadata": {},
   "source": [
    "## Informazioni sull'Ambiente di Sviluppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa70d673-a3ed-4600-a58b-eed52bacad89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Last updated: Sat Mar 16 2024\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.8\n",
      "IPython version      : 8.22.2\n",
      "\n",
      "Compiler    : Clang 16.0.6 \n",
      "OS          : Darwin\n",
      "Release     : 23.4.0\n",
      "Machine     : arm64\n",
      "Processor   : arm\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n",
      "numpy : 1.26.4\n",
      "pandas: 2.2.1\n",
      "\n",
      "Watermark: 2.4.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w -m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "2fb22ef73048ab480306a3a8971dd9d9ae74918bef9be9e93ea3e01644b9825e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
