<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Analisi dei dati per psicologi - 35&nbsp; Monte Carlo a Catena di Markov</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/chapter_4/15_stan_beta_binomial.html" rel="next">
<link href="../../chapters/chapter_4/06_balance_prior_post.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/chapter_4/01_intro_bayes.html">Inferenza Bayesiana</a></li><li class="breadcrumb-item"><a href="../../chapters/chapter_4/10_metropolis.html"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Monte Carlo a Catena di Markov</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Analisi dei dati per psicologi</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ccaudek/psicometria/" title="Eseguire il codice" class="quarto-navigation-tool px-1" aria-label="Eseguire il codice"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benvenuti</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Python</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_1/00_prelims.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Preliminari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_1/01_python_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Python (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_1/02_python_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Python (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_1/03_numpy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">NumPy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_1/04_pandas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Pandas (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_1/05_pandas_aggregate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Pandas (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_1/06_pandas_functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Pandas (3)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_1/07_matplotlib.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Matplotlib</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_1/08_seaborn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Seaborn</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Fondamenti</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_2/00_scientific_method.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">La scienza dei dati e il metodo scientifico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_2/01_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Concetti chiave</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_2/02_measurement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">La misurazione in psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_2/03_freq_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Distribuzioni di Frequenze</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_2/04_loc_scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Indici di posizione e di scala</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_2/05_correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Le relazioni tra variabili</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_2/06_causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Causalità dai dati osservazionali</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Probabilità</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/01_intro_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Introduzione al calcolo delle probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/02_conditional_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Probabilità condizionata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/03_bayes_theorem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/04a_random_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/04b_expval_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Proprietà delle variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/04c_sampling_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/05_joint_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Probabilità congiunta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/06_density_func.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">La funzione di densità di probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/07_discr_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. discrete</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/08_cont_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. continue</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/09_likelihood.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/10_grid_gauss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Calcolo della Distribuzione a Posteriori Gaussiana tramite Metodo a Griglia</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Inferenza Bayesiana</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/01_intro_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Modellazione bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/02_subj_prop.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Pensare ad una proporzione in termini soggettivi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/03_conjugate_families_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/04_conjugate_families_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/05_summary_posterior.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Sintesi a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/06_balance_prior_post.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">L’influenza della distribuzione a priori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/10_metropolis.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Monte Carlo a Catena di Markov</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/15_stan_beta_binomial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Linguaggio Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/16_stan_summary_posterior.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Metodi di sintesi della distribuzione a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/17_stan_diagnostics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Diagnostica delle catene markoviane</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/18_stan_prediction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">La predizione bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/19_stan_odds_ratio.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Analisi bayesiana dell’odds-ratio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/22_stan_normal_normal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Inferenza bayesiana su una media</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Modelli lineari</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_03_reglin_bayesian.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Modello di regressione lineare bayesiano</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_04_synt_sugar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Zucchero sintattico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_05_two_means.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Confronto tra le medie di due gruppi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_05a_stan_multreg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Regressione multipla con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_06_hier_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Il modello lineare gerarchico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_07_robust_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Regressione robusta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_08_specification_error.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Errore di specificazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_09_causal_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Inferenza causale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_21_stan_binomial_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Regressione binomiale con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_22_stan_logistic_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Regressione logistica con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_23_stan_poisson_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Regressione di Poisson con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_24_stan_mixed_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">Modelli misti con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_30_entropy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Entropia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_31_kl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">Divergenza KL e ELPD</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_32_stan_loo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">Validazione Incrociata Leave-One-Out</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_35_missing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">Dati mancanti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_40_rescorla_wagner.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Apprendimento per rinforzo</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Appendici</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a00_installation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Ambiente di lavoro</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a01_markdown.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Jupyter Notebook</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a02_shell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">La Shell</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a03_colab_tutorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Colab: un breve tutorial</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a04_virtual_env.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Ambienti virtuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a10_math_symbols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Simbologia di base</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a11_numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Numeri binari, interi, razionali, irrazionali e reali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a13_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">H</span>&nbsp; <span class="chapter-title">Insiemi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a13a_probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">I</span>&nbsp; <span class="chapter-title">Sigma algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a14_combinatorics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">J</span>&nbsp; <span class="chapter-title">Calcolo combinatorio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a15_calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">K</span>&nbsp; <span class="chapter-title">Per liberarvi dai terrori preliminari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a44_montecarlo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">L</span>&nbsp; <span class="chapter-title">Simulazione Monte Carlo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a46_stan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">M</span>&nbsp; <span class="chapter-title">Linguaggio Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../99-references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indice</h2>
   
  <ul>
  <li><a href="#introduzione" id="toc-introduzione" class="nav-link active" data-scroll-target="#introduzione">Introduzione</a></li>
  <li><a href="#il-denominatore-bayesiano" id="toc-il-denominatore-bayesiano" class="nav-link" data-scroll-target="#il-denominatore-bayesiano"><span class="header-section-number">35.1</span> Il denominatore bayesiano</a></li>
  <li><a href="#il-metodo-di-monte-carlo" id="toc-il-metodo-di-monte-carlo" class="nav-link" data-scroll-target="#il-metodo-di-monte-carlo"><span class="header-section-number">35.2</span> Il Metodo di Monte Carlo</a></li>
  <li><a href="#le-catene-di-markov" id="toc-le-catene-di-markov" class="nav-link" data-scroll-target="#le-catene-di-markov"><span class="header-section-number">35.3</span> Le Catene di Markov</a></li>
  <li><a href="#estrazione-di-campioni-dalla-distribuzione-a-posteriori" id="toc-estrazione-di-campioni-dalla-distribuzione-a-posteriori" class="nav-link" data-scroll-target="#estrazione-di-campioni-dalla-distribuzione-a-posteriori"><span class="header-section-number">35.4</span> Estrazione di campioni dalla distribuzione a posteriori</a>
  <ul class="collapse">
  <li><a href="#simulazione-con-distribuzione-target-nota" id="toc-simulazione-con-distribuzione-target-nota" class="nav-link" data-scroll-target="#simulazione-con-distribuzione-target-nota"><span class="header-section-number">35.4.1</span> Simulazione con distribuzione target nota</a></li>
  </ul></li>
  <li><a href="#algoritmo-di-metropolis" id="toc-algoritmo-di-metropolis" class="nav-link" data-scroll-target="#algoritmo-di-metropolis"><span class="header-section-number">35.5</span> Algoritmo di Metropolis</a>
  <ul class="collapse">
  <li><a href="#passaggi-fondamentali-dellalgoritmo-di-metropolis" id="toc-passaggi-fondamentali-dellalgoritmo-di-metropolis" class="nav-link" data-scroll-target="#passaggi-fondamentali-dellalgoritmo-di-metropolis"><span class="header-section-number">35.5.1</span> Passaggi Fondamentali dell’Algoritmo di Metropolis</a></li>
  <li><a href="#dettagli-dellalgoritmo" id="toc-dettagli-dellalgoritmo" class="nav-link" data-scroll-target="#dettagli-dellalgoritmo"><span class="header-section-number">35.5.2</span> Dettagli dell’Algoritmo</a></li>
  <li><a href="#implementazione-dellalgoritmo-di-metropolis" id="toc-implementazione-dellalgoritmo-di-metropolis" class="nav-link" data-scroll-target="#implementazione-dellalgoritmo-di-metropolis"><span class="header-section-number">35.5.3</span> Implementazione dell’Algoritmo di Metropolis</a></li>
  </ul></li>
  <li><a href="#aspetti-computazionali" id="toc-aspetti-computazionali" class="nav-link" data-scroll-target="#aspetti-computazionali"><span class="header-section-number">35.6</span> Aspetti computazionali</a>
  <ul class="collapse">
  <li><a href="#warm-upburn-in" id="toc-warm-upburn-in" class="nav-link" data-scroll-target="#warm-upburn-in"><span class="header-section-number">35.6.1</span> Warm-up/Burn-in</a></li>
  <li><a href="#sintesi-della-distribuzione-a-posteriori" id="toc-sintesi-della-distribuzione-a-posteriori" class="nav-link" data-scroll-target="#sintesi-della-distribuzione-a-posteriori"><span class="header-section-number">35.6.2</span> Sintesi della distribuzione a posteriori</a></li>
  </ul></li>
  <li><a href="#diagnostiche-della-soluzione-mcmc" id="toc-diagnostiche-della-soluzione-mcmc" class="nav-link" data-scroll-target="#diagnostiche-della-soluzione-mcmc"><span class="header-section-number">35.7</span> Diagnostiche della soluzione MCMC</a>
  <ul class="collapse">
  <li><a href="#catene-multiple" id="toc-catene-multiple" class="nav-link" data-scroll-target="#catene-multiple"><span class="header-section-number">35.7.1</span> Catene multiple</a></li>
  <li><a href="#stazionarietà" id="toc-stazionarietà" class="nav-link" data-scroll-target="#stazionarietà"><span class="header-section-number">35.7.2</span> Stazionarietà</a></li>
  <li><a href="#autocorrelazione" id="toc-autocorrelazione" class="nav-link" data-scroll-target="#autocorrelazione"><span class="header-section-number">35.7.3</span> Autocorrelazione</a></li>
  <li><a href="#approcci-grafici-tracce-delle-serie-temporali-trace-plots" id="toc-approcci-grafici-tracce-delle-serie-temporali-trace-plots" class="nav-link" data-scroll-target="#approcci-grafici-tracce-delle-serie-temporali-trace-plots"><span class="header-section-number">35.7.4</span> Approcci Grafici: Tracce delle Serie Temporali (Trace Plots)</a></li>
  <li><a href="#test-statistici-per-la-convergenza" id="toc-test-statistici-per-la-convergenza" class="nav-link" data-scroll-target="#test-statistici-per-la-convergenza"><span class="header-section-number">35.7.5</span> Test Statistici per la Convergenza</a></li>
  <li><a href="#effective-sample-size-ess" id="toc-effective-sample-size-ess" class="nav-link" data-scroll-target="#effective-sample-size-ess"><span class="header-section-number">35.7.6</span> Effective sample size (ESS)</a></li>
  </ul></li>
  <li><a href="#caso-normale-normale" id="toc-caso-normale-normale" class="nav-link" data-scroll-target="#caso-normale-normale"><span class="header-section-number">35.8</span> Caso Normale-Normale</a>
  <ul class="collapse">
  <li><a href="#calcolo-dei-parametri-del-posterior-analitico" id="toc-calcolo-dei-parametri-del-posterior-analitico" class="nav-link" data-scroll-target="#calcolo-dei-parametri-del-posterior-analitico"><span class="header-section-number">35.8.1</span> Calcolo dei Parametri del Posterior Analitico</a></li>
  <li><a href="#codice-per-il-grafico" id="toc-codice-per-il-grafico" class="nav-link" data-scroll-target="#codice-per-il-grafico"><span class="header-section-number">35.8.2</span> Codice per il Grafico</a></li>
  </ul></li>
  <li><a href="#commenti-e-considerazioni-finali" id="toc-commenti-e-considerazioni-finali" class="nav-link" data-scroll-target="#commenti-e-considerazioni-finali"><span class="header-section-number">35.9</span> Commenti e considerazioni finali</a></li>
  <li><a href="#informazioni-sullambiente-di-sviluppo" id="toc-informazioni-sullambiente-di-sviluppo" class="nav-link" data-scroll-target="#informazioni-sullambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.dev/ccaudek/psicometria/blob/main/chapters/chapter_4/10_metropolis.ipynb" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/chapter_4/01_intro_bayes.html">Inferenza Bayesiana</a></li><li class="breadcrumb-item"><a href="../../chapters/chapter_4/10_metropolis.html"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Monte Carlo a Catena di Markov</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-metropolis" class="quarto-section-identifier"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Monte Carlo a Catena di Markov</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><strong>Prerequisiti</strong></p>
<p><strong>Concetti e competenze chiave</strong></p>
<ul>
<li>Utilizzare metodi di Monte Carlo per stimare valori attesi e probabilità, evitando calcoli di integrali complessi.</li>
<li>Apprendere il ruolo delle catene di Markov per generare sequenze di campioni dalla distribuzione a posteriori.</li>
<li>Implementare e comprendere l’algoritmo di Metropolis come strumento per il campionamento dalla distribuzione a posteriori.</li>
<li>Valutare la convergenza delle catene di Markov e utilizzare strumenti come il trace plot e l’autocorrelazione per diagnosticare la qualità del campionamento.</li>
<li>Riconoscere l’importanza della fase di burn-in e la necessità di più catene per verificare la stazionarietà e ridurre l’autocorrelazione.</li>
</ul>
<p><strong>Preparazione del Notebook</strong></p>
<div id="cell-2" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:09:40.345584Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:09:40.320370Z&quot;}" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>run ..<span class="op">/</span>..<span class="op">/</span>config.py <span class="co"># Import the configuration settings</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats <span class="im">as</span> stats</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.graphics <span class="im">import</span> tsaplots</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="introduzione" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="introduzione">Introduzione</h2>
<p>In precedenza, abbiamo esplorato diversi esempi di inferenza bayesiana relativi alla distribuzione a posteriori di un singolo parametro, come nel caso del modello bernoulliano. Abbiamo anche discusso l’utilizzo di approcci come l’approssimazione tramite griglia e i metodi dei priori coniugati per ottenere o approssimare la distribuzione a posteriori. In questo capitolo, ci concentreremo sul metodo di simulazione e spiegheremo perché sono necessari approcci speciali noti come metodi di Monte Carlo a Catena di Markov (MCMC).</p>
</section>
<section id="il-denominatore-bayesiano" class="level2" data-number="35.1">
<h2 data-number="35.1" class="anchored" data-anchor-id="il-denominatore-bayesiano"><span class="header-section-number">35.1</span> Il denominatore bayesiano</h2>
<p>Nell’approccio bayesiano, il nostro obiettivo principale è determinare la distribuzione a posteriori <span class="math inline">\(p(\theta \mid y)\)</span> di un parametro <span class="math inline">\(\theta\)</span>, utilizzando sia i dati osservati $ y $ che la distribuzione a priori <span class="math inline">\(p(\theta)\)</span>. Questo processo si basa sul teorema di Bayes:</p>
<p><span class="math display">\[
p(\theta \mid y) = \frac{p(y \mid \theta) p(\theta)}{\int p(y \mid \theta) p(\theta) d\theta}.
\]</span></p>
<p>In questa formula, il denominatore <span class="math inline">\(\int p(y \mid \theta) p(\theta) d\theta\)</span> rappresenta l’integrazione (o la somma, nel caso di variabili discrete) su tutti i possibili valori di <span class="math inline">\(\theta\)</span>, fornendo così la probabilità marginale di <span class="math inline">\(y\)</span>. Questo assicura che $ p(y) $ sia una distribuzione di probabilità valida che si integra (o si somma) a 1.</p>
<p>Tuttavia, spesso incontriamo una sfida significativa: il calcolo dell’evidenza <span class="math inline">\(p(y) = \int p(y \mid \theta) p(\theta) d\theta\)</span> può essere estremamente complesso, specialmente per modelli più articolati, rendendo difficile ottenere con precisione la distribuzione a posteriori.</p>
<p>Una soluzione possibile è rappresentata dalle distribuzioni a priori coniugate, che offrono un metodo analitico per determinare la distribuzione a posteriori. Tuttavia, questo limita la selezione delle distribuzioni a priori e di verosimiglianza.</p>
<p>Un metodo per superare questa limitazione è ricorrere a soluzioni numeriche, ma i metodi di campionamento a griglia sono applicabili solo nel caso di modelli con un numero di parametri molto piccolo.</p>
<p>La soluzione generale è utilizzare i Metodi di Monte Carlo a Catena di Markov (MCMC). Questi metodi consentono di derivare la distribuzione a posteriori basandosi su presupposti teorici e senza restrizioni nella scelta delle distribuzioni. L’approccio Monte Carlo si basa sulla generazione di sequenze di numeri casuali per creare un ampio campione di osservazioni dalla distribuzione a posteriori. Da questi campioni, possiamo poi stimare empiricamente le proprietà di interesse. Questo approccio richiede l’uso di metodi computazionalmente intensivi e, con la crescente potenza di calcolo dei computer moderni, tali metodi stanno diventando sempre più accessibili e popolari nell’analisi dei dati.</p>
</section>
<section id="il-metodo-di-monte-carlo" class="level2" data-number="35.2">
<h2 data-number="35.2" class="anchored" data-anchor-id="il-metodo-di-monte-carlo"><span class="header-section-number">35.2</span> Il Metodo di Monte Carlo</h2>
<p>Nei capitoli precedenti abbiamo già esplorato l’efficacia della simulazione nel campo della teoria delle probabilità. Un esempio classico è il problema di Monty Hall, che mostra come la simulazione ripetuta possa fornire stime affidabili per la media e la varianza di variabili casuali. Inoltre, applicando la legge dei grandi numeri, queste stime diventano più accurate all’aumentare del numero di simulazioni. Ciò evidenzia la potenza dei metodi Monte Carlo, che evitano la necessità di calcolare integrali complessi.</p>
<p>Il Metodo di Monte Carlo, sviluppato durante il Progetto Manhattan negli anni ’40, utilizza numeri casuali per risolvere problemi matematici complessi. Originariamente ideato da Stanislaw Ulam e successivamente implementato da John von Neumann, il metodo prende il nome dallo zio giocatore d’azzardo di Ulam, come suggerito da Nicholas Metropolis. Da allora, questo metodo è diventato una tecnica fondamentale in diverse discipline, contribuendo significativamente alla risoluzione di problemi complessi.</p>
<p>La metodologia di Monte Carlo genera un’ampia serie di punti casuali per stimare quantità di interesse, come l’integrazione numerica. Un esempio classico è l’approssimazione dell’integrale di un cerchio in 2D, dove il rapporto tra il numero di punti che cadono all’interno del cerchio e tutti i campioni fornisce un’approssimazione dell’area (per un esempio numerico, si veda <a href="../chapter_7/a44_montecarlo.html" class="quarto-xref"><span>Appendice L</span></a>).</p>
<p>Per illustrare ulteriormente questo concetto, consideriamo ora una distribuzione continua <span class="math inline">\(p(\theta \mid y)\)</span> con una media <span class="math inline">\(\mu\)</span>. Se siamo in grado di generare una sequenza di campioni casuali <span class="math inline">\(\theta^{(1)}, \theta^{(2)}, \dots, \theta^{(T)}\)</span> indipendenti e identicamente distribuiti secondo <span class="math inline">\(p(\theta \mid y)\)</span>, possiamo stimare il valore atteso teorico di <span class="math inline">\(\theta\)</span> utilizzando la media campionaria <span class="math inline">\(\frac{1}{T} \sum_{i=1}^T \theta^{(t)}\)</span>. Questa approssimazione diventa sempre più accurata man mano che aumenta il numero di campioni <span class="math inline">\(T\)</span>, grazie alla Legge Forte dei Grandi Numeri.</p>
<p>Un altro vantaggio del Metodo di Monte Carlo è la sua capacità di approssimare la probabilità che una variabile casuale <span class="math inline">\(\theta\)</span> cada all’interno di un intervallo specifico <span class="math inline">\((l, u)\)</span>. Questo può essere ottenuto calcolando la media campionaria della funzione indicatrice <span class="math inline">\(I(l &lt; \theta &lt; u)\)</span> per ogni realizzazione <span class="math inline">\(\theta^{(t)}\)</span>, cioè <span class="math inline">\(Pr(l &lt; \theta &lt; u) \approx \frac{\text{numero di realizzazioni } \theta^{(t)} \in (l, u)}{T}\)</span>.</p>
<p>Nonostante la loro efficacia, un limite dei metodi Monte Carlo tradizionali risiede nella generazione efficiente di un elevato numero di campioni <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span>. In risposta a questa sfida, i metodi di Monte Carlo basati su catene di Markov (MCMC) offrono una soluzione potente per simulare da distribuzioni complesse attraverso catene di Markov. L’evoluzione di questi algoritmi ha trasformato radicalmente la statistica e il calcolo scientifico, permettendo la simulazione da una vasta gamma di distribuzioni, anche in spazi di alta dimensionalità.</p>
</section>
<section id="le-catene-di-markov" class="level2" data-number="35.3">
<h2 data-number="35.3" class="anchored" data-anchor-id="le-catene-di-markov"><span class="header-section-number">35.3</span> Le Catene di Markov</h2>
<p>Le catene di Markov, ideate da Andrey Markov nel 1906, rappresentano un tentativo di estendere la legge dei grandi numeri a contesti in cui le variabili casuali non sono indipendenti. Tradizionalmente, la statistica si concentra su sequenze di variabili casuali indipendenti e identicamente distribuite (i.i.d.), simboleggiate come <span class="math inline">\(X_0, X_1, \ldots, X_n, \ldots\)</span>. In tali sequenze, ogni variabile è indipendente dalle altre e segue la stessa distribuzione, con <span class="math inline">\(n\)</span> che rappresenta un indice temporale discreto. Tuttavia, questa assunzione di indipendenza non è sempre realistica nei modelli di fenomeni complessi, portando alla necessità di esplorare forme alternative di dipendenza tra variabili.</p>
<p>Per superare le limitazioni dell’indipendenza, le catene di Markov introducono una cosiddetta “dipendenza a un passo”, incarnata nella “proprietà di Markov”. Questa proprietà stabilisce che la previsione di un evento futuro <span class="math inline">\(X_{n+1}\)</span> dipende unicamente dall’evento immediatamente precedente <span class="math inline">\(X_n\)</span>, indipendentemente dagli eventi passati <span class="math inline">\(X_0, X_1, X_2, \ldots, X_{n-1}\)</span>. La proprietà di Markov è espressa matematicamente come:</p>
<p><span class="math display">\[
P(X_{n+1} = j | X_n = i, X_{n-1} = i_{n-1}, \ldots, X_0 = i_0) = P(X_{n+1} = j | X_n = i).
\]</span></p>
<p>Questa proprietà afferma che la previsione di un evento futuro dipende solo dall’evento immediatamente precedente, semplificando i calcoli relativi alle probabilità condizionali.</p>
<p>Le catene di Markov rappresentano un framework fondamentale per la modellazione delle dipendenze tra variabili casuali, una nozione cruciale in numerosi campi della statistica e della scienza dei dati, inclusa la metodologia MCMC (Markov Chain Monte Carlo). In particolare, nell’ambito dell’analisi bayesiana, l’uso di MCMC si rivela di estrema importanza, soprattutto quando non è possibile calcolare in modo analitico la distribuzione a posteriori.</p>
<p>L’algoritmo di Metropolis rappresenta una delle implementazioni più semplici del metodo MCMC. Questo algoritmo sfrutta la natura dipendente delle catene di Markov per navigare in modo efficace attraverso lo spazio della distribuzione a posteriori. Questo aspetto rende il MCMC uno strumento di grande potenza per affrontare problemi complessi in cui i metodi analitici tradizionali non possono essere applicati (per ulteriori dettagli, si veda <a href="../chapter_7/a44_montecarlo.html" class="quarto-xref"><span>Appendice L</span></a>). In breve, il MCMC consente di generare un vasto insieme di valori per il parametro <span class="math inline">\(\theta\)</span>. Idealmente, questi valori riflettono la distribuzione a posteriori <span class="math inline">\(p(\theta \mid y)\)</span> quando questa non può essere ottenuta direttamente. Questa caratteristica rende il MCMC uno strumento essenziale per risolvere problemi complessi in cui i metodi analitici convenzionali non sono applicabili.</p>
</section>
<section id="estrazione-di-campioni-dalla-distribuzione-a-posteriori" class="level2" data-number="35.4">
<h2 data-number="35.4" class="anchored" data-anchor-id="estrazione-di-campioni-dalla-distribuzione-a-posteriori"><span class="header-section-number">35.4</span> Estrazione di campioni dalla distribuzione a posteriori</h2>
<p>Nella discussione seguente ci porremo l’obiettivo di comprendere come utilizzare l’algoritmo di Metropolis per approssimare la distribuzione a posteriori <span class="math inline">\(p(\theta \mid y)\)</span>. A questo fine, il capitolo è strutturato in varie sezioni che facilitano la comprensione progressiva del tema.</p>
<ul>
<li>Inizieremo discutendo di come la distribuzione a posteriori possa essere approssimata mediante tecniche di simulazione convenzionali. Questa prima parte presuppone che la distribuzione target, o “a posteriori,” sia già conosciuta o disponibile per l’analisi.</li>
<li>In seguito, passeremo a illustrare come l’algoritmo di Metropolis possa essere utilizzato per affrontare situazioni in cui la distribuzione a posteriori non è direttamente nota. In questi casi, spesso abbiamo a disposizione informazioni riguardanti la distribuzione a priori e la funzione di verosimiglianza, che possono essere utilizzate per ottenere un’approssimazione efficace della distribuzione a posteriori.</li>
</ul>
<p>A titolo esemplificativo, utilizzeremo il dataset <code>moma_sample.csv</code>, il quale costituisce un campione casuale di 100 artisti provenienti dal Museo di Arte Moderna di New York (MoMA) e contiene diverse informazioni relative a ciascun artista.</p>
<p>Il nostro interesse è focalizzato sulla determinazione della probabilità che un artista presente nel MoMA appartenga alla generazione X o a una generazione successiva (nati dopo il 1965). Questa probabilità sarà indicata come <span class="math inline">\(\pi\)</span>. Iniziamo importando i dati.</p>
<div id="cell-6" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:15:48.008788Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:15:47.995389Z&quot;}" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>moma_sample <span class="op">=</span> pd.read_csv(<span class="st">"../../data/moma_sample.csv"</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Esaminiamo le prime cinque righe del DataFrame.</p>
<div id="cell-8" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:09:49.939453Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:09:49.924447Z&quot;}" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>moma_sample.head()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">artist</th>
<th data-quarto-table-cell-role="th">country</th>
<th data-quarto-table-cell-role="th">birth</th>
<th data-quarto-table-cell-role="th">death</th>
<th data-quarto-table-cell-role="th">alive</th>
<th data-quarto-table-cell-role="th">genx</th>
<th data-quarto-table-cell-role="th">gender</th>
<th data-quarto-table-cell-role="th">count</th>
<th data-quarto-table-cell-role="th">year_acquired_min</th>
<th data-quarto-table-cell-role="th">year_acquired_max</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>Ad Gerritsen</td>
<td>dutch</td>
<td>1940</td>
<td>2015.0</td>
<td>False</td>
<td>False</td>
<td>male</td>
<td>1</td>
<td>1981</td>
<td>1981</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Kirstine Roepstorff</td>
<td>danish</td>
<td>1972</td>
<td>NaN</td>
<td>True</td>
<td>True</td>
<td>female</td>
<td>3</td>
<td>2005</td>
<td>2005</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Lisa Baumgardner</td>
<td>american</td>
<td>1958</td>
<td>2015.0</td>
<td>False</td>
<td>False</td>
<td>female</td>
<td>2</td>
<td>2016</td>
<td>2016</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>David Bates</td>
<td>american</td>
<td>1952</td>
<td>NaN</td>
<td>True</td>
<td>False</td>
<td>male</td>
<td>1</td>
<td>2001</td>
<td>2001</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>Simon Levy</td>
<td>american</td>
<td>1946</td>
<td>NaN</td>
<td>True</td>
<td>False</td>
<td>male</td>
<td>1</td>
<td>2012</td>
<td>2012</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<p>Dai dati osserviamo che solo 14 artisti su 100 appartengono alla generazione X o a una generazione successiva.</p>
<div id="cell-10" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:09:53.119523Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:09:53.086277Z&quot;}" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> moma_sample[<span class="st">"genx"</span>].value_counts()</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>genx
False    86
True     14
Name: count, dtype: int64</code></pre>
</div>
</div>
<p>Il valore campionato <span class="math inline">\(y = 14\)</span> riflette le caratteristiche del campione che è stato osservato. Tuttavia, poiché il MOMA contiene opere di migliaia di artisti, sorge una domanda riguardante il vero valore di <span class="math inline">\(\theta\)</span> (la probabilità di appartenere alla generazione X o a una generazione successiva) all’interno di questa popolazione.</p>
<p>Possiamo interpretare i dati <span class="math inline">\(y = 14\)</span> come l’esito di una variabile casuale Binomiale con parametri <span class="math inline">\(N = 100\)</span> e <span class="math inline">\(\theta\)</span> sconosciuto.</p>
<p>Supponiamo che le nostre credenze pregresse riguardo a <span class="math inline">\(\theta\)</span> possano essere modellate attraverso una distribuzione Beta(4, 6).</p>
<p>Sfruttando le proprietà delle distribuzioni coniugate, possiamo calcolare la distribuzione a posteriori esatta:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>Y <span class="op">~</span> Binomiale(<span class="dv">100</span>, π)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>θ <span class="op">=</span> Beta(<span class="dv">4</span>, <span class="dv">6</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>θ <span class="op">|</span> (Y <span class="op">=</span> <span class="dv">14</span>) <span class="op">~</span> Beta(<span class="dv">4</span> <span class="op">+</span> <span class="dv">14</span>, <span class="dv">6</span> <span class="op">+</span> <span class="dv">100</span> <span class="op">-</span> <span class="dv">14</span>) → Beta(<span class="dv">18</span>, <span class="dv">92</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Nella figura successiva è rappresentata la distribuzione a posteriori del parametro <span class="math inline">\(\theta\)</span>, insieme alla distribuzione a priori scelta.</p>
<div id="cell-12" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:10:00.582390Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:10:00.377742Z&quot;}" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>prior_density <span class="op">=</span> stats.beta.pdf(x, <span class="dv">4</span>, <span class="dv">6</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>posterior_density <span class="op">=</span> stats.beta.pdf(x, <span class="dv">18</span>, <span class="dv">92</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x, prior_density, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">"Prior: Beta(4, 6)"</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x, posterior_density, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">"Posterior: Beta(18, 92)"</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Parameter Value"</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Density"</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Prior and Posterior Densities"</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_metropolis_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Se vogliamo conoscere il valore della media a posteriori di <span class="math inline">\(\theta\)</span>, il risultato esatto è</p>
<p><span class="math display">\[
\bar{\theta}_{post} = \frac{\alpha}{\alpha + \beta} = \frac{18}{18 + 92} \approx 0.1636.
\]</span></p>
<section id="simulazione-con-distribuzione-target-nota" class="level3" data-number="35.4.1">
<h3 data-number="35.4.1" class="anchored" data-anchor-id="simulazione-con-distribuzione-target-nota"><span class="header-section-number">35.4.1</span> Simulazione con distribuzione target nota</h3>
<p>Usiamo ora una simulazione numerica per stimare la media a posteriori di <span class="math inline">\(\theta\)</span>. Conoscendo la forma della distribuzione a posteriori <span class="math inline">\(Beta(18, 92)\)</span>, possiamo generare un campione di osservazioni casuali da questa distribuzione. Successivamente, calcoliamo la media delle osservazioni ottenute per ottenere un’approssimazione della media a posteriori.</p>
<p>Se vogliamo ottenere un risultato approssimato con poche osservazioni (ad esempio, 10), possiamo procedere con la seguente simulazione:</p>
<div id="cell-15" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:10:13.375419Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:10:13.359907Z&quot;}" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> stats.beta(<span class="dv">18</span>, <span class="dv">92</span>).rvs(<span class="dv">10</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.18694021 0.12067309 0.17576971 0.1608302  0.12912987 0.20527983
 0.14209658 0.15057358 0.12001648 0.13024873]</code></pre>
</div>
</div>
<div id="cell-16" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:10:16.372388Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:10:16.367267Z&quot;}" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>np.mean(y)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>0.1521558285153169</code></pre>
</div>
</div>
<p>Tuttavia, con solo 10 campioni l’approssimazione potrebbe non essere molto accurata. Più aumentiamo il numero di campioni (cioè il numero di osservazioni casuali generate), più precisa sarà l’approssimazione. Aumentando il numero di campioni, ad esempio a 10000, otteniamo un risultato più preciso:</p>
<div id="cell-18" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:10:18.652167Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:10:18.635620Z&quot;}" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>stats.beta(<span class="dv">18</span>, <span class="dv">92</span>).rvs(<span class="dv">10000</span>).mean()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>0.16380917953899665</code></pre>
</div>
</div>
<p>Quando il numero di campioni a posteriori diventa molto grande, la distribuzione campionaria <em>converge</em> alla densità della popolazione. Questo concetto si applica non solo alla media, ma anche ad altre statistiche descrittive, come la moda e la varianza.</p>
<p>È importante sottolineare che l’applicazione della simulazione di Monte Carlo è efficace per calcolare distribuzioni a posteriori solo quando conosciamo la distribuzione stessa e possiamo utilizzare funzioni Python per estrarre campioni casuali da tale distribuzione. Ciò è stato possibile nel caso della distribuzione a posteriori <span class="math inline">\(Beta(18, 92)\)</span>.</p>
<p>Tuttavia, questa situazione ideale non si verifica sempre nella pratica, poiché le distribuzioni a priori coniugate alla verosimiglianza sono spesso rare. Per esempio, nel caso di una verosimiglianza binomiale e una distribuzione a priori gaussiana, l’espressione</p>
<p><span class="math display">\[
p(\theta \mid y) = \frac{\mathrm{e}^{-(\theta - 1 / 2)^2} \theta^y (1 - \theta)^{n - y}} {\int_0^1 \mathrm{e}^{-(t - 1 / 2)^2} t^y (1 - t)^{n - y} dt}
\]</span></p>
<p>rende impossibile calcolare analiticamente la distribuzione a posteriori di <span class="math inline">\(\theta\)</span>, precludendo quindi l’utilizzo diretto di Python per generare campioni casuali.</p>
<p>In queste circostanze, però, è possibile ottenere campioni casuali dalla distribuzione a posteriori mediante l’uso di metodi Monte Carlo basati su Catena di Markov (MCMC). Gli algoritmi MCMC, come ad esempio l’algoritmo Metropolis, costituiscono una classe di metodi che consentono di estrarre campioni casuali dalla distribuzione a posteriori <em>senza richiedere la conoscenza della sua rappresentazione analitica</em>. Le tecniche MCMC sono ampiamente adottate per risolvere problemi di inferenza bayesiana e rappresentano il principale strumento computazionale per ottenere stime approssimate di distribuzioni a posteriori in situazioni complesse e non analiticamente trattabili.</p>
</section>
</section>
<section id="algoritmo-di-metropolis" class="level2" data-number="35.5">
<h2 data-number="35.5" class="anchored" data-anchor-id="algoritmo-di-metropolis"><span class="header-section-number">35.5</span> Algoritmo di Metropolis</h2>
<p>L’algoritmo di Metropolis è un metodo avanzato per il campionamento da distribuzioni probabilistiche complesse. Appartiene alla famiglia dei metodi Monte Carlo Markov Chain (MCMC) e combina strategie di campionamento Monte Carlo con catene di Markov per navigare nello spazio dei parametri in modo intelligente. Questo consente di ottenere campioni rappresentativi della distribuzione di interesse indipendentemente dal punto di partenza, riducendo il rischio di bias nei risultati.</p>
<section id="passaggi-fondamentali-dellalgoritmo-di-metropolis" class="level3" data-number="35.5.1">
<h3 data-number="35.5.1" class="anchored" data-anchor-id="passaggi-fondamentali-dellalgoritmo-di-metropolis"><span class="header-section-number">35.5.1</span> Passaggi Fondamentali dell’Algoritmo di Metropolis</h3>
<ol type="1">
<li><strong>Scegli un valore iniziale <span class="math inline">\(\theta_1\)</span>. Imposta <span class="math inline">\(t = 1\)</span>.</strong></li>
<li><strong>Campiona un possibile nuovo valore <span class="math inline">\(\theta_p\)</span> basato su una distribuzione di proposta <span class="math inline">\(g(\theta_p \mid \theta_t)\)</span></strong>. Di solito, si usa una distribuzione normale <span class="math inline">\(N(\theta_t, \tau)\)</span> come distribuzione di proposta, dove <span class="math inline">\(\tau\)</span> funge da parametro di regolazione che controlla la dimensione del passo.</li>
<li><strong>Calcola il rapporto <span class="math inline">\(\alpha = \frac{p(\theta_p \mid y)}{p(\theta_t \mid y)}\)</span>.</strong></li>
<li><strong>Se <span class="math inline">\(\alpha \geq 1\)</span>, imposta <span class="math inline">\(\theta_{t+1} = \theta_p\)</span>.</strong></li>
<li><strong>Se <span class="math inline">\(\alpha &lt; 1\)</span>, imposta <span class="math inline">\(\theta_{t+1} = \theta_p\)</span> con probabilità <span class="math inline">\(\alpha\)</span>.</strong> Altrimenti, imposta <span class="math inline">\(\theta_{t+1} = \theta_t\)</span>.</li>
<li><strong>Ripeti dal passo 2 per campionare un nuovo valore <span class="math inline">\(\theta_p\)</span>.</strong></li>
</ol>
</section>
<section id="dettagli-dellalgoritmo" class="level3" data-number="35.5.2">
<h3 data-number="35.5.2" class="anchored" data-anchor-id="dettagli-dellalgoritmo"><span class="header-section-number">35.5.2</span> Dettagli dell’Algoritmo</h3>
<ul>
<li><p><strong>Distribuzione di Proposta</strong>: La distribuzione di proposta <span class="math inline">\(g(\theta_p \mid \theta_t)\)</span> è usata per generare nuovi campioni di <span class="math inline">\(\theta_p\)</span> basati sul valore corrente <span class="math inline">\(\theta_t\)</span>. Una scelta comune è la distribuzione normale <span class="math inline">\(N(\theta_t, \tau)\)</span>, dove <span class="math inline">\(\tau\)</span> è un parametro di tuning che controlla la dimensione dei passi del campionamento. Un valore di <span class="math inline">\(\tau\)</span> troppo grande o troppo piccolo può influenzare negativamente l’efficienza del campionamento.</p></li>
<li><p><strong>Calcolo del Rapporto <span class="math inline">\(\alpha\)</span></strong>: Il rapporto <span class="math inline">\(\alpha\)</span> è dato dalla probabilità a posteriori del nuovo valore <span class="math inline">\(\theta_p\)</span> rispetto alla probabilità a posteriori del valore corrente <span class="math inline">\(\theta_t\)</span>. Questo rapporto determina l’accettazione o il rifiuto del nuovo campione.</p></li>
<li><p><strong>Decisione di Accettazione</strong>:</p>
<ul>
<li>Se <span class="math inline">\(\alpha \geq 1\)</span>, il nuovo valore <span class="math inline">\(\theta_p\)</span> è sempre accettato.</li>
<li>Se <span class="math inline">\(\alpha &lt; 1\)</span>, il nuovo valore <span class="math inline">\(\theta_p\)</span> è accettato con probabilità <span class="math inline">\(\alpha\)</span>. Se non viene accettato, il valore corrente <span class="math inline">\(\theta_t\)</span> è mantenuto per il prossimo passo.</li>
</ul></li>
</ul>
<p>Questo processo permette di esplorare lo spazio dei parametri <span class="math inline">\(\theta\)</span> generando una catena di Markov che, nel tempo, converge verso la distribuzione a posteriori <span class="math inline">\(p(\theta \mid y)\)</span>. Utilizzando questa catena, possiamo stimare empiricamente le proprietà della distribuzione a posteriori.</p>
<p>La procedura continua con l’iterazione dei passi dal 2 al 5, permettendo alla catena di campioni di convergere alla distribuzione target. Inizialmente, i campioni potrebbero non essere rappresentativi, ma dopo un sufficiente numero di iterazioni (il cosiddetto “burn-in”), la distribuzione dei punti accettati dovrebbe avvicinarsi a quella che si intende esplorare.</p>
<p>L’efficienza di questo algoritmo deriva dalla sua abilità nel mantenere un equilibrio tra l’esplorazione di nuove possibilità e l’utilizzo di quelle già note. Questo viene realizzato attraverso l’adozione di un meccanismo che accetta i punti suggeriti in modo probabilistico, facilitando così la raccolta di un campione che riflette accuratamente la distribuzione di probabilità complessa sotto indagine.</p>
<p>Per una visualizzazione del comportamento dell’algoritmo di Metropolis nell’esplorare lo spazio dei parametri, si può consultare <a href="https://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/">questo post</a>. La distinzione tra i diversi metodi MCMC si basa principalmente sul tipo di distribuzione proposta e sul criterio di accettazione dei punti nel campionamento.</p>
</section>
<section id="implementazione-dellalgoritmo-di-metropolis" class="level3" data-number="35.5.3">
<h3 data-number="35.5.3" class="anchored" data-anchor-id="implementazione-dellalgoritmo-di-metropolis"><span class="header-section-number">35.5.3</span> Implementazione dell’Algoritmo di Metropolis</h3>
<p>Iniziamo col definire la distribuzione a priori. In questo esempio, la distribuzione a priori è una distribuzione Beta(4, 6).</p>
<div id="cell-23" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:12:12.051103Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:12:12.046959Z&quot;}" data-execution_count="14">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prior(p):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> stats.beta.pdf(p, alpha, beta)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Definiamo la verosimiglianza. Il problema presente richiede una verosimiglianza binomiale.</p>
<div id="cell-25" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:12:13.814524Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:12:13.809362Z&quot;}" data-execution_count="15">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> likelihood(p):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> <span class="dv">14</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> stats.binom.pmf(y, n, p)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La distribuzione a posteriori non normalizzata è il prodotto della distribuzione a priori e della verosimiglianza. Si noti che, per il motivo spiegato prima, non è necessario normalizzare la distribuzione a posteriori.</p>
<div id="cell-27" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:12:16.160214Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:12:16.148084Z&quot;}" data-execution_count="16">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> posterior(p):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> likelihood(p) <span class="op">*</span> prior(p)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Nell’implementazione di un algoritmo di Metropolis in ambito bayesiano, vi sono diversi aspetti da considerare attentamente. Ecco alcuni punti cruciali:</p>
<ol type="1">
<li><p><strong>Simmetria della Distribuzione Proposta</strong>: È fondamentale che la distribuzione proposta sia simmetrica. Questa è una condizione necessaria per il funzionamento dell’algoritmo di Metropolis, ma non per quello di Metropolis-Hastings.</p></li>
<li><p><strong>Valore Iniziale</strong>: Il valore iniziale scelto dovrebbe essere plausibile in base alla distribuzione a priori, in modo da facilitare la convergenza dell’algoritmo.</p></li>
<li><p><strong>Probabilità Zero</strong>: Nel caso in cui la verosimiglianza o il prior assumano un valore di zero, il rapporto tra le densità di probabilità (<code>pdfratio</code>) all’interno dell’algoritmo di Metropolis risulterà indefinito. Pertanto, è importante garantire che il valore <code>x_star</code>, generato dalla distribuzione proposta, cada sempre entro i limiti del supporto sia del prior che della verosimiglianza.</p></li>
</ol>
<p>Di seguito è presentato il codice dell’algoritmo di Metropolis.</p>
<div id="cell-29" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Definizione della funzione dell'algoritmo di Metropolis.</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="co"># nsamp: Numero di campioni da generare.</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co"># xinit: Valore iniziale da cui iniziare il sampling.</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> metropolis(nsamp, xinit):</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Inizializza un array vuoto per conservare i campioni generati.</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> np.empty(nsamp)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Imposta il primo valore (valore iniziale) da cui partire per la generazione dei campioni.</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    x_prev <span class="op">=</span> xinit</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Inizia un ciclo che si ripeterà per il numero di volte specificato da nsamp (numero di campioni da generare).</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(nsamp):</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Genera un nuovo punto (x_star) usando una distribuzione normale (gaussiana).</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Questo nuovo punto è generato in modo da essere "vicino" al punto precedente (x_prev),</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># con una deviazione standard di 0.1. Questo significa che la maggior parte dei punti</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># sarà entro 0.1 unità da x_prev, ma alcuni potrebbero essere più lontani.</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>        x_star <span class="op">=</span> np.random.normal(x_prev, <span class="fl">0.1</span>)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Verifica che il nuovo punto (x_star) sia un valore plausibile nel contesto del problema.</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Qui, l'assunzione è che x_star debba essere tra 0 e 1. Se non lo è, il punto è rifiutato.</span></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="dv">0</span> <span class="op">&lt;=</span> x_star <span class="op">&lt;=</span> <span class="dv">1</span>:</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calcola il valore della funzione di densità di probabilità posterior per il nuovo punto e il punto precedente.</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>            <span class="co"># La funzione posterior è definita altrove e rappresenta il prodotto del prior e della likelihood.</span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>            p_star <span class="op">=</span> posterior(x_star)</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>            p_prev <span class="op">=</span> posterior(x_prev)</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calcola il rapporto tra le densità posterior del nuovo punto e del punto precedente.</span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Questo rapporto determina la probabilità di accettare il nuovo punto.</span></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Se p_prev è 0, per evitare la divisione per zero, il rapporto è impostato a 1.</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>            pdfratio <span class="op">=</span> p_star <span class="op">/</span> p_prev <span class="cf">if</span> p_prev <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">1</span></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Genera un numero casuale tra 0 e 1.</span></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>            random_chance <span class="op">=</span> np.random.uniform()</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calcola il rapporto tra la densità posteriore del nuovo punto e quella del punto precedente.</span></span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Se il nuovo punto è migliore o uguale, questo rapporto sarà &gt;= 1.</span></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Se il nuovo punto è peggiore, il rapporto sarà un numero fra 0 e 1.</span></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>            acceptance_probability <span class="op">=</span> <span class="bu">min</span>(<span class="dv">1</span>, pdfratio)</span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Se il numero casuale è minore dell'acceptance_probability, accettiamo il nuovo punto.</span></span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Ciò significa che un punto migliore o uguale viene sempre accettato (perché random_chance sarà sempre &lt; 1),</span></span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a>            <span class="co"># mentre un punto peggiore ha una possibilità basata sul quanto è peggiore.</span></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> random_chance <span class="op">&lt;</span> acceptance_probability:</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>                samples[i] <span class="op">=</span> x_star  <span class="co"># Accetta il nuovo punto.</span></span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>                x_prev <span class="op">=</span> (</span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>                    x_star  <span class="co"># Aggiorna il punto precedente con il nuovo punto accettato.</span></span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>                samples[i] <span class="op">=</span> (</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>                    x_prev  <span class="co"># Mantiene il punto precedente se il nuovo punto non è accettato.</span></span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a>            samples[i] <span class="op">=</span> (</span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a>                x_prev  <span class="co"># Se x_star non è nel supporto, conserva il punto precedente.</span></span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Dopo aver generato il numero desiderato di campioni, ritorna l'array dei campioni.</span></span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> samples</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>L’idea fondamentale dietro la fase dell’algoritmo di Metropolis successiva al calcolo di <code>p_star</code> e <code>p_prev</code> è decidere se “muoversi” verso un nuovo punto basandosi su quanto è probabile (o “buono”) quel punto rispetto al punto attuale, in termini della densità posteriore. Qui, la “probabilità” di un punto è data dalla sua densità posteriore, che è un modo per misurare quanto bene un certo valore del parametro si adatta ai dati osservati, dato un modello.</p>
<p>Ecco come funziona:</p>
<ol type="1">
<li><p><strong>Generazione di un numero casuale</strong>.</p></li>
<li><p><strong>Confronto tra i punti</strong>: Si confronta il “valore” del nuovo punto (<code>x_star</code>) con quello del punto precedente (<code>x_prev</code>). Questo “valore” è dato dalla densità posteriore: più alto è, meglio è.</p></li>
<li><p><strong>Decisione</strong>:</p>
<ul>
<li>Se il nuovo punto è <strong>migliore</strong> del precedente (ovvero, ha una densità posteriore <strong>maggiore o uguale</strong>), lo accettiamo sempre.</li>
<li>Se il nuovo punto è <strong>peggiore</strong> del precedente (ha una densità posteriore <strong>minore</strong>), non lo rifiutiamo subito. Invece, gli diamo una chance di essere scelto, ma questa chance è più piccola quanto più il nuovo punto è “peggiore”.</li>
</ul></li>
</ol>
<p>In termini di codice, questa logica si traduce così:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Genera un numero casuale tra 0 e 1.</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>random_chance <span class="op">=</span> np.random.uniform()</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcola il rapporto tra la densità posteriore del nuovo punto e quella del punto precedente.</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Se il nuovo punto è migliore o uguale, questo rapporto sarà &gt;= 1.</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Se il nuovo punto è peggiore, il rapporto sarà un numero fra 0 e 1.</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>acceptance_probability <span class="op">=</span> <span class="bu">min</span>(<span class="dv">1</span>, pdfratio)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Se il numero casuale è minore dell'acceptance_probability, accettiamo il nuovo punto.</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Ciò significa che un punto migliore o uguale viene sempre accettato (perché random_chance sarà sempre &lt; 1),</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="co"># mentre un punto peggiore ha una possibilità basata sul quanto è peggiore.</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> random_chance <span class="op">&lt;</span> acceptance_probability:</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    samples[i] <span class="op">=</span> x_star  <span class="co"># Accetta il nuovo punto.</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>    x_prev <span class="op">=</span> x_star      <span class="co"># Aggiorna il punto precedente con il nuovo punto accettato.</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>    samples[i] <span class="op">=</span> x_prev  <span class="co"># Mantiene il punto precedente se il nuovo punto non è accettato.</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In sintesi, questo meccanismo consente all’algoritmo di esplorare lo spazio dei parametri in modo efficiente, accettando sempre miglioramenti e, occasionalmente, facendo passi in direzioni non ottimali per evitare di rimanere intrappolati in “minimi locali”, ovvero in soluzioni che sembrano buone rispetto a quelle vicine ma non sono le migliori globalmente.</p>
<p>Si osservi un punto importante: nel calcolo di <code>pdfratio</code>, il rapporto tra la densità a posteriori del parametro proposto <code>x_star</code> e quella del parametro corrente <code>x_prev</code>, la costante di normalizzazione si annulla grazie alla regola di Bayes. Questo lascia nel rapporto solamente le componenti relative alla verosimiglianza e alla distribuzione a priori, che sono entrambe facilmente calcolabili. Matematicamente, questo si esprime come:</p>
<p><span class="math display">\[
\begin{equation}
\text{pdfratio} = \frac{p(\theta^* \mid y)}{p(\theta^{\text{prev}} \mid y)} = \frac{\frac{p(y \mid \theta^*) p(\theta^*)}{p(y)}}{\frac{p(y \mid \theta^{\text{prev}}) p(\theta^{\text{prev}})}{p(y)}}
= \frac{p(y \mid \theta^*) p(\theta^*)}{p(y \mid \theta^{\text{prev}}) p(\theta^{\text{prev}})}
\end{equation}
\]</span> (eq-ratio-metropolis)</p>
<p>Eseguiamo dunque il campionamento usando l’algoritmo che abbiamo definito.</p>
<div id="cell-32" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:12:39.577063Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:12:24.866146Z&quot;}" data-execution_count="18">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">100_000</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>samps <span class="op">=</span> metropolis(n_samples, <span class="fl">0.5</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In somma, l’algoritmo Metropolis accetta come input il numero <code>nsamp</code> di passi da simulare e il punto di partenza. Come output, l’algoritmo restituisce una catena di valori del parametro, specificamente la sequenza <span class="math inline">\(\theta^{(1)}, \theta^{(2)}, \ldots, \theta^{\text{nsamp}}\)</span>. Uno degli aspetti cruciali per la riuscita dell’algoritmo è il raggiungimento della stazionarietà da parte della catena. In genere, i primi 1000-5000 valori vengono scartati in quanto rappresentano il periodo di “burn-in” della catena. Dopo un determinato numero di passi <span class="math inline">\(k\)</span>, la catena converge e i valori diventano campioni effettivi dalla distribuzione a posteriori <span class="math inline">\(p(\theta \mid y)\)</span>.</p>
<p>Il modello descritto è stato inizialmente proposto da Metropolis et al.&nbsp;nel 1953 <span class="citation" data-cites="metropolist_etal_1953">(<a href="../../99-references.html#ref-metropolist_etal_1953" role="doc-biblioref">Metropolis et al. 1953</a>)</span>. Hastings nel 1970 introdusse un’estensione nota come algoritmo Metropolis-Hastings <span class="citation" data-cites="hastings_1970">(<a href="../../99-references.html#ref-hastings_1970" role="doc-biblioref">Hastings 1970</a>)</span>. Altre varianti includono il campionatore di Gibbs, introdotto da Geman e Geman nel 1984 <span class="citation" data-cites="geman_geman_1984">(<a href="../../99-references.html#ref-geman_geman_1984" role="doc-biblioref">Geman e Geman 1984</a>)</span>, l’Hamiltonian Monte Carlo <span class="citation" data-cites="duane1987hybrid">(<a href="../../99-references.html#ref-duane1987hybrid" role="doc-biblioref">Duane et al. 1987</a>)</span>, e il No-U-Turn Sampler (NUTS) utilizzato in pacchetti come PyMC e Stan <span class="citation" data-cites="hoffman2014no">(<a href="../../99-references.html#ref-hoffman2014no" role="doc-biblioref">Hoffman, Gelman, et al. 2014</a>)</span>. Per un’analisi più dettagliata e intuitiva dell’algoritmo Metropolis, si rimanda a <span class="citation" data-cites="doingbayesian">Kruschke (<a href="../../99-references.html#ref-doingbayesian" role="doc-biblioref">2014</a>)</span>.</p>
<p>Un elemento chiave da considerare nell’uso dell’algoritmo Metropolis è il tasso di accettazione, che è il rapporto tra il numero di valori del parametro proposti e il numero di quei valori che vengono effettivamente accettati. Un limite di questo algoritmo è la sua inefficienza relativa: rispetto alle sue varianti più moderne, l’algoritmo Metropolis tende ad essere meno efficiente.</p>
</section>
</section>
<section id="aspetti-computazionali" class="level2" data-number="35.6">
<h2 data-number="35.6" class="anchored" data-anchor-id="aspetti-computazionali"><span class="header-section-number">35.6</span> Aspetti computazionali</h2>
<section id="warm-upburn-in" class="level3" data-number="35.6.1">
<h3 data-number="35.6.1" class="anchored" data-anchor-id="warm-upburn-in"><span class="header-section-number">35.6.1</span> Warm-up/Burn-in</h3>
<p>Una catena di Markov ha bisogno di alcune iterazioni per raggiungere la distribuzione stazionaria. Queste iterazioni sono comunemente chiamate iterazioni di warm-up o burn-in (a seconda dell’algoritmo e del software utilizzato) e vengono di solito scartate. In molti programmi software, la prima metà delle iterazioni viene considerata come iterazioni di warm-up, quindi, nel caso presente, anche se abbiamo ottenuto 100000 iterazioni, ne utilizzeremo solo 50000.</p>
</section>
<section id="sintesi-della-distribuzione-a-posteriori" class="level3" data-number="35.6.2">
<h3 data-number="35.6.2" class="anchored" data-anchor-id="sintesi-della-distribuzione-a-posteriori"><span class="header-section-number">35.6.2</span> Sintesi della distribuzione a posteriori</h3>
<p>L’array <code>samps</code> contiene 100000 valori di una catena di Markov. Escludiamo i primi 50000 valori considerati come burn-in e consideriamo i restanti 50000 valori come un campione casuale estratto dalla distribuzione a posteriori <span class="math inline">\(p(\theta \mid y)\)</span>.</p>
<p>Mediante i valori della catena così ottenuta è facile trovare una stima a posteriori del parametro <span class="math inline">\(\theta\)</span>. Per esempio, possiamo trovare la stima della media a posteriori.</p>
<div id="cell-35" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:12:44.227523Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:12:44.222391Z&quot;}" data-execution_count="19">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>burnin <span class="op">=</span> <span class="bu">int</span>(n_samples <span class="op">*</span> <span class="fl">0.5</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>burnin</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>50000</code></pre>
</div>
</div>
<div id="cell-36" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:12:46.060202Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:12:46.047272Z&quot;}" data-execution_count="20">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>np.mean(samps[burnin:])</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>0.16408968344081415</code></pre>
</div>
</div>
<p>Oppure possiamo stimare la deviazione standard della distribuzione a posteriori.</p>
<div id="cell-38" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:12:47.966732Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:12:47.958107Z&quot;}" data-execution_count="21">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>np.std(samps[burnin:])</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>0.03513787973422409</code></pre>
</div>
</div>
<p>Visualizziamo un <em>trace plot</em> dei valori della catena di Markov dopo il periodo di burn-in.</p>
<div id="cell-40" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:12:50.867938Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:12:50.519308Z&quot;}" data-execution_count="22">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>plt.plot(samps[burnin:])</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"sample"</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"theta"</span>)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_metropolis_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Il trace plot indica che la catena inizia con un valore casuale per poi spostarsi rapidamente nell’area intorno a 0.16, che è l’area con alta densità a posteriori. Successivamente, oscilla intorno a quel valore per le iterazioni successive.</p>
<div id="cell-42" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:12:54.187856Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:12:54.034529Z&quot;}" data-execution_count="23">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>plt.plot(samps[:<span class="dv">500</span>])</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"sample"</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"theta"</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_metropolis_files/figure-html/cell-19-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>L’istogramma mostrato di seguito, sul quale è stata sovrapposta la distribuzione a posteriori derivata analiticamente – specificamente una <span class="math inline">\(\text{Beta}(25, 17)\)</span> – dimostra che la catena converge effettivamente alla distribuzione a posteriori desiderata.</p>
<div id="cell-44" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:12:58.593324Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:12:58.427645Z&quot;}" data-execution_count="24">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>plt.hist(samps[burnin:], bins<span class="op">=</span><span class="dv">30</span>, alpha<span class="op">=</span><span class="fl">0.4</span>, label<span class="op">=</span><span class="st">"MCMC distribution"</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the true function</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>plt.plot(x, stats.beta.pdf(x, <span class="dv">18</span>, <span class="dv">92</span>), <span class="st">"C0"</span>, label<span class="op">=</span><span class="st">"True distribution"</span>)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_metropolis_files/figure-html/cell-20-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>È possibile usare la funzione <code>summary</code> del pacchetto AriviZ per calolare l’intervallo di credibilità, ovvero l’intervallo che contiene la proporzione indicata dei valori estratti a caso dalla distribuzione a posteriori.</p>
<div id="cell-46" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:01.815307Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:01.783434Z&quot;}" data-execution_count="25">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>az.summary(samps[burnin:], kind<span class="op">=</span><span class="st">"stats"</span>, hdi_prob<span class="op">=</span><span class="fl">0.94</span>, round_to<span class="op">=</span><span class="dv">2</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">mean</th>
<th data-quarto-table-cell-role="th">sd</th>
<th data-quarto-table-cell-role="th">hdi_3%</th>
<th data-quarto-table-cell-role="th">hdi_97%</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">x</td>
<td>0.16</td>
<td>0.04</td>
<td>0.1</td>
<td>0.23</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<p>Un KDE plot corrispondente all’istogramma precedente si può generare usando <code>az.plot_posterior()</code>. La curva rappresenta l’intera distribuzione a posteriori e viene calcolata utilizzando la stima della densità del kernel (KDE) che serve a “lisciare” l’istogramma.</p>
<div id="cell-48" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:05.281861Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:05.140042Z&quot;}" data-execution_count="26">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>az.plot_posterior(samps[burnin:])</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_metropolis_files/figure-html/cell-22-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>L’HDI è una scelta comune nelle statistiche bayesiane e valori arrotondati come 50% o 95% sono molto popolari. Ma ArviZ utilizza il 94% come valore predefinito. La ragione di questa scelta è che il 94% è vicino al valore ampiamente utilizzato del 95%, ma è anche diverso da questo, così da servire da “amichevole promemoria” che non c’è niente di speciale nella soglia del 5%. Idealmente sarebbe opportuno scegliere un valore che si adatti alle specifiche esigenze dell’analisi statistica che si sta svolgendo, o almeno riconoscere che si sta usando un valore arbitrario.</p>
</section>
</section>
<section id="diagnostiche-della-soluzione-mcmc" class="level2" data-number="35.7">
<h2 data-number="35.7" class="anchored" data-anchor-id="diagnostiche-della-soluzione-mcmc"><span class="header-section-number">35.7</span> Diagnostiche della soluzione MCMC</h2>
<section id="catene-multiple" class="level3" data-number="35.7.1">
<h3 data-number="35.7.1" class="anchored" data-anchor-id="catene-multiple"><span class="header-section-number">35.7.1</span> Catene multiple</h3>
<p>Poiché ciascuno stato in una catena di Markov dipende dagli stati precedenti, il valore o i valori iniziali possono influenzare i valori campionati. Una soluzione per verificare la sensibilità rispetto ai valori iniziali è utilizzare più catene, ognuna con diversi valori iniziali. Se più catene campionano la stessa distribuzione target, queste dovrebbero mescolarsi bene, ovvero intersecarsi l’una con l’altra in un trace plot.</p>
</section>
<section id="stazionarietà" class="level3" data-number="35.7.2">
<h3 data-number="35.7.2" class="anchored" data-anchor-id="stazionarietà"><span class="header-section-number">35.7.2</span> Stazionarietà</h3>
<p>Un punto importante da verificare è se il campionatore ha raggiunto la sua distribuzione stazionaria. La convergenza di una catena di Markov alla distribuzione stazionaria viene detta “mixing”.</p>
</section>
<section id="autocorrelazione" class="level3" data-number="35.7.3">
<h3 data-number="35.7.3" class="anchored" data-anchor-id="autocorrelazione"><span class="header-section-number">35.7.3</span> Autocorrelazione</h3>
<p>Ogni passo nell’algoritmo MCMC è chiamato <em>iterazione</em>. I valori campionati sono dipendenti, il che significa che il valore all’iterazione <span class="math inline">\(m\)</span> dipende dal valore all’iterazione <span class="math inline">\(m-1\)</span>. Questa è una differenza importante rispetto alle funzioni che generano campioni casuali indipendenti, come <code>beta(25, 17).rvs()</code>. I valori campionati formano una <em>catena di Markov</em>, il che significa che ciascun valore campionato è correlato con il valore precedente (ad esempio, se <span class="math inline">\(\theta(m)\)</span> è grande, <span class="math inline">\(\theta(m+1)\)</span> sarà anch’esso grande).</p>
<p>Informazioni sul “mixing” della catena di Markov sono fornite dall’autocorrelazione. L’autocorrelazione misura la correlazione tra i valori successivi di una catena di Markov. Il valore <span class="math inline">\(m\)</span>-esimo della serie ordinata viene confrontato con un altro valore ritardato di una quantità <span class="math inline">\(k\)</span> (dove <span class="math inline">\(k\)</span> è l’entità del ritardo) per verificare quanto si correli al variare di <span class="math inline">\(k\)</span>. L’autocorrelazione di ordine 1 (<em>lag 1</em>) misura la correlazione tra valori successivi della catena di Markow (cioè, la correlazione tra <span class="math inline">\(\theta^{(m)}\)</span> e <span class="math inline">\(\theta^{(m-1)}\)</span>); l’autocorrelazione di ordine 2 (<em>lag 2</em>) misura la correlazione tra valori della catena di Markow separati da due “passi” (cioè, la correlazione tra <span class="math inline">\(\theta^{(m)}\)</span> e <span class="math inline">\(\theta^{(m-2)}\)</span>); e così via.</p>
<p>L’autocorrelazione di ordine <span class="math inline">\(k\)</span> è data da <span class="math inline">\(\rho_k\)</span> e può essere stimata come:</p>
<p><span class="math display">\[
\begin{align}
\rho_k &amp;= \frac{Cov(\theta_m, \theta_{m+k})}{Var(\theta_m)}\notag\\
&amp;= \frac{\sum_{m=1}^{n-k}(\theta_m - \bar{\theta})(\theta_{m-k} - \bar{\theta})}{\sum_{m=1}^{n-k}(\theta_m - \bar{\theta})^2} \qquad\text{con }\quad \bar{\theta} = \frac{1}{n}\sum_{m=1}^{n}\theta_m.
\end{align}
\]</span> (eq-autocor)</p>
<p>Per fare un esempio pratico, simuliamo dei dati autocorrelati.</p>
<div id="cell-50" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:14.103239Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:14.088762Z&quot;}" data-execution_count="40">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> pd.array([<span class="dv">22</span>, <span class="dv">24</span>, <span class="dv">25</span>, <span class="dv">25</span>, <span class="dv">28</span>, <span class="dv">29</span>, <span class="dv">34</span>, <span class="dv">37</span>, <span class="dv">40</span>, <span class="dv">44</span>, <span class="dv">51</span>, <span class="dv">48</span>, <span class="dv">47</span>, <span class="dv">50</span>, <span class="dv">51</span>])</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;IntegerArray&gt;
[22, 24, 25, 25, 28, 29, 34, 37, 40, 44, 51, 48, 47, 50, 51]
Length: 15, dtype: Int64</code></pre>
</div>
</div>
<p>L’autocorrelazione di ordine 1 è semplicemente la correlazione tra ciascun elemento e quello successivo nella sequenza.</p>
<div id="cell-52" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:19.851915Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:19.838140Z&quot;}" data-execution_count="41">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>sm.tsa.acf(x)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>array([ 1.        ,  0.83174224,  0.65632458,  0.49105012,  0.27863962,
        0.03102625, -0.16527446, -0.30369928, -0.40095465, -0.45823389,
       -0.45047733, -0.36933174])</code></pre>
</div>
</div>
<p>Nell’esempio, il vettore <code>x</code> è una sequenza temporale di 15 elementi. Il vettore <span class="math inline">\(x'\)</span> include gli elementi con gli indici da 0 a 13 nella sequenza originaria, mentre il vettore <span class="math inline">\(x''\)</span> include gli elementi 1:14. Gli elementi delle coppie ordinate dei due vettori avranno dunque gli indici <span class="math inline">\((0, 1)\)</span>, <span class="math inline">\((1, 2), (2, 3), \dots (13, 14)\)</span> degli elementi della sequenza originaria. La correlazione di Pearson tra i vettori <span class="math inline">\(x'\)</span> e <span class="math inline">\(x''\)</span> corrisponde all’autocorrelazione di ordine 1 della serie temporale.</p>
<p>Nell’output precedente</p>
<ul>
<li>0.83174224 è l’autocorrelazione di ordine 1 (lag = 1),</li>
<li>0.65632458 è l’autocorrelazione di ordine 2 (lag = 2),</li>
<li>0.49105012 è l’autocorrelazione di ordine 3 (lag = 3),</li>
<li>ecc.</li>
</ul>
<p>È possibile specificare il numero di ritardi (<em>lag</em>) da utilizzare con l’argomento <code>nlags</code>:</p>
<div id="cell-54" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:23.261460Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:23.237239Z&quot;}" data-execution_count="42">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>sm.tsa.acf(x, nlags<span class="op">=</span><span class="dv">4</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="42">
<pre><code>array([1.        , 0.83174224, 0.65632458, 0.49105012, 0.27863962])</code></pre>
</div>
</div>
<p>In Python possiamo creare un grafico della funzione di autocorrelazione (correlogramma) per una serie temporale usando la funzione <code>tsaplots.plot_acf()</code> dalla libreria <code>statsmodels</code>.</p>
<div id="cell-56" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:25.740566Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:25.579637Z&quot;}" data-execution_count="43">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>tsaplots.plot_acf(x, lags<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_metropolis_files/figure-html/cell-26-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Per i dati dell’esempio in discussione otteniamo la situazione seguente.</p>
<div id="cell-58" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:32.510199Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:30.039818Z&quot;}" data-execution_count="44">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>tsaplots.plot_acf(samps[burnin:], lags<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_metropolis_files/figure-html/cell-27-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Il correlogramma è uno strumento grafico usato per la valutazione della tendenza di una catena di Markov nel tempo. Il correlogramma si costruisce a partire dall’autocorrelazione <span class="math inline">\(\rho_k\)</span> di una catena di Markov in funzione del ritardo <span class="math inline">\(k\)</span> con cui l’autocorrelazione è calcolata: nel grafico ogni barretta verticale riporta il valore dell’autocorrelazione (sull’asse delle ordinate) in funzione del ritardo (sull’asse delle ascisse).</p>
<p>In situazioni ottimali l’autocorrelazione diminuisce rapidamente ed è effettivamente pari a 0 per piccoli lag. Ciò indica che i valori della catena di Markov che si trovano a più di soli pochi passi di distanza gli uni dagli altri non risultano associati tra loro, il che fornisce una conferma del “mixing” della catena di Markov, ossia della convergenza alla distribuzione stazionaria. Nelle analisi bayesiane, una delle strategie che consentono di ridurre l’autocorrelazione è quella di assottigliare l’output immagazzinando solo ogni <span class="math inline">\(m\)</span>-esimo punto dopo il periodo di burn-in. Una tale strategia va sotto il nome di <em>thinning</em>.</p>
<p>Nel seguente correlogramma, analizziamo la medesima catena di Markov. Tuttavia, in questa occasione applichiamo un “thinning” (sottocampionamento) con un fattore di 5.</p>
<div id="cell-60" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:37.102817Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:36.966176Z&quot;}" data-execution_count="45">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>thin <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>sampsthin <span class="op">=</span> samps[burnin::thin]</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>tsaplots.plot_acf(sampsthin, lags<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_metropolis_files/figure-html/cell-28-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Si può notare come l’autocorrelazione diminuisce molto più rapidamente.</p>
<section id="tasso-di-accettazione" class="level4" data-number="35.7.3.1">
<h4 data-number="35.7.3.1" class="anchored" data-anchor-id="tasso-di-accettazione"><span class="header-section-number">35.7.3.1</span> Tasso di accettazione</h4>
<p>Quando si utilizza l’algoritmo Metropolis, è importante monitorare il tasso di accettazione e assicurarsi che sia nell’intervallo ottimale. Se si accetta quasi sempre il candidato proposto, probabilmente significa che, in ogni iterazione, la catena salta solo di un piccolo passo (in modo che il rapporto di accettazione sia vicino a 1 ogni volta). Di conseguenza, la catena impiegherà molte iterazioni per raggiungere altre regioni della distribuzione stazionaria e i campioni consecutivi saranno molto fortemente correlati. D’altra parte, se il tasso di accettazione è molto basso, la catena rimarrà bloccata nella stessa posizione per molte iterazioni prima di spostarsi verso uno stato diverso. Per l’algoritmo Metropolis base con un singolo parametro con una distribuzione proposta Gaussiana normale, un tasso di accettazione ottimale è compreso tra il 40% e il 50%.</p>
</section>
<section id="test-di-convergenza" class="level4" data-number="35.7.3.2">
<h4 data-number="35.7.3.2" class="anchored" data-anchor-id="test-di-convergenza"><span class="header-section-number">35.7.3.2</span> Test di convergenza</h4>
<p>Per valutare la convergenza di una catena di Markov Monte Carlo (MCMC), esistono diversi metodi, tra cui approcci grafici e test statistici. Ecco una spiegazione più chiara e dettagliata:</p>
</section>
</section>
<section id="approcci-grafici-tracce-delle-serie-temporali-trace-plots" class="level3" data-number="35.7.4">
<h3 data-number="35.7.4" class="anchored" data-anchor-id="approcci-grafici-tracce-delle-serie-temporali-trace-plots"><span class="header-section-number">35.7.4</span> Approcci Grafici: Tracce delle Serie Temporali (Trace Plots)</h3>
<p>Le tracce delle serie temporali, o trace plots, sono grafici che mostrano l’evoluzione dei valori campionati rispetto al numero di iterazioni. Questi grafici sono utili per valutare visivamente se la catena ha raggiunto la convergenza. Segni che indicano una potenziale convergenza includono:</p>
<ul>
<li><strong>Assenza di Tendenze</strong>: Non ci sono trend ascendenti o discendenti nel corso delle iterazioni.</li>
<li><strong>Costanza dell’Ampiezza</strong>: La variabilità dei valori campionati rimane costante nel tempo, senza significative fluttuazioni.</li>
<li><strong>Mancanza di Periodicità</strong>: Non si osservano cicli o ripetizioni regolari che potrebbero indicare la presenza di correlazioni residue.</li>
</ul>
</section>
<section id="test-statistici-per-la-convergenza" class="level3" data-number="35.7.5">
<h3 data-number="35.7.5" class="anchored" data-anchor-id="test-statistici-per-la-convergenza"><span class="header-section-number">35.7.5</span> Test Statistici per la Convergenza</h3>
<p>Oltre agli approcci grafici, esistono test statistici specifici che possono aiutare a determinare se la catena ha raggiunto uno stato stazionario.</p>
<section id="test-di-geweke" class="level4" data-number="35.7.5.1">
<h4 data-number="35.7.5.1" class="anchored" data-anchor-id="test-di-geweke"><span class="header-section-number">35.7.5.1</span> Test di Geweke</h4>
<p>Il test di Geweke è una procedura che confronta le medie di due segmenti della catena di campionamento, tipicamente il primo 10% e l’ultimo 50% dei campioni, dopo aver escluso un iniziale periodo di “burn-in” (una fase iniziale durante la quale la catena potrebbe non essere ancora convergente). La premessa di base è che, se la catena è in uno stato stazionario, le medie di questi due segmenti dovrebbero essere sostanzialmente uguali. Differenze significative tra queste medie possono indicare che la catena non ha ancora raggiunto la convergenza.</p>
</section>
<section id="geweke-z-score" class="level4" data-number="35.7.5.2">
<h4 data-number="35.7.5.2" class="anchored" data-anchor-id="geweke-z-score"><span class="header-section-number">35.7.5.2</span> Geweke Z-score</h4>
<p>Una variante del test di Geweke è lo z-score di Geweke, che offre un modo quantitativo per valutare le differenze tra i segmenti della catena. Questo test calcola uno z-score che confronta le medie dei due segmenti tenendo conto della varianza. Un valore di z-score:</p>
<ul>
<li><strong>Al di sotto di 2 (in valore assoluto)</strong> suggerisce che non ci sono differenze significative tra i segmenti, indicando che la catena potrebbe essere in stato stazionario.</li>
<li><strong>Superiore a 2 (in valore assoluto)</strong> indica che esiste una differenza significativa tra i segmenti, suggerendo che la catena non ha raggiunto la convergenza e potrebbe essere necessario un periodo di burn-in più esteso.</li>
</ul>
<p>Entrambi i metodi forniscono strumenti utili per valutare la convergenza delle catene MCMC. È importante notare che nessun test può garantire con certezza la convergenza, ma l’utilizzo congiunto di approcci grafici e test statistici può offrire una buona indicazione dello stato della catena.</p>
</section>
</section>
<section id="effective-sample-size-ess" class="level3" data-number="35.7.6">
<h3 data-number="35.7.6" class="anchored" data-anchor-id="effective-sample-size-ess"><span class="header-section-number">35.7.6</span> Effective sample size (ESS)</h3>
<p>Quando le iterazioni sono dipendenti, ogni iterazione contiene informazioni sovrapposte con le iterazioni precedenti. In altre parole, quando si ottengono 500 campioni dipendenti dalla distribuzione a posteriori, questi contengono solo informazioni equivalenti a &lt; 500 campioni indipendenti. L’ESS (Effective Sample Size) quantifica la quantità effettiva di informazioni, quindi una catena con ESS = n conterrà approssimativamente la stessa quantità di informazioni di n campioni indipendenti. In generale, vogliamo che l’ESS sia almeno 400 per un’utilizzazione generale nel riassumere la distribuzione a posteriori.</p>
</section>
</section>
<section id="caso-normale-normale" class="level2" data-number="35.8">
<h2 data-number="35.8" class="anchored" data-anchor-id="caso-normale-normale"><span class="header-section-number">35.8</span> Caso Normale-Normale</h2>
<p>Consideriamo ora il caso Normale-Normale di cui è possibile trovare una soluzione analitica. Supponiamo, come prior, una <span class="math inline">\(\mathcal{N}(30, 5\)</span>.</p>
<div id="cell-64" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:44.508917Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:44.503276Z&quot;}" data-execution_count="46">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prior(mu):</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> stats.norm.pdf(mu, <span class="dv">30</span>, <span class="dv">5</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Per la verosimiglianza del parametro <span class="math inline">\(\mu\)</span>, supponiamo <span class="math inline">\(\sigma\)</span> nota e uguale alla deviazione standard del campione.</p>
<div id="cell-66" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:46.627729Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:46.624746Z&quot;}" data-execution_count="47">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> likelihood(mu, data):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    std_data <span class="op">=</span> np.std(data)  <span class="co"># Calcola la deviazione standard dei dati</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.prod(stats.norm.pdf(data, mu, std_data))</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Definiamo il posterior non normalizzato:</p>
<div id="cell-68" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:48.769380Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:48.764214Z&quot;}" data-execution_count="48">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> posterior(mu, data):</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> likelihood(mu, data) <span class="op">*</span> prior(mu)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Modifichiamo ora l’algoritmo di Metropolis descritto sopra per adattarlo al caso presente.</p>
<div id="cell-70" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:51.026747Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:51.021109Z&quot;}" data-execution_count="49">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Algoritmo di Metropolis per il caso normale-normale</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> metropolis_for_normal(nsamp, xinit, data):</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> np.empty(nsamp)</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    x_prev <span class="op">=</span> xinit</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(nsamp):</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>        x_star <span class="op">=</span> np.random.normal(x_prev, <span class="fl">0.5</span>)  <span class="co"># Genera un nuovo punto dalla proposta</span></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calcola il rapporto di accettazione e accetta il nuovo punto con una certa probabilità</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> posterior(x_star, data) <span class="op">/</span> posterior(x_prev, data) <span class="op">&gt;</span> np.random.uniform():</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>            x_prev <span class="op">=</span> x_star</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>        samples[i] <span class="op">=</span> x_prev</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> samples</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Vediamo cosa fa la presente versione dell’algoritmo di Metropolis passo dopo passo:</p>
<ol type="1">
<li><p><strong>Ciclo sui Campioni</strong>: <code>for i in range(nsamp):</code> inizia un ciclo che si ripeterà <code>nsamp</code> volte, dove <code>nsamp</code> è il numero totale di campioni che vogliamo generare. Ogni iterazione di questo ciclo produrrà un campione dalla distribuzione di interesse.</p></li>
<li><p><strong>Generazione di un Nuovo Punto</strong>: <code>x_star = np.random.normal(x_prev, 0.5)</code> genera un nuovo punto (<code>x_star</code>) come proposta per il prossimo passo del campionamento. Questo è fatto campionando da una distribuzione normale con media uguale all’ultimo punto accettato (<code>x_prev</code>) e una deviazione standard di <code>0.5</code>. Questa distribuzione è detta distribuzione di proposta e serve a esplorare lo spazio dei parametri.</p></li>
<li><p><strong>Calcolo del Rapporto di Accettazione</strong>:</p>
<ul>
<li>Il rapporto di accettazione è calcolato come <code>posterior(x_star, data) / posterior(x_prev, data)</code>, che è il rapporto tra la probabilità del posterior del nuovo punto proposto (<code>x_star</code>) e la probabilità del posterior dell’ultimo punto accettato (<code>x_prev</code>).</li>
<li>Questo rapporto indica quanto sia preferibile il nuovo punto rispetto al precedente, basandosi sulla funzione <code>posterior</code>, che calcola la probabilità a posteriori del modello dato il parametro e i dati osservati.</li>
</ul></li>
<li><p><strong>Decisione di Accettazione del Nuovo Punto</strong>:</p>
<ul>
<li>La decisione se accettare o meno il nuovo punto (<code>x_star</code>) si basa su un confronto del rapporto di accettazione con un numero casuale uniformemente distribuito tra 0 e 1 (<code>np.random.uniform()</code>).</li>
<li>Se il rapporto di accettazione è maggiore di questo numero casuale, il nuovo punto è accettato come il prossimo punto nella catena (<code>x_prev = x_star</code>). Ciò significa che il nuovo punto ha una probabilità a posteriori più alta rispetto al punto precedente, o è stato “fortunato” nel processo di selezione casuale, consentendo all’algoritmo di esplorare lo spazio dei parametri anche in zone di minore probabilità.</li>
<li>Se il nuovo punto non viene accettato, la catena rimane nel punto precedente (<code>x_prev</code>), e questo punto viene nuovamente aggiunto all’array dei campioni.</li>
</ul></li>
<li><p><strong>Salvataggio del Campione</strong>: <code>samples[i] = x_prev</code> salva il punto corrente (che può essere il nuovo punto accettato o il punto precedente se il nuovo punto è stato rifiutato) nell’array <code>samples</code>. Questo processo si ripete fino a quando non si raggiunge il numero desiderato di campioni.</p></li>
</ol>
<p>Come dati, usiamo il campione di 30 valori BDI-II ottenuti dai soggetti clinici esaminati da {cite}<code>zetsche_2019future</code>.</p>
<div id="cell-73" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:55.370275Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:55.362498Z&quot;}" data-execution_count="50">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>    <span class="fl">26.0</span>, <span class="fl">35.0</span>, <span class="dv">30</span>, <span class="dv">25</span>, <span class="dv">44</span>, <span class="dv">30</span>, <span class="dv">33</span>, <span class="dv">43</span>, <span class="dv">22</span>, <span class="dv">43</span>, <span class="dv">24</span>, <span class="dv">19</span>, <span class="dv">39</span>, <span class="dv">31</span>, <span class="dv">25</span>, <span class="dv">28</span>, <span class="dv">35</span>, <span class="dv">30</span>, <span class="dv">26</span>, <span class="dv">31</span>,</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>    <span class="dv">41</span>, <span class="dv">36</span>, <span class="dv">26</span>, <span class="dv">35</span>, <span class="dv">33</span>, <span class="dv">28</span>, <span class="dv">27</span>, <span class="dv">34</span>, <span class="dv">27</span>, <span class="dv">22</span>,</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>])</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Procediamo con l’esecuzione dell’algoritmo di Metropolis.</p>
<div id="cell-75" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:14:13.286088Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:57.908754Z&quot;}" data-execution_count="51">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> metropolis_for_normal(<span class="dv">100_000</span>, np.mean(y), y)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>samples.shape</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>(100000,)</code></pre>
</div>
</div>
<section id="calcolo-dei-parametri-del-posterior-analitico" class="level3" data-number="35.8.1">
<h3 data-number="35.8.1" class="anchored" data-anchor-id="calcolo-dei-parametri-del-posterior-analitico"><span class="header-section-number">35.8.1</span> Calcolo dei Parametri del Posterior Analitico</h3>
<p>Nel caso normale-normale, possiamo derivare analiticamente la distribuzione posteriore quando sia il prior che la likelihood sono distribuzioni normali. La bellezza di questo approccio sta nella forma chiusa del posterior, che è anch’esso una distribuzione normale con parametri specifici facilmente calcolabili. Ecco come si fa:</p>
<ol type="1">
<li><p><strong>Media Posteriore (<span class="math inline">\(\mu_{post}\)</span>)</strong>: La media del posterior è un peso tra la media del campione e la media del prior, dove i pesi sono determinati dalle varianze del prior e dei dati.</p>
<p><span class="math display">\[
\mu_{post} = \frac{\frac{\mu_{prior}}{\sigma_{prior}^2} + \frac{\sum y_i}{\sigma_{data}^2}}{\frac{1}{\sigma_{prior}^2} + \frac{n}{\sigma_{data}^2}}
\]</span></p></li>
<li><p><strong>Varianza Posteriore (<span class="math inline">\(\sigma_{post}^2\)</span>)</strong>: La varianza del posterior è determinata dalle varianze del prior e dei dati.</p>
<p><span class="math display">\[
\sigma_{post}^2 = \left(\frac{1}{\sigma_{prior}^2} + \frac{n}{\sigma_{data}^2}\right)^{-1}
\]</span></p></li>
</ol>
<p>Dove: - <span class="math inline">\(\mu_{prior}\)</span> è la media del prior (in questo caso, 30), - <span class="math inline">\(\sigma_{prior}^2\)</span> è la varianza del prior (<span class="math inline">\(5^2\)</span> in questo caso), - <span class="math inline">\(\sigma_{data}^2\)</span> è la varianza dei dati (calcolata dai dati), - <span class="math inline">\(n\)</span> è il numero di osservazioni, - <span class="math inline">\(\sum y_i\)</span> è la somma delle osservazioni.</p>
</section>
<section id="codice-per-il-grafico" class="level3" data-number="35.8.2">
<h3 data-number="35.8.2" class="anchored" data-anchor-id="codice-per-il-grafico"><span class="header-section-number">35.8.2</span> Codice per il Grafico</h3>
<p>Per produrre il grafico con l’istogramma dei campioni dal posterior (usando l’algoritmo di Metropolis) e la curva della distribuzione posteriore analitica, usiamo i parametri calcolati:</p>
<div id="cell-77" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:14:27.238252Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:14:27.043718Z&quot;}" data-execution_count="52">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Parametri del prior</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>mu_prior <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>std_prior <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>var_prior <span class="op">=</span> std_prior <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Dati osservati</span></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(y)</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>sum_y <span class="op">=</span> np.<span class="bu">sum</span>(y)</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>var_data <span class="op">=</span> np.var(y, ddof<span class="op">=</span><span class="dv">1</span>)  <span class="co"># ddof=1 for sample variance</span></span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcolo dei parametri posterior</span></span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>mu_post <span class="op">=</span> (mu_prior <span class="op">/</span> var_prior <span class="op">+</span> sum_y <span class="op">/</span> var_data) <span class="op">/</span> (<span class="dv">1</span> <span class="op">/</span> var_prior <span class="op">+</span> n <span class="op">/</span> var_data)</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>var_post <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">/</span> var_prior <span class="op">+</span> n <span class="op">/</span> var_data)</span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>std_post <span class="op">=</span> np.sqrt(var_post)</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Generazione dei punti x per il grafico</span></span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(mu_post <span class="op">-</span> <span class="dv">4</span> <span class="op">*</span> std_post, mu_post <span class="op">+</span> <span class="dv">4</span> <span class="op">*</span> std_post, <span class="dv">1000</span>)</span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Istogramma dei campioni dal posterior</span></span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a>plt.hist(samples[burnin:], bins<span class="op">=</span><span class="dv">30</span>, alpha<span class="op">=</span><span class="fl">0.4</span>, density<span class="op">=</span><span class="va">True</span>, label<span class="op">=</span><span class="st">"MCMC Samples Distribution"</span>)</span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Curva della distribuzione posteriore analitica</span></span>
<span id="cb47-23"><a href="#cb47-23" aria-hidden="true" tabindex="-1"></a>plt.plot(x, stats.norm.pdf(x, mu_post, std_post), <span class="st">"C1"</span>, label<span class="op">=</span><span class="st">"Analytical Posterior Distribution"</span>)</span>
<span id="cb47-24"><a href="#cb47-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-25"><a href="#cb47-25" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb47-26"><a href="#cb47-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="10_metropolis_files/figure-html/cell-35-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Questo codice mostra come integrare l’analisi MCMC con l’approccio analitico per il caso normale-normale, offrendo sia una visualizzazione dei risultati del sampling che la conferma attraverso la soluzione analitica.</p>
</section>
</section>
<section id="commenti-e-considerazioni-finali" class="level2" data-number="35.9">
<h2 data-number="35.9" class="anchored" data-anchor-id="commenti-e-considerazioni-finali"><span class="header-section-number">35.9</span> Commenti e considerazioni finali</h2>
<p>In generale, la distribuzione a posteriori dei parametri di un modello statistico non può essere determinata per via analitica. Tale problema viene invece affrontato facendo ricorso ad una classe di algoritmi per il campionamento da distribuzioni di probabilità che sono estremamente onerosi dal punto di vista computazionale e che possono essere utilizzati nelle applicazioni pratiche solo grazie alla potenza di calcolo dei moderni computer. Lo sviluppo di software che rendono sempre più semplice l’uso dei metodi MCMC, insieme all’incremento della potenza di calcolo dei computer, ha contribuito a rendere sempre più popolare il metodo dell’inferenza bayesiana che, in questo modo, può essere estesa a problemi di qualunque grado di complessità.</p>
</section>
<section id="informazioni-sullambiente-di-sviluppo" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="informazioni-sullambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</h2>
<div id="cell-81" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:14:34.256099Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:14:34.201359Z&quot;}" data-execution_count="53">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext watermark</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>watermark <span class="op">-</span>n <span class="op">-</span>u <span class="op">-</span>v <span class="op">-</span>iv <span class="op">-</span>w <span class="op">-</span>m</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Last updated: Sat Jun 15 2024

Python implementation: CPython
Python version       : 3.12.3
IPython version      : 8.25.0

Compiler    : Clang 16.0.6 
OS          : Darwin
Release     : 23.4.0
Machine     : arm64
Processor   : arm
CPU cores   : 8
Architecture: 64bit

sys        : 3.12.3 | packaged by conda-forge | (main, Apr 15 2024, 18:35:20) [Clang 16.0.6 ]
matplotlib : 3.8.4
arviz      : 0.18.0
statsmodels: 0.14.2
scipy      : 1.13.1
numpy      : 1.26.4
seaborn    : 0.13.2
pymc       : 5.15.1
pandas     : 2.2.2

Watermark: 2.4.3
</code></pre>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-duane1987hybrid" class="csl-entry" role="listitem">
Duane, Simon, Anthony D Kennedy, Brian J Pendleton, e Duncan Roweth. 1987. <span>«Hybrid monte carlo»</span>. <em>Physics letters B</em> 195 (2): 216–22.
</div>
<div id="ref-geman_geman_1984" class="csl-entry" role="listitem">
Geman, Stuart, e Donald Geman. 1984. <span>«Stochastic relaxation, <span>Gibbs</span> distributions, and the <span>Bayesian</span> restoration of images»</span>. <em>IEEE Transactions on pattern analysis and machine intelligence</em> 6: 721–41.
</div>
<div id="ref-hastings_1970" class="csl-entry" role="listitem">
Hastings, W. Keith. 1970. <span>«<span>Monte</span> <span>Carlo</span> sampling methods using <span>Markov</span> chains and their applications»</span>. <em>Biometrika</em> 57 (1): 97–109.
</div>
<div id="ref-hoffman2014no" class="csl-entry" role="listitem">
Hoffman, Matthew D, Andrew Gelman, et al. 2014. <span>«The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo.»</span> <em>Journal of Machine Learning Research</em> 15 (1): 1593–623.
</div>
<div id="ref-doingbayesian" class="csl-entry" role="listitem">
Kruschke, John. 2014. <em>Doing Bayesian data analysis: <span>A</span> tutorial with <span>R</span>, JAGS, and <span>Stan</span></em>. Academic Press.
</div>
<div id="ref-metropolist_etal_1953" class="csl-entry" role="listitem">
Metropolis, Nicholas, Arianna W. Rosenbluth, Marshall N. Rosenbluth, Augusta H. Teller, e Edward Teller. 1953. <span>«Equation of state calculations by fast computing machines»</span>. <em>The Journal of Chemical Physics</em> 21 (6): 1087–92.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiato!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiato!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/ccaudek\.github\.io\/psicometria\/intro\.html");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../chapters/chapter_4/06_balance_prior_post.html" class="pagination-link" aria-label="L'influenza della distribuzione a priori">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">L’influenza della distribuzione a priori</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/chapter_4/15_stan_beta_binomial.html" class="pagination-link" aria-label="Linguaggio Stan">
        <span class="nav-page-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Linguaggio Stan</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.dev/ccaudek/psicometria/blob/main/chapters/chapter_4/10_metropolis.ipynb" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li></ul></div></div></div></footer></body></html>