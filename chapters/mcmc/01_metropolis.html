<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Corrado Caudek">

<title>50&nbsp; Monte Carlo a Catena di Markov – Data Science per Psicologi</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/mcmc/02_ppl.html" rel="next">
<link href="../../chapters/mcmc/introduction_mcmc.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/mcmc/introduction_mcmc.html">MCMC</a></li><li class="breadcrumb-item"><a href="../../chapters/mcmc/01_metropolis.html"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Monte Carlo a Catena di Markov</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Data Science per Psicologi</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ccaudek/psicometria/" title="Eseguire il codice" class="quarto-navigation-tool px-1" aria-label="Eseguire il codice"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Attiva/disattiva la modalità oscura"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benvenuti</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../prefazione.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Python</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/introduction_python.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/00_prelims.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Preliminari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/01_python_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Python (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/02_python_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Python (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/03_numpy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">NumPy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/04_pandas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Pandas (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/05_pandas_aggregate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Pandas (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/06_pandas_functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Pandas (3)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/07_matplotlib.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Matplotlib</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/08_seaborn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Seaborn</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Fondamenti</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/introduction_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/00_uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Abbracciare l’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/01_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Concetti chiave</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/02_measurement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">La misurazione in psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/03_data_analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">L’analisi dei dati psicologici</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">EDA</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/introduction_eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/01_project_structure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Le fasi del progetto di analisi dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/02_data_cleaning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Flusso di lavoro per la pulizia dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/03_dplyr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Data tidying</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/04_exploring_qualitative_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Esplorare i dati qualitativi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/05_exploring_numeric_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Esplorare i dati numerici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/06_data_visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Principi della visualizzazione dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/07_loc_scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Indicatori di tendenza centrale e variabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/08_correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Relazioni tra variabili: correlazione e covarianza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/09_causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Causalità dai dati osservazionali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/10_estimand.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Estimandi teorici e estimandi empirici</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Probabilità</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/introduction_probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/01_intro_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Interpretazione della probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/02_prob_spaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Misura di Probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/03_prob_on_general_spaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Fondamenti della probabilità: assiomi di Kolmogorov e sigma-algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/04_conditional_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Probabilità condizionata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/05_random_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/06_expval_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Proprietà delle variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/07_bayes_theorem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/08_sampling_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/09_joint_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Probabilità congiunta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/10_density_func.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">La funzione di densità di probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11_discr_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. discrete</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/12_cont_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. continue</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/13_qq_plot.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Diagramma quantile-quantile</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/14_likelihood.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/15_simulation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Simulazioni</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Inferenza</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/introduction_bayes_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/01_intro_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">La quantificazione dell’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/02_bayesian_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Inferenza bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/03_subj_prop.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Pensare ad una proporzione in termini soggettivi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/04_grid_gauss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Calcolo della Distribuzione a Posteriori Gaussiana tramite Metodo a Griglia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/05_conjugate_families_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/06_conjugate_families_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/07_summary_posterior.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Sintesi a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/08_balance_prior_post.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">L’influenza della distribuzione a priori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/09_gamma_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Modello coniugato Gamma-Poisson</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/10_gamma_exponential_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Modello gamma-esponenziale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/11_post_pred_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Distribuzione predittiva a posteriori</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">MCMC</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/introduction_mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/01_metropolis.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Monte Carlo a Catena di Markov</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/02_ppl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Linguaggi di programmazione probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/03_stan_language.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Linguaggio Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/04_stan_summary_posterior.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">Metodi di sintesi della distribuzione a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/05_stan_diagnostics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Diagnostica delle catene markoviane</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/06_stan_prediction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">La predizione bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/07_bayesian_workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">Flusso di lavoro bayesiano</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/08_stan_odds_ratio.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">Analisi bayesiana dell’odds-ratio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/09_stan_normal_normal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Inferenza bayesiana su una media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/10_stan_two_groups.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">59</span>&nbsp; <span class="chapter-title">Confronto tra due gruppi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/11_stan_poisson_model_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">60</span>&nbsp; <span class="chapter-title">Modello di Poisson (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/12_stan_poisson_model_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">61</span>&nbsp; <span class="chapter-title">Modello di Poisson (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/13_stan_exponential_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">62</span>&nbsp; <span class="chapter-title">Modello esponenziale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/14_stan_gaussian_mixture.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">63</span>&nbsp; <span class="chapter-title">Modelli Mistura Gaussiani</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/15_stan_nuisance_parameters.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">64</span>&nbsp; <span class="chapter-title">Modelli con più di un parametro</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/16_stan_hier_beta_binom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">65</span>&nbsp; <span class="chapter-title">Modello gerarchico beta-binomiale con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/17_stan_categorical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">66</span>&nbsp; <span class="chapter-title">Modello categoriale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/18_cmdstanr_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">67</span>&nbsp; <span class="chapter-title">Introduzione a CmdStanR</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/19_approximations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">68</span>&nbsp; <span class="chapter-title">Metodi approssimativi nell’inferenza Bayesiana</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Regressione</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/introduction_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/01_reglin_bayesian.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">69</span>&nbsp; <span class="chapter-title">Modello bayesiano di regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/02_non_centered_param.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">70</span>&nbsp; <span class="chapter-title">Non-Centered Parameterization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/03_beauty_sex_power.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">71</span>&nbsp; <span class="chapter-title">Bellezza, sesso e potere</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/04_synt_sugar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">Zucchero sintattico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/05_two_means.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">Confronto tra le medie di due gruppi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/06_prediction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Predizione e inferenza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/07_design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Disegno della ricerca e potere statistico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/08_linear_algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">Elementi di algebra lineare</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/09_stan_multreg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Il modello di regressione multipla</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/10_hier_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">78</span>&nbsp; <span class="chapter-title">Il modello lineare gerarchico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/11_stan_mixed_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">79</span>&nbsp; <span class="chapter-title">Modelli misti con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/12_specification_error.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">80</span>&nbsp; <span class="chapter-title">Errore di specificazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/14_interactions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">81</span>&nbsp; <span class="chapter-title">Interazioni statistiche</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/16_missing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">82</span>&nbsp; <span class="chapter-title">Dati mancanti</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">Causalità</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/causal_inference/01_rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">Trial controllati randomizzati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/causal_inference/02_ate_att_atu.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">84</span>&nbsp; <span class="chapter-title">Concetti di ATE, ATT e ATU</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/causal_inference/03_endogenous_treatments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">85</span>&nbsp; <span class="chapter-title">Inferenza causale con trattamenti endogeni</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/causal_inference/04_mult_reg_causal_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">86</span>&nbsp; <span class="chapter-title">Inferenza causale nella regressione multipla</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/causal_inference/05_collider_bias.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">87</span>&nbsp; <span class="chapter-title">Collider bias</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">GLM</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/introduction_glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/01_robust_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">88</span>&nbsp; <span class="chapter-title">Regressione robusta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/02_stan_binomial_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">89</span>&nbsp; <span class="chapter-title">Modelli lineari generalizzati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/03_stan_logistic_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">90</span>&nbsp; <span class="chapter-title">Regressione logistica con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/04_stan_poisson_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">91</span>&nbsp; <span class="chapter-title">Regressione di Poisson con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/05_simchon_2023.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">92</span>&nbsp; <span class="chapter-title">Tweets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/06_stan_rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">93</span>&nbsp; <span class="chapter-title">Incorporare dati storici di controllo in una RCT</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/07_stan_mediation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">94</span>&nbsp; <span class="chapter-title">Modello di mediazione con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/08_uses_and_abuses.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">95</span>&nbsp; <span class="chapter-title">Usi e abusi dei modelli lineari</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Entropia</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/introduction_entropy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/01_entropy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">96</span>&nbsp; <span class="chapter-title">Entropia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/02_kl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">97</span>&nbsp; <span class="chapter-title">La divergenza di Kullback-Leibler</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/03_model_comparison.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">98</span>&nbsp; <span class="chapter-title">Divergenza KL, LPPD, ELPD e LOO-CV</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/04_loo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">99</span>&nbsp; <span class="chapter-title">Validazione Incrociata Leave-One-Out</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/05_regularization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">100</span>&nbsp; <span class="chapter-title">Regolarizzazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/06_cognitive_modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">101</span>&nbsp; <span class="chapter-title">Dall’analisi descrittiva alla modellazione cognitiva</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/07_inductive_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">102</span>&nbsp; <span class="chapter-title">Limiti dell’inferenza induttiva</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false">
 <span class="menu-text">Dinamiche</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/introduction_dynamic_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/01_canoeing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">103</span>&nbsp; <span class="chapter-title">Dinamiche post-errore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/02_change_across_time.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">104</span>&nbsp; <span class="chapter-title">Modellare il cambiamento nel tempo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/03_rescorla_wagner.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">105</span>&nbsp; <span class="chapter-title">Apprendimento per rinforzo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/04_affect.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">106</span>&nbsp; <span class="chapter-title">Le emozioni influenzano le emozioni</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/05_sequential_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">107</span>&nbsp; <span class="chapter-title">Analisi dinamica delle sequenze di apprendimento</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/07_bipolar_disorder.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">108</span>&nbsp; <span class="chapter-title">Modello di Markov nascosto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/08_haslbeck.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">109</span>&nbsp; <span class="chapter-title">Recuperare le dinamiche intra-personali dalle serie temporali psicologiche</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false">
 <span class="menu-text">Modelli cognitivi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/cognitive_models/introduction_cognitive_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/cognitive_models/01_ddm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">110</span>&nbsp; <span class="chapter-title">Drift Diffusion Model</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false">
 <span class="menu-text">Frequentismo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/introduction_frequentist_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/01_intro_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">111</span>&nbsp; <span class="chapter-title">Introduzione all’inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/02_conf_interv.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">112</span>&nbsp; <span class="chapter-title">Intervallo di confidenza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/03_test_ipotesi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">113</span>&nbsp; <span class="chapter-title">Significatività statistica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/04_two_ind_samples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">114</span>&nbsp; <span class="chapter-title">Test t di Student per campioni indipendenti</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="false">
 <span class="menu-text">Crisi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/introduction_replication_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/01_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">115</span>&nbsp; <span class="chapter-title">La Crisi della Replicazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/02_limits_stat_freq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">116</span>&nbsp; <span class="chapter-title">Limiti dell’inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/03_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">117</span>&nbsp; <span class="chapter-title">La grandezza dell’effetto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/04_s_m_errors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">118</span>&nbsp; <span class="chapter-title">Errori di segno e errori di grandezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/05_p_values.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">119</span>&nbsp; <span class="chapter-title">La fragilità del <em>p</em>-valore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/06_changes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">120</span>&nbsp; <span class="chapter-title">Proposte di cambiamento</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/07_integrity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">121</span>&nbsp; <span class="chapter-title">Integrità della ricerca</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="false">
 <span class="menu-text">Epilogo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/epiloque/epiloque.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Considerazioni Conclusive</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../99-references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliografia</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" role="navigation" aria-expanded="false">
 <span class="menu-text">Appendici</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-16" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a00_installation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Ambiente di lavoro</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01_markdown.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Jupyter Notebook</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a02_shell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">La Shell</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a03_colab_tutorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Colab: un breve tutorial</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a04_virtual_env.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Ambienti virtuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a10_math_symbols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Simbologia di base</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a11_numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Numeri e intervalli</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a12_sum_notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">H</span>&nbsp; <span class="chapter-title">Sommatorie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a13_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">I</span>&nbsp; <span class="chapter-title">Insiemi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a14_combinatorics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">J</span>&nbsp; <span class="chapter-title">Calcolo combinatorio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a15_calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">K</span>&nbsp; <span class="chapter-title">Per liberarvi dai terrori preliminari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a20_kde_plot.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">L</span>&nbsp; <span class="chapter-title">Kernel Density Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a30_prob_tutorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">M</span>&nbsp; <span class="chapter-title">Esercizi di probabilità discreta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a40_rng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">N</span>&nbsp; <span class="chapter-title">Generazione di numeri casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a44_montecarlo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">O</span>&nbsp; <span class="chapter-title">Simulazione Monte Carlo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a46_stan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">P</span>&nbsp; <span class="chapter-title">Linguaggio Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a47_first_order_markov.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">Q</span>&nbsp; <span class="chapter-title">Catene di Markov</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a48_hidden_markov_processes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">R</span>&nbsp; <span class="chapter-title">Processi di Markov Nascosti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a50_lin_fun.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">S</span>&nbsp; <span class="chapter-title">La funzione lineare</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a51_r_squared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">T</span>&nbsp; <span class="chapter-title">Teorema della scomposizione della devianza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a53_entropy_rv_cont.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">U</span>&nbsp; <span class="chapter-title">Entropia di una variabile casuale continua</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a55_rescorla_wagner.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">V</span>&nbsp; <span class="chapter-title">Apprendimento per rinforzo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a60_ttest_exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">W</span>&nbsp; <span class="chapter-title">Esercizi sull’inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a70_predict_counts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">X</span>&nbsp; <span class="chapter-title">La predizione delle frequenze</span></span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-17" role="navigation" aria-expanded="false">
 <span class="menu-text">Soluzioni degli esercizi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-17" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-17" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/solutions_probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">Y</span>&nbsp; <span class="chapter-title">Probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/solutions_bayesian_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">Z</span>&nbsp; <span class="chapter-title">Inferenza bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/solutions_causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/solutions_mult_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">\</span>&nbsp; <span class="chapter-title">Regressione multipla</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/solutions_rescorla_wagner.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">]</span>&nbsp; <span class="chapter-title">Modello Rescorla-Wagner</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/solutions_entropia.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">^</span>&nbsp; <span class="chapter-title">Entropia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/solutions_replication_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">_</span>&nbsp; <span class="chapter-title">Crisi della replicazione</span></span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indice</h2>
   
  <ul>
  <li><a href="#introduzione" id="toc-introduzione" class="nav-link active" data-scroll-target="#introduzione">Introduzione</a></li>
  <li><a href="#il-denominatore-bayesiano" id="toc-il-denominatore-bayesiano" class="nav-link" data-scroll-target="#il-denominatore-bayesiano"><span class="header-section-number">50.1</span> Il denominatore bayesiano</a></li>
  <li><a href="#il-metodo-monte-carlo-e-le-sue-limitazioni" id="toc-il-metodo-monte-carlo-e-le-sue-limitazioni" class="nav-link" data-scroll-target="#il-metodo-monte-carlo-e-le-sue-limitazioni"><span class="header-section-number">50.2</span> Il metodo Monte Carlo e le sue limitazioni</a></li>
  <li><a href="#perché-i-metodi-mcmc-sono-necessari" id="toc-perché-i-metodi-mcmc-sono-necessari" class="nav-link" data-scroll-target="#perché-i-metodi-mcmc-sono-necessari"><span class="header-section-number">50.3</span> Perché i metodi MCMC sono necessari</a></li>
  <li><a href="#le-catene-di-markov" id="toc-le-catene-di-markov" class="nav-link" data-scroll-target="#le-catene-di-markov"><span class="header-section-number">50.4</span> Le Catene di Markov</a>
  <ul>
  <li><a href="#catene-di-markov-e-metodi-mcmc" id="toc-catene-di-markov-e-metodi-mcmc" class="nav-link" data-scroll-target="#catene-di-markov-e-metodi-mcmc"><span class="header-section-number">50.4.1</span> Catene di Markov e Metodi MCMC</a></li>
  <li><a href="#condizioni-fondamentali-per-le-catene-di-markov" id="toc-condizioni-fondamentali-per-le-catene-di-markov" class="nav-link" data-scroll-target="#condizioni-fondamentali-per-le-catene-di-markov"><span class="header-section-number">50.4.2</span> Condizioni fondamentali per le Catene di Markov</a></li>
  <li><a href="#algoritmi-mcmc" id="toc-algoritmi-mcmc" class="nav-link" data-scroll-target="#algoritmi-mcmc"><span class="header-section-number">50.4.3</span> Algoritmi MCMC</a></li>
  <li><a href="#conclusione" id="toc-conclusione" class="nav-link" data-scroll-target="#conclusione"><span class="header-section-number">50.4.4</span> Conclusione</a></li>
  </ul></li>
  <li><a href="#estrazione-di-campioni-dalla-distribuzione-a-posteriori" id="toc-estrazione-di-campioni-dalla-distribuzione-a-posteriori" class="nav-link" data-scroll-target="#estrazione-di-campioni-dalla-distribuzione-a-posteriori"><span class="header-section-number">50.5</span> Estrazione di campioni dalla distribuzione a posteriori</a>
  <ul>
  <li><a href="#simulazione-con-distribuzione-target-nota" id="toc-simulazione-con-distribuzione-target-nota" class="nav-link" data-scroll-target="#simulazione-con-distribuzione-target-nota"><span class="header-section-number">50.5.1</span> Simulazione con distribuzione target nota</a></li>
  <li><a href="#algoritmo-di-metropolis" id="toc-algoritmo-di-metropolis" class="nav-link" data-scroll-target="#algoritmo-di-metropolis"><span class="header-section-number">50.5.2</span> Algoritmo di Metropolis</a></li>
  <li><a href="#principio-di-funzionamento" id="toc-principio-di-funzionamento" class="nav-link" data-scroll-target="#principio-di-funzionamento"><span class="header-section-number">50.5.3</span> Principio di Funzionamento</a></li>
  <li><a href="#burn-in-e-convergenza" id="toc-burn-in-e-convergenza" class="nav-link" data-scroll-target="#burn-in-e-convergenza"><span class="header-section-number">50.5.4</span> Burn-in e Convergenza</a></li>
  <li><a href="#meccanismo-di-accettazione-e-rifiuto" id="toc-meccanismo-di-accettazione-e-rifiuto" class="nav-link" data-scroll-target="#meccanismo-di-accettazione-e-rifiuto"><span class="header-section-number">50.5.5</span> Meccanismo di Accettazione e Rifiuto</a></li>
  <li><a href="#passaggi-fondamentali-dellalgoritmo-di-metropolis" id="toc-passaggi-fondamentali-dellalgoritmo-di-metropolis" class="nav-link" data-scroll-target="#passaggi-fondamentali-dellalgoritmo-di-metropolis"><span class="header-section-number">50.5.6</span> Passaggi Fondamentali dell’Algoritmo di Metropolis</a></li>
  <li><a href="#dettagli-aggiuntivi" id="toc-dettagli-aggiuntivi" class="nav-link" data-scroll-target="#dettagli-aggiuntivi"><span class="header-section-number">50.5.7</span> Dettagli Aggiuntivi</a></li>
  </ul></li>
  <li><a href="#esempio-di-implementazione" id="toc-esempio-di-implementazione" class="nav-link" data-scroll-target="#esempio-di-implementazione"><span class="header-section-number">50.6</span> Esempio di Implementazione</a>
  <ul>
  <li><a href="#simmetria-della-distribuzione-proposta" id="toc-simmetria-della-distribuzione-proposta" class="nav-link" data-scroll-target="#simmetria-della-distribuzione-proposta"><span class="header-section-number">50.6.1</span> Simmetria della Distribuzione Proposta</a></li>
  <li><a href="#scelta-del-valore-iniziale" id="toc-scelta-del-valore-iniziale" class="nav-link" data-scroll-target="#scelta-del-valore-iniziale"><span class="header-section-number">50.6.2</span> Scelta del Valore Iniziale</a></li>
  <li><a href="#gestione-della-probabilità-zero" id="toc-gestione-della-probabilità-zero" class="nav-link" data-scroll-target="#gestione-della-probabilità-zero"><span class="header-section-number">50.6.3</span> Gestione della Probabilità Zero</a></li>
  </ul></li>
  <li><a href="#meccanismo-di-accettazione" id="toc-meccanismo-di-accettazione" class="nav-link" data-scroll-target="#meccanismo-di-accettazione"><span class="header-section-number">50.7</span> Meccanismo di Accettazione</a>
  <ul>
  <li><a href="#generazione-del-nuovo-stato" id="toc-generazione-del-nuovo-stato" class="nav-link" data-scroll-target="#generazione-del-nuovo-stato"><span class="header-section-number">50.7.1</span> Generazione del Nuovo Stato</a></li>
  <li><a href="#confronto-tra-stati" id="toc-confronto-tra-stati" class="nav-link" data-scroll-target="#confronto-tra-stati"><span class="header-section-number">50.7.2</span> Confronto tra Stati</a></li>
  <li><a href="#calcolo-del-rapporto-di-accettazione" id="toc-calcolo-del-rapporto-di-accettazione" class="nav-link" data-scroll-target="#calcolo-del-rapporto-di-accettazione"><span class="header-section-number">50.7.3</span> Calcolo del Rapporto di Accettazione</a></li>
  </ul></li>
  <li><a href="#tasso-di-accettazione-e-efficienza" id="toc-tasso-di-accettazione-e-efficienza" class="nav-link" data-scroll-target="#tasso-di-accettazione-e-efficienza"><span class="header-section-number">50.8</span> Tasso di Accettazione e Efficienza</a></li>
  <li><a href="#esecuzione-del-campionamento" id="toc-esecuzione-del-campionamento" class="nav-link" data-scroll-target="#esecuzione-del-campionamento"><span class="header-section-number">50.9</span> Esecuzione del Campionamento</a></li>
  <li><a href="#aspetti-computazionali" id="toc-aspetti-computazionali" class="nav-link" data-scroll-target="#aspetti-computazionali"><span class="header-section-number">50.10</span> Aspetti computazionali</a>
  <ul>
  <li><a href="#warm-upburn-in" id="toc-warm-upburn-in" class="nav-link" data-scroll-target="#warm-upburn-in"><span class="header-section-number">50.10.1</span> Warm-up/Burn-in</a></li>
  <li><a href="#sec-moma-post-estimates" id="toc-sec-moma-post-estimates" class="nav-link" data-scroll-target="#sec-moma-post-estimates"><span class="header-section-number">50.10.2</span> Sintesi della distribuzione a posteriori</a></li>
  </ul></li>
  <li><a href="#catene-di-markov-e-convergenza" id="toc-catene-di-markov-e-convergenza" class="nav-link" data-scroll-target="#catene-di-markov-e-convergenza"><span class="header-section-number">50.11</span> Catene di Markov e Convergenza</a></li>
  <li><a href="#diagnostiche-della-soluzione-mcmc" id="toc-diagnostiche-della-soluzione-mcmc" class="nav-link" data-scroll-target="#diagnostiche-della-soluzione-mcmc"><span class="header-section-number">50.12</span> Diagnostiche della soluzione MCMC</a>
  <ul>
  <li><a href="#stazionarietà-e-convergenza" id="toc-stazionarietà-e-convergenza" class="nav-link" data-scroll-target="#stazionarietà-e-convergenza"><span class="header-section-number">50.12.1</span> Stazionarietà e Convergenza</a>
  <ul>
  <li><a href="#valutazione-visuale-trace-plots-e-grafici-di-densità" id="toc-valutazione-visuale-trace-plots-e-grafici-di-densità" class="nav-link" data-scroll-target="#valutazione-visuale-trace-plots-e-grafici-di-densità"><span class="header-section-number">50.12.1.1</span> Valutazione Visuale: Trace Plots e Grafici di Densità</a></li>
  </ul></li>
  <li><a href="#autocorrelazione-nelle-catene-di-markov-mcmc" id="toc-autocorrelazione-nelle-catene-di-markov-mcmc" class="nav-link" data-scroll-target="#autocorrelazione-nelle-catene-di-markov-mcmc"><span class="header-section-number">50.12.2</span> Autocorrelazione nelle catene di Markov MCMC</a>
  <ul>
  <li><a href="#tasso-di-accettazione" id="toc-tasso-di-accettazione" class="nav-link" data-scroll-target="#tasso-di-accettazione"><span class="header-section-number">50.12.2.1</span> Tasso di accettazione</a></li>
  </ul></li>
  <li><a href="#test-statistici-per-la-convergenza" id="toc-test-statistici-per-la-convergenza" class="nav-link" data-scroll-target="#test-statistici-per-la-convergenza"><span class="header-section-number">50.12.3</span> Test Statistici per la Convergenza</a>
  <ul>
  <li><a href="#test-di-geweke" id="toc-test-di-geweke" class="nav-link" data-scroll-target="#test-di-geweke"><span class="header-section-number">50.12.3.1</span> Test di Geweke</a></li>
  <li><a href="#geweke-z-score" id="toc-geweke-z-score" class="nav-link" data-scroll-target="#geweke-z-score"><span class="header-section-number">50.12.3.2</span> Geweke Z-score</a></li>
  </ul></li>
  <li><a href="#dimensione-del-campione-effettiva-ess" id="toc-dimensione-del-campione-effettiva-ess" class="nav-link" data-scroll-target="#dimensione-del-campione-effettiva-ess"><span class="header-section-number">50.12.4</span> Dimensione del campione effettiva (ESS)</a></li>
  <li><a href="#calcolo-della-statistica-di-gelman-rubin-hatr" id="toc-calcolo-della-statistica-di-gelman-rubin-hatr" class="nav-link" data-scroll-target="#calcolo-della-statistica-di-gelman-rubin-hatr"><span class="header-section-number">50.12.5</span> Calcolo della Statistica di Gelman-Rubin (<span class="math inline">\(\hat{R}\)</span>)</a></li>
  </ul></li>
  <li><a href="#vantaggi-del-campionamento-mcmc-rispetto-alle-soluzioni-analitiche" id="toc-vantaggi-del-campionamento-mcmc-rispetto-alle-soluzioni-analitiche" class="nav-link" data-scroll-target="#vantaggi-del-campionamento-mcmc-rispetto-alle-soluzioni-analitiche"><span class="header-section-number">50.13</span> Vantaggi del Campionamento MCMC rispetto alle Soluzioni Analitiche</a>
  <ul>
  <li><a href="#facilità-di-manipolazione-e-flessibilità" id="toc-facilità-di-manipolazione-e-flessibilità" class="nav-link" data-scroll-target="#facilità-di-manipolazione-e-flessibilità"><span class="header-section-number">50.13.1</span> Facilità di Manipolazione e Flessibilità</a></li>
  </ul></li>
  <li><a href="#caso-normale-normale" id="toc-caso-normale-normale" class="nav-link" data-scroll-target="#caso-normale-normale"><span class="header-section-number">50.14</span> Caso Normale-Normale</a>
  <ul>
  <li><a href="#calcolo-dei-parametri-del-posterior-analitico" id="toc-calcolo-dei-parametri-del-posterior-analitico" class="nav-link" data-scroll-target="#calcolo-dei-parametri-del-posterior-analitico"><span class="header-section-number">50.14.1</span> Calcolo dei Parametri del Posterior Analitico</a></li>
  <li><a href="#codice-per-il-grafico" id="toc-codice-per-il-grafico" class="nav-link" data-scroll-target="#codice-per-il-grafico"><span class="header-section-number">50.14.2</span> Codice per il Grafico</a></li>
  </ul></li>
  <li><a href="#commenti-e-considerazioni-finali" id="toc-commenti-e-considerazioni-finali" class="nav-link" data-scroll-target="#commenti-e-considerazioni-finali"><span class="header-section-number">50.15</span> Commenti e Considerazioni Finali</a></li>
  <li><a href="#informazioni-sullambiente-di-sviluppo" id="toc-informazioni-sullambiente-di-sviluppo" class="nav-link" data-scroll-target="#informazioni-sullambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.dev/ccaudek/psicometria/blob/main/chapters/mcmc/01_metropolis.ipynb" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/mcmc/introduction_mcmc.html">MCMC</a></li><li class="breadcrumb-item"><a href="../../chapters/mcmc/01_metropolis.html"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Monte Carlo a Catena di Markov</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-metropolis" class="quarto-section-identifier"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Monte Carlo a Catena di Markov</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><strong>Prerequisiti</strong></p>
<ul>
<li>Leggere l’<a href="../appendix/a44_montecarlo.html" class="quarto-xref"><span>Appendice O</span></a>.</li>
<li>Leggere l’<a href="../appendix/a47_first_order_markov.html" class="quarto-xref"><span>Appendice Q</span></a>.</li>
</ul>
<p><strong>Concetti e competenze chiave</strong></p>
<ul>
<li>Utilizzare metodi di Monte Carlo per stimare valori attesi e probabilità, evitando calcoli di integrali complessi.</li>
<li>Apprendere il ruolo delle catene di Markov per generare sequenze di campioni dalla distribuzione a posteriori.</li>
<li>Implementare e comprendere l’algoritmo di Metropolis come strumento per il campionamento dalla distribuzione a posteriori.</li>
<li>Valutare la convergenza delle catene di Markov e utilizzare strumenti come il trace plot e l’autocorrelazione per diagnosticare la qualità del campionamento.</li>
<li>Riconoscere l’importanza della fase di burn-in e la necessità di più catene per verificare la stazionarietà e ridurre l’autocorrelazione.</li>
</ul>
<p><strong>Preparazione del Notebook</strong></p>
<div id="cell-2" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard library imports</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Third-party imports</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> arviz <span class="im">as</span> az</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.graphics <span class="im">import</span> tsaplots</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cmdstanpy <span class="im">import</span> cmdstan_path, CmdStanModel</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cmdstanpy</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>cmdstanpy.utils.get_logger().setLevel(logging.ERROR)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Configuration</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="bu">sum</span>(<span class="bu">map</span>(<span class="bu">ord</span>, <span class="st">"metropolis"</span>))</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(seed<span class="op">=</span>seed)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>sns.set_theme(palette<span class="op">=</span><span class="st">"colorblind"</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>az.style.use(<span class="st">"arviz-darkgrid"</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>config InlineBackend.figure_format <span class="op">=</span> <span class="st">"retina"</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Define directories</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>home_directory <span class="op">=</span> os.path.expanduser(<span class="st">"~"</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>project_directory <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>home_directory<span class="sc">}</span><span class="ss">/_repositories/psicometria"</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="introduzione" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="introduzione">Introduzione</h2>
<p>In precedenza, abbiamo esplorato diversi esempi di inferenza bayesiana riguardanti la distribuzione a posteriori di un singolo parametro, come nel caso del modello bernoulliano. Abbiamo anche trattato metodi come l’approssimazione tramite griglia e l’utilizzo dei priori coniugati per ottenere o approssimare la distribuzione a posteriori. In questo capitolo, ci concentreremo sul metodo di simulazione <font color="orange">Monte Carlo a Catena di Markov (MCMC)</font>.</p>
<p>Il metodo MCMC è una tecnica computazionale utilizzata per approssimare distribuzioni di probabilità complesse, generando una sequenza di campioni (correlati) attraverso una catena di Markov, in cui ogni campione viene ottenuto tramite una transizione iterativa con probabilità attentamente progettate.</p>
<p>Il metodo MCMC rappresenta l’approccio moderno per approssimare distribuzioni a posteriori complesse. L’idea di base è simile al concetto di considerare la distribuzione a posteriori come una popolazione da cui estraiamo campioni ripetutamente. Con un numero sufficientemente grande di campioni (ad esempio 1000), la distribuzione del campione si avvicina molto alla distribuzione della popolazione, consentendo stime affidabili dei parametri incogniti.</p>
<p>Una differenza rispetto all’analogia precedente è che i campioni generati con MCMC sono correlati: se il primo campione ha un valore alto, anche il successivo ha maggiori probabilità di essere alto. Questo accade perché non abbiamo un modo diretto per estrarre campioni dalla distribuzione a posteriori, che spesso ha una forma molto complessa; utilizziamo invece algoritmi che ci permettono di arrivarci indirettamente. La correlazione tra i campioni non rappresenta un problema rilevante, ma rende necessario estrarre un numero maggiore di campioni per compensare questa correlazione e ottenere stime accurate.</p>
</section>
<section id="il-denominatore-bayesiano" class="level2" data-number="50.1">
<h2 data-number="50.1" class="anchored" data-anchor-id="il-denominatore-bayesiano"><span class="header-section-number">50.1</span> Il denominatore bayesiano</h2>
<p>Nell’approccio bayesiano, l’obiettivo principale è determinare la distribuzione a posteriori <span class="math inline">\(p(\theta \mid y)\)</span> di un parametro <span class="math inline">\(\theta\)</span>, utilizzando i dati osservati <span class="math inline">\(y\)</span> e la distribuzione a priori <span class="math inline">\(p(\theta)\)</span>. Questo si ottiene attraverso il teorema di Bayes:</p>
<p><span class="math display">\[
p(\theta \mid y) = \frac{p(y \mid \theta) p(\theta)}{\int p(y \mid \theta) p(\theta) d\theta}.
\]</span></p>
<p>Il denominatore <span class="math inline">\(\int p(y \mid \theta) p(\theta) d\theta\)</span> rappresenta la probabilità marginale di <span class="math inline">\(y\)</span>, chiamata <font color="orange">evidenza</font>. Tale integrale garantisce che <span class="math inline">\(p(\theta \mid y)\)</span> sia una distribuzione di probabilità valida. Tuttavia, il calcolo di questo integrale è spesso complesso, soprattutto in modelli articolati o ad alta dimensionalità, rendendo difficile ottenere una rappresentazione esplicita della distribuzione a posteriori.</p>
<p>Una possibile semplificazione analitica è l’uso di distribuzioni a priori coniugate, che offrono una soluzione esatta per la distribuzione a posteriori. Tuttavia, questo approccio è limitato a casi specifici e impone forti vincoli sulla scelta delle distribuzioni a priori e delle verosimiglianze.</p>
<p>Un approccio più generale è ricorrere a soluzioni numeriche. In precedenza abbiamo discusso il metodo di campionamento a griglia. Tuttavia, i metodi di campionamento a griglia, sebbene efficaci per modelli con pochi parametri, diventano impraticabili man mano che il numero di parametri aumenta, poiché richiedono una copertura densa dell’intero spazio parametrico. Di conseguenza, per modelli più complessi e con più parametri, si rende necessario un metodo che possa esplorare lo spazio dei parametri in maniera più efficiente.</p>
</section>
<section id="il-metodo-monte-carlo-e-le-sue-limitazioni" class="level2" data-number="50.2">
<h2 data-number="50.2" class="anchored" data-anchor-id="il-metodo-monte-carlo-e-le-sue-limitazioni"><span class="header-section-number">50.2</span> Il metodo Monte Carlo e le sue limitazioni</h2>
<p>Il metodo Monte Carlo fornisce una soluzione a questo problema generando campioni casuali dalla distribuzione a posteriori <span class="math inline">\(p(\theta \mid y)\)</span>. L’idea centrale è semplice: se possiamo generare un numero sufficiente di campioni casuali dalla distribuzione a posteriori, possiamo usare questi campioni per stimare le proprietà d’interesse, come la media o la varianza del parametro <span class="math inline">\(\theta\)</span>. Questa procedura ci permette di evitare il calcolo diretto dell’integrale complicato nel denominatore del teorema di Bayes.</p>
<p>Per esempio, se fossimo in grado di generare una serie di campioni <span class="math inline">\(\theta^{(1)}, \theta^{(2)}, \dots, \theta^{(T)}\)</span> dalla distribuzione a posteriori, potremmo approssimare il valore atteso di <span class="math inline">\(\theta\)</span> con la media campionaria:</p>
<p><span class="math display">\[
\mathbb{E}[\theta] \approx \frac{1}{T} \sum_{t=1}^T \theta^{(t)}.
\]</span></p>
<p>Tuttavia, un problema significativo nei metodi Monte Carlo tradizionali è che generare campioni indipendenti dalla distribuzione a posteriori non è semplice, soprattutto quando questa distribuzione ha una forma complessa, è multimodale o definita su spazi di alta dimensionalità. Le regioni di alta densità, che contribuiscono maggiormente al valore dell’integrale, possono essere difficili da individuare e campionare adeguatamente. Per ottenere una buona copertura dello spazio dei parametri, sarebbe necessario generare un numero enorme di campioni, rendendo il metodo Monte Carlo inefficiente e computazionalmente oneroso.</p>
</section>
<section id="perché-i-metodi-mcmc-sono-necessari" class="level2" data-number="50.3">
<h2 data-number="50.3" class="anchored" data-anchor-id="perché-i-metodi-mcmc-sono-necessari"><span class="header-section-number">50.3</span> Perché i metodi MCMC sono necessari</h2>
<p>È qui che entrano in gioco i Metodi Monte Carlo a Catena di Markov (MCMC). Questi metodi risolvono il problema generando campioni <font color="orange">dipendenti</font> dalla distribuzione a posteriori, sfruttando la struttura di una catena di Markov. A differenza dei campioni indipendenti utilizzati nei metodi Monte Carlo tradizionali, i metodi MCMC costruiscono una sequenza di campioni, in cui ciascun campione dipende dal precedente. Questa dipendenza permette di esplorare in modo più efficiente le regioni di alta densità della distribuzione a posteriori, riducendo il numero di campioni necessari per ottenere stime accurate.</p>
<p>In pratica, MCMC consente di evitare di campionare inutilmente da regioni di bassa densità, concentrandosi invece sulle aree più rilevanti della distribuzione. Questo approccio è particolarmente potente nei contesti ad alta dimensionalità o in presenza di distribuzioni multimodali, dove i metodi Monte Carlo tradizionali risulterebbero inefficaci o richiederebbero un numero sproporzionato di campioni.</p>
<p>In sintesi, i metodi Monte Carlo classici sono limitati quando si tratta di campionare da distribuzioni complesse e multidimensionali. I metodi MCMC, invece, offrono una soluzione efficiente e flessibile, permettendo di esplorare le distribuzioni a posteriori anche in contesti complessi, senza la necessità di campionare indipendentemente ogni punto. Nel prossimo paragrafo introdurremo i concetti fondamentali delle catene di Markov e vedremo come queste vengono utilizzate nei metodi MCMC per campionare efficacemente da distribuzioni a posteriori difficili da trattare analiticamente.</p>
</section>
<section id="le-catene-di-markov" class="level2" data-number="50.4">
<h2 data-number="50.4" class="anchored" data-anchor-id="le-catene-di-markov"><span class="header-section-number">50.4</span> Le Catene di Markov</h2>
<p>Le catene di Markov, introdotte da Andrey Markov nel 1906, rappresentano un’estensione della legge dei grandi numeri per descrivere sequenze di variabili casuali <font color="orange">non indipendenti</font> (per maggiori dettagli, si veda l’<a href="../appendix/a47_first_order_markov.html" class="quarto-xref"><span>Appendice Q</span></a>). Nella statistica tradizionale, si lavora spesso con sequenze di variabili casuali indipendenti e identicamente distribuite (i.i.d.), come <span class="math inline">\(X_0, X_1, \ldots, X_n, \ldots\)</span>, dove ogni variabile è indipendente dalle altre e segue la stessa distribuzione. Tuttavia, nei modelli più realistici che descrivono fenomeni complessi, l’indipendenza tra variabili è un’assunzione troppo rigida e spesso irrealistica.</p>
<p>Le catene di Markov superano questo limite introducendo una dipendenza locale, detta <font color="orange">dipendenza a un passo</font>, formalizzata nella cosiddetta <font color="orange">proprietà di Markov</font>. Secondo questa proprietà, il valore futuro di una variabile casuale <span class="math inline">\(X_{n+1}\)</span> dipende unicamente dal valore attuale <span class="math inline">\(X_n\)</span>, ignorando tutta la storia precedente della catena. Questo permette di semplificare notevolmente i calcoli relativi alle probabilità condizionali. La proprietà di Markov è formalmente espressa come:</p>
<p><span class="math display">\[
P(X_{n+1} = j | X_n = i, X_{n-1} = i_{n-1}, \ldots, X_0 = i_0) = P(X_{n+1} = j | X_n = i).
\]</span></p>
<p>In altre parole, la previsione di un evento futuro dipende soltanto dallo stato attuale e non da tutti gli eventi precedenti, semplificando così il processo di modellazione. Questa caratteristica rende le catene di Markov particolarmente utili per descrivere sistemi dinamici in cui gli eventi successivi sono influenzati solo dallo stato immediatamente precedente.</p>
<section id="catene-di-markov-e-metodi-mcmc" class="level3" data-number="50.4.1">
<h3 data-number="50.4.1" class="anchored" data-anchor-id="catene-di-markov-e-metodi-mcmc"><span class="header-section-number">50.4.1</span> Catene di Markov e Metodi MCMC</h3>
<p>Le catene di Markov sono fondamentali nei metodi Monte Carlo a Catena di Markov (MCMC) perché forniscono un modo efficiente per generare sequenze di campioni che approssimano distribuzioni di probabilità complesse. Mentre i metodi Monte Carlo classici generano campioni indipendenti, i metodi MCMC costruiscono una sequenza di campioni dipendenti attraverso una catena di Markov, in cui ciascun campione è ottenuto in base al campione precedente. Questo approccio consente di concentrarsi sulle regioni di alta probabilità della distribuzione, migliorando l’efficienza del campionamento.</p>
<p>Per esempio, consideriamo una distribuzione di probabilità <span class="math inline">\(P(x_1, x_2, ..., x_n)\)</span> definita su un insieme di variabili <span class="math inline">\(x_1, x_2, ..., x_n\)</span>. Nei metodi MCMC, si genera una sequenza di configurazioni <span class="math inline">\(\{x(0)\}, \{x(1)\}, \{x(2)\}, \dots\)</span>, tale che la frequenza con cui ogni configurazione <span class="math inline">\(\{x\}\)</span> viene visitata è proporzionale alla sua probabilità <span class="math inline">\(P(x)\)</span>. In questo modo, le configurazioni più probabili vengono visitate più spesso, garantendo che l’algoritmo converga alla distribuzione di interesse.</p>
</section>
<section id="condizioni-fondamentali-per-le-catene-di-markov" class="level3" data-number="50.4.2">
<h3 data-number="50.4.2" class="anchored" data-anchor-id="condizioni-fondamentali-per-le-catene-di-markov"><span class="header-section-number">50.4.2</span> Condizioni fondamentali per le Catene di Markov</h3>
<p>Affinché un algoritmo MCMC funzioni correttamente e converga alla distribuzione desiderata, la catena di Markov deve soddisfare alcune condizioni fondamentali:</p>
<ul>
<li><strong>Proprietà di Markov:</strong> La prossima configurazione dipende solo dalla configurazione attuale, non dalla storia passata. Questo garantisce che l’evoluzione della catena sia “locale” e non influenzata dagli stati remoti.</li>
<li><strong>Irriducibilità:</strong> Ogni configurazione della catena può essere raggiunta da qualsiasi altra in un numero finito di passi. Ciò assicura che l’intero spazio dei parametri possa essere esplorato.</li>
<li><strong>Aperiodicità:</strong> La catena non segue cicli fissi e non ritorna sistematicamente allo stesso stato dopo un certo numero di passi. Questo garantisce che la catena possa esplorare lo spazio dei parametri in modo casuale.</li>
<li><strong>Condizione di bilanciamento dettagliato:</strong> La probabilità di passare da uno stato a un altro deve essere bilanciata dalla probabilità di tornare allo stato iniziale, assicurando così che la distribuzione di equilibrio della catena sia proprio la distribuzione a posteriori desiderata.</li>
</ul>
</section>
<section id="algoritmi-mcmc" class="level3" data-number="50.4.3">
<h3 data-number="50.4.3" class="anchored" data-anchor-id="algoritmi-mcmc"><span class="header-section-number">50.4.3</span> Algoritmi MCMC</h3>
<p>Esistono diversi algoritmi basati su MCMC, ognuno con caratteristiche specifiche:</p>
<ul>
<li><p><strong>Metropolis-Hastings:</strong> Questo è uno degli algoritmi più noti. Si basa sulla generazione di una configurazione proposta che viene accettata o rifiutata in base a un criterio di probabilità. Se la configurazione proposta ha una probabilità più alta, viene accettata; se ha una probabilità più bassa, può essere accettata con una certa probabilità, che dipende dal rapporto tra le probabilità delle due configurazioni.</p></li>
<li><p><strong>Gibbs Sampling:</strong> In questo algoritmo, le variabili vengono aggiornate una alla volta, campionando ogni variabile dalla sua distribuzione condizionale data la configurazione corrente delle altre variabili. È particolarmente utile quando le distribuzioni condizionali sono note o facili da campionare.</p></li>
<li><p><strong>Hamiltonian Monte Carlo (HMC):</strong> Utilizza principi della meccanica hamiltoniana per esplorare lo spazio dei parametri in modo più efficiente, considerando non solo le probabilità, ma anche le “forze” che muovono i campioni attraverso lo spazio dei parametri. Questo approccio è particolarmente vantaggioso per modelli complessi e ad alta dimensionalità, poiché consente di generare campioni lontani dallo stato corrente senza ricorrere a piccoli passi.</p></li>
</ul>
</section>
<section id="conclusione" class="level3" data-number="50.4.4">
<h3 data-number="50.4.4" class="anchored" data-anchor-id="conclusione"><span class="header-section-number">50.4.4</span> Conclusione</h3>
<p>Le catene di Markov forniscono il fondamento teorico e pratico per i metodi MCMC, offrendo un modo efficiente per esplorare lo spazio dei parametri nei modelli complessi. Grazie alle proprietà specifiche delle catene di Markov, i metodi MCMC permettono di affrontare problemi di inferenza bayesiana che sarebbero intrattabili con approcci analitici o con i metodi Monte Carlo classici. Essendo flessibili e potenti, le catene di Markov continueranno a essere uno strumento fondamentale nella statistica e nella scienza dei dati.</p>
</section>
</section>
<section id="estrazione-di-campioni-dalla-distribuzione-a-posteriori" class="level2" data-number="50.5">
<h2 data-number="50.5" class="anchored" data-anchor-id="estrazione-di-campioni-dalla-distribuzione-a-posteriori"><span class="header-section-number">50.5</span> Estrazione di campioni dalla distribuzione a posteriori</h2>
<p>In questo capitolo presenteremo l’algoritmo di Metropolis, che è uno dei più semplici e potenti metodi MCMC. Sfruttando la struttura delle catene di Markov, esplora in modo efficiente lo spazio dei parametri, permettendo di ottenere campioni dalla distribuzione a posteriori anche in casi complessi, dove i metodi analitici falliscono. In sostanza, il MCMC genera un gran numero di valori per il parametro <span class="math inline">\(\theta\)</span> che, nel loro insieme, approssimano la distribuzione di interesse <span class="math inline">\(p(\theta \mid y)\)</span>. A questo fine, il capitolo è strutturato in varie sezioni che facilitano la comprensione progressiva del tema.</p>
<ul>
<li>Inizieremo discutendo di come la distribuzione a posteriori possa essere approssimata mediante tecniche di simulazione convenzionali. Questa prima parte presuppone che la distribuzione target, o “a posteriori,” sia già conosciuta o disponibile per l’analisi.</li>
<li>In seguito, passeremo a illustrare come l’algoritmo di Metropolis possa essere utilizzato per affrontare situazioni in cui la distribuzione a posteriori non è direttamente nota. In questi casi, spesso abbiamo a disposizione informazioni riguardanti la distribuzione a priori e la funzione di verosimiglianza, che possono essere utilizzate per ottenere un’approssimazione efficace della distribuzione a posteriori.</li>
</ul>
<p>A titolo esemplificativo, utilizzeremo il dataset <code>moma_sample.csv</code>, il quale costituisce un campione casuale di 100 artisti provenienti dal Museo di Arte Moderna di New York (MoMA) e contiene diverse informazioni relative a ciascun artista.</p>
<p>Il nostro interesse è focalizzato sulla determinazione della probabilità che un artista presente nel MoMA appartenga alla generazione X o a una generazione successiva (nati dopo il 1965). Questa probabilità sarà indicata come <span class="math inline">\(\pi\)</span>. Iniziamo importando i dati.</p>
<div id="cell-6" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>file_path <span class="op">=</span> os.path.join(project_directory, <span class="st">"data"</span>, <span class="st">"moma_sample.csv"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>moma_sample <span class="op">=</span> pd.read_csv(file_path)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Esaminiamo le prime cinque righe del DataFrame.</p>
<div id="cell-8" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:09:49.939453Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:09:49.924447Z&quot;}" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>moma_sample.head()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">artist</th>
<th data-quarto-table-cell-role="th">country</th>
<th data-quarto-table-cell-role="th">birth</th>
<th data-quarto-table-cell-role="th">death</th>
<th data-quarto-table-cell-role="th">alive</th>
<th data-quarto-table-cell-role="th">genx</th>
<th data-quarto-table-cell-role="th">gender</th>
<th data-quarto-table-cell-role="th">count</th>
<th data-quarto-table-cell-role="th">year_acquired_min</th>
<th data-quarto-table-cell-role="th">year_acquired_max</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>Ad Gerritsen</td>
<td>dutch</td>
<td>1940</td>
<td>2015.0</td>
<td>False</td>
<td>False</td>
<td>male</td>
<td>1</td>
<td>1981</td>
<td>1981</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Kirstine Roepstorff</td>
<td>danish</td>
<td>1972</td>
<td>NaN</td>
<td>True</td>
<td>True</td>
<td>female</td>
<td>3</td>
<td>2005</td>
<td>2005</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Lisa Baumgardner</td>
<td>american</td>
<td>1958</td>
<td>2015.0</td>
<td>False</td>
<td>False</td>
<td>female</td>
<td>2</td>
<td>2016</td>
<td>2016</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>David Bates</td>
<td>american</td>
<td>1952</td>
<td>NaN</td>
<td>True</td>
<td>False</td>
<td>male</td>
<td>1</td>
<td>2001</td>
<td>2001</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>Simon Levy</td>
<td>american</td>
<td>1946</td>
<td>NaN</td>
<td>True</td>
<td>False</td>
<td>male</td>
<td>1</td>
<td>2012</td>
<td>2012</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Dai dati osserviamo che solo 14 artisti su 100 appartengono alla generazione X o a una generazione successiva.</p>
<div id="cell-10" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:09:53.119523Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:09:53.086277Z&quot;}" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> moma_sample[<span class="st">"genx"</span>].value_counts()</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>genx
False    86
True     14
Name: count, dtype: int64</code></pre>
</div>
</div>
<p>Il valore campionato <span class="math inline">\(y = 14\)</span> riflette le caratteristiche del campione che è stato osservato. Tuttavia, poiché il MOMA contiene opere di migliaia di artisti, sorge una domanda riguardante il vero valore di <span class="math inline">\(\theta\)</span> (la probabilità di appartenere alla generazione X o a una generazione successiva) all’interno di questa popolazione.</p>
<p>Possiamo interpretare i dati <span class="math inline">\(y = 14\)</span> come l’esito di una variabile casuale Binomiale con parametri <span class="math inline">\(N = 100\)</span> e <span class="math inline">\(\theta\)</span> sconosciuto.</p>
<p>Supponiamo che le nostre credenze pregresse riguardo a <span class="math inline">\(\theta\)</span> possano essere modellate attraverso una distribuzione Beta(4, 6).</p>
<p>Sfruttando le proprietà delle distribuzioni coniugate, possiamo calcolare la distribuzione a posteriori esatta:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>Y <span class="op">~</span> Binomiale(<span class="dv">100</span>, π)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>θ <span class="op">=</span> Beta(<span class="dv">4</span>, <span class="dv">6</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>θ <span class="op">|</span> (Y <span class="op">=</span> <span class="dv">14</span>) <span class="op">~</span> Beta(<span class="dv">4</span> <span class="op">+</span> <span class="dv">14</span>, <span class="dv">6</span> <span class="op">+</span> <span class="dv">100</span> <span class="op">-</span> <span class="dv">14</span>) → Beta(<span class="dv">18</span>, <span class="dv">92</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Nella figura successiva è rappresentata la distribuzione a posteriori del parametro <span class="math inline">\(\theta\)</span>, insieme alla distribuzione a priori scelta.</p>
<div id="cell-12" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:10:00.582390Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:10:00.377742Z&quot;}" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>prior_density <span class="op">=</span> stats.beta.pdf(x, <span class="dv">4</span>, <span class="dv">6</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>posterior_density <span class="op">=</span> stats.beta.pdf(x, <span class="dv">18</span>, <span class="dv">92</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x, prior_density, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">"Prior: Beta(4, 6)"</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x, posterior_density, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">"Posterior: Beta(18, 92)"</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Parameter Value"</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Density"</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Prior and Posterior Densities"</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01_metropolis_files/figure-html/cell-6-output-1.png" width="731" height="491" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Se vogliamo conoscere il valore della media a posteriori di <span class="math inline">\(\theta\)</span>, il risultato esatto è</p>
<p><span class="math display">\[
\bar{\theta}_{post} = \frac{\alpha}{\alpha + \beta} = \frac{18}{18 + 92} \approx 0.1636.
\]</span></p>
<section id="simulazione-con-distribuzione-target-nota" class="level3" data-number="50.5.1">
<h3 data-number="50.5.1" class="anchored" data-anchor-id="simulazione-con-distribuzione-target-nota"><span class="header-section-number">50.5.1</span> Simulazione con distribuzione target nota</h3>
<p>Usiamo ora una simulazione numerica per stimare la media a posteriori di <span class="math inline">\(\theta\)</span>. Conoscendo la forma della distribuzione a posteriori <span class="math inline">\(Beta(18, 92)\)</span>, possiamo generare un campione di osservazioni casuali da questa distribuzione. Successivamente, calcoliamo la media delle osservazioni ottenute per ottenere un’approssimazione della media a posteriori.</p>
<p>Se vogliamo ottenere un risultato approssimato con poche osservazioni (ad esempio, 10), possiamo procedere con la seguente simulazione:</p>
<div id="cell-15" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:10:13.375419Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:10:13.359907Z&quot;}" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> stats.beta(<span class="dv">18</span>, <span class="dv">92</span>).rvs(<span class="dv">10</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.12971627 0.15527801 0.14714901 0.20647646 0.1897427  0.16158686
 0.17558128 0.20738074 0.23121331 0.1330954 ]</code></pre>
</div>
</div>
<div id="cell-16" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:10:16.372388Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:10:16.367267Z&quot;}" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>np.mean(y)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>0.17372200509655458</code></pre>
</div>
</div>
<p>Tuttavia, con solo 10 campioni l’approssimazione potrebbe non essere molto accurata. Più aumentiamo il numero di campioni (cioè il numero di osservazioni casuali generate), più precisa sarà l’approssimazione. Aumentando il numero di campioni, ad esempio a 10000, otteniamo un risultato più preciso:</p>
<div id="cell-18" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:10:18.652167Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:10:18.635620Z&quot;}" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>stats.beta(<span class="dv">18</span>, <span class="dv">92</span>).rvs(<span class="dv">10000</span>).mean()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>0.1635283857789979</code></pre>
</div>
</div>
<p>Quando il numero di campioni a posteriori diventa molto grande, la distribuzione campionaria <em>converge</em> alla densità della popolazione. Questo concetto si applica non solo alla media, ma anche ad altre statistiche descrittive, come la moda e la varianza.</p>
<p>È importante sottolineare che l’applicazione della simulazione di Monte Carlo è efficace per calcolare distribuzioni a posteriori solo quando conosciamo la distribuzione stessa e possiamo utilizzare funzioni Python per estrarre campioni casuali da tale distribuzione. Ciò è stato possibile nel caso della distribuzione a posteriori <span class="math inline">\(Beta(18, 92)\)</span>.</p>
<p>Tuttavia, questa situazione ideale non si verifica sempre nella pratica, poiché le distribuzioni a priori coniugate alla verosimiglianza sono spesso rare. Per esempio, nel caso di una verosimiglianza binomiale e una distribuzione a priori gaussiana, l’espressione</p>
<p><span class="math display">\[
p(\theta \mid y) = \frac{\mathrm{e}^{-(\theta - 1 / 2)^2} \theta^y (1 - \theta)^{n - y}} {\int_0^1 \mathrm{e}^{-(t - 1 / 2)^2} t^y (1 - t)^{n - y} dt}
\]</span></p>
<p>rende impossibile calcolare analiticamente la distribuzione a posteriori di <span class="math inline">\(\theta\)</span>, precludendo quindi l’utilizzo diretto di Python per generare campioni casuali.</p>
<p>In queste circostanze, però, è possibile ottenere campioni casuali dalla distribuzione a posteriori mediante l’uso di metodi Monte Carlo basati su Catena di Markov (MCMC). Gli algoritmi MCMC, come ad esempio l’algoritmo Metropolis, costituiscono una classe di metodi che consentono di estrarre campioni casuali dalla distribuzione a posteriori <em>senza richiedere la conoscenza della sua rappresentazione analitica</em>. Le tecniche MCMC sono ampiamente adottate per risolvere problemi di inferenza bayesiana e rappresentano il principale strumento computazionale per ottenere stime approssimate di distribuzioni a posteriori in situazioni complesse e non analiticamente trattabili.</p>
</section>
<section id="algoritmo-di-metropolis" class="level3" data-number="50.5.2">
<h3 data-number="50.5.2" class="anchored" data-anchor-id="algoritmo-di-metropolis"><span class="header-section-number">50.5.2</span> Algoritmo di Metropolis</h3>
<p>L’algoritmo di Metropolis appartiene alla famiglia dei metodi Monte Carlo basati su catene di Markov (MCMC), sfruttando le proprietà di queste catene per generare campioni da una distribuzione target. Il suo obiettivo principale è di esplorare lo spazio dei parametri in modo efficiente, producendo campioni che approssimano la distribuzione a posteriori di interesse.</p>
</section>
<section id="principio-di-funzionamento" class="level3" data-number="50.5.3">
<h3 data-number="50.5.3" class="anchored" data-anchor-id="principio-di-funzionamento"><span class="header-section-number">50.5.3</span> Principio di Funzionamento</h3>
<p>L’algoritmo inizia da un valore iniziale per i parametri e, in ogni iterazione, genera un nuovo campione tramite una distribuzione di proposta (solitamente una distribuzione normale centrata sul valore corrente). Successivamente, decide se accettare il nuovo campione in base al confronto tra le densità posteriori del nuovo campione e di quello precedente. Questa accettazione avviene in modo probabilistico, favorendo campioni con una densità più alta ma consentendo anche l’accettazione di campioni peggiori per evitare che la catena rimanga bloccata in minimi locali.</p>
</section>
<section id="burn-in-e-convergenza" class="level3" data-number="50.5.4">
<h3 data-number="50.5.4" class="anchored" data-anchor-id="burn-in-e-convergenza"><span class="header-section-number">50.5.4</span> Burn-in e Convergenza</h3>
<p>Poiché i primi campioni potrebbero non rappresentare bene la distribuzione target, si esclude spesso una porzione iniziale della catena (fase di burn-in). Con il progredire delle iterazioni, i campioni si distribuiscono in accordo con la distribuzione stazionaria desiderata, indipendentemente dallo stato iniziale scelto. Questo processo permette di esplorare lo spazio dei parametri in modo efficiente.</p>
</section>
<section id="meccanismo-di-accettazione-e-rifiuto" class="level3" data-number="50.5.5">
<h3 data-number="50.5.5" class="anchored" data-anchor-id="meccanismo-di-accettazione-e-rifiuto"><span class="header-section-number">50.5.5</span> Meccanismo di Accettazione e Rifiuto</h3>
<p>L’algoritmo di Metropolis bilancia due esigenze opposte:</p>
<ol type="1">
<li><strong>Esplorazione</strong> di nuove aree dello spazio dei parametri.</li>
<li><strong>Sfruttamento</strong> delle informazioni già acquisite dai campioni precedenti.</li>
</ol>
<p>Utilizzando una regola probabilistica per accettare campioni peggiori (con minore densità a posteriori), l’algoritmo evita di restare intrappolato in minimi locali, esplorando così in modo più completo l’intera distribuzione.</p>
</section>
<section id="passaggi-fondamentali-dellalgoritmo-di-metropolis" class="level3" data-number="50.5.6">
<h3 data-number="50.5.6" class="anchored" data-anchor-id="passaggi-fondamentali-dellalgoritmo-di-metropolis"><span class="header-section-number">50.5.6</span> Passaggi Fondamentali dell’Algoritmo di Metropolis</h3>
<ol type="1">
<li><strong>Scelta di uno stato iniziale <span class="math inline">\(\theta_1\)</span> e impostazione del contatore <span class="math inline">\(t = 1\)</span>.</strong>
<ul>
<li>Questo è il punto di partenza della catena, dove <span class="math inline">\(\theta_1\)</span> rappresenta il primo campione.</li>
</ul></li>
<li><strong>Proposta di un nuovo campione <span class="math inline">\(\theta_p\)</span>.</strong>
<ul>
<li>Un nuovo valore <span class="math inline">\(\theta_p\)</span> viene generato da una distribuzione di proposta <span class="math inline">\(g(\theta_p \mid \theta_t)\)</span>, solitamente una distribuzione normale centrata sul campione corrente <span class="math inline">\(\theta_t\)</span> con una deviazione standard <span class="math inline">\(\tau\)</span> che controlla l’ampiezza dei passi.</li>
</ul></li>
<li><strong>Verifica dei vincoli del campione proposto.</strong>
<ul>
<li>Se il nuovo campione deve rispettare dei vincoli (ad esempio, essere compreso tra 0 e 1 per probabilità), campioni non validi vengono automaticamente rifiutati.</li>
</ul></li>
<li><strong>Calcolo del rapporto di accettazione <span class="math inline">\(\alpha\)</span>.</strong>
<ul>
<li>Si calcola <span class="math inline">\(\alpha = \frac{p(\theta_p \mid y)}{p(\theta_t \mid y)}\)</span>, che rappresenta il rapporto tra le densità a posteriori del nuovo campione <span class="math inline">\(\theta_p\)</span> e del campione corrente <span class="math inline">\(\theta_t\)</span>. Questo valore guida la decisione di accettazione.</li>
</ul></li>
<li><strong>Decisione di accettazione.</strong>
<ul>
<li>Se <span class="math inline">\(\alpha \geq 1\)</span>, il nuovo campione <span class="math inline">\(\theta_p\)</span> viene accettato incondizionatamente.</li>
<li>Se <span class="math inline">\(\alpha &lt; 1\)</span>, il campione <span class="math inline">\(\theta_p\)</span> viene accettato con probabilità <span class="math inline">\(\alpha\)</span>. In caso di rifiuto, si mantiene il campione corrente <span class="math inline">\(\theta_t\)</span> per la prossima iterazione.</li>
</ul></li>
<li><strong>Ripetizione del processo.</strong>
<ul>
<li>Si ripetono i passaggi dal 2 al 5 fino a ottenere il numero desiderato di campioni.</li>
</ul></li>
</ol>
</section>
<section id="dettagli-aggiuntivi" class="level3" data-number="50.5.7">
<h3 data-number="50.5.7" class="anchored" data-anchor-id="dettagli-aggiuntivi"><span class="header-section-number">50.5.7</span> Dettagli Aggiuntivi</h3>
<ul>
<li><p><strong>Distribuzione di proposta</strong>: La distribuzione di proposta <span class="math inline">\(g(\theta_p \mid \theta_t)\)</span> genera nuovi campioni attorno a <span class="math inline">\(\theta_t\)</span>. Tipicamente si usa una normale <span class="math inline">\(N(\theta_t, \tau)\)</span>, dove <span class="math inline">\(\tau\)</span> controlla quanto il nuovo campione si discosta da quello corrente. Scegliere un <span class="math inline">\(\tau\)</span> troppo piccolo può rendere l’esplorazione lenta, mentre un <span class="math inline">\(\tau\)</span> troppo grande può far rifiutare troppi campioni, riducendo l’efficienza.</p></li>
<li><p><strong>Rapporto di accettazione <span class="math inline">\(\alpha\)</span></strong>: Se il nuovo campione ha una densità a posteriori maggiore del campione corrente, viene sempre accettato. Se ha una densità inferiore, viene accettato con probabilità <span class="math inline">\(\alpha\)</span>, il che consente di esplorare anche regioni meno probabili della distribuzione.</p></li>
<li><p><strong>Accettazione probabilistica</strong>: Accettare campioni peggiori occasionalmente aiuta l’algoritmo a evitare di bloccarsi in minimi locali. Questo è uno dei punti di forza dell’algoritmo di Metropolis, che garantisce una buona esplorazione dello spazio dei parametri.</p></li>
</ul>
</section>
</section>
<section id="esempio-di-implementazione" class="level2" data-number="50.6">
<h2 data-number="50.6" class="anchored" data-anchor-id="esempio-di-implementazione"><span class="header-section-number">50.6</span> Esempio di Implementazione</h2>
<p>Per la presente simulazione, ci serviremo dell’implementazione in Python proposta da <a href="https://elizaveta-semenova.com/">Elizaveta Semenova</a>, che offre una presentazione particolarmente concisa ed efficace della logica sottostante l’algoritmo. Iniziamo a definire alcune funzioni.</p>
<p>Definiamo una funzione <code>prior</code> che accetta come argomento il valore di <span class="math inline">\(\theta\)</span> e restituisce la densità della distribuzione Beta(4, 6).</p>
<div id="cell-23" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define prior distribution (Beta(4,6))</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prior(p):</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> stats.beta.pdf(p, alpha, beta)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Definiamo la funzione <code>likelihood</code>, che accetta come argomento il valore di <span class="math inline">\(\theta\)</span> e restituisce la densità della funzione di verosimiglianza binomiale per il caso di 14 successi su 100 prove.</p>
<div id="cell-25" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define likelihood function (Binomial with y = 14 successes out of n = 100 trials)</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> likelihood(p):</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> <span class="dv">14</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> stats.binom.pmf(y, n, p)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Infine, definiamo la funzione <code>posterior</code>, che accetta come argomento il valore di <span class="math inline">\(\theta\)</span> e restituisce la densità della distribuzione a posteriori non normalizzata, calcolata come il prodotto della distribuzione a priori e della verosimiglianza.</p>
<div id="cell-27" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define posterior as the product of prior and likelihood</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> posterior(p):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> likelihood(p) <span class="op">*</span> prior(p)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Definiamo la distribuzione proposta come una distribuzione gaussiana centrata sullo stato corrente con deviazione standard definita a priori.</p>
<div id="cell-29" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Proposal distribution (normal distribution around current state)</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> proposal_distribution(current_state, proposal_sigma):</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.random.normal(current_state, proposal_sigma)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Definiamo la distribuzione target che corrispone dalla distribuzione a posteriori.</p>
<div id="cell-31" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Target distribution is the posterior we are sampling from</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> target_distribution(p):</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> posterior(p)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Procediamo ora con un’implementazione dell’algoritmo di Metropolis, considerando i dati relativi agli artisti della Generazione X presenti al MoMA. In questa simulazione, come indicato in precedenza, adottiamo una distribuzione a priori per <span class="math inline">\(\theta\)</span> (la probabilità di appartenere alla Generazione X) modellata secondo una (arbitraria) distribuzione Beta(4, 6).</p>
<div id="cell-33" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Metropolis-Hastings algorithm</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> metropolis_hastings(num_samples, initial_state, proposal_sigma):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Start with the initial state and prepare to store samples</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> [initial_state]</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    current_state <span class="op">=</span> initial_state</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_samples):</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Propose a new state from a normal distribution around the current state</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        proposed_state <span class="op">=</span> proposal_distribution(current_state, proposal_sigma)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Ensure the proposed state is within valid bounds (between 0 and 1 for probabilities)</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="dv">0</span> <span class="op">&lt;=</span> proposed_state <span class="op">&lt;=</span> <span class="dv">1</span>:</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calculate acceptance ratio</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>            acceptance_ratio <span class="op">=</span> <span class="bu">min</span>(</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>                <span class="dv">1</span>,</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>                target_distribution(proposed_state)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>                <span class="op">/</span> target_distribution(current_state),</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Accept or reject the proposed state</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">1</span>) <span class="op">&lt;</span> acceptance_ratio:</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>                current_state <span class="op">=</span> proposed_state</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Append the current state (accepted or rejected) to the samples</span></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>        samples.append(current_state)</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> samples</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Nell’implementazione dell’algoritmo di Metropolis proposta ci sono diversi punti cruciali da considerare, che contribuiscono al funzionamento e alla correttezza dell’algoritmo.</p>
<section id="simmetria-della-distribuzione-proposta" class="level3" data-number="50.6.1">
<h3 data-number="50.6.1" class="anchored" data-anchor-id="simmetria-della-distribuzione-proposta"><span class="header-section-number">50.6.1</span> Simmetria della Distribuzione Proposta</h3>
<p>L’algoritmo di Metropolis richiede che la distribuzione di proposta sia simmetrica. Questo significa che la probabilità di proporre uno stato <span class="math inline">\(\theta_p\)</span> partendo da <span class="math inline">\(\theta_t\)</span> deve essere uguale alla probabilità di proporre <span class="math inline">\(\theta_t\)</span> partendo da <span class="math inline">\(\theta_p\)</span>. Nel tuo caso, la distribuzione normale utilizzata (<span class="math inline">\(N(\theta_t, \sigma)\)</span>) soddisfa questa condizione, poiché è simmetrica attorno al valore corrente <span class="math inline">\(\theta_t\)</span>. È importante notare che questa condizione è necessaria per l’algoritmo di Metropolis, ma non per l’algoritmo di Metropolis-Hastings, che permette distribuzioni proposte asimmetriche.</p>
</section>
<section id="scelta-del-valore-iniziale" class="level3" data-number="50.6.2">
<h3 data-number="50.6.2" class="anchored" data-anchor-id="scelta-del-valore-iniziale"><span class="header-section-number">50.6.2</span> Scelta del Valore Iniziale</h3>
<p>Il valore iniziale <code>initial_state</code> dovrebbe essere scelto con attenzione. Idealmente, questo valore dovrebbe trovarsi in una regione con alta densità della distribuzione a priori, in modo da facilitare la convergenza dell’algoritmo verso la distribuzione target. In alcuni casi, una scelta di partenza lontana dalla distribuzione a posteriori potrebbe richiedere un periodo di burn-in più lungo, poiché l’algoritmo impiega più tempo a esplorare lo spazio dei parametri e a stabilizzarsi.</p>
</section>
<section id="gestione-della-probabilità-zero" class="level3" data-number="50.6.3">
<h3 data-number="50.6.3" class="anchored" data-anchor-id="gestione-della-probabilità-zero"><span class="header-section-number">50.6.3</span> Gestione della Probabilità Zero</h3>
<p>Un altro aspetto fondamentale è garantire che il nuovo campione proposto, <code>proposed_state</code>, rientri nei limiti del supporto della verosimiglianza e del prior. Nel tuo esempio, <code>proposed_state</code> è limitato all’intervallo [0, 1], come appropriato per i problemi di probabilità. Se il campione proposto cadesse al di fuori di questo intervallo, la densità a posteriori risulterebbe zero, causando un’accettazione impossibile o un rapporto indefinito.</p>
</section>
</section>
<section id="meccanismo-di-accettazione" class="level2" data-number="50.7">
<h2 data-number="50.7" class="anchored" data-anchor-id="meccanismo-di-accettazione"><span class="header-section-number">50.7</span> Meccanismo di Accettazione</h2>
<p>Nel processo successivo alla generazione di un nuovo stato proposto, l’algoritmo deve decidere se accettare o meno questo stato in base alla sua <strong>densità a posteriori</strong>, calcolata come il prodotto tra la <strong>verosimiglianza</strong> e il <strong>prior</strong>. Questo confronto avviene nel seguente modo:</p>
<section id="generazione-del-nuovo-stato" class="level3" data-number="50.7.1">
<h3 data-number="50.7.1" class="anchored" data-anchor-id="generazione-del-nuovo-stato"><span class="header-section-number">50.7.1</span> Generazione del Nuovo Stato</h3>
<p>Il nuovo stato <code>proposed_state</code> viene generato da una distribuzione di proposta centrata sullo stato corrente <code>current_state</code>. La deviazione standard della distribuzione, definita dal parametro <code>proposal_sigma</code>, determina quanto il nuovo stato può distanziarsi dallo stato attuale. In pratica, un valore di <code>proposal_sigma</code> troppo piccolo potrebbe rallentare l’esplorazione dello spazio dei parametri, mentre un valore troppo grande potrebbe portare a un tasso di rifiuto elevato.</p>
</section>
<section id="confronto-tra-stati" class="level3" data-number="50.7.2">
<h3 data-number="50.7.2" class="anchored" data-anchor-id="confronto-tra-stati"><span class="header-section-number">50.7.2</span> Confronto tra Stati</h3>
<p>Il cuore dell’algoritmo è il confronto tra la densità a posteriori del nuovo stato (<code>proposed_state</code>) e quella dello stato corrente (<code>current_state</code>). Se il nuovo stato ha una densità a posteriori <strong>maggiore o uguale</strong> rispetto al vecchio stato, viene <strong>sempre accettato</strong>. Altrimenti, viene accettato con una probabilità data dal <strong>rapporto di accettazione</strong>.</p>
</section>
<section id="calcolo-del-rapporto-di-accettazione" class="level3" data-number="50.7.3">
<h3 data-number="50.7.3" class="anchored" data-anchor-id="calcolo-del-rapporto-di-accettazione"><span class="header-section-number">50.7.3</span> Calcolo del Rapporto di Accettazione</h3>
<p>Il rapporto di accettazione <span class="math inline">\(\alpha\)</span> viene calcolato come:</p>
<p><span class="math display">\[
\alpha = \min\left(1, \frac{p(\text{proposed state})}{p(\text{current state})}\right),
\]</span></p>
<p>dove <span class="math inline">\(p(\text{proposed state})\)</span> è la densità a posteriori del nuovo stato e <span class="math inline">\(p(\text{current state})\)</span> è quella dello stato attuale. Se il nuovo stato ha una probabilità maggiore, sarà accettato. Se invece la sua probabilità è minore, verrà accettato con una probabilità pari a <span class="math inline">\(\alpha\)</span>. In questo modo, lo schema probabilistico permette all’algoritmo di esplorare anche aree con minore densità, evitando di restare bloccato in minimi locali.</p>
</section>
</section>
<section id="tasso-di-accettazione-e-efficienza" class="level2" data-number="50.8">
<h2 data-number="50.8" class="anchored" data-anchor-id="tasso-di-accettazione-e-efficienza"><span class="header-section-number">50.8</span> Tasso di Accettazione e Efficienza</h2>
<p>Uno degli aspetti cruciali nell’uso dell’algoritmo è il <strong>tasso di accettazione</strong>, ovvero il rapporto tra il numero di proposte accettate e il numero totale di proposte. Un tasso di accettazione troppo basso indica che l’algoritmo sta rifiutando troppi campioni e quindi l’esplorazione dello spazio dei parametri è inefficiente. Viceversa, un tasso di accettazione troppo alto potrebbe indicare che la distribuzione proposta non si sta spostando abbastanza tra le iterazioni, con il rischio di una convergenza lenta.</p>
<p>In generale, un <strong>tasso di accettazione compreso tra il 20% e il 40%</strong> è spesso considerato ottimale per garantire un buon bilanciamento tra esplorazione e efficienza.</p>
</section>
<section id="esecuzione-del-campionamento" class="level2" data-number="50.9">
<h2 data-number="50.9" class="anchored" data-anchor-id="esecuzione-del-campionamento"><span class="header-section-number">50.9</span> Esecuzione del Campionamento</h2>
<p>Procediamo con l’esecuzione del campionamento, implementando l’algoritmo di Metropolis come precedentemente illustrato</p>
<div id="cell-36" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:12:39.577063Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:12:24.866146Z&quot;}" data-execution_count="15">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>initial_state <span class="op">=</span> <span class="fl">0.5</span>  <span class="co"># Starting at the midpoint of the probability range</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>proposal_sigma <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># Standard deviation for the proposal distribution</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>num_samples <span class="op">=</span> <span class="dv">10000</span>  <span class="co"># Number of samples to generate</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> metropolis_hastings(num_samples, initial_state, proposal_sigma)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La funzione <code>metropolis_hastings()</code> accetta come input il numero <code>num_samples</code> di passi da simulare, il punto di partenza e la deviazione standard della distribuzione proposta gaussiana. Come output, restituisce una catena di valori del parametro, specificamente la sequenza <span class="math inline">\(\theta^{(1)}, \theta^{(2)}, \ldots, \theta^{\text{nsamp}}\)</span>.</p>
</section>
<section id="aspetti-computazionali" class="level2" data-number="50.10">
<h2 data-number="50.10" class="anchored" data-anchor-id="aspetti-computazionali"><span class="header-section-number">50.10</span> Aspetti computazionali</h2>
<section id="warm-upburn-in" class="level3" data-number="50.10.1">
<h3 data-number="50.10.1" class="anchored" data-anchor-id="warm-upburn-in"><span class="header-section-number">50.10.1</span> Warm-up/Burn-in</h3>
<p>Uno degli aspetti cruciali per la riuscita dell’algoritmo è il raggiungimento della stazionarietà da parte della catena. In genere, i primi 1000-5000 valori vengono scartati in quanto rappresentano il periodo di “burn-in” della catena. Dopo un determinato numero di passi <span class="math inline">\(k\)</span>, la catena converge e i valori diventano campioni effettivi dalla distribuzione a posteriori <span class="math inline">\(p(\theta \mid y)\)</span>.</p>
</section>
<section id="sec-moma-post-estimates" class="level3" data-number="50.10.2">
<h3 data-number="50.10.2" class="anchored" data-anchor-id="sec-moma-post-estimates"><span class="header-section-number">50.10.2</span> Sintesi della distribuzione a posteriori</h3>
<p>L’array <code>samples</code> contiene 10000 valori di una catena di Markov. Escludiamo i primi 5000 valori considerati come burn-in e consideriamo i restanti 5000 valori come un campione casuale estratto dalla distribuzione a posteriori <span class="math inline">\(p(\theta \mid y)\)</span>.</p>
<p>Mediante i valori della catena così ottenuta è facile trovare una stima a posteriori del parametro <span class="math inline">\(\theta\)</span>. Per esempio, possiamo trovare la stima della media a posteriori.</p>
<div id="cell-39" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>burnin <span class="op">=</span> <span class="bu">int</span>(num_samples <span class="op">*</span> <span class="fl">0.5</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>burnin</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>5000</code></pre>
</div>
</div>
<div id="cell-40" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>np.mean(samples[burnin:])</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>0.16389183286201028</code></pre>
</div>
</div>
<p>Oppure possiamo stimare la deviazione standard della distribuzione a posteriori.</p>
<div id="cell-42" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:12:47.966732Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:12:47.958107Z&quot;}" data-execution_count="12">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>np.std(samples[burnin:])</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>0.0353754937892814</code></pre>
</div>
</div>
<p>Il trace plot indica che la catena inizia con un valore casuale per poi spostarsi rapidamente nell’area intorno a 0.16, che è l’area con alta densità a posteriori. Successivamente, oscilla intorno a quel valore per le iterazioni successive. Visualizziamo un <em>trace plot</em> dei primi 200 valori della catena di Markov.</p>
<div id="cell-44" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>plt.plot(samples[<span class="dv">0</span>:<span class="dv">200</span>])</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"sample"</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"theta"</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01_metropolis_files/figure-html/cell-20-output-1.png" width="731" height="491" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Visualizziamo un <em>trace plot</em> dei valori della catena di Markov dopo il periodo di burn-in.</p>
<div id="cell-46" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:12:50.867938Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:12:50.519308Z&quot;}" data-execution_count="39">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>plt.plot(samples[burnin:])</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"sample"</span>)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"theta"</span>)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01_metropolis_files/figure-html/cell-21-output-1.png" width="731" height="491" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>L’istogramma mostrato di seguito, sul quale è stata sovrapposta la distribuzione a posteriori derivata analiticamente – specificamente una <span class="math inline">\(\text{Beta}(25, 17)\)</span> – dimostra che la catena converge effettivamente alla distribuzione a posteriori desiderata.</p>
<div id="cell-48" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:12:58.593324Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:12:58.427645Z&quot;}" data-execution_count="41">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>plt.hist(samples[burnin:], bins<span class="op">=</span><span class="dv">20</span>, alpha<span class="op">=</span><span class="fl">0.4</span>, label<span class="op">=</span><span class="st">"MCMC distribution"</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the true function</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>plt.plot(x, stats.beta.pdf(x, <span class="dv">18</span>, <span class="dv">92</span>), <span class="st">"C0"</span>, label<span class="op">=</span><span class="st">"True distribution"</span>)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01_metropolis_files/figure-html/cell-22-output-1.png" width="731" height="491" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Come previsto, la distribuzione a posteriori calcolata con l’algorimo di Metropolis converge verso la soluzione analitica.</p>
<p>È possibile usare la funzione <code>summary</code> del pacchetto ArviZ per calolare l’intervallo di credibilità, ovvero l’intervallo che contiene la proporzione indicata dei valori estratti a caso dalla distribuzione a posteriori.</p>
<div id="cell-50" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:01.815307Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:01.783434Z&quot;}" data-execution_count="49">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>samples_array <span class="op">=</span> np.array(samples) <span class="co"># per potere usare ArViZ</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>az.summary(samples_array[burnin:], kind<span class="op">=</span><span class="st">"stats"</span>, hdi_prob<span class="op">=</span><span class="fl">0.94</span>, round_to<span class="op">=</span><span class="dv">2</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">mean</th>
<th data-quarto-table-cell-role="th">sd</th>
<th data-quarto-table-cell-role="th">hdi_3%</th>
<th data-quarto-table-cell-role="th">hdi_97%</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">x</td>
<td>0.16</td>
<td>0.03</td>
<td>0.11</td>
<td>0.23</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Un KDE plot corrispondente all’istogramma precedente si può generare usando <code>az.plot_posterior()</code>. La curva rappresenta l’intera distribuzione a posteriori e viene calcolata utilizzando la stima della densità del kernel (KDE) che serve a “lisciare” l’istogramma.</p>
<div id="cell-52" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:05.281861Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:05.140042Z&quot;}" data-execution_count="50">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>az.plot_posterior(samples_array[burnin:])</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01_metropolis_files/figure-html/cell-24-output-1.png" width="731" height="491" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>L’HDI è una scelta comune nelle statistiche bayesiane e valori arrotondati come 50% o 95% sono molto popolari. Ma ArviZ utilizza il 94% come valore predefinito. La ragione di questa scelta è che il 94% è vicino al valore ampiamente utilizzato del 95%, ma è anche diverso da questo, così da servire da “amichevole promemoria” che non c’è niente di speciale nella soglia del 5%. Idealmente sarebbe opportuno scegliere un valore che si adatti alle specifiche esigenze dell’analisi statistica che si sta svolgendo, o almeno riconoscere che si sta usando un valore arbitrario.</p>
</section>
</section>
<section id="catene-di-markov-e-convergenza" class="level2" data-number="50.11">
<h2 data-number="50.11" class="anchored" data-anchor-id="catene-di-markov-e-convergenza"><span class="header-section-number">50.11</span> Catene di Markov e Convergenza</h2>
<p>Nell’ambito delle simulazioni Monte Carlo, una <em>catena</em> rappresenta una sequenza di valori campionati dall’algoritmo durante le sue iterazioni. Ogni valore nella catena corrisponde a un possibile stato del sistema che stiamo modellando. In altre parole, una catena traccia il percorso che l’algoritmo segue nello spazio dei parametri, esplorando le diverse configurazioni possibili.</p>
<p>Per verificare se l’algoritmo ha raggiunto la convergenza e se i campioni generati rappresentano effettivamente la distribuzione di interesse, è utile eseguire <em>multiple catene</em>. Ogni catena parte da un punto iniziale diverso nello spazio dei parametri.</p>
<p><strong>I vantaggi delle multiple catene:</strong></p>
<ul>
<li><strong>Diagnostica della convergenza:</strong> Confrontando le diverse catene, possiamo valutare se si stabilizzano verso la stessa distribuzione. Se le catene si mescolano bene, ovvero si intersecano frequentemente nel grafico dei valori campionati (trace plot), è un forte indicatore di convergenza.</li>
<li><strong>Robustezza:</strong> L’utilizzo di multiple catene rende l’analisi meno sensibile alla scelta del punto di partenza. Se una singola catena potesse rimanere “intrappolata” in una regione dello spazio dei parametri, multiple catene aumentano la probabilità di esplorare lo spazio in modo più completo.</li>
</ul>
</section>
<section id="diagnostiche-della-soluzione-mcmc" class="level2" data-number="50.12">
<h2 data-number="50.12" class="anchored" data-anchor-id="diagnostiche-della-soluzione-mcmc"><span class="header-section-number">50.12</span> Diagnostiche della soluzione MCMC</h2>
<section id="stazionarietà-e-convergenza" class="level3" data-number="50.12.1">
<h3 data-number="50.12.1" class="anchored" data-anchor-id="stazionarietà-e-convergenza"><span class="header-section-number">50.12.1</span> Stazionarietà e Convergenza</h3>
<p>Un aspetto cruciale nell’analisi delle catene di Markov MCMC è la <strong>convergenza</strong> alla <strong>distribuzione stazionaria</strong>. Intuitivamente, la catena converge quando i campioni generati rappresentano fedelmente la distribuzione di interesse, indipendentemente dal punto di partenza. Questo fenomeno è spesso indicato come “mixing”.</p>
<section id="valutazione-visuale-trace-plots-e-grafici-di-densità" class="level4" data-number="50.12.1.1">
<h4 data-number="50.12.1.1" class="anchored" data-anchor-id="valutazione-visuale-trace-plots-e-grafici-di-densità"><span class="header-section-number">50.12.1.1</span> Valutazione Visuale: Trace Plots e Grafici di Densità</h4>
<ul>
<li><em>Trace Plots:</em> Questi grafici visualizzano l’evoluzione dei parametri nel tempo. Una catena convergente mostra tracce stabili e senza trend evidenti. Tracce irregolari o con andamenti sistematici suggeriscono problemi di convergenza.</li>
<li><em>Grafici di Densità:</em> Confrontando i grafici di densità dei campioni con la distribuzione teorica, è possibile valutare visivamente se la catena sta esplorando adeguatamente lo spazio dei parametri. Una buona convergenza si manifesta con una sovrapposizione significativa tra i due grafici.</li>
</ul>
<p><strong>Segni di Convergenza:</strong></p>
<ul>
<li><em>Stabilità:</em> I valori campionati oscillano attorno a un valore medio costante, senza trend marcati.</li>
<li><em>Omogeneità:</em> La variabilità dei campioni rimane relativamente uniforme nel tempo.</li>
<li><em>Assenza di Periodicità:</em> Non si osservano pattern ciclici o ripetitivi.</li>
</ul>
<p>In sintesi, i trace plots e i grafici di densità offrono strumenti visivi rapidi per valutare la convergenza di una catena di Markov MCMC. Una convergenza soddisfacente è fondamentale per garantire la validità delle inferenze statistiche basate sui campioni generati.</p>
</section>
</section>
<section id="autocorrelazione-nelle-catene-di-markov-mcmc" class="level3" data-number="50.12.2">
<h3 data-number="50.12.2" class="anchored" data-anchor-id="autocorrelazione-nelle-catene-di-markov-mcmc"><span class="header-section-number">50.12.2</span> Autocorrelazione nelle catene di Markov MCMC</h3>
<p>A differenza dei generatori di numeri casuali indipendenti, gli algoritmi MCMC producono una sequenza di campioni <em>correlati</em>. Ogni valore campionato dipende da quello precedente, formando una <em>catena di Markov</em>. Questa interdipendenza è un aspetto fondamentale dell’MCMC.</p>
<p>L’<em>autocorrelazione</em> quantifica il grado di dipendenza tra valori distanti di una certa quantità (detta <em>lag</em>) nella catena. Un’alta autocorrelazione a lag bassi indica una forte dipendenza tra campioni successivi. Al contrario, una rapida diminuzione dell’autocorrelazione al crescere del lag suggerisce che la catena “miscela” bene, ovvero esplora lo spazio dei parametri in modo efficiente.</p>
<ul>
<li><em>Lag 1:</em> Misura la correlazione tra valori consecutivi nella catena.</li>
<li><em>Lag 2:</em> Misura la correlazione tra valori separati da un passo intermedio.</li>
<li><em>Lag k:</em> Generalizza il concetto ai valori separati da k passi.</li>
</ul>
<p>Un <em>correlogramma</em> è un grafico che mostra l’autocorrelazione in funzione del lag. Un decadimento rapido dell’autocorrelazione verso zero indica una buona convergenza della catena.</p>
<p>L’autocorrelazione di ordine <span class="math inline">\(k\)</span> è data da <span class="math inline">\(\rho_k\)</span> e può essere stimata come:</p>
<p><span id="eq-autocor"><span class="math display">\[
\begin{align}
\rho_k &amp;= \frac{Cov(\theta_m, \theta_{m+k})}{Var(\theta_m)}\notag\\
&amp;= \frac{\sum_{m=1}^{n-k}(\theta_m - \bar{\theta})(\theta_{m-k} - \bar{\theta})}{\sum_{m=1}^{n-k}(\theta_m - \bar{\theta})^2} \qquad\text{con }\quad \bar{\theta} = \frac{1}{n}\sum_{m=1}^{n}\theta_m.
\end{align}
\tag{50.1}\]</span></span></p>
<p>Per fare un esempio pratico, simuliamo dei dati autocorrelati.</p>
<div id="cell-56" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:14.103239Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:14.088762Z&quot;}" data-execution_count="40">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> pd.array([<span class="dv">22</span>, <span class="dv">24</span>, <span class="dv">25</span>, <span class="dv">25</span>, <span class="dv">28</span>, <span class="dv">29</span>, <span class="dv">34</span>, <span class="dv">37</span>, <span class="dv">40</span>, <span class="dv">44</span>, <span class="dv">51</span>, <span class="dv">48</span>, <span class="dv">47</span>, <span class="dv">50</span>, <span class="dv">51</span>])</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;IntegerArray&gt;
[22, 24, 25, 25, 28, 29, 34, 37, 40, 44, 51, 48, 47, 50, 51]
Length: 15, dtype: Int64</code></pre>
</div>
</div>
<p>L’autocorrelazione di ordine 1 è semplicemente la correlazione tra ciascun elemento e quello successivo nella sequenza.</p>
<div id="cell-58" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:19.851915Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:19.838140Z&quot;}" data-execution_count="41">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>sm.tsa.acf(x)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>array([ 1.        ,  0.83174224,  0.65632458,  0.49105012,  0.27863962,
        0.03102625, -0.16527446, -0.30369928, -0.40095465, -0.45823389,
       -0.45047733, -0.36933174])</code></pre>
</div>
</div>
<p>Nell’esempio, il vettore <code>x</code> è una sequenza temporale di 15 elementi. Il vettore <span class="math inline">\(x'\)</span> include gli elementi con gli indici da 0 a 13 nella sequenza originaria, mentre il vettore <span class="math inline">\(x''\)</span> include gli elementi 1:14. Gli elementi delle coppie ordinate dei due vettori avranno dunque gli indici <span class="math inline">\((0, 1)\)</span>, <span class="math inline">\((1, 2), (2, 3), \dots (13, 14)\)</span> degli elementi della sequenza originaria. La correlazione di Pearson tra i vettori <span class="math inline">\(x'\)</span> e <span class="math inline">\(x''\)</span> corrisponde all’autocorrelazione di ordine 1 della serie temporale.</p>
<p>Nell’output precedente</p>
<ul>
<li>0.83174224 è l’autocorrelazione di ordine 1 (lag = 1),</li>
<li>0.65632458 è l’autocorrelazione di ordine 2 (lag = 2),</li>
<li>0.49105012 è l’autocorrelazione di ordine 3 (lag = 3),</li>
<li>ecc.</li>
</ul>
<p>È possibile specificare il numero di ritardi (<em>lag</em>) da utilizzare con l’argomento <code>nlags</code>:</p>
<div id="cell-60" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:23.261460Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:23.237239Z&quot;}" data-execution_count="42">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>sm.tsa.acf(x, nlags<span class="op">=</span><span class="dv">4</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="42">
<pre><code>array([1.        , 0.83174224, 0.65632458, 0.49105012, 0.27863962])</code></pre>
</div>
</div>
<p>In Python possiamo creare un grafico della funzione di autocorrelazione (correlogramma) per una serie temporale usando la funzione <code>tsaplots.plot_acf()</code> dalla libreria <code>statsmodels</code>.</p>
<div id="cell-62" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:25.740566Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:25.579637Z&quot;}" data-execution_count="43">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>tsaplots.plot_acf(x, lags<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01_metropolis_files/figure-html/cell-28-output-1.png" width="731" height="491" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Per i dati dell’esempio in discussione otteniamo la situazione seguente.</p>
<div id="cell-64" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:32.510199Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:30.039818Z&quot;}" data-execution_count="44">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>tsaplots.plot_acf(samps[burnin:], lags<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01_metropolis_files/figure-html/cell-29-output-1.png" width="731" height="491" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In situazioni ottimali l’autocorrelazione diminuisce rapidamente ed è effettivamente pari a 0 per piccoli lag. Ciò indica che i valori della catena di Markov che si trovano a più di soli pochi passi di distanza gli uni dagli altri non risultano associati tra loro, il che fornisce una conferma del “mixing” della catena di Markov, ossia della convergenza alla distribuzione stazionaria. Nelle analisi bayesiane, una delle strategie che consentono di ridurre l’autocorrelazione è quella di assottigliare l’output immagazzinando solo ogni <span class="math inline">\(m\)</span>-esimo punto dopo il periodo di burn-in. Una tale strategia va sotto il nome di <em>thinning</em>.</p>
<p>Nel seguente correlogramma, analizziamo la medesima catena di Markov. Tuttavia, in questa occasione applichiamo un “thinning” (sottocampionamento) con un fattore di 5.</p>
<div id="cell-66" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:37.102817Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:36.966176Z&quot;}" data-execution_count="45">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>thin <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>sampsthin <span class="op">=</span> samps[burnin::thin]</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>tsaplots.plot_acf(sampsthin, lags<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01_metropolis_files/figure-html/cell-30-output-1.png" width="731" height="491" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Si può notare come l’autocorrelazione diminuisce molto più rapidamente.</p>
<section id="tasso-di-accettazione" class="level4" data-number="50.12.2.1">
<h4 data-number="50.12.2.1" class="anchored" data-anchor-id="tasso-di-accettazione"><span class="header-section-number">50.12.2.1</span> Tasso di accettazione</h4>
<p>Quando si utilizza l’algoritmo Metropolis, è importante monitorare il tasso di accettazione e assicurarsi che sia nell’intervallo ottimale. Se si accetta quasi sempre il candidato proposto, probabilmente significa che, in ogni iterazione, la catena salta solo di un piccolo passo (in modo che il rapporto di accettazione sia vicino a 1 ogni volta). Di conseguenza, la catena impiegherà molte iterazioni per raggiungere altre regioni della distribuzione stazionaria e i campioni consecutivi saranno molto fortemente correlati. D’altra parte, se il tasso di accettazione è molto basso, la catena rimarrà bloccata nella stessa posizione per molte iterazioni prima di spostarsi verso uno stato diverso. Per l’algoritmo Metropolis base con un singolo parametro con una distribuzione proposta Gaussiana normale, un tasso di accettazione ottimale è compreso tra il 40% e il 50%.</p>
</section>
</section>
<section id="test-statistici-per-la-convergenza" class="level3" data-number="50.12.3">
<h3 data-number="50.12.3" class="anchored" data-anchor-id="test-statistici-per-la-convergenza"><span class="header-section-number">50.12.3</span> Test Statistici per la Convergenza</h3>
<p>Oltre agli approcci grafici, esistono test statistici specifici che possono aiutare a determinare se la catena ha raggiunto uno stato stazionario.</p>
<section id="test-di-geweke" class="level4" data-number="50.12.3.1">
<h4 data-number="50.12.3.1" class="anchored" data-anchor-id="test-di-geweke"><span class="header-section-number">50.12.3.1</span> Test di Geweke</h4>
<p>Il test di Geweke è una procedura che confronta le medie di due segmenti della catena di campionamento, tipicamente il primo 10% e l’ultimo 50% dei campioni, dopo aver escluso un iniziale periodo di “burn-in” (una fase iniziale durante la quale la catena potrebbe non essere ancora convergente). La premessa di base è che, se la catena è in uno stato stazionario, le medie di questi due segmenti dovrebbero essere sostanzialmente uguali. Differenze significative tra queste medie possono indicare che la catena non ha ancora raggiunto la convergenza.</p>
</section>
<section id="geweke-z-score" class="level4" data-number="50.12.3.2">
<h4 data-number="50.12.3.2" class="anchored" data-anchor-id="geweke-z-score"><span class="header-section-number">50.12.3.2</span> Geweke Z-score</h4>
<p>Una variante del test di Geweke è lo z-score di Geweke, che offre un modo quantitativo per valutare le differenze tra i segmenti della catena. Questo test calcola uno z-score che confronta le medie dei due segmenti tenendo conto della varianza. Un valore di z-score:</p>
<ul>
<li><strong>Al di sotto di 2 (in valore assoluto)</strong> suggerisce che non ci sono differenze significative tra i segmenti, indicando che la catena potrebbe essere in stato stazionario.</li>
<li><strong>Superiore a 2 (in valore assoluto)</strong> indica che esiste una differenza significativa tra i segmenti, suggerendo che la catena non ha raggiunto la convergenza e potrebbe essere necessario un periodo di burn-in più esteso.</li>
</ul>
<p>Entrambi i metodi forniscono strumenti utili per valutare la convergenza delle catene MCMC. È importante notare che nessun test può garantire con certezza la convergenza, ma l’utilizzo congiunto di approcci grafici e test statistici può offrire una buona indicazione dello stato della catena.</p>
</section>
</section>
<section id="dimensione-del-campione-effettiva-ess" class="level3" data-number="50.12.4">
<h3 data-number="50.12.4" class="anchored" data-anchor-id="dimensione-del-campione-effettiva-ess"><span class="header-section-number">50.12.4</span> Dimensione del campione effettiva (ESS)</h3>
<p>La correlazione tra campioni consecutivi in una catena MCMC riduce l’informazione effettiva contenuta in ogni iterazione. La <strong>dimensione del campione effettiva (ESS)</strong> quantifica questa perdita di informazione dovuta alla dipendenza tra i campioni, stimando il numero equivalente di campioni indipendenti. Un valore basso di ESS indica una forte correlazione tra i campioni e una convergenza più lenta della catena.</p>
<p>L’ESS descrive l’efficacia del campionamento dipendente in termini di campioni indipendenti estratti dalla stessa distribuzione. Rappresenta un indicatore dell’efficienza del campionamento e dell’autocorrelazione della catena.</p>
<p>La formula per stimare la dimensione del campione effettiva (ESS) di una catena di Markov è:</p>
<p><span class="math display">\[
\text{ESS} = \frac{N}{1 + 2 \sum_{t=1}^{T} \rho_t},
\]</span></p>
<p>dove:</p>
<ul>
<li><span class="math inline">\(N\)</span> è il numero totale di campioni nella catena,</li>
<li><span class="math inline">\(T\)</span> è il lag, ovvero il numero massimo di termini di autocorrelazione considerati,</li>
<li><span class="math inline">\(\rho_t\)</span> è l’autocorrelazione al lag <span class="math inline">\(t\)</span>, ossia la correlazione tra due campioni consecutivi separati da <span class="math inline">\(t\)</span> iterazioni.</li>
</ul>
<p>In pratica, <span class="math inline">\(T\)</span> viene scelto in modo tale che <span class="math inline">\(\rho_T\)</span> sia sufficientemente piccolo, indicando che l’autocorrelazione è quasi svanita. La somma <span class="math inline">\(\sum_{t=1}^T \rho_t\)</span> viene quindi troncata approssimativamente a <span class="math inline">\(T\)</span>, poiché i contributi delle autocorrelazioni successive diventano trascurabili.</p>
</section>
<section id="calcolo-della-statistica-di-gelman-rubin-hatr" class="level3" data-number="50.12.5">
<h3 data-number="50.12.5" class="anchored" data-anchor-id="calcolo-della-statistica-di-gelman-rubin-hatr"><span class="header-section-number">50.12.5</span> Calcolo della Statistica di Gelman-Rubin (<span class="math inline">\(\hat{R}\)</span>)</h3>
<p>Per calcolare la statistica di Gelman-Rubin (spesso indicata come <span class="math inline">\(\hat{R}\)</span>), è necessario eseguire più catene e confrontare la variabilità all’interno di ciascuna catena con la variabilità tra le catene. Ecco i passaggi per calcolare <span class="math inline">\(\hat{R}\)</span>:</p>
<ol type="1">
<li><p>Esegui <span class="math inline">\(m\)</span> catene di Markov di lunghezza <span class="math inline">\(n\)</span>, dove <span class="math inline">\(m\)</span> è solitamente maggiore di 1.</p></li>
<li><p>Per ciascun parametro scalare <span class="math inline">\(\theta\)</span>, calcola la varianza all’interno delle catene (<span class="math inline">\(W\)</span>) e la varianza tra le catene (<span class="math inline">\(B\)</span>).</p></li>
<li><p>Calcola la varianza combinata <span class="math inline">\(\hat{V}\)</span> come media ponderata delle varianze all’interno delle catene.</p></li>
<li><p>Calcola il fattore di riduzione della scala potenziale <span class="math inline">\(\hat{R}\)</span> come la radice quadrata del rapporto tra la varianza combinata <span class="math inline">\(\hat{V}\)</span> e la varianza all’interno delle catene <span class="math inline">\(W\)</span>:</p>
<p><span class="math display">\[
\hat{R} = \sqrt{\frac{\hat{V}}{W}}.
\]</span></p></li>
<li><p>Se <span class="math inline">\(\hat{R}\)</span> è vicino a 1, ciò indica che le catene sono in convergenza.</p></li>
</ol>
<p>La statistica di Gelman-Rubin <span class="math inline">\(\hat{R}\)</span> è una misura di convergenza per le catene di Markov. Essa quantifica il grado di accordo tra più catene, fornendo uno strumento diagnostico per valutare la convergenza nelle simulazioni MCMC.</p>
</section>
</section>
<section id="vantaggi-del-campionamento-mcmc-rispetto-alle-soluzioni-analitiche" class="level2" data-number="50.13">
<h2 data-number="50.13" class="anchored" data-anchor-id="vantaggi-del-campionamento-mcmc-rispetto-alle-soluzioni-analitiche"><span class="header-section-number">50.13</span> Vantaggi del Campionamento MCMC rispetto alle Soluzioni Analitiche</h2>
<p>Il campionamento MCMC offre notevoli vantaggi pratici rispetto alle soluzioni analitiche nella statistica bayesiana, in particolare quando si tratta di manipolare distribuzioni a posteriori. Sebbene l’impossibilità di calcolare analiticamente la distribuzione a posteriori sia spesso la motivazione principale per l’uso di MCMC, i benefici di questo approccio si estendono ben oltre questa necessità <span class="citation" data-cites="buerkner2024brms">(<a href="../../99-references.html#ref-buerkner2024brms" role="doc-biblioref">Bürkner 2024</a>)</span>.</p>
<section id="facilità-di-manipolazione-e-flessibilità" class="level3" data-number="50.13.1">
<h3 data-number="50.13.1" class="anchored" data-anchor-id="facilità-di-manipolazione-e-flessibilità"><span class="header-section-number">50.13.1</span> Facilità di Manipolazione e Flessibilità</h3>
<p>Il vantaggio chiave del campionamento MCMC risiede nella semplicità con cui si possono manipolare i campioni ottenuti. Mentre le densità calcolate analiticamente possono richiedere trasformazioni matematiche complesse, i campioni MCMC possono essere facilmente trasformati con operazioni dirette. Questa flessibilità si manifesta in diversi aspetti:</p>
<ol type="1">
<li><p><strong>Trasformazioni di Variabili</strong>: Consideriamo un caso in cui siamo interessati alla varianza residua (<span class="math inline">\(\sigma^2\)</span>) in un modello, ma abbiamo campioni solo della deviazione standard residua (<span class="math inline">\(\sigma\)</span>). Con il campionamento MCMC, la trasformazione è immediata:</p>
<p><span class="math display">\[
(\sigma^{(s)})^2 = \sigma^{2(s)},
\]</span></p>
<p>dove <span class="math inline">\(s\)</span> indica il singolo campione. Questa operazione si traduce semplicemente nell’elevare al quadrato ogni campione di <span class="math inline">\(\sigma\)</span>, ottenendo direttamente campioni validi di <span class="math inline">\(\sigma^2\)</span>. In contrasto, con una densità analitica di <span class="math inline">\(\sigma\)</span>, la trasformazione richiederebbe l’applicazione dell’aggiustamento del Jacobiano, un processo matematicamente più complesso.</p></li>
<li><p><strong>Combinazione di Parametri</strong>: Il MCMC semplifica notevolmente la combinazione di parametri in modelli statistici. Per una quantità <span class="math inline">\(\theta\)</span> che dipende da parametri <span class="math inline">\(\beta_1\)</span> e <span class="math inline">\(\beta_2\)</span> attraverso una funzione <span class="math inline">\(f\)</span>, possiamo calcolare:</p>
<p><span class="math display">\[\theta^{(s)} = f(\beta_1^{(s)}, \beta_2^{(s)}).\]</span></p>
<p>Questa operazione si estende facilmente a combinazioni complesse e funzioni non lineari, contrastando nettamente con la complessità di derivare analiticamente la distribuzione di <span class="math inline">\(\theta\)</span>.</p></li>
</ol>
<p>In conclusione, il campionamento MCMC non è solo una necessità quando le soluzioni analitiche sono introvabili, ma offre vantaggi in termini di facilità di manipolazione, flessibilità computazionale e applicabilità pratica.</p>
</section>
</section>
<section id="caso-normale-normale" class="level2" data-number="50.14">
<h2 data-number="50.14" class="anchored" data-anchor-id="caso-normale-normale"><span class="header-section-number">50.14</span> Caso Normale-Normale</h2>
<p>Per fare un altro esempio, consideriamo ora il caso Normale-Normale di cui è possibile trovare una soluzione analitica. Supponiamo, come prior, una <span class="math inline">\(\mathcal{N}(30, 5\)</span>.</p>
<div id="cell-71" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:44.508917Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:44.503276Z&quot;}" data-execution_count="46">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prior(mu):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> stats.norm.pdf(mu, <span class="dv">30</span>, <span class="dv">5</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Per la verosimiglianza del parametro <span class="math inline">\(\mu\)</span>, supponiamo <span class="math inline">\(\sigma\)</span> nota e uguale alla deviazione standard del campione.</p>
<div id="cell-73" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:46.627729Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:46.624746Z&quot;}" data-execution_count="47">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> likelihood(mu, data):</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    std_data <span class="op">=</span> np.std(data)  <span class="co"># Calcola la deviazione standard dei dati</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.prod(stats.norm.pdf(data, mu, std_data))</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Definiamo il posterior non normalizzato:</p>
<div id="cell-75" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:48.769380Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:48.764214Z&quot;}" data-execution_count="48">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> posterior(mu, data):</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> likelihood(mu, data) <span class="op">*</span> prior(mu)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Modifichiamo ora l’algoritmo di Metropolis descritto sopra per adattarlo al caso presente.</p>
<div id="cell-77" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:51.026747Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:51.021109Z&quot;}" data-execution_count="49">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Algoritmo di Metropolis per il caso normale-normale</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> metropolis_for_normal(nsamp, xinit, data):</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> np.empty(nsamp)</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>    x_prev <span class="op">=</span> xinit</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(nsamp):</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>        x_star <span class="op">=</span> np.random.normal(x_prev, <span class="fl">0.5</span>)  <span class="co"># Genera un nuovo punto dalla proposta</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calcola il rapporto di accettazione e accetta il nuovo punto con una certa probabilità</span></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> posterior(x_star, data) <span class="op">/</span> posterior(x_prev, data) <span class="op">&gt;</span> np.random.uniform():</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>            x_prev <span class="op">=</span> x_star</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>        samples[i] <span class="op">=</span> x_prev</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> samples</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Vediamo cosa fa la presente versione dell’algoritmo di Metropolis passo dopo passo:</p>
<ol type="1">
<li><p><strong>Ciclo sui Campioni</strong>: <code>for i in range(nsamp):</code> inizia un ciclo che si ripeterà <code>nsamp</code> volte, dove <code>nsamp</code> è il numero totale di campioni che vogliamo generare. Ogni iterazione di questo ciclo produrrà un campione dalla distribuzione di interesse.</p></li>
<li><p><strong>Generazione di un Nuovo Punto</strong>: <code>x_star = np.random.normal(x_prev, 0.5)</code> genera un nuovo punto (<code>x_star</code>) come proposta per il prossimo passo del campionamento. Questo è fatto campionando da una distribuzione normale con media uguale all’ultimo punto accettato (<code>x_prev</code>) e una deviazione standard di <code>0.5</code>. Questa distribuzione è detta distribuzione di proposta e serve a esplorare lo spazio dei parametri.</p></li>
<li><p><strong>Calcolo del Rapporto di Accettazione</strong>:</p>
<ul>
<li>Il rapporto di accettazione è calcolato come <code>posterior(x_star, data) / posterior(x_prev, data)</code>, che è il rapporto tra la probabilità del posterior del nuovo punto proposto (<code>x_star</code>) e la probabilità del posterior dell’ultimo punto accettato (<code>x_prev</code>).</li>
<li>Questo rapporto indica quanto sia preferibile il nuovo punto rispetto al precedente, basandosi sulla funzione <code>posterior</code>, che calcola la probabilità a posteriori del modello dato il parametro e i dati osservati.</li>
</ul></li>
<li><p><strong>Decisione di Accettazione del Nuovo Punto</strong>:</p>
<ul>
<li>La decisione se accettare o meno il nuovo punto (<code>x_star</code>) si basa su un confronto del rapporto di accettazione con un numero casuale uniformemente distribuito tra 0 e 1 (<code>np.random.uniform()</code>).</li>
<li>Se il rapporto di accettazione è maggiore di questo numero casuale, il nuovo punto è accettato come il prossimo punto nella catena (<code>x_prev = x_star</code>). Ciò significa che il nuovo punto ha una probabilità a posteriori più alta rispetto al punto precedente, o è stato “fortunato” nel processo di selezione casuale, consentendo all’algoritmo di esplorare lo spazio dei parametri anche in zone di minore probabilità.</li>
<li>Se il nuovo punto non viene accettato, la catena rimane nel punto precedente (<code>x_prev</code>), e questo punto viene nuovamente aggiunto all’array dei campioni.</li>
</ul></li>
<li><p><strong>Salvataggio del Campione</strong>: <code>samples[i] = x_prev</code> salva il punto corrente (che può essere il nuovo punto accettato o il punto precedente se il nuovo punto è stato rifiutato) nell’array <code>samples</code>. Questo processo si ripete fino a quando non si raggiunge il numero desiderato di campioni.</p></li>
</ol>
<p>Come dati, usiamo il campione di 30 valori BDI-II ottenuti dai soggetti clinici esaminati da {cite}<code>zetsche_2019future</code>.</p>
<div id="cell-80" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:13:55.370275Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:55.362498Z&quot;}" data-execution_count="50">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>    <span class="fl">26.0</span>, <span class="fl">35.0</span>, <span class="dv">30</span>, <span class="dv">25</span>, <span class="dv">44</span>, <span class="dv">30</span>, <span class="dv">33</span>, <span class="dv">43</span>, <span class="dv">22</span>, <span class="dv">43</span>, <span class="dv">24</span>, <span class="dv">19</span>, <span class="dv">39</span>, <span class="dv">31</span>, <span class="dv">25</span>, <span class="dv">28</span>, <span class="dv">35</span>, <span class="dv">30</span>, <span class="dv">26</span>, <span class="dv">31</span>,</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>    <span class="dv">41</span>, <span class="dv">36</span>, <span class="dv">26</span>, <span class="dv">35</span>, <span class="dv">33</span>, <span class="dv">28</span>, <span class="dv">27</span>, <span class="dv">34</span>, <span class="dv">27</span>, <span class="dv">22</span>,</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>])</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Procediamo con l’esecuzione dell’algoritmo di Metropolis.</p>
<div id="cell-82" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:14:13.286088Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:13:57.908754Z&quot;}" data-execution_count="51">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> metropolis_for_normal(<span class="dv">100_000</span>, np.mean(y), y)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>samples.shape</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>(100000,)</code></pre>
</div>
</div>
<section id="calcolo-dei-parametri-del-posterior-analitico" class="level3" data-number="50.14.1">
<h3 data-number="50.14.1" class="anchored" data-anchor-id="calcolo-dei-parametri-del-posterior-analitico"><span class="header-section-number">50.14.1</span> Calcolo dei Parametri del Posterior Analitico</h3>
<p>Nel caso normale-normale, possiamo derivare analiticamente la distribuzione posteriore quando sia il prior che la likelihood sono distribuzioni normali. La bellezza di questo approccio sta nella forma chiusa del posterior, che è anch’esso una distribuzione normale con parametri specifici facilmente calcolabili. Ecco come si fa:</p>
<ol type="1">
<li><p><strong>Media Posteriore (<span class="math inline">\(\mu_{post}\)</span>)</strong>: La media del posterior è un peso tra la media del campione e la media del prior, dove i pesi sono determinati dalle varianze del prior e dei dati.</p>
<p><span class="math display">\[
\mu_{post} = \frac{\frac{\mu_{prior}}{\sigma_{prior}^2} + \frac{\sum y_i}{\sigma_{data}^2}}{\frac{1}{\sigma_{prior}^2} + \frac{n}{\sigma_{data}^2}}
\]</span></p></li>
<li><p><strong>Varianza Posteriore (<span class="math inline">\(\sigma_{post}^2\)</span>)</strong>: La varianza del posterior è determinata dalle varianze del prior e dei dati.</p>
<p><span class="math display">\[
\sigma_{post}^2 = \left(\frac{1}{\sigma_{prior}^2} + \frac{n}{\sigma_{data}^2}\right)^{-1}
\]</span></p></li>
</ol>
<p>Dove: - <span class="math inline">\(\mu_{prior}\)</span> è la media del prior (in questo caso, 30), - <span class="math inline">\(\sigma_{prior}^2\)</span> è la varianza del prior (<span class="math inline">\(5^2\)</span> in questo caso), - <span class="math inline">\(\sigma_{data}^2\)</span> è la varianza dei dati (calcolata dai dati), - <span class="math inline">\(n\)</span> è il numero di osservazioni, - <span class="math inline">\(\sum y_i\)</span> è la somma delle osservazioni.</p>
</section>
<section id="codice-per-il-grafico" class="level3" data-number="50.14.2">
<h3 data-number="50.14.2" class="anchored" data-anchor-id="codice-per-il-grafico"><span class="header-section-number">50.14.2</span> Codice per il Grafico</h3>
<p>Per produrre il grafico con l’istogramma dei campioni dal posterior (usando l’algoritmo di Metropolis) e la curva della distribuzione posteriore analitica, usiamo i parametri calcolati:</p>
<div id="cell-84" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:14:27.238252Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:14:27.043718Z&quot;}" data-execution_count="52">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Parametri del prior</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>mu_prior <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>std_prior <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>var_prior <span class="op">=</span> std_prior <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Dati osservati</span></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(y)</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>sum_y <span class="op">=</span> np.<span class="bu">sum</span>(y)</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>var_data <span class="op">=</span> np.var(y, ddof<span class="op">=</span><span class="dv">1</span>)  <span class="co"># ddof=1 for sample variance</span></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcolo dei parametri posterior</span></span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>mu_post <span class="op">=</span> (mu_prior <span class="op">/</span> var_prior <span class="op">+</span> sum_y <span class="op">/</span> var_data) <span class="op">/</span> (<span class="dv">1</span> <span class="op">/</span> var_prior <span class="op">+</span> n <span class="op">/</span> var_data)</span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>var_post <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">/</span> var_prior <span class="op">+</span> n <span class="op">/</span> var_data)</span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>std_post <span class="op">=</span> np.sqrt(var_post)</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Generazione dei punti x per il grafico</span></span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(mu_post <span class="op">-</span> <span class="dv">4</span> <span class="op">*</span> std_post, mu_post <span class="op">+</span> <span class="dv">4</span> <span class="op">*</span> std_post, <span class="dv">1000</span>)</span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Istogramma dei campioni dal posterior</span></span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a>plt.hist(samples[burnin:], bins<span class="op">=</span><span class="dv">30</span>, alpha<span class="op">=</span><span class="fl">0.4</span>, density<span class="op">=</span><span class="va">True</span>, label<span class="op">=</span><span class="st">"MCMC Samples Distribution"</span>)</span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Curva della distribuzione posteriore analitica</span></span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a>plt.plot(x, stats.norm.pdf(x, mu_post, std_post), <span class="st">"C1"</span>, label<span class="op">=</span><span class="st">"Analytical Posterior Distribution"</span>)</span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="01_metropolis_files/figure-html/cell-37-output-1.png" width="731" height="491" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Questo codice mostra come integrare l’analisi MCMC con l’approccio analitico per il caso normale-normale, offrendo sia una visualizzazione dei risultati del sampling che la conferma attraverso la soluzione analitica.</p>
</section>
</section>
<section id="commenti-e-considerazioni-finali" class="level2" data-number="50.15">
<h2 data-number="50.15" class="anchored" data-anchor-id="commenti-e-considerazioni-finali"><span class="header-section-number">50.15</span> Commenti e Considerazioni Finali</h2>
<p>In molti casi, la distribuzione a posteriori dei parametri di un modello statistico non ha una forma analitica risolvibile. Per affrontare questa limitazione, si utilizzano metodi Monte Carlo basati su catene di Markov (MCMC). Questi algoritmi permettono di campionare efficacemente dalla distribuzione a posteriori, anche per modelli complessi, generando una sequenza di valori che approssima la distribuzione desiderata. L’algoritmo di Metropolis-Hastings <span class="citation" data-cites="hastings_1970">(<a href="../../99-references.html#ref-hastings_1970" role="doc-biblioref">Hastings 1970</a>)</span>, un’estensione dell’algoritmo di Metropolis originale <span class="citation" data-cites="metropolist_etal_1953">(<a href="../../99-references.html#ref-metropolist_etal_1953" role="doc-biblioref">Metropolis et al. 1953</a>)</span>, è uno dei metodi MCMC più ampiamente utilizzati.</p>
<p>In sintesi, l’algoritmo segue questi passaggi principali:</p>
<ul>
<li><strong>Generazione del nuovo stato proposto</strong>: Si crea un nuovo stato vicino a quello corrente utilizzando una distribuzione di proposta.</li>
<li><strong>Confronto tra densità posteriori</strong>: Si confrontano le densità a posteriori del nuovo stato proposto e dello stato corrente.</li>
<li><strong>Accettazione probabilistica</strong>: Il nuovo stato viene sempre accettato se ha una densità posteriore maggiore, oppure accettato con una certa probabilità se ha una densità minore.</li>
<li><strong>Burn-in e tasso di accettazione</strong>: I primi campioni vengono scartati (fase di burn-in) per garantire che la catena abbia raggiunto la distribuzione stazionaria, e si monitora il tasso di accettazione per ottimizzare l’efficienza del campionamento.</li>
</ul>
<p>Questo approccio consente di ottenere campioni che approssimano la distribuzione a posteriori, ma l’algoritmo di Metropolis può presentare limiti di efficienza, soprattutto per problemi ad alta dimensionalità o distribuzioni con geometrie complesse. Un aspetto cruciale è il <strong>tasso di accettazione</strong>, che rappresenta il rapporto tra il numero di proposte accettate e il numero totale di proposte. Un tasso troppo basso può indicare che la catena esplora lo spazio dei parametri in modo inefficiente, mentre un tasso troppo alto può segnalare che i passi effettuati sono troppo piccoli per consentire una buona esplorazione.</p>
<p>Rispetto alle varianti più moderne, l’algoritmo di Metropolis tende a essere meno efficiente. Metodi come il <strong>No-U-Turn Sampler (NUTS)</strong> e l’<strong>Hamiltonian Monte Carlo (HMC)</strong> offrono miglioramenti significativi, specialmente in spazi di parametri di grandi dimensioni. NUTS, ad esempio, viene utilizzato in strumenti avanzati come <strong>Stan</strong> e <strong>PyMC</strong> <span class="citation" data-cites="hoffman2014no">(<a href="../../99-references.html#ref-hoffman2014no" role="doc-biblioref">Hoffman, Gelman, et al. 2014</a>)</span>, permettendo un’esplorazione più rapida e accurata della distribuzione a posteriori.</p>
<p>Tra gli altri algoritmi MCMC degni di nota troviamo il <strong>campionatore di Gibbs</strong> <span class="citation" data-cites="geman_geman_1984">(<a href="../../99-references.html#ref-geman_geman_1984" role="doc-biblioref">Geman e Geman 1984</a>)</span> e l’<strong>Hamiltonian Monte Carlo</strong> <span class="citation" data-cites="duane1987hybrid">(<a href="../../99-references.html#ref-duane1987hybrid" role="doc-biblioref">Duane et al. 1987</a>)</span>. Questi metodi, insieme a Metropolis-Hastings, formano la base di numerose tecniche moderne per il campionamento da distribuzioni complesse. Per un approfondimento dettagliato sulle tecniche MCMC, si consiglia di consultare <span class="citation" data-cites="hanada2022mcmc">Hanada e Matsuura (<a href="../../99-references.html#ref-hanada2022mcmc" role="doc-biblioref">2022</a>)</span>.</p>
</section>
<section id="informazioni-sullambiente-di-sviluppo" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="informazioni-sullambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</h2>
<div id="cell-88" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-06-15T18:14:34.256099Z&quot;,&quot;start_time&quot;:&quot;2024-06-15T18:14:34.201359Z&quot;}" data-execution_count="53">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext watermark</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>watermark <span class="op">-</span>n <span class="op">-</span>u <span class="op">-</span>v <span class="op">-</span>iv <span class="op">-</span>w <span class="op">-</span>m</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Last updated: Sat Jun 15 2024

Python implementation: CPython
Python version       : 3.12.3
IPython version      : 8.25.0

Compiler    : Clang 16.0.6 
OS          : Darwin
Release     : 23.4.0
Machine     : arm64
Processor   : arm
CPU cores   : 8
Architecture: 64bit

sys        : 3.12.3 | packaged by conda-forge | (main, Apr 15 2024, 18:35:20) [Clang 16.0.6 ]
matplotlib : 3.8.4
arviz      : 0.18.0
statsmodels: 0.14.2
scipy      : 1.13.1
numpy      : 1.26.4
seaborn    : 0.13.2
pymc       : 5.15.1
pandas     : 2.2.2

Watermark: 2.4.3
</code></pre>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-buerkner2024brms" class="csl-entry" role="listitem">
Bürkner, Paul-Christian. 2024. <em>The brms Book: Applied Bayesian Regression Modelling Using R and Stan (Early Draft)</em>. <a href="https://paulbuerkner.com/software/brms-book">https://paulbuerkner.com/software/brms-book</a>.
</div>
<div id="ref-duane1987hybrid" class="csl-entry" role="listitem">
Duane, Simon, Anthony D Kennedy, Brian J Pendleton, e Duncan Roweth. 1987. <span>«Hybrid monte carlo»</span>. <em>Physics letters B</em> 195 (2): 216–22.
</div>
<div id="ref-geman_geman_1984" class="csl-entry" role="listitem">
Geman, Stuart, e Donald Geman. 1984. <span>«Stochastic relaxation, <span>Gibbs</span> distributions, and the <span>Bayesian</span> restoration of images»</span>. <em>IEEE Transactions on pattern analysis and machine intelligence</em> 6: 721–41.
</div>
<div id="ref-hanada2022mcmc" class="csl-entry" role="listitem">
Hanada, Masanori, e So Matsuura. 2022. <em>MCMC from Scratch</em>. Springer.
</div>
<div id="ref-hastings_1970" class="csl-entry" role="listitem">
Hastings, W. Keith. 1970. <span>«<span>Monte</span> <span>Carlo</span> sampling methods using <span>Markov</span> chains and their applications»</span>. <em>Biometrika</em> 57 (1): 97–109.
</div>
<div id="ref-hoffman2014no" class="csl-entry" role="listitem">
Hoffman, Matthew D, Andrew Gelman, et al. 2014. <span>«The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo.»</span> <em>Journal of Machine Learning Research</em> 15 (1): 1593–623.
</div>
<div id="ref-metropolist_etal_1953" class="csl-entry" role="listitem">
Metropolis, Nicholas, Arianna W. Rosenbluth, Marshall N. Rosenbluth, Augusta H. Teller, e Edward Teller. 1953. <span>«Equation of state calculations by fast computing machines»</span>. <em>The Journal of Chemical Physics</em> 21 (6): 1087–92.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiato!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiato!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/ccaudek\.github\.io\/psicometria\/intro\.html");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../chapters/mcmc/introduction_mcmc.html" class="pagination-link" aria-label="Introduzione">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Introduzione</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/mcmc/02_ppl.html" class="pagination-link" aria-label="Linguaggi di programmazione probabilistici">
        <span class="nav-page-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Linguaggi di programmazione probabilistici</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Data Science per Psicologi è stato scritto da Corrado Caudek.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.dev/ccaudek/psicometria/blob/main/chapters/mcmc/01_metropolis.ipynb" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Questo libro è stato realizzato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>