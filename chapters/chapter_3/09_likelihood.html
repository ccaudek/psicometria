<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Analisi dei dati per psicologi - 32&nbsp; La verosimiglianza</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/chapter_3/10_grid_gauss.html" rel="next">
<link href="../../chapters/chapter_3/08_cont_rv_distr.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/chapter_3/introduction_chapter_3.html">Probabilit√†</a></li><li class="breadcrumb-item"><a href="../../chapters/chapter_3/09_likelihood.html"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Analisi dei dati per psicologi</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ccaudek/psicometria/" title="Eseguire il codice" class="quarto-navigation-tool px-1" aria-label="Eseguire il codice"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benvenuti</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Python</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_1/introduction_chapter_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduzione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_1/00_prelims.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Preliminari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_1/01_python_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Python (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_1/02_python_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Python (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_1/03_numpy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">NumPy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_1/04_pandas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Pandas (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_1/05_pandas_aggregate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Pandas (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_1/06_pandas_functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pandas (3)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_1/07_matplotlib.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Matplotlib</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_1/08_seaborn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Seaborn</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Fondamenti</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_2/introduction_chapter_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Introduzione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_2/00_scientific_method.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Il Ruolo della Data Science nella Psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_2/01_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Concetti chiave</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_2/02_measurement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">La misurazione in psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_2/03_freq_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Analisi esplorativa dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_2/04_loc_scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Indici di posizione e di scala</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_2/05_correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Le relazioni tra variabili</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_2/06_causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Causalit√† dai dati osservazionali</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Probabilit√†</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/introduction_chapter_3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Introduzione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/01a_intro_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Interpretazione della probabilit√†</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/01b_prob_spaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Misura di Probabilit√†</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/01c_prob_on_general_spaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Misure e distribuzioni di probabilit√†</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/02_conditional_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Probabilit√† condizionata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/03_bayes_theorem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/04a_random_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/04b_expval_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Propriet√† delle variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/04c_sampling_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/05_joint_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Probabilit√† congiunta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/06_density_func.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">La funzione di densit√† di probabilit√†</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/07_discr_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. discrete</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/08_cont_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. continue</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/09_likelihood.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_3/10_grid_gauss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Calcolo della Distribuzione a Posteriori Gaussiana tramite Metodo a Griglia</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Inferenza Bayesiana</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/introduction_chapter_4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Introduzione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/01_intro_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Modellazione bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/02_subj_prop.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Pensare ad una proporzione in termini soggettivi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/03_conjugate_families_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/04_conjugate_families_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/05_summary_posterior.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Sintesi a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/06_balance_prior_post.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">L‚Äôinfluenza della distribuzione a priori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/10_metropolis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Monte Carlo a Catena di Markov</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/15_stan_beta_binomial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Linguaggio Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/16_stan_summary_posterior.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Metodi di sintesi della distribuzione a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/17_stan_diagnostics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Diagnostica delle catene markoviane</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/18_stan_prediction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">La predizione bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/19_stan_odds_ratio.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Analisi bayesiana dell‚Äôodds-ratio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/22_stan_normal_normal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Inferenza bayesiana su una media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/23_stan_two_groups.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Confronto tra due gruppi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/24_stan_hier_beta_binom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Modello gerarchico beta-binomiale con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_4/25_stan_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Modello di Poisson</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Modelli lineari</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/introduction_chapter_5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Introduzione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_03_reglin_bayesian.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Modello di regressione lineare bayesiano</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_04_synt_sugar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">Zucchero sintattico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_05_two_means.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Confronto tra le medie di due gruppi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_05a_stan_multreg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">Regressione multipla con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_06_hier_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">Il modello lineare gerarchico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_07_robust_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">Regressione robusta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_08_specification_error.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Errore di specificazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_09_causal_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">59</span>&nbsp; <span class="chapter-title">Inferenza causale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_21_stan_binomial_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">60</span>&nbsp; <span class="chapter-title">Regressione binomiale con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_22_stan_logistic_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">61</span>&nbsp; <span class="chapter-title">Regressione logistica con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_23_stan_poisson_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">62</span>&nbsp; <span class="chapter-title">Regressione di Poisson con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_24_stan_mixed_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">63</span>&nbsp; <span class="chapter-title">Modelli misti con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_30_entropy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">64</span>&nbsp; <span class="chapter-title">Entropia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_31_kl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">65</span>&nbsp; <span class="chapter-title">Divergenza KL e ELPD</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_32_stan_loo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">66</span>&nbsp; <span class="chapter-title">Validazione Incrociata Leave-One-Out</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_35_missing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">67</span>&nbsp; <span class="chapter-title">Dati mancanti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_5/05_40_rescorla_wagner.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">68</span>&nbsp; <span class="chapter-title">Apprendimento per rinforzo</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Inferenza frequentista</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_6/introduction_chapter_6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">69</span>&nbsp; <span class="chapter-title">Introduzione</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../99-references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliografia</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
 <span class="menu-text">Appendici</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a00_installation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Ambiente di lavoro</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a01_markdown.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Jupyter Notebook</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a02_shell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">La Shell</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a03_colab_tutorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Colab: un breve tutorial</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a04_virtual_env.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Ambienti virtuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a10_math_symbols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Simbologia di base</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a11_numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Numeri binari, interi, razionali, irrazionali e reali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a12_sum_notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">H</span>&nbsp; <span class="chapter-title">Simbolo di somma (sommatorie)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a13_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">I</span>&nbsp; <span class="chapter-title">Insiemi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a13a_probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">J</span>&nbsp; <span class="chapter-title">Sigma algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a14_combinatorics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">K</span>&nbsp; <span class="chapter-title">Calcolo combinatorio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a15_calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">L</span>&nbsp; <span class="chapter-title">Per liberarvi dai terrori preliminari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a20_kde_plot.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">M</span>&nbsp; <span class="chapter-title">Kernel Density Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a30_prob_tutorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">N</span>&nbsp; <span class="chapter-title">Esercizi di probabilit√† discreta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a40_rng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">O</span>&nbsp; <span class="chapter-title">Generazione di numeri casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a44_montecarlo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">P</span>&nbsp; <span class="chapter-title">Simulazione Monte Carlo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a45_mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">Q</span>&nbsp; <span class="chapter-title">Catene di Markov</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a46_stan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">R</span>&nbsp; <span class="chapter-title">Linguaggio Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a50_lin_fun.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">S</span>&nbsp; <span class="chapter-title">La funzione lineare</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a50_reglin_ml.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">T</span>&nbsp; <span class="chapter-title">Modello di Regressione Bivariato e ML</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a51_reglin_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">U</span>&nbsp; <span class="chapter-title">Regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a60_ttest_exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">V</span>&nbsp; <span class="chapter-title">Esercizi sull‚Äôinferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a70_predict_counts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">W</span>&nbsp; <span class="chapter-title">La predizione delle frequenze</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter_7/a100_solutions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">X</span>&nbsp; <span class="chapter-title">Soluzioni degli Esercizi</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indice</h2>
   
  <ul>
  <li><a href="#introduzione" id="toc-introduzione" class="nav-link active" data-scroll-target="#introduzione">Introduzione</a></li>
  <li><a href="#il-principio-della-verosimiglianza-e-la-sua-formalizzazione" id="toc-il-principio-della-verosimiglianza-e-la-sua-formalizzazione" class="nav-link" data-scroll-target="#il-principio-della-verosimiglianza-e-la-sua-formalizzazione"><span class="header-section-number">32.1</span> Il Principio della Verosimiglianza e la sua Formalizzazione</a></li>
  <li><a href="#verosimiglianza-binomiale" id="toc-verosimiglianza-binomiale" class="nav-link" data-scroll-target="#verosimiglianza-binomiale"><span class="header-section-number">32.2</span> Verosimiglianza Binomiale</a>
  <ul class="collapse">
  <li><a href="#interpretazione-della-funzione-di-verosimiglianza" id="toc-interpretazione-della-funzione-di-verosimiglianza" class="nav-link" data-scroll-target="#interpretazione-della-funzione-di-verosimiglianza"><span class="header-section-number">32.2.1</span> Interpretazione della Funzione di Verosimiglianza</a></li>
  </ul></li>
  <li><a href="#massima-verosimiglianza" id="toc-massima-verosimiglianza" class="nav-link" data-scroll-target="#massima-verosimiglianza"><span class="header-section-number">32.3</span> Massima verosimiglianza</a>
  <ul class="collapse">
  <li><a href="#la-funzione-di-log-verosimiglianza" id="toc-la-funzione-di-log-verosimiglianza" class="nav-link" data-scroll-target="#la-funzione-di-log-verosimiglianza"><span class="header-section-number">32.3.1</span> La Funzione di Log-Verosimiglianza</a></li>
  <li><a href="#verosimiglianza-congiunta" id="toc-verosimiglianza-congiunta" class="nav-link" data-scroll-target="#verosimiglianza-congiunta"><span class="header-section-number">32.3.2</span> Verosimiglianza Congiunta</a></li>
  </ul></li>
  <li><a href="#la-verosimiglianza-marginale" id="toc-la-verosimiglianza-marginale" class="nav-link" data-scroll-target="#la-verosimiglianza-marginale"><span class="header-section-number">32.4</span> La Verosimiglianza Marginale</a></li>
  <li><a href="#modello-gaussiano-e-verosimiglianza" id="toc-modello-gaussiano-e-verosimiglianza" class="nav-link" data-scroll-target="#modello-gaussiano-e-verosimiglianza"><span class="header-section-number">32.5</span> Modello Gaussiano e Verosimiglianza</a>
  <ul class="collapse">
  <li><a href="#caso-di-una-singola-osservazione" id="toc-caso-di-una-singola-osservazione" class="nav-link" data-scroll-target="#caso-di-una-singola-osservazione"><span class="header-section-number">32.5.1</span> Caso di una Singola Osservazione</a></li>
  <li><a href="#campione-indipendente-di-osservazioni-da-una-distribuzione-gaussiana" id="toc-campione-indipendente-di-osservazioni-da-una-distribuzione-gaussiana" class="nav-link" data-scroll-target="#campione-indipendente-di-osservazioni-da-una-distribuzione-gaussiana"><span class="header-section-number">32.5.2</span> Campione indipendente di osservazioni da una distribuzione gaussiana</a></li>
  <li><a href="#la-stima-di-massima-verosimiglianza-per-mu" id="toc-la-stima-di-massima-verosimiglianza-per-mu" class="nav-link" data-scroll-target="#la-stima-di-massima-verosimiglianza-per-mu"><span class="header-section-number">32.5.3</span> La Stima di Massima Verosimiglianza per <span class="math inline">\(\mu\)</span></a></li>
  </ul></li>
  <li><a href="#conclusione-e-riflessioni-finali" id="toc-conclusione-e-riflessioni-finali" class="nav-link" data-scroll-target="#conclusione-e-riflessioni-finali"><span class="header-section-number">32.6</span> Conclusione e Riflessioni Finali</a></li>
  <li><a href="#informazioni-sullambiente-di-sviluppo" id="toc-informazioni-sullambiente-di-sviluppo" class="nav-link" data-scroll-target="#informazioni-sullambiente-di-sviluppo">Informazioni sull‚ÄôAmbiente di Sviluppo</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.dev/ccaudek/psicometria/blob/main/chapters/chapter_3/09_likelihood.ipynb" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/chapter_3/introduction_chapter_3.html">Probabilit√†</a></li><li class="breadcrumb-item"><a href="../../chapters/chapter_3/09_likelihood.html"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-likelihood" class="quarto-section-identifier"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><strong>Prerequisiti</strong></p>
<p><strong>Concetti e Competenze Chiave</strong></p>
<ul>
<li>Comprendere il concetto di verosimiglianza e il suo ruolo nella dei parametri.</li>
<li>Generare grafici della funzione di verosimiglianza binomiale.</li>
<li>Generare grafici della funzione di verosimiglianza del modello gaussiano.</li>
<li>Interpretare i grafici della funzione di verosimiglianza.</li>
<li>Comprendere il concetto di stima di massima verosimiglianza.</li>
</ul>
<p><strong>Preparazione del Notebook</strong></p>
<div id="271de755-b218-48e3-a1c5-6bd1d0a55266" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats <span class="im">as</span> stats</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.integrate <span class="im">import</span> quad</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> arviz <span class="im">as</span> az</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="bb772cc1-f190-4e46-9fcd-2eb1bcc59ac2" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>seed: <span class="bu">int</span> <span class="op">=</span> <span class="bu">sum</span>(<span class="bu">map</span>(<span class="bu">ord</span>, <span class="st">"likelihood"</span>))</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>rng: np.random.Generator <span class="op">=</span> np.random.default_rng(seed<span class="op">=</span>seed)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>sns.set_theme(palette<span class="op">=</span><span class="st">"colorblind"</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>az.style.use(<span class="st">"arviz-darkgrid"</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>config InlineBackend.figure_format <span class="op">=</span> <span class="st">"retina"</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="introduzione" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="introduzione">Introduzione</h2>
<p>Oltre agli approcci frequentisti e bayesiani, esiste un terzo metodo fondamentale nell‚Äôambito dell‚Äôinferenza statistica: la metodologia basata sulla verosimiglianza. Questo approccio consente ai ricercatori di valutare l‚Äôevidenza relativa quando si confrontano due modelli o ipotesi, in maniera simile alla metodologia bayesiana. Ci√≤ che lo distingue √® il suo esplicito rifiuto di incorporare informazioni pregresse (priori) nelle analisi statistiche.</p>
<p>Questo capitolo si concentra sulla funzione di verosimiglianza, concetto centrale che si estende attraverso tutti e tre gli approcci statistici, fungendo da collegamento tra i dati osservati e i parametri di un modello statistico specifico. La rilevanza della funzione di verosimiglianza risiede nella sua capacit√† di fornire un fondamento robusto per l‚Äôinterpretazione e la quantificazione dell‚Äôadeguatezza dei dati ai modelli teorici. Questo la rende uno strumento cruciale per l‚Äôinferenza statistica, indispensabile per comprendere e valutare la conformit√† dei dati rispetto alle teorie proposte.</p>
</section>
<section id="il-principio-della-verosimiglianza-e-la-sua-formalizzazione" class="level2" data-number="32.1">
<h2 data-number="32.1" class="anchored" data-anchor-id="il-principio-della-verosimiglianza-e-la-sua-formalizzazione"><span class="header-section-number">32.1</span> Il Principio della Verosimiglianza e la sua Formalizzazione</h2>
<p>La funzione di verosimiglianza e la funzione di densit√† (o massa) di probabilit√† sono due concetti fondamentali in statistica che, nonostante condividano la stessa espressione matematica, rivestono ruoli e interpretazioni distinti a seconda del contesto in cui vengono applicati. La chiave per distinguere tra i due concetti risiede nel modo in cui trattiamo i dati e i parametri del modello.</p>
<p>Nel caso della funzione di densit√† (o massa) di probabilit√†, i parametri del modello sono fissati e l‚Äôobiettivo √® valutare la probabilit√† di osservare un certo insieme di dati. Qui, i dati sono variabili, mentre i parametri sono considerati costanti. Per esempio, in un esperimento in cui lanciamo una moneta diverse volte, potremmo usare una distribuzione binomiale per calcolare la probabilit√† di ottenere un certo numero di teste, assumendo un valore noto e fisso per la probabilit√† di ottenere testa in un singolo lancio.</p>
<p>Al contrario, nella funzione di verosimiglianza, manteniamo i dati osservati come fissi e variamo i parametri del modello per valutare quanto bene questi ultimi si adattino ai dati osservati. Questo processo ci permette di esplorare la plausibilit√† di diversi valori dei parametri dati gli stessi dati. L‚Äôobiettivo √® identificare il set di parametri che meglio spiega i dati osservati.</p>
<p>Formalmente, la relazione tra la funzione di verosimiglianza e la funzione di densit√† di probabilit√† √® espressa come segue:</p>
<p><span class="math display">\[
L(\theta | y) \propto p(y | \theta),
\]</span></p>
<p>dove <span class="math inline">\(L(\theta | y)\)</span> rappresenta la funzione di verosimiglianza per i parametri <span class="math inline">\(\theta\)</span> dati gli osservazioni <span class="math inline">\(y\)</span>, e <span class="math inline">\(p(y | \theta)\)</span> indica la probabilit√† (o densit√†) di osservare i dati <span class="math inline">\(y\)</span> dato un certo set di parametri <span class="math inline">\(\theta\)</span>.</p>
<p>Prendiamo l‚Äôesempio del lancio di una moneta. Se osserviamo 7 teste su 10 lanci, la funzione di massa di probabilit√† della distribuzione binomiale ci permette di calcolare la probabilit√† di questo esito per un dato valore di <span class="math inline">\(p\)</span> (la probabilit√† di testa). In questo contesto, <span class="math inline">\(p\)</span> √® fisso e i dati (<span class="math inline">\(y = 7\)</span> teste in <span class="math inline">\(n = 10\)</span> lanci) sono variabili.</p>
<p>Dall‚Äôaltro lato, se consideriamo <span class="math inline">\(p\)</span> variabile, la funzione di verosimiglianza ci permette di valutare come diversi valori di <span class="math inline">\(p\)</span> si adattano all‚Äôesito osservato di 7 teste su 10 lanci, mantenendo i dati osservati fissi.</p>
<p>√à importante sottolineare che, bench√© le due funzioni condividano la stessa forma matematica, il loro utilizzo e interpretazione sono profondamente diversi. La funzione di densit√† di probabilit√† si concentra sulla probabilit√† degli esiti dati i parametri, mentre la funzione di verosimiglianza valuta la plausibilit√† dei parametri dati gli esiti. Questa distinzione √® cruciale per l‚Äôinferenza statistica, permettendoci di stimare i parametri del modello che meglio si adattano ai dati osservati e di comprendere in modo pi√π approfondito la struttura e le caratteristiche del fenomeno studiato.</p>
</section>
<section id="verosimiglianza-binomiale" class="level2" data-number="32.2">
<h2 data-number="32.2" class="anchored" data-anchor-id="verosimiglianza-binomiale"><span class="header-section-number">32.2</span> Verosimiglianza Binomiale</h2>
<p>Proseguendo con l‚Äôesempio della distribuzione binomiale, approfondiamo la rilevanza della funzione di verosimiglianza nell‚Äôanalisi statistica attraverso uno scenario pratico. Supponiamo di condurre un esperimento con un numero definito di prove <span class="math inline">\(n\)</span>, ognuna delle quali pu√≤ terminare con un successo o un fallimento, come nel caso dei lanci di una moneta. Se registriamo <span class="math inline">\(y\)</span> successi e <span class="math inline">\(n - y\)</span> fallimenti, la probabilit√† di osservare esattamente <span class="math inline">\(y\)</span> successi segue la funzione di massa di probabilit√† (FMP) binomiale, che √® definita come:</p>
<p><span class="math display">\[
P(Y = y) = \binom{n}{y} \theta^y (1 - \theta)^{n - y},
\]</span></p>
<p>dove <span class="math inline">\(\theta\)</span> √® la probabilit√† di successo in una singola prova di Bernoulli.</p>
<p>Nell‚Äôutilizzo della funzione di verosimiglianza, ci concentriamo su come i diversi valori di <span class="math inline">\(\theta\)</span> possono spiegare i dati osservati <span class="math inline">\(y\)</span>. La verosimiglianza √® espressa come:</p>
<p><span class="math display">\[
\mathcal{L}(\theta \mid y) = \theta^y (1 - \theta)^{n - y},
\]</span></p>
<p>dato che il coefficiente binomiale <span class="math inline">\(\binom{n}{y}\)</span>, non dipendendo da <span class="math inline">\(\theta\)</span>, pu√≤ essere omesso per la semplicit√† della formulazione.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 32.1</strong></span> Per esemplificare, immaginiamo uno studio su un gruppo di 30 individui, di cui 23 presentano un atteggiamento negativo verso il futuro, un indicatore comune in pazienti con depressione <span class="citation" data-cites="zetsche_2019future">(<a href="../../99-references.html#ref-zetsche_2019future" role="doc-biblioref">Zetsche, Buerkner, e Renneberg 2019</a>)</span>. Qui, i nostri dati <span class="math inline">\(y\)</span> e <span class="math inline">\(n\)</span> sono fissi, e la funzione di verosimiglianza per <span class="math inline">\(\theta\)</span> sconosciuto diventa:</p>
<p><span class="math display">\[
\mathcal{L}(\theta \mid y) = \frac{(23 + 7)!}{23!7!} \theta^{23} (1 - \theta)^7.
\]</span></p>
<p>Valutando questa funzione per una serie di valori di <span class="math inline">\(\theta\)</span> possiamo determinare quale valore di <span class="math inline">\(\theta\)</span> rende i dati osservati pi√π verosimili. Procediamo simulando 100 valori equidistanti di <span class="math inline">\(\theta\)</span> nell‚Äôintervallo [0, 1] e calcoliamo la verosimiglianza per ciascuno di questi valori.</p>
<div id="cb433da7-f10f-472c-9a5c-cf312d42ff68" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">23</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Creiamo i possibili valori del parametro <span class="math inline">\(\theta\)</span> per i quali calcoleremo la verosimiglianza.</p>
<div id="06e02b06-0fb8-4843-be54-dfadd07c6315" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.linspace(<span class="fl">0.0</span>, <span class="fl">1.0</span>, num<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(theta)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.         0.01010101 0.02020202 0.03030303 0.04040404 0.05050505
 0.06060606 0.07070707 0.08080808 0.09090909 0.1010101  0.11111111
 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717
 0.18181818 0.19191919 0.2020202  0.21212121 0.22222222 0.23232323
 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929
 0.3030303  0.31313131 0.32323232 0.33333333 0.34343434 0.35353535
 0.36363636 0.37373737 0.38383838 0.39393939 0.4040404  0.41414141
 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747
 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354
 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.5959596
 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566
 0.66666667 0.67676768 0.68686869 0.6969697  0.70707071 0.71717172
 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778
 0.78787879 0.7979798  0.80808081 0.81818182 0.82828283 0.83838384
 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.8989899
 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596
 0.96969697 0.97979798 0.98989899 1.        ]</code></pre>
</div>
</div>
<p>Per esempio, ponendo <span class="math inline">\(\theta = 0.1\)</span> otteniamo il seguente valore dell‚Äôordinata della funzione di verosimiglianza:</p>
<p><span class="math display">\[
\mathcal{L}(\theta \mid y) = \frac{(23 + 7)!}{23!7!} 0.1^{23} + (1-0.1)^7.
\]</span></p>
<div id="3747aefb-2424-4fef-8ad5-724738c9c093" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>stats.binom.pmf(y, n, <span class="fl">0.1</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>9.7371682902e-18</code></pre>
</div>
</div>
<p>Ponendo <span class="math inline">\(\theta = 0.2\)</span> otteniamo il seguente valore dell‚Äôordinata della funzione di verosimiglianza:</p>
<p><span class="math display">\[
\mathcal{L}(\theta \mid y) = \frac{(23 + 7)!}{23!7!} 0.2^{23} + (1-0.2)^7.
\]</span></p>
<div id="fe975edb-365a-4471-a305-ec16385f24bc" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>stats.binom.pmf(y, n, <span class="fl">0.2</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>3.58141723492221e-11</code></pre>
</div>
</div>
<p>Se ripetiamo questo processo 100 volte, una volta per ciascuno dei valori <span class="math inline">\(\theta\)</span> che abbiamo elencato sopra, otteniamo 100 coppie di punti <span class="math inline">\(\theta\)</span> e <span class="math inline">\(f(\theta)\)</span>. A tale fine, definiamo la seguente funzione.</p>
<div id="7a114165-85fa-4a71-a379-4e38efae1a81" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> like(r, n, theta):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> math.comb(n, r) <span class="op">*</span> theta<span class="op">**</span>r <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> theta) <span class="op">**</span> (n <span class="op">-</span> r)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La curva che interpola i punti ottenuti √® la funzione di verosimiglianza, come indicato dalla figura seguente.</p>
<div id="5e75d37a-ae54-41e0-ae27-56cf7e3b7e95" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>plt.plot(theta, like(r<span class="op">=</span>y, n<span class="op">=</span>n, theta<span class="op">=</span>theta), <span class="st">"-"</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Funzione di verosimiglianza"</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Valore della variabile casuale theta [0, 1]"</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Verosimiglianza"</span>)<span class="op">;</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="09_likelihood_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
<section id="interpretazione-della-funzione-di-verosimiglianza" class="level3" data-number="32.2.1">
<h3 data-number="32.2.1" class="anchored" data-anchor-id="interpretazione-della-funzione-di-verosimiglianza"><span class="header-section-number">32.2.1</span> Interpretazione della Funzione di Verosimiglianza</h3>
<p>L‚Äôinterpretazione della funzione di verosimiglianza ci permette di misurare l‚Äôadattamento dei vari valori di <span class="math inline">\(\theta\)</span> ai dati. Il valore che massimizza la funzione indica la stima pi√π plausibile di <span class="math inline">\(\theta\)</span> dati i dati osservati. In termini pratici, se per esempio il valore che massimizza la verosimiglianza √® <span class="math inline">\(\theta = 0.767\)</span>, ci√≤ suggerisce che la probabilit√† pi√π plausibile di successo (o atteggiamento negativo) nella nostra popolazione di studio √® del 76.7%.</p>
<p>La determinazione numerica di questo valore ottimale pu√≤ avvenire attraverso tecniche computazionali, come l‚Äôidentificazione del punto di massimo della funzione di verosimiglianza tramite metodi di ottimizzazione. L‚Äôuso di librerie statistiche e matematiche in linguaggi di programmazione come Python consente di effettuare queste analisi con precisione e efficienza, offrendo una stima accurata del parametro <span class="math inline">\(\theta\)</span> che meglio si adatta ai dati osservati.</p>
<p>Questa metodologia, basata sull‚Äôuso della funzione di verosimiglianza, √® cruciale per l‚Äôinferenza statistica, permettendo agli scienziati di stimare i parametri dei modelli statistici in modo informato e di valutare l‚Äôadeguatezza di tali modelli in rappresentanza dei dati reali.</p>
<p>In pratica, per identificare numericamente il valore ottimale di <span class="math inline">\(\theta\)</span>, si pu√≤ localizzare l‚Äôindice nel vettore dei valori di verosimiglianza dove questa raggiunge il suo picco. Metodi computazionali, come l‚Äôuso della funzione <code>argmax</code> in NumPy, possono automatizzare questo processo. Una volta individuato l‚Äôindice che massimizza la verosimiglianza, si pu√≤ risalire al valore corrispondente di <span class="math inline">\(\theta\)</span> nel vettore dei parametri, ottenendo cos√¨ la stima di <span class="math inline">\(\theta\)</span> che rende i dati osservati pi√π plausibili.</p>
<div id="c30a330a-6f10-4a8b-97ae-4d6581fe51ac" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> like(r<span class="op">=</span>y, n<span class="op">=</span>n, theta<span class="op">=</span>theta)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>l.argmax()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>76</code></pre>
</div>
</div>
<div id="b0342493-69a4-499d-b5c0-a99e057e20e9" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>theta[<span class="dv">76</span>]</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>0.7676767676767677</code></pre>
</div>
</div>
<p>√à importante notare che, invece di utilizzare la funzione <code>like()</code> che abbiamo definito precedentemente per motivi didattici, √® possibile ottenere lo stesso risultato utilizzando in modo equivalente la funzione <code>binom.pmf()</code>.</p>
<div id="e3ed8345-e756-497e-a65f-842d03a2ab10" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>plt.plot(theta, stats.binom.pmf(y, n, theta), <span class="st">"-"</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Funzione di verosimiglianza"</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Valore della variabile casuale theta [0, 1]"</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Verosimiglianza"</span>)<span class="op">;</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="09_likelihood_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="massima-verosimiglianza" class="level2" data-number="32.3">
<h2 data-number="32.3" class="anchored" data-anchor-id="massima-verosimiglianza"><span class="header-section-number">32.3</span> Massima verosimiglianza</h2>
<p>Tra tutti i possibili valori <span class="math inline">\(\theta\)</span> cerchiao il valore che massimizzi la probabilit√† dei dati osservati, ovvero, cerchiamo il valore <span class="math inline">\(\theta\)</span> che corrisponde al massimo della funzione di verosimiglianza.</p>
<p>Parliamo di ‚Äúminimizzazione‚Äù quando l ‚Äôobiettivo √® trovare il punto pi√π basso in una valle (minimizzare) o di ‚Äúmassimizzazione‚Äù, quando l‚Äôobiettivo √® quello di trovare il punto pi√π alto su una collina, a seconda della funzione. Nel caso della funzione di verosimiglianza, cerchiamo il punto in cui questa funzione raggiunge il suo valore massimo, ma poich√© molti algoritmi sono progettati per trovare minimi, possiamo cercare il minimo del negativo della funzione di verosimiglianza, che corrisponde al massimo della funzione stessa.</p>
<p>La Strategia di Base</p>
<ul>
<li>Punto di Partenza: L‚Äôalgoritmo inizia da un punto di partenza, che pu√≤ essere scelto casualmente o basato su una qualche ipotesi ragionevole.</li>
<li>Esplorazione: L‚Äôalgoritmo esplora la ‚Äúsuperficie‚Äù della funzione, muovendosi in direzioni che sembrano portare verso il punto pi√π basso (o pi√π alto, se stiamo massimizzando). Questo √® simile a sentire la pendenza del terreno intorno a noi per decidere in quale direzione camminare.</li>
<li>Aggiustamento: Man mano che procede, l‚Äôalgoritmo aggiusta la sua traiettoria basandosi su ci√≤ che ha ‚Äúsentito‚Äù durante l‚Äôesplorazione. Se trova una discesa, continua in quella direzione; se incontra una salita, prova una direzione differente.</li>
<li>Convergenza: Il processo continua finch√© l‚Äôalgoritmo non trova un punto in cui non ci sono pi√π discese significative in nessuna direzione, suggerendo che ha trovato il punto pi√π basso (o il punto pi√π alto, se stiamo massimizzando) raggiungibile da quel percorso.</li>
</ul>
<p>Esistono diversi metodi che l‚Äôalgoritmo pu√≤ utilizzare per decidere come muoversi. Alcuni esempi includono:</p>
<ul>
<li>Discesa pi√π ripida (Gradient Descent): Utilizza il gradiente (la direzione e la pendenza della collina) per decidere in quale direzione muoversi.</li>
<li>Newton-Raphson: Utilizza sia il gradiente sia la ‚Äúcurvatura‚Äù della funzione per fare passi pi√π informati verso il minimo.</li>
<li>Algoritmi Genetici: Ispirati dall‚Äôevoluzione biologica, questi algoritmi ‚Äúevolvono‚Äù una soluzione attraverso iterazioni che simulano la selezione naturale.</li>
</ul>
<p>In termini intuitivi, dunque, l‚Äôottimizzazione √® un processo metodico di esplorazione e aggiustamento basato su feedback immediato dalla funzione che stiamo cercando di ottimizzare, con l‚Äôobiettivo di trovare il punto di massimo o minimo valore.</p>
<p>Definiamo dunque il negativo della funzione di verosimiglianza per l‚Äôottimizzazione (trovare il massimo della funzione):</p>
<div id="bc09df21-31de-43d3-99b4-8cbdfc55c8f1" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> negative_likelihood(theta, n, y):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calcolo del negativo della funzione di verosimiglianza</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span> (math.comb(n, y) <span class="op">*</span> theta<span class="op">**</span>y <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> theta) <span class="op">**</span> (n <span class="op">-</span> y))</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Utilizziamo ora <code>scipy.optimize.minimize</code> per trovare il valore di theta che massimizza la verosimiglianza. Bisogna specificare un valore iniziale per theta, qui assumiamo 0.5 come punto di partenza. I vincoli su theta sono che deve essere compreso tra 0 e 1.</p>
<div id="0eab72ba-76f4-41c3-a8cc-49409180d35d" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> minimize(negative_likelihood, x0<span class="op">=</span><span class="fl">0.5</span>, args<span class="op">=</span>(n, y), bounds<span class="op">=</span>[(<span class="dv">0</span>, <span class="dv">1</span>)])</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>result.x</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>array([0.76666666])</code></pre>
</div>
</div>
<section id="la-funzione-di-log-verosimiglianza" class="level3" data-number="32.3.1">
<h3 data-number="32.3.1" class="anchored" data-anchor-id="la-funzione-di-log-verosimiglianza"><span class="header-section-number">32.3.1</span> La Funzione di Log-Verosimiglianza</h3>
<p>Proseguendo con il nostro approfondimento sull‚Äôanalisi statistica mediante la funzione di verosimiglianza, ci spostiamo verso una sua trasformazione matematica spesso preferita dagli statistici: la funzione di log-verosimiglianza. Il passaggio alla log-verosimiglianza, definita come il logaritmo naturale della funzione di verosimiglianza:</p>
<p><span id="eq-loglike-definition"><span class="math display">\[
\ell(\theta) = \log \mathcal{L}(\theta \mid y),
\tag{32.1}\]</span></span></p>
<p>non altera la posizione del massimo della funzione originale grazie alla propriet√† di monotonicit√† del logaritmo. In termini pratici, ci√≤ significa che il valore di <span class="math inline">\(\theta\)</span> che massimizza la log-verosimiglianza, <span class="math inline">\(\hat{\theta}\)</span>, √® lo stesso che massimizza la verosimiglianza originale:</p>
<p><span class="math display">\[
\hat{\theta} = \arg \max_{\theta \in \Theta} \ell(\theta) = \arg \max_{\theta \in \Theta} \mathcal{L}(\theta).
\]</span></p>
<p>Nell‚Äôanalisi di un campione di osservazioni, l‚Äôuso della log-verosimiglianza semplifica il processo di massimizzazione, che pu√≤ risultare complicato con la verosimiglianza tradizionale, soprattutto quando si gestiscono numeri molto piccoli. Questa semplificazione avviene perch√© la log-verosimiglianza trasforma il prodotto delle probabilit√† in una somma di logaritmi, rendendo il problema pi√π semplice e numericamente stabile. L‚Äôespressione della log-verosimiglianza per un modello binomiale, ad esempio, si presenta come segue:</p>
<p><span class="math display">\[
\ell(\theta \mid y) = \log(\theta^y (1 - \theta)^{n - y}) = y \log(\theta) + (n - y) \log(1 - \theta).
\]</span></p>
<p>Questa formulazione trasforma il prodotto delle probabilit√† di osservazioni indipendenti in una somma, facilitando notevolmente i calcoli, specialmente per dataset di grandi dimensioni o in presenza di calcoli complessi. La forma logaritmica √® pi√π gestibile e si presta meglio all‚Äôapplicazione di tecniche di ottimizzazione numerica, grazie alla sua maggiore stabilit√† e alla riduzione di problemi come l‚Äôunderflow, comuni quando si lavora con probabilit√† molto piccole.</p>
<p>Ritornando all‚Äôesempio della distribuzione binomiale, l‚Äôapplicazione della log-verosimiglianza per il calcolo del parametro <span class="math inline">\(\theta\)</span> che meglio si adatta ai dati osservati pu√≤ essere eseguita con efficienza attraverso metodi computazionali. Per esempio, l‚Äôutilizzo di funzioni specifiche disponibili in pacchetti statistici, come <code>binom.logpmf()</code> in Python, permette di calcolare direttamente la log-verosimiglianza di un dato set di osservazioni per diversi valori di <span class="math inline">\(\theta\)</span>. Questo approccio facilita la ricerca del valore di <span class="math inline">\(\theta\)</span> che massimizza la log-verosimiglianza, fornendo una stima accurata e computazionalmente efficiente del parametro.</p>
<p>L‚Äôadozione della funzione di log-verosimiglianza, quindi, non solo consente di affrontare i limiti pratici legati alla manipolazione di piccole probabilit√†, ma offre anche un quadro concettuale chiaro per l‚Äôinterpretazione della plausibilit√† dei parametri del modello alla luce dei dati osservati. Questa trasformazione logaritmica rappresenta un passaggio cruciale nell‚Äôanalisi inferenziale, consentendo di stimare i parametri dei modelli statistici con maggiore precisione e affidabilit√†.</p>
<p>Per illustrare questo concetto, riprendiamo l‚Äôesempio precedente e applichiamo la funzione di log-verosimiglianza per identificare il valore di $ $ che massimizza questa funzione. La rappresentazione grafica della funzione di log-verosimiglianza fornisce ulteriori intuizioni sul comportamento di questa funzione.</p>
<div id="e59a4a11-fdcd-41e4-9f8f-c526fe5f1c47" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> <span class="dv">23</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>plt.plot(theta, stats.binom.logpmf(y, n, theta), <span class="st">"-"</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Funzione di log-verosimiglianza"</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Valore della variabile casuale theta [0, 1]"</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Log-verosimiglianza"</span>)<span class="op">;</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="09_likelihood_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Il risultato replica quello trovato in precedenza con la funzione di verosimiglianza.</p>
<div id="51fe9970-46c4-4fbd-a554-97d20dcbeac5" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>ll <span class="op">=</span> stats.binom.logpmf(y, n, theta)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>ll.argmax()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>76</code></pre>
</div>
</div>
<div id="bd008466-3f1d-4bb7-9705-6f97c6d4fc28" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>theta[<span class="dv">76</span>]</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>0.7676767676767677</code></pre>
</div>
</div>
<p>Definizione della funzione del negativo della log-verosimiglianza con correzioni per evitare errori di dominio:</p>
<div id="078c29c3-3d45-4b26-a774-0154caa16ec9" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> corrected_negative_log_likelihood(theta, n, y):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Assicurarsi che theta sia all'interno di un intervallo valido per evitare errori di logaritmo</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.clip(theta, <span class="fl">1e-10</span>, <span class="dv">1</span><span class="op">-</span><span class="fl">1e-10</span>)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span> (y <span class="op">*</span> np.log(theta) <span class="op">+</span> (n <span class="op">-</span> y) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">-</span> theta))</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Utilizzo di <code>scipy.optimize.minimize</code> per trovare il valore di theta che massimizza la log-verosimiglianza:</p>
<div id="1823ee04-f058-424c-9188-f50d01867cfb" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>result_log_likelihood_corrected <span class="op">=</span> minimize(</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    corrected_negative_log_likelihood, x0<span class="op">=</span>[<span class="fl">0.5</span>], args<span class="op">=</span>(n, y), bounds<span class="op">=</span>[(<span class="dv">0</span>, <span class="dv">1</span>)]</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="a424f60a-c8a9-43ce-973c-f9dafbf9f2f4" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Il risultato ottimizzato per theta utilizzando la log-verosimiglianza corretta</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>optimized_theta <span class="op">=</span> result_log_likelihood_corrected.x</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>optimized_theta</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>array([0.76666666])</code></pre>
</div>
</div>
</section>
<section id="verosimiglianza-congiunta" class="level3" data-number="32.3.2">
<h3 data-number="32.3.2" class="anchored" data-anchor-id="verosimiglianza-congiunta"><span class="header-section-number">32.3.2</span> Verosimiglianza Congiunta</h3>
<p>Proseguendo nella nostra esplorazione dell‚Äôinferenza statistica attraverso la funzione di verosimiglianza, ci concentriamo ora sul caso in cui abbiamo pi√π osservazioni, tutte provenienti dalla stessa distribuzione binomiale e considerate indipendenti ed identicamente distribuite (IID). Tale scenario si presenta frequentemente nelle applicazioni pratiche, dove un insieme di <span class="math inline">\(n\)</span> osservazioni <span class="math inline">\(Y = [y_1, y_2, \ldots, y_n]\)</span> viene raccolto sotto le stesse condizioni sperimentali.</p>
<p>La chiave per analizzare queste osservazioni congiuntamente risiede nel calcolo della probabilit√† congiunta di <span class="math inline">\(y_1, y_2, \ldots, y_n\)</span> data un‚Äôunica probabilit√† di successo <span class="math inline">\(\theta\)</span> comune a tutte le prove. L‚Äôindipendenza delle osservazioni ci consente di esprimere questa probabilit√† congiunta come il prodotto delle probabilit√† individuali di ciascuna osservazione:</p>
<p><span class="math display">\[
p(y_1, y_2, \ldots, y_n \mid \theta) = \prod_{i=1}^{n} p(y_i \mid \theta) = \prod_{i=1}^{n} \text{Binomiale}(y_i \mid \theta).
\]</span></p>
<p>La bellezza di questo approccio sta nel fatto che la verosimiglianza congiunta, che rappresenta la plausibilit√† complessiva di <span class="math inline">\(\theta\)</span> data l‚Äôintera sequenza di osservazioni <span class="math inline">\(Y\)</span>, √® semplicemente il prodotto delle verosimiglianze individuali di ogni osservazione <span class="math inline">\(y_i\)</span> rispetto a <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
\mathcal{L}(\theta \mid Y) = \prod_{i=1}^{n} \mathcal{L}(\theta \mid y_i) = \prod_{i=1}^{n} p(y_i \mid \theta).
\]</span></p>
<p>Questa formulazione della verosimiglianza congiunta non solo evidenzia quanto bene il parametro <span class="math inline">\(\theta\)</span> si adatta all‚Äôintero set di dati <span class="math inline">\(Y\)</span>, ma offre anche una base metodologica solida per stimare <span class="math inline">\(\theta\)</span>. Il parametro che massimizza la verosimiglianza congiunta, noto come stimatore di massima verosimiglianza (MLE) di <span class="math inline">\(\theta\)</span>, √® quello che si ritiene essere il pi√π plausibile data l‚Äôosservazione dei dati.</p>
<p>Quando abbiamo pi√π gruppi di osservazioni bernoulliane indipendenti ed identicamente distribuite (iid), la funzione di log-verosimiglianza congiunta per tutti i gruppi pu√≤ essere espressa come la somma delle log-verosimiglianze di ciascun gruppo. Ci√≤ √® dovuto alla propriet√† che il logaritmo del prodotto √® la somma dei logaritmi.</p>
<p>Supponiamo di avere i seguenti dati per 4 gruppi di osservazioni:</p>
<ul>
<li>Gruppo 1: 30 prove con 23 successi</li>
<li>Gruppo 2: 28 prove con 21 successi</li>
<li>Gruppo 3: 40 prove con 31 successi</li>
<li>Gruppo 4: 36 prove con 29 successi</li>
</ul>
<p>La funzione di log-verosimiglianza congiunta per questi dati, assumendo una singola probabilit√† di successo <span class="math inline">\(\theta\)</span> per tutti i gruppi, √® data da:</p>
<p><span class="math display">\[
\log L(\theta) = \sum_{i=1}^{4} \left[ y_i \log(\theta) + (n_i - y_i) \log(1 - \theta) \right],
\]</span></p>
<p>dove <span class="math inline">\(n_i\)</span> e <span class="math inline">\(y_i\)</span> sono rispettivamente il numero di prove e il numero di successi nel <span class="math inline">\(i\)</span>-esimo gruppo.</p>
<p>Per trovare il valore di <span class="math inline">\(\theta\)</span> che massimizza questa funzione di log-verosimiglianza, possiamo usare il metodo di ottimizzazione <code>scipy.optimize.minimize</code>, come abbiamo fatto in precedenza. Definiamo prima la funzione di log-verosimiglianza congiunta (usiamo <code>np.clip</code> per evitare errori):</p>
<div id="aaf8a603-f96a-481b-aac7-91129e725d99" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_verosimiglianza_congiunta(theta, dati):</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.clip(theta, <span class="fl">1e-10</span>, <span class="dv">1</span><span class="op">-</span><span class="fl">1e-10</span>)  <span class="co"># Evita valori esattamente 0 o 1</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    log_likelihood <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n, y <span class="kw">in</span> dati:</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>        log_likelihood <span class="op">+=</span> y <span class="op">*</span> np.log(theta) <span class="op">+</span> (n <span class="op">-</span> y) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">-</span> theta)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>log_likelihood  <span class="co"># Restituisce il negativo per l'ottimizzazione</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="f35f2fd5-2c3b-484a-8aea-f44e7f72b55d" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Dati dei gruppi: (prove, successi)</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>dati_gruppi <span class="op">=</span> [(<span class="dv">30</span>, <span class="dv">23</span>), (<span class="dv">28</span>, <span class="dv">20</span>), (<span class="dv">40</span>, <span class="dv">29</span>), (<span class="dv">36</span>, <span class="dv">29</span>)]</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dati_gruppi)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[(30, 23), (28, 20), (40, 29), (36, 29)]</code></pre>
</div>
</div>
<p>Ottimizzazione con la funzione <code>log_verosimiglianza_congiunta</code></p>
<div id="7c569f4a-d7f4-4ea3-94e7-86d300a46a9c" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> minimize(</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    log_verosimiglianza_congiunta, x0<span class="op">=</span>[<span class="fl">0.5</span>], args<span class="op">=</span>(dati_gruppi,), bounds<span class="op">=</span>[(<span class="dv">0</span>, <span class="dv">1</span>)]</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Il risultato ottimizzato per theta con la funzione corretta</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>result.x</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>array([0.75373134])</code></pre>
</div>
</div>
<div id="300c7b56-61a2-4b78-b23a-8e659c06b2b9" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Intervallo di valori di theta da esplorare</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>theta_values <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="dv">100</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcolo dei valori di log-verosimiglianza per ogni theta</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>log_likelihood_values <span class="op">=</span> [log_verosimiglianza_congiunta(theta, dati_gruppi) <span class="cf">for</span> theta <span class="kw">in</span> theta_values]</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Creazione del grafico</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>plt.plot(theta_values, log_likelihood_values, label<span class="op">=</span><span class="st">'Log-verosimiglianza'</span>)</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Theta'</span>)</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Log-verosimiglianza negativa'</span>)</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Funzione di Log-verosimiglianza'</span>)</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="09_likelihood_files/figure-html/cell-24-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="la-verosimiglianza-marginale" class="level2" data-number="32.4">
<h2 data-number="32.4" class="anchored" data-anchor-id="la-verosimiglianza-marginale"><span class="header-section-number">32.4</span> La Verosimiglianza Marginale</h2>
<p>Avanzando nella nostra discussione sulla verosimiglianza, approfondiamo ora un passaggio cruciale nell‚Äôapplicazione della teoria bayesiana: il concetto di verosimiglianza marginale. Questo approccio si rivela essenziale quando affrontiamo situazioni in cui il parametro di interesse, <span class="math inline">\(\theta\)</span>, non √® definito da un valore singolo e fisso, ma √® invece descritto da una distribuzione di probabilit√† che riflette la nostra incertezza o variabilit√† su di esso.</p>
<p>In contesti pratici, non √® raro incontrare scenari in cui <span class="math inline">\(\theta\)</span> pu√≤ assumere una gamma di valori, ciascuno con una probabilit√† associata, piuttosto che un valore deterministico. L‚Äôintegrazione del parametro <span class="math inline">\(\theta\)</span> permette di calcolare la probabilit√† complessiva (o verosimiglianza) di osservare un determinato risultato dati tutti i possibili valori di <span class="math inline">\(\theta\)</span>, piuttosto che appoggiarsi a un‚Äôanalisi basata su un singolo valore di <span class="math inline">\(\theta\)</span>.</p>
<p>Consideriamo, per esempio, una situazione in cui stiamo osservando una sequenza di prove binomiali, con un risultato specifico di interesse (ad esempio, <span class="math inline">\(k=7\)</span> successi su <span class="math inline">\(n=10\)</span> prove). Se <span class="math inline">\(\theta\)</span> rappresenta la probabilit√† di successo in ciascuna prova e pu√≤ assumere un insieme discreto di valori (per esempio, 0.1, 0.5, e 0.9) con probabilit√† uniforme, la verosimiglianza di osservare il nostro risultato specifico pu√≤ essere espressa come:</p>
<p><span class="math display">\[p(k=7, n=10) = \sum_{\theta \in \{0.1, 0.5, 0.9\}} \binom{10}{7} \theta^7 (1-\theta)^3 p(\theta),\]</span></p>
<p>dove <span class="math inline">\(p(\theta)\)</span> rappresenta la probabilit√† associata a ciascun possibile valore di <span class="math inline">\(\theta\)</span>.</p>
<p>Tuttavia, in molte applicazioni reali, <span class="math inline">\(\theta\)</span> pu√≤ variare continuamente all‚Äôinterno di un intervallo, come tra 0 e 1 per una distribuzione binomiale. In questi casi, il calcolo della verosimiglianza marginale richiede l‚Äôutilizzo dell‚Äôintegrazione su tutto lo spazio dei valori possibili di <span class="math inline">\(\theta\)</span>, riflettendo la gamma continua di possibili probabilit√† di successo. La formula si estende quindi a:</p>
<p><span class="math display">\[p(k=7, n=10) = \int_{0}^{1} \binom{10}{7} \theta^7 (1-\theta)^3 p(\theta) d\theta,\]</span></p>
<p>dove <span class="math inline">\(p(\theta) d\theta\)</span> rappresenta la densit√† di probabilit√† di <span class="math inline">\(\theta\)</span> su un intervallo infinitesimale, e l‚Äôintegrale copre tutti i possibili valori di <span class="math inline">\(\theta\)</span> da 0 a 1. Implementare questo calcolo nell‚Äôambito di uno spazio continuo richiede l‚Äôutilizzo di tecniche di integrazione.</p>
<p>Vediamo come sia possibile eseguire questo calcolo in Python, utilizzando la libreria <code>scipy</code> per l‚Äôintegrazione su uno spazio continuo:</p>
<div id="ba1aaa69-7d39-4461-9a78-e5dad7b098d2" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Definire la funzione di verosimiglianza</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> likelihood(theta):</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> stats.binom.pmf(k<span class="op">=</span><span class="dv">7</span>, n<span class="op">=</span><span class="dv">10</span>, p<span class="op">=</span>theta)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcolare la verosimiglianza marginale integrando su Œ∏</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>marginal_likelihood, _ <span class="op">=</span> quad(<span class="kw">lambda</span> theta: likelihood(theta), <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"La verosimiglianza marginale √®:"</span>, marginal_likelihood)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>La verosimiglianza marginale √®: 0.09090909090909094</code></pre>
</div>
</div>
<p>Questo codice esegue l‚Äôintegrazione della funzione di verosimiglianza binomiale su tutti i possibili valori di Œ∏ (da 0 a 1), fornendo cos√¨ la verosimiglianza marginale per il nostro esempio. Questo processo ci permette di considerare l‚Äôincertezza su Œ∏, offrendo una visione completa della verosimiglianza dell‚Äôevento osservato senza fissare Œ∏ a un singolo valore.</p>
<p>Numericamente, nell‚Äôesempio della verosimiglianza basata su una distribuzione binomiale precedente, la verosimiglianza marginale √® effettivamente interpretata come l‚Äôarea sottesa dalla funzione di verosimiglianza, calcolata integrandola su tutto l‚Äôintervallo dei possibili valori di <span class="math inline">\(\theta\)</span> (da 0 a 1, nel contesto di probabilit√†). Questa operazione di integrazione fornisce un valore che quantifica l‚Äôarea sotto la curva della funzione di verosimiglianza. Importante sottolineare, questo valore non corrisponde alla probabilit√† dei dati dati i parametri, dato che la verosimiglianza non √® una densit√† di probabilit√† sui parametri. Piuttosto, esso misura in che misura l‚Äôintero modello, considerando tutti i possibili valori del parametro, √® in grado di spiegare i dati osservati.</p>
<p>La vera importanza della verosimiglianza marginale emerge nel contesto dell‚Äôinferenza bayesiana: essa agisce come fattore di normalizzazione nella formula di Bayes. Nello specifico, la verosimiglianza marginale normalizza la funzione risultante dal prodotto tra la verosimiglianza e la distribuzione a priori dei parametri (il numeratore nella formula di Bayes), garantendo che il risultato sia una distribuzione di probabilit√† valida sullo spazio dei parametri. In altre parole, la verosimiglianza marginale assicura che l‚Äôarea sotto la curva della distribuzione posteriore sia esattamente 1, rendendola cos√¨ una vera distribuzione di probabilit√†.</p>
</section>
<section id="modello-gaussiano-e-verosimiglianza" class="level2" data-number="32.5">
<h2 data-number="32.5" class="anchored" data-anchor-id="modello-gaussiano-e-verosimiglianza"><span class="header-section-number">32.5</span> Modello Gaussiano e Verosimiglianza</h2>
<p>Ampliamo ora la nostra analisi al caso della distribuzione gaussiana. Inizieremo con la verosimiglianza associata a una singola osservazione $ Y $, per poi estendere la discussione a un insieme di osservazioni gaussiane indipendenti e identicamente distribuite (IID).</p>
<section id="caso-di-una-singola-osservazione" class="level3" data-number="32.5.1">
<h3 data-number="32.5.1" class="anchored" data-anchor-id="caso-di-una-singola-osservazione"><span class="header-section-number">32.5.1</span> Caso di una Singola Osservazione</h3>
<p>Iniziamo esaminiamo il caso di una singola osservazione. Quale esempio, prendiamo in considerazione la situazione in cui una variabile casuale rappresenta il Quoziente d‚ÄôIntelligenza (QI) di un individuo. Se consideriamo la distribuzione del QI come gaussiana, possiamo esprimere la funzione di verosimiglianza per un singolo valore osservato di QI tramite la formula della distribuzione gaussiana, che misura la probabilit√† di osservare quel particolare valore di QI dato un insieme di parametri specifici, <span class="math inline">\(\mu\)</span> (la media) e <span class="math inline">\(\sigma\)</span> (la deviazione standard). La verosimiglianza offre quindi un modo per quantificare quanto bene i parametri <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma\)</span> si accordano con il valore osservato di QI.</p>
<p>Supponiamo che il QI osservato sia 114 e, per semplicit√†, assumiamo che la deviazione standard <span class="math inline">\(\sigma\)</span> sia conosciuta e pari a 15. Vogliamo esaminare un‚Äôampia gamma di possibili valori per la media <span class="math inline">\(\mu\)</span>, diciamo tra 70 e 160, e valutare quale di questi valori rende pi√π plausibile l‚Äôosservazione fatta Definiamo quindi un insieme di 1000 valori per <span class="math inline">\(\mu\)</span> da esplorare:</p>
<div id="1e7bf1a3-c601-42fb-8312-db4033571208" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> np.linspace(<span class="fl">70.0</span>, <span class="fl">160.0</span>, num<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">114</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La nostra analisi consiste nell‚Äôapplicare la funzione di densit√† di probabilit√† gaussiana a ciascuno di questi 1000 valori di <span class="math inline">\(\mu\)</span>, mantenendo fisso il valore osservato di QI, <span class="math inline">\(y=114\)</span>, e la deviazione standard, <span class="math inline">\(\sigma=15\)</span>. In questo modo, possiamo costruire la funzione di verosimiglianza che esprime la plausibilit√† di ciascun valore di <span class="math inline">\(\mu\)</span> alla luce del QI osservato.</p>
<p>Il calcolo specifico della densit√† di probabilit√† per ogni valore di <span class="math inline">\(\mu\)</span> pu√≤ essere eseguito con la funzione <code>norm.pdf</code> di <code>scipy.stats</code>, che accetta il valore osservato <span class="math inline">\(y\)</span>, un array di medie (i nostri valori di <span class="math inline">\(\mu\)</span>) e la deviazione standard <span class="math inline">\(\sigma\)</span>. Per un singolo valore <code>mu</code> = 70, otteniamo</p>
<div id="5979501b-ef77-429a-85e0-538901b4d4f8" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>stats.norm.pdf(y, loc<span class="op">=</span><span class="dv">70</span>, scale<span class="op">=</span><span class="dv">15</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>0.00036007041207962535</code></pre>
</div>
</div>
<p>Per il valore <code>mu</code> = 70.05 otteniamo</p>
<div id="53682fa2-40c9-4ce6-b8c8-600b08543ba1" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>stats.norm.pdf(y, loc<span class="op">=</span><span class="fl">70.05</span>, scale<span class="op">=</span><span class="dv">15</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>0.00036360634900376967</code></pre>
</div>
</div>
<p>e cos√¨ via. Se usiamo utti i 1000 valori possibili di <code>mu</code>, otteniamo un vettore di 1000 risultati:</p>
<div id="726e752b-6324-4b3f-b1d2-f2b0556cccde" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>f_mu <span class="op">=</span> stats.norm.pdf(y, loc<span class="op">=</span>mu, scale<span class="op">=</span><span class="dv">15</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Questo passaggio ci fornisce un array di valori che rappresentano la verosimiglianza di ciascun valore di <span class="math inline">\(\mu\)</span> data l‚Äôosservazione <span class="math inline">\(y\)</span>. Tracciando questi valori <code>f_mu</code> in funzione di <span class="math inline">\(\mu\)</span>, otteniamo una curva di verosimiglianza che illustra visivamente quanto bene ciascun valore di <span class="math inline">\(\mu\)</span> si adatta al dato osservato <code>y</code> = 114:</p>
<div id="fd4c49fa-4144-4874-a9db-8fe0b3d1910a" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>plt.plot(mu, f_mu, <span class="st">"-"</span>)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Funzione di verosimiglianza per QI = 114"</span>)</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Valore di mu [70, 160]"</span>)</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Verosimiglianza"</span>)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>plt.xlim([<span class="dv">70</span>, <span class="dv">160</span>])</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="09_likelihood_files/figure-html/cell-30-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Abbiamo dunque proceduto come nel caso della distribuzione binomiale esaminata in precedenza. Abbiamo utilizzato la formula</p>
<p><span class="math display">\[
f(x | \mu, \sigma) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right),
\]</span></p>
<p>tenendo costante il valore <span class="math inline">\(x\)</span> = 114 e considerando noto <span class="math inline">\(\sigma\)</span> = 15, e abbiamo applicato la formula 1000 volte facendo variare <code>mu</code> ogni volta utilizziando ciascuno dei valori definiti con <code>np.linspace(70.0, 160.0, num=1000)</code>.</p>
<p>La moda della distribuzione, si trova con</p>
<div id="47eabf93-c1d8-4cc9-841f-be3f70daf2b9" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>optimal_mu <span class="op">=</span> mu[f_mu.argmax()]</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(optimal_mu)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>113.96396396396396</code></pre>
</div>
</div>
<p>In questo esempio, otteniamo il valore <span class="math inline">\(\mu\)</span> = 113.96 che massimizza la verosimiglianza.</p>
<p>Per calcolare il massimo della log-verosimiglianza per una distribuzione Gaussiana usando la funzione <code>optimize()</code> di SciPy, possiamo seguire questi passi. Partiamo dalla formula della densit√† di probabilit√† della distribuzione gaussiana per una singola osservazione <span class="math inline">\(y\)</span>, con media <span class="math inline">\(\mu\)</span> e deviazione standard <span class="math inline">\(\sigma\)</span>. La formula √®:</p>
<p><span class="math display">\[
f(y \mid \mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi}} \exp \left( -\frac{(y - \mu)^2}{2\sigma^2} \right)
\]</span></p>
<p>Poich√© abbiamo una singola osservazione <span class="math inline">\(y\)</span>, la funzione di verosimiglianza coincide con la funzione di densit√† di probabilit√†. Quindi, prendiamo il logaritmo naturale di entrambi i lati della equazione della densit√† di probabilit√† gaussiana per ottenere la log-verosimiglianza:</p>
<p><span class="math display">\[
\log f(y \mid \mu, \sigma) = \log \left( \frac{1}{\sigma \sqrt{2\pi}} \exp \left( -\frac{(y - \mu)^2}{2\sigma^2} \right) \right)
\]</span></p>
<p>Applichiamo le propriet√† dei logaritmi. Ricordiamo che:</p>
<ul>
<li><span class="math inline">\(\log(ab) = \log(a) + \log(b)\)</span></li>
<li><span class="math inline">\(\log\left(\frac{1}{a}\right) = -\log(a)\)</span></li>
<li><span class="math inline">\(\log(e^x) = x\)</span></li>
</ul>
<p>Quindi, possiamo scrivere:</p>
<p><span class="math display">\[
\log f(y \mid \mu, \sigma) = \log\left(\frac{1}{\sigma \sqrt{2\pi}}\right) + \log\left(\exp \left( -\frac{(y - \mu)^2}{2\sigma^2} \right)\right)
\]</span></p>
<p><span class="math display">\[
= -\log(\sigma \sqrt{2\pi}) -\frac{(y - \mu)^2}{2\sigma^2}.
\]</span></p>
<p>Ricordando che <span class="math inline">\(\log(ab) = \log(a) + \log(b)\)</span>, possiamo scrivere <span class="math inline">\(\log(\sigma \sqrt{2\pi})\)</span> come la somma di due logaritmi:</p>
<p><span class="math display">\[
-\log(\sigma \sqrt{2\pi}) = -\log(\sigma) - \log(\sqrt{2\pi}).
\]</span></p>
<p>E dato che <span class="math inline">\(\log(\sqrt{2\pi}) = \frac{1}{2}\log(2\pi)\)</span>, possiamo sostituire per ottenere:</p>
<p><span class="math display">\[
-\log(\sigma) - \frac{1}{2}\log(2\pi).
\]</span></p>
<p>Combinando tutto, otteniamo:</p>
<p><span class="math display">\[
\log L(\mu; y, \sigma) = -\frac{1}{2} \log(2 \pi) - \log(\sigma) - \frac{(y - \mu)^2}{2 \sigma^2}.
\]</span></p>
<p>Questa √® la trasformata logaritmica della funzione di densit√† di probabilit√† gaussiana per una singola osservazione, che rappresenta la log-verosimiglianza di osservare <span class="math inline">\(y\)</span> dato <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma\)</span>.</p>
<p>Vogliamo trovare il valore di <span class="math inline">\(\mu\)</span> che massimizza questa funzione di log-verosimiglianza. Siccome <code>optimize()</code> di SciPy minimizza una funzione, possiamo passare il negativo della log-verosimiglianza per trovare il massimo.</p>
<div id="fd8d1548-fee9-4d37-bb54-269803b2c908" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Dati osservati</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>y_obs <span class="op">=</span> <span class="dv">114</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Definizione della funzione negativa della log-verosimiglianza</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> negative_log_likelihood(mu, y, sigma):</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> np.log(<span class="dv">2</span> <span class="op">*</span> np.pi) <span class="op">+</span> np.log(sigma) <span class="op">+</span> ((y <span class="op">-</span> mu)<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> sigma<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Ottimizzazione per trovare il valore di mu che massimizza la log-verosimiglianza</span></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> minimize(negative_log_likelihood, x0<span class="op">=</span><span class="dv">0</span>, args<span class="op">=</span>(y_obs, sigma))</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Il risultato ottimizzato per mu</span></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>result.x</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>array([113.99997648])</code></pre>
</div>
</div>
<p>Il valore di <span class="math inline">\(\mu\)</span> che massimizza la log-verosimiglianza per una distribuzione Gaussiana con <span class="math inline">\(y = 114\)</span> e <span class="math inline">\(\sigma = 15\)</span> √® circa <span class="math inline">\(114\)</span>. Questo risultato dimostra che, nel caso di una distribuzione Gaussiana con una singola osservazione e deviazione standard nota, il massimo della log-verosimiglianza si ottiene quando la media stimata <span class="math inline">\(\mu\)</span> √® molto vicina al valore osservato <span class="math inline">\(y\)</span>.</p>
</section>
<section id="campione-indipendente-di-osservazioni-da-una-distribuzione-gaussiana" class="level3" data-number="32.5.2">
<h3 data-number="32.5.2" class="anchored" data-anchor-id="campione-indipendente-di-osservazioni-da-una-distribuzione-gaussiana"><span class="header-section-number">32.5.2</span> Campione indipendente di osservazioni da una distribuzione gaussiana</h3>
<p>Passiamo ora all‚Äôesame di un contesto pi√π complesso: quello di un campione composto da <span class="math inline">\(n\)</span> osservazioni indipendenti, tutte provenienti da una distribuzione gaussiana. Consideriamo questo insieme di osservazioni come realizzazioni indipendenti ed identicamente distribuite (i.i.d.) di una variabile casuale <span class="math inline">\(X\)</span>, che segue una distribuzione normale con media $ $ e deviazione standard <span class="math inline">\(\sigma\)</span>, entrambi parametri sconosciuti. Denotiamo questa situazione con la notazione <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>.</p>
<p>In presenza di osservazioni i.i.d., la densit√† di probabilit√† congiunta del campione √® il prodotto delle funzioni di densit√† per ogni singola osservazione. Matematicamente, ci√≤ si esprime attraverso l‚Äôequazione:</p>
<p><span class="math display">\[ p(y_1, y_2, \ldots, y_n | \mu, \sigma) = \prod_{i=1}^{n} p(y_i | \mu, \sigma), \]</span></p>
<p>dove <span class="math inline">\(p(y_i | \mu, \sigma)\)</span> indica la funzione di densit√† gaussiana per l‚Äôosservazione <span class="math inline">\(y_i\)</span>, parametrizzata da <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma\)</span>.</p>
<p>Se manteniamo i dati osservati come costanti, ci√≤ che cambia in questa equazione quando variamo $ $ e <span class="math inline">\(\sigma\)</span> sono le probabilit√† associate ad ogni configurazione dei parametri, portandoci cos√¨ alla funzione di verosimiglianza congiunta per il campione.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 32.2</strong></span> Consideriamo, per illustrare questa dinamica, il caso di uno studio clinico che misura i punteggi del Beck Depression Inventory II (BDI-II) su trenta partecipanti. Supponiamo che questi punteggi seguano una distribuzione normale. Dati i punteggi BDI-II per i trenta partecipanti, il nostro obiettivo √® costruire una funzione di verosimiglianza per questi dati, assumendo che la deviazione standard <span class="math inline">\(\sigma\)</span> sia nota e pari alla deviazione standard campionaria di 6.50.</p>
<p>Per la totalit√† del campione, la densit√† di probabilit√† congiunta diventa quindi il prodotto delle densit√† per ogni osservazione. Di conseguenza, la funzione di verosimiglianza per il campione intero √® rappresentata dal prodotto delle densit√† di probabilit√† di tutte le osservazioni.</p>
<p>In questo contesto, ogni possibile valore di <span class="math inline">\(\mu\)</span> viene valutato in termini di verosimiglianza. Per esemplificare, consideriamo un range di 1000 valori per <span class="math inline">\(\mu\)</span> e calcoliamo la funzione di verosimiglianza per ognuno di questi. Per rendere pi√π gestibili i calcoli, utilizziamo il logaritmo della funzione di verosimiglianza.</p>
<p>Definendo una funzione <code>log_likelihood</code> in Python che accetta i punteggi BDI-II <span class="math inline">\(y\)</span>, un valore medio <span class="math inline">\(\mu\)</span>, e imposta <span class="math inline">\(\sigma\)</span> al valore noto, possiamo calcolare la log-verosimiglianza per un‚Äôampia gamma di valori di <span class="math inline">\(\mu\)</span> entro un intervallo specifico. Ci√≤ ci permette di visualizzare la credibilit√† relativa di ciascun valore di <span class="math inline">\(\mu\)</span> alla luce dei dati osservati.</p>
<p>Infine, il valore di <span class="math inline">\(\mu\)</span> che massimizza la funzione di log-verosimiglianza corrisponde alla stima di massima verosimiglianza di <span class="math inline">\(\mu\)</span> data la distribuzione dei punteggi BDI-II nel campione. Questo valore, nel nostro esempio, coincide con la media campionaria dei punteggi BDI-II, offrendo una stima concorde con l‚Äôintuizione che la media del campione sia un buon rappresentante del parametro <span class="math inline">\(\mu\)</span> in una distribuzione normale.</p>
<p>I dati sono:</p>
<div id="4a117251-93a9-4dfa-960e-09bf6f9019ff" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> [</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    <span class="dv">26</span>, <span class="dv">35</span>, <span class="dv">30</span>, <span class="dv">25</span>, <span class="dv">44</span>, <span class="dv">30</span>, <span class="dv">33</span>, <span class="dv">43</span>, <span class="dv">22</span>, <span class="dv">43</span>, <span class="dv">24</span>, <span class="dv">19</span>, <span class="dv">39</span>, <span class="dv">31</span>, <span class="dv">25</span>,</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    <span class="dv">28</span>, <span class="dv">35</span>, <span class="dv">30</span>, <span class="dv">26</span>, <span class="dv">31</span>, <span class="dv">41</span>, <span class="dv">36</span>, <span class="dv">26</span>, <span class="dv">35</span>, <span class="dv">33</span>, <span class="dv">28</span>, <span class="dv">27</span>, <span class="dv">34</span>, <span class="dv">27</span>, <span class="dv">22</span>,</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Il nostro scopo √® sviluppare una funzione di verosimiglianza utilizzando le 30 osservazioni indicate sopra. Basandoci su studi precedenti, ipotizziamo che questi punteggi seguano una distribuzione normale. Assumiamo inoltre che la deviazione standard <span class="math inline">\(\sigma\)</span> sia nota e corrisponda a quella osservata nel campione, ossia 6.50.</p>
<p>Per la prima osservazione del campione, dove <span class="math inline">\(y_1 = 26\)</span>, la funzione di densit√† di probabilit√† si esprime come:</p>
<p><span class="math display">\[
f(26 \,|\, \mu, \sigma = 6.50) = \frac{1}{6.50\sqrt{2\pi}} \exp \left( -\frac{(26 - \mu)^2}{2 \cdot 6.50^2} \right).
\]</span></p>
<p>Estendendo questo calcolo all‚Äôintero campione, la funzione di densit√† di probabilit√† congiunta si ottiene come il prodotto delle densit√† di tutte le osservazioni individuali:</p>
<p><span class="math display">\[
f(y \,|\, \mu, \sigma = 6.50) = \prod_{i=1}^{n} f(y_i \,|\, \mu, \sigma = 6.50).
\]</span></p>
<p>Di conseguenza, la funzione di verosimiglianza, indicata con <span class="math inline">\(\mathcal{L}(\mu, \sigma = 6.50 \,|\, y)\)</span>, si determina moltiplicando insieme le densit√† di probabilit√† di tutte le osservazioni nel campione:</p>
<p><span class="math display">\[
\begin{aligned}
\mathcal{L}(\mu, \sigma=6.50 \,|\, y) &amp;= \prod_{i=1}^{30} \frac{1}{6.50\sqrt{2\pi}} \exp \left( -\frac{(y_i - \mu)^2}{2 \cdot 6.50^2} \right) \\
&amp;= \left( \frac{1}{6.50\sqrt{2\pi}} \right)^{30} \exp\left( -\sum_{i=1}^{30} \frac{(y_i - \mu)^2}{2 \cdot 6.50^2} \right).
\end{aligned}
\]</span></p>
<p>In questa formula, <span class="math inline">\(\mu\)</span> rappresenta il parametro di interesse, la media della distribuzione, la cui stima massimizza la funzione di verosimiglianza. Se si considerano 1000 valori differenti per <span class="math inline">\(\mu\)</span>, dovremmo calcolare la funzione di verosimiglianza per ciascuno di questi valori.</p>
<p>Per rendere i calcoli pi√π gestibili, √® consigliabile utilizzare il logaritmo della funzione di verosimiglianza. In Python, possiamo definire una funzione <code>log_likelihood()</code> che accetta come argomenti <code>y</code>, <code>mu</code> e <code>sigma = true_sigma</code>. Per semplificare, impostiamo <code>true_sigma</code> uguale alla deviazione standard osservata nel campione.</p>
<div id="f29f8230-961c-4469-82bd-9d4d034b5999" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>true_sigma <span class="op">=</span> np.std(y)</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(true_sigma)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>6.495810615739622</code></pre>
</div>
</div>
<div id="52306120-fefd-4548-bfef-98a3c68097d0" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_likelihood(y, mu, sigma<span class="op">=</span>true_sigma):</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">sum</span>(stats.norm.logpdf(y, loc<span class="op">=</span>mu, scale<span class="op">=</span>true_sigma))</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Consideriamo, ad esempio, il valore <span class="math inline">\(\mu_0 = \bar{y}\)</span>, ovvero</p>
<div id="c366b5cd-cff0-427f-bd55-8eb45374a171" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>bar_y <span class="op">=</span> np.mean(y)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(bar_y)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>30.933333333333334</code></pre>
</div>
</div>
<p>L‚Äôordinata della funzione di log-verosimiglianza in corrispondenza di <span class="math inline">\(\mu = 30.93\)</span> √®</p>
<div id="3a11b51f-e578-4b55-a83d-2a0c48555c35" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>log_likelihood(y, <span class="fl">30.93</span>, sigma<span class="op">=</span>true_sigma)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>-98.70288339960591</code></pre>
</div>
</div>
<p>Troviamo ora i valori della log-verosimiglianza per ciascuno dei 1000 valori <span class="math inline">\(\mu\)</span> nell‚Äôintervallo <span class="math inline">\([\bar{y} - 2 \sigma, \bar{y} + 2 \sigma]\)</span>. Iniziamo a definire il vettore <code>mu</code>.</p>
<div id="41cb06e4-e7e5-4dbb-b9f4-939ae49cc0bc" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> np.linspace(np.mean(y) <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> np.std(y), np.mean(y) <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.std(y), num<span class="op">=</span><span class="dv">1000</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Troviamo il valore dell‚Äôordinata della funzione di log-verosimiglianza in corrispondenza di ciascuno dei 1000 valori <code>mu</code> che abbiamo definito.</p>
<div id="3bbcf36a-d7c4-4ba0-b559-789de63f21e8" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>ll <span class="op">=</span> [log_likelihood(y, mu_val, true_sigma) <span class="cf">for</span> mu_val <span class="kw">in</span> mu]</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Nel caso di un solo parametro sconosciuto (nel caso presente, <span class="math inline">\(\mu\)</span>) √® possibile rappresentare la log-verosimiglianza con una curva che interpola i punti (<code>mu</code>, <code>ll</code>). Tale funzione descrive la <em>credibilit√† relativa</em> che pu√≤ essere attribuita ai valori del parametro <span class="math inline">\(\mu\)</span> alla luce dei dati osservati.</p>
<div id="492abac6-60a8-412c-9f85-0f7fcfba36d0" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>plt.plot(mu, ll, <span class="st">"-"</span>)</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Funzione di log-verosimiglianza"</span>)</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Valore della variabile casuale mu"</span>)</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Log-verosimiglianza"</span>)</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span>np.mean(y), alpha<span class="op">=</span><span class="fl">0.4</span>, ls<span class="op">=</span><span class="st">"--"</span>)<span class="op">;</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="09_likelihood_files/figure-html/cell-40-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Il valore <span class="math inline">\(\mu\)</span> pi√π credibile corrisponde al massimo della funzione di log-verosimiglinza e viene detto <em>stima di massima verosimiglianza</em>.</p>
<p>Il massimo della funzione di log-verosimiglianza, ovvero 30.93 per l‚Äôesempio in discussione, √® identico alla media dei dati campionari.</p>
<p>Per applicare lo stesso approccio usato precedentemente con <code>optimize</code> ad un campione di dati, anzich√© a una singola osservazione, possiamo modificare la funzione di log-verosimiglianza per prendere in considerazione tutte le osservazioni nel campione. La log-verosimiglianza per un campione da una distribuzione Gaussiana, dove ogni osservazione <span class="math inline">\(y_i\)</span> ha la stessa media <span class="math inline">\(\mu\)</span> e deviazione standard <span class="math inline">\(\sigma\)</span>, √® la somma delle log-verosimiglianze di ogni osservazione individuale.</p>
<p>La formula modificata per il campione sar√†:</p>
<p><span class="math display">\[
\log L(\mu; y, \sigma) = \sum_{i=1}^{n} \left[ -\frac{1}{2} \log(2 \pi) - \log(\sigma) - \frac{(y_i - \mu)^2}{2 \sigma^2} \right],
\]</span></p>
<p>dove <span class="math inline">\(y\)</span> √® l‚Äôarray delle osservazioni e <span class="math inline">\(n\)</span> √® il numero di osservazioni nel campione.</p>
<p>Poich√©, per semplicit√†, assumiamo <span class="math inline">\(\sigma\)</span> come la deviazione standard del campione, prima calcoleremo <span class="math inline">\(\sigma\)</span> dal campione fornito e poi useremo quel valore per l‚Äôottimizzazione della log-verosimiglianza, cercando il valore di <span class="math inline">\(\mu\)</span> che la massimizza.</p>
<div id="4dd4af98-0adc-48c3-9295-0a5e400fea06" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcolo della deviazione standard del campione</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>sigma_sample <span class="op">=</span> np.std(y, ddof<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Definizione della funzione negativa della log-verosimiglianza per il campione</span></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> negative_log_likelihood_sample(mu, y, sigma):</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(y)</span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> n <span class="op">*</span> <span class="fl">0.5</span> <span class="op">*</span> np.log(<span class="dv">2</span> <span class="op">*</span> np.pi) <span class="op">+</span> n <span class="op">*</span> np.log(sigma) <span class="op">+</span> np.<span class="bu">sum</span>((y <span class="op">-</span> mu)<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> sigma<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Ottimizzazione per trovare il valore di mu che massimizza la log-verosimiglianza per il campione</span></span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>result_sample <span class="op">=</span> minimize(negative_log_likelihood_sample, x0<span class="op">=</span>np.mean(y), args<span class="op">=</span>(y, sigma_sample))</span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Il risultato ottimizzato per mu</span></span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a>result_sample.x</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>array([30.93333333])</code></pre>
</div>
</div>
<p>Il valore di <span class="math inline">\(\mu\)</span> che massimizza la log-verosimiglianza per il campione di dati fornito, assumendo noto il valore di <span class="math inline">\(\sigma\)</span> (la deviazione standard del campione), √® circa <span class="math inline">\(30.93\)</span>. Questo rappresenta la stima ottimale per la media della distribuzione Gaussiana che meglio si adatta al campione di dati dato.</p>
</div>
</section>
<section id="la-stima-di-massima-verosimiglianza-per-mu" class="level3" data-number="32.5.3">
<h3 data-number="32.5.3" class="anchored" data-anchor-id="la-stima-di-massima-verosimiglianza-per-mu"><span class="header-section-number">32.5.3</span> La Stima di Massima Verosimiglianza per <span class="math inline">\(\mu\)</span></h3>
<p>Per determinare il valore di <span class="math inline">\(\mu\)</span> che massimizza la funzione di log-verosimiglianza, procediamo calcolando la sua derivata parziale rispetto a <span class="math inline">\(\mu\)</span> e impostando il risultato uguale a zero:</p>
<ol type="1">
<li><p>Partiamo dalla funzione di log-verosimiglianza, che √® data da:</p>
<p><span class="math display">\[
\ell = -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \mu)^2.
\]</span></p></li>
<li><p>Calcoliamo la derivata parziale di $ $ rispetto a $ $:</p>
<p><span class="math display">\[
\frac{\partial \ell}{\partial \mu} = \sum_{i=1}^n \frac{(y_i - \mu)}{\sigma^2}.
\]</span></p></li>
<li><p>Impostiamo la derivata uguale a zero per trovare il punto di massimo:</p>
<p><span class="math display">\[
\frac{1}{\sigma^2} \sum_{i=1}^n (y_i - \mu) = 0.
\]</span></p></li>
</ol>
<p>Risolvendo questa equazione per $ $, otteniamo la stima di massima verosimiglianza:</p>
<p><span class="math display">\[
\hat{\mu}_{MLE} = \frac{1}{n} \sum_{i=1}^n y_i = \bar{y}.
\]</span></p>
<p>Questa formula ci mostra che la stima di massima verosimiglianza per <span class="math inline">\(\mu\)</span> corrisponde semplicemente alla media aritmetica delle osservazioni.</p>
<p>Questo processo pu√≤ essere analogamente applicato per stimare <span class="math inline">\(\sigma^2\)</span>, la varianza, e si trova che la stima di massima verosimiglianza per <span class="math inline">\(\sigma^2\)</span> √® pari alla varianza campionaria.</p>
<p>In conclusione, all‚Äôinterno di una distribuzione gaussiana, le stime di massima verosimiglianza per <span class="math inline">\(\mu\)</span> (la media) e <span class="math inline">\(\sigma^2\)</span> (la varianza) coincidono con la media campionaria e la varianza campionaria, rispettivamente.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 32.3</strong></span> Consideriamo un esempio relativo all‚Äôapprendimento per rinforzo. Lo scopo degli studi sull‚Äôapprendimento per rinforzo √® quello di comprendere come le persone imparano a massimizzare le loro ricompense in situazioni in cui la scelta migliore √® inizialmente sconosciuta. In modo pi√π specifico, consideriamo il seguente problema di apprendimento. Un partecipante deve effettuare ripetutamente delle scelte tra diverse opzioni o azioni, e dopo ogni scelta riceve una ricompensa numerica estratta da una distribuzione di probabilit√† che dipende dall‚Äôazione selezionata. L‚Äôobiettivo del partecipante √® massimizzare la ricompensa totale attesa durante un certo periodo di tempo, ad esempio, durante 100 scelte. Per descrivere questa situazione, viene spesso utilizzata la metafora di un giocatore che deve fare una serie di <span class="math inline">\(T\)</span> scelte tra <span class="math inline">\(K\)</span> slot machine (conosciute anche come ‚Äúmulti-armed bandits‚Äù) al fine di massimizzare le sue vincite. Se nella scelta <span class="math inline">\(t\)</span> viene selezionata la slot machine <span class="math inline">\(k\)</span>, viene ottenuta una ricompensa <span class="math inline">\(r_t\)</span> che ha valore <code>1</code> con una probabilit√† di successo <span class="math inline">\(\mu^k_t\)</span>, altrimenti ha valore <code>0</code>. Le probabilit√† di successo sono diverse per ogni slot machine e inizialmente sono sconosciute al partecipante. Nella versione pi√π semplice di questo compito, le probabilit√† di successo rimangono costanti nel tempo.</p>
<p>Il modello di Rescorla-Wagner √® un modello di apprendimento associativo che descrive come gli animali o gli umani aggiornano le loro aspettative di rinforzo in risposta a stimoli. Il modello pu√≤ essere descritto con la seguente formula di aggiornamento:</p>
<p><span class="math display">\[ V_{t+1} = V_t + \alpha (\lambda - V_t), \]</span></p>
<p>dove:</p>
<ul>
<li><span class="math inline">\(V_t\)</span> √® il valore predetto del rinforzo al tempo <span class="math inline">\(t\)</span>,</li>
<li><span class="math inline">\(\alpha\)</span> √® il tasso di apprendimento, un parametro che vogliamo stimare,</li>
<li><span class="math inline">\(\lambda\)</span> √® l‚Äôintensit√† del rinforzo,</li>
<li><span class="math inline">\(V_{t+1}\)</span> √® il valore aggiornato dopo aver sperimentato il rinforzo.</li>
</ul>
<p>Per semplificare, consideriamo un caso in cui gli stimoli si presentano in maniera binaria (rinforzo presente o assente), e <span class="math inline">\(\lambda\)</span> √® noto. L‚Äôobiettivo √® stimare il valore di <span class="math inline">\(\alpha\)</span> che massimizza la verosimiglianza dei dati osservati sotto il modello.</p>
<p>La funzione di verosimiglianza per questo modello dipende dalla differenza tra i valori predetti e gli effettivi rinforzi ricevuti. Tuttavia, la formulazione esatta della funzione di verosimiglianza pu√≤ variare a seconda della specifica formulazione del problema e dei dati disponibili. Per mantenere le cose semplici, consideriamo una versione semplificata in cui la ‚Äúverosimiglianza‚Äù √® basata sulla somma dei quadrati degli errori (SSE) tra i rinforzi previsti e quelli osservati (anche se tecnicamente questo non √® un approccio basato sulla verosimiglianza nel senso statistico classico).</p>
<p>Per questo esempio, assumiamo di avere un semplice set di dati di rinforzi osservati e vogliamo trovare il valore di <span class="math inline">\(\alpha\)</span> che minimizza l‚ÄôSSE:</p>
<p><span class="math display">\[ SSE = \sum_{t=1}^{n} (\lambda - V_t)^2. \]</span></p>
<p>Ecco un esempio di implementazione in Python che utilizza <code>scipy.optimize.minimize</code> per stimare <span class="math inline">\(\alpha\)</span>:</p>
<div id="d5659652-f986-49ff-80ec-710c1ab85abc" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Dati di esempio: rinforzi osservati (lambda)</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="co"># In questo esempio, assumiamo lambda = 1 per rinforzo presente e lambda = 0 per rinforzo assente</span></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="co"># per semplicit√†. In pratica, lambda potrebbe essere diverso a seconda degli esperimenti.</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>rinforzi_osservati <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>]  <span class="co"># Esempio di sequenza di rinforzi</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="396ece00-1b4d-4f76-905b-b24df6f31fdb" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Funzione che calcola l'SSE per un dato valore di alpha</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sse(alpha, rinforzi, V0<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> V0</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>    sse <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> lambda_ <span class="kw">in</span> rinforzi:</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>        sse <span class="op">+=</span> (lambda_ <span class="op">-</span> V)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>        V <span class="op">+=</span> alpha <span class="op">*</span> (lambda_ <span class="op">-</span> V)  <span class="co"># Aggiornamento del valore secondo il modello Rescorla-Wagner</span></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sse</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Ottimizzazione per trovare il valore di alpha che minimizza l'SSE</span></span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>result_alpha <span class="op">=</span> minimize(sse, x0<span class="op">=</span><span class="fl">0.5</span>, args<span class="op">=</span>(rinforzi_osservati,))</span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Il risultato ottimizzato per alpha</span></span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a>result_alpha.x</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="43">
<pre><code>array([0.29739989])</code></pre>
</div>
</div>
<p>Il valore di <span class="math inline">\(\alpha\)</span> (tasso di apprendimento) che minimizza la somma dei quadrati degli errori (SSE) per il modello di Rescorla-Wagner, dato il campione di rinforzi osservati, √® circa <span class="math inline">\(0.297\)</span>. Questo suggerisce che il tasso di apprendimento ottimale per adattare il modello ai dati osservati in questo esempio semplificato √® di circa 0.297, secondo l‚Äôapproccio di minimizzazione dell‚Äôerrore utilizzato qui.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 32.4</strong></span> Consideriamo ora un esempio relativo alla distribuzione esponenziale. Supponiamo che i seguenti siano i tempi di attesa per un certo evento:</p>
<div id="d453296c-aa5c-4f4d-9f4b-e022a1f7c343" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array([<span class="dv">27</span>, <span class="dv">64</span>, <span class="dv">3</span>, <span class="dv">18</span>, <span class="dv">8</span>])</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Definiamo la funzione di log-verosimiglianza negativa. Per iniziare, ricordiamo che la funzione di densit√† di probabilit√† (PDF) per una distribuzione esponenziale, dato un tasso <span class="math inline">\(\lambda\)</span>, √® definita come:</p>
<p><span class="math display">\[
f(x; \lambda) = \lambda e^{-\lambda x} \quad \text{per } x \geq 0.
\]</span></p>
<p>La verosimiglianza (<span class="math inline">\(L\)</span>) di osservare un insieme di dati <span class="math inline">\(\{x_1, x_2, ..., x_n\}\)</span> dato un parametro <span class="math inline">\(\lambda\)</span> √® il prodotto delle funzioni di densit√† di probabilit√† per ogni punto dati, assumendo che ciascun dato sia indipendente dagli altri. Quindi, per <span class="math inline">\(n\)</span> dati osservati, la funzione di verosimiglianza √®:</p>
<p><span class="math display">\[
L(\lambda) = \prod_{i=1}^{n} f(x_i; \lambda) = \prod_{i=1}^{n} \lambda e^{-\lambda x_i}
\]</span></p>
<p>La log-verosimiglianza (<span class="math inline">\(\log(L(\lambda))\)</span>) √® il logaritmo naturale di <span class="math inline">\(L(\lambda)\)</span>. Utilizziamo il logaritmo per semplificare la moltiplicazione in una somma, il che rende pi√π semplici sia il calcolo che la differenziazione. Pertanto, la log-verosimiglianza diventa:</p>
<p><span class="math display">\[
\log(L(\lambda)) = \log\left(\prod_{i=1}^{n} \lambda e^{-\lambda x_i}\right) = \sum_{i=1}^{n} \log(\lambda e^{-\lambda x_i}) = \sum_{i=1}^{n} (\log(\lambda) - \lambda x_i)
\]</span></p>
<p>Il motivo per utilizzare il negativo della log-verosimiglianza, cio√® <span class="math inline">\(-\log(L(\lambda))\)</span>, nelle tecniche di ottimizzazione, √® perch√© molte librerie e funzioni di ottimizzazione sono progettate per minimizzare una funzione obiettivo piuttosto che massimizzarla. Dato che vogliamo trovare il valore di <span class="math inline">\(\lambda\)</span> che massimizza la log-verosimiglianza (e quindi la verosimiglianza), possiamo invece minimizzare il suo negativo. Di conseguenza, la funzione obiettivo che passiamo all‚Äôalgoritmo di minimizzazione √®:</p>
<p><span class="math display">\[
-\log(L(\lambda)) = -\sum_{i=1}^{n} (\log(\lambda) - \lambda x_i)
\]</span></p>
<p>Scriviamo la funzione in Python:</p>
<div id="740022bc-bf1c-4306-84e5-fd81d7b796ba" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> neg_log_likelihood(lambda_, data, eps<span class="op">=</span><span class="fl">1e-8</span>):</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>    lambda_ <span class="op">=</span> np.clip(lambda_, eps, <span class="va">None</span>)  <span class="co"># Assicura che lambda_ sia almeno eps</span></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.<span class="bu">sum</span>(np.log(lambda_) <span class="op">-</span> lambda_ <span class="op">*</span> data)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Minimizzaziamo la funzione di log-verosimiglianza negativa:</p>
<div id="e8674b5c-60ff-4ed4-9adf-bc8865b93e53" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> minimize(neg_log_likelihood, x0<span class="op">=</span><span class="fl">0.1</span>, args<span class="op">=</span>(data,), bounds<span class="op">=</span>[(<span class="dv">0</span>, <span class="va">None</span>)])</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Il valore di lambda che massimizza la log-verosimiglianza √®: </span><span class="sc">{</span>result<span class="sc">.</span>x[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Il valore di lambda che massimizza la log-verosimiglianza √®: 0.04166666292998713</code></pre>
</div>
</div>
<p>Avendo trovato il tasso <span class="math inline">\(\lambda\)</span>, la stima di massima verosimiglianza del tempo di attesa medio diventa:</p>
<div id="4d4fcbb8-20c2-4ad8-a73b-90a2a8183f35" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="op">/</span> result.x</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>array([24.00000215])</code></pre>
</div>
</div>
<p>Visualizzazione.</p>
<div id="eeb2b512-3bb6-4d93-b74b-96aef7c23082" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>lambda_opt <span class="op">=</span> result.x[<span class="dv">0</span>]</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>lambda_array <span class="op">=</span> np.geomspace(<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">100</span>)</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>LL <span class="op">=</span> [<span class="op">-</span>neg_log_likelihood(L, data) <span class="cf">for</span> L <span class="kw">in</span> lambda_array]</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>plt.plot(lambda_array, LL, label<span class="op">=</span><span class="st">'Log-likelihood'</span>)</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>plt.axvline(lambda_opt, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="ss">f'Optimal $\lambda$ = </span><span class="sc">{</span>lambda_opt<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'$\lambda$'</span>)</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Log-likelihood'</span>)</span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Log-likelihood over a range of $\lambda$ values'</span>)</span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>&lt;&gt;:6: SyntaxWarning: invalid escape sequence '\l'
&lt;&gt;:7: SyntaxWarning: invalid escape sequence '\l'
&lt;&gt;:9: SyntaxWarning: invalid escape sequence '\l'
&lt;&gt;:6: SyntaxWarning: invalid escape sequence '\l'
&lt;&gt;:7: SyntaxWarning: invalid escape sequence '\l'
&lt;&gt;:9: SyntaxWarning: invalid escape sequence '\l'
/var/folders/s7/z86r4t9j6yx376cm120nln6w0000gn/T/ipykernel_53240/73438383.py:6: SyntaxWarning: invalid escape sequence '\l'
  plt.axvline(lambda_opt, color='r', linestyle='--', label=f'Optimal $\lambda$ = {lambda_opt:.4f}')
/var/folders/s7/z86r4t9j6yx376cm120nln6w0000gn/T/ipykernel_53240/73438383.py:7: SyntaxWarning: invalid escape sequence '\l'
  plt.xlabel('$\lambda$')
/var/folders/s7/z86r4t9j6yx376cm120nln6w0000gn/T/ipykernel_53240/73438383.py:9: SyntaxWarning: invalid escape sequence '\l'
  plt.title('Log-likelihood over a range of $\lambda$ values')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="09_likelihood_files/figure-html/cell-48-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="conclusione-e-riflessioni-finali" class="level2" data-number="32.6">
<h2 data-number="32.6" class="anchored" data-anchor-id="conclusione-e-riflessioni-finali"><span class="header-section-number">32.6</span> Conclusione e Riflessioni Finali</h2>
<p>La funzione di verosimiglianza rappresenta un elemento cruciale che collega i dati osservati ai parametri di un modello statistico. Essa fornisce una misura della plausibilit√† dei dati in relazione a diversi valori possibili dei parametri del modello. La strutturazione di una funzione di verosimiglianza richiede la considerazione di tre componenti fondamentali: il modello statistico che si presume abbia generato i dati, l‚Äôinsieme di valori possibili per i parametri di tale modello e le osservazioni empiriche che effettivamente abbiamo a disposizione.</p>
<p>La funzione di verosimiglianza √® centrale nella pratica dell‚Äôinferenza statistica. Essa ci permette di quantificare quanto bene differenti set di parametri potrebbero aver generato i dati osservati. Questo √® fondamentale sia per la selezione del modello che per la stima dei parametri, e pertanto √® indispensabile per un‚Äôanalisi dati rigorosa e per un‚Äôinterpretazione accurata dei risultati.</p>
<p>Un‚Äôapplicazione pratica e illustrativa dei principi esposti in questo capitolo √® fornita nella sezione sul modello Rescorla-Wagner, che √® un esempio di come la teoria della verosimiglianza possa essere applicata per affrontare questioni empiriche in psicologia.</p>
<p>In sintesi, la comprensione e l‚Äôapplicazione appropriata della funzione di verosimiglianza sono passaggi essenziali nel processo di analisi dati. Essa costituisce uno strumento indispensabile per chi √® impegnato nella ricerca empirica e nell‚Äôinterpretazione di dati complessi.</p>
<div class="callout callout-style-simple callout-warning">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>All‚Äôesame ti verr√† chiesto di:</p>
<ul>
<li>Calcolare la funzione di verosimiglianza binomiale e riportare il valore della funzione in corrispondenza di specifici valori <span class="math inline">\(\theta\)</span>.</li>
<li>Calcolare la funzione di verosimiglianza del modello gaussiano, per <span class="math inline">\(\sigma\)</span> noto, e riportare il valore della funzione in corrispondenza di specifici valori <span class="math inline">\(\mu\)</span>.</li>
<li>Calcolare la stima di massima verosimiglianza.</li>
<li>Rispondere a domande che implicano una adeguata comprensione del concetto di funzione di verosimiglianza.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="informazioni-sullambiente-di-sviluppo" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="informazioni-sullambiente-di-sviluppo">Informazioni sull‚ÄôAmbiente di Sviluppo</h2>
<div id="b32126bb-3cd1-4041-8e88-7575a7532963" class="cell" data-tags="[]" data-execution_count="4">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext watermark</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>watermark <span class="op">-</span>n <span class="op">-</span>u <span class="op">-</span>v <span class="op">-</span>iv <span class="op">-</span>w <span class="op">-</span>m</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>SyntaxError: invalid syntax (2307612016.py, line 1)</code></pre>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-zetsche_2019future" class="csl-entry" role="listitem">
Zetsche, Ulrike, Paul-Christian Buerkner, e Babette Renneberg. 2019. <span>¬´Future expectations in clinical depression: biased or realistic?¬ª</span> <em>Journal of Abnormal Psychology</em> 128 (7): 678.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiato!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiato!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/ccaudek\.github\.io\/psicometria\/intro\.html");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../chapters/chapter_3/08_cont_rv_distr.html" class="pagination-link" aria-label="Distribuzioni di v.c. continue">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. continue</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/chapter_3/10_grid_gauss.html" class="pagination-link" aria-label="Calcolo della Distribuzione a Posteriori Gaussiana tramite Metodo a Griglia">
        <span class="nav-page-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Calcolo della Distribuzione a Posteriori Gaussiana tramite Metodo a Griglia</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.dev/ccaudek/psicometria/blob/main/chapters/chapter_3/09_likelihood.ipynb" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li></ul></div></div></div></footer></body></html>