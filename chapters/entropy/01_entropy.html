<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Corrado Caudek">

<title>Data Science per Psicologi - 83&nbsp; Entropia</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/entropy/02_kl.html" rel="next">
<link href="../../chapters/entropy/introduction_entropy.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/entropy/introduction_entropy.html">Entropia</a></li><li class="breadcrumb-item"><a href="../../chapters/entropy/01_entropy.html"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">Entropia</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Data Science per Psicologi</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ccaudek/psicometria/" title="Eseguire il codice" class="quarto-navigation-tool px-1" aria-label="Eseguire il codice"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benvenuti</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../prefazione.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefazione</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
 <span class="menu-text">Python</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/introduction_python.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/00_prelims.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Preliminari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/01_python_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Python (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/02_python_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Python (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/03_numpy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">NumPy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/04_pandas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Pandas (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/05_pandas_aggregate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Pandas (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/06_pandas_functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Pandas (3)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/07_matplotlib.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Matplotlib</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/python/08_seaborn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Seaborn</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
 <span class="menu-text">Fondamenti</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/introduction_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/01_key_notions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Concetti chiave</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/02_measurement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">La misurazione in psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/key_notions/03_data_analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">L’analisi dei dati psicologici</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
 <span class="menu-text">EDA</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/introduction_eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/01_project_structure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Le fasi del progetto di analisi dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/02_data_cleaning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Flusso di lavoro per la pulizia dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/03_dplyr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Data tidying</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/04_freq_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Analisi esplorativa dei dati</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/05_loc_scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Indici di posizione e di scala</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/06_correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Le relazioni tra variabili</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/07_causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Causalità dai dati osservazionali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/eda/08_estimand.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Estimandi teorici e estimandi empirici</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">
 <span class="menu-text">Probabilità</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/introduction_probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/01_intro_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Interpretazione della probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/02_prob_spaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Misura di Probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/03_prob_on_general_spaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Fondamenti della probabilità: assiomi di Kolmogorov e sigma-algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/04_conditional_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Probabilità condizionata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/05_bayes_theorem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/06_random_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/07_expval_var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Proprietà delle variabili casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/08_sampling_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Stime, stimatori e parametri</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/09_joint_prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Probabilità congiunta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/10_density_func.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">La funzione di densità di probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/11_discr_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. discrete</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/12_cont_rv_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. continue</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/13_likelihood.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">La verosimiglianza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/probability/14_simulation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Simulazioni</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">
 <span class="menu-text">Inferenza</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/introduction_bayes_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/01_intro_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Modellazione bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/02_subj_prop.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Pensare ad una proporzione in termini soggettivi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/03_grid_gauss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Calcolo della Distribuzione a Posteriori Gaussiana tramite Metodo a Griglia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/04_conjugate_families_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/05_conjugate_families_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/06_summary_posterior.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Sintesi a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/07_balance_prior_post.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">L’influenza della distribuzione a priori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/08_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Modello di Poisson</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/09_exponential_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Modello Esponenziale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/10_post_pred_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Distribuzione predittiva a posteriori</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false">
 <span class="menu-text">MCMC</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/introduction_mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/01_metropolis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Monte Carlo a Catena di Markov</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/02_stan_beta_binomial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Linguaggio Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/03_stan_summary_posterior.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Metodi di sintesi della distribuzione a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/04_stan_diagnostics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Diagnostica delle catene markoviane</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/05_stan_prediction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">La predizione bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/06_stan_odds_ratio.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Analisi bayesiana dell’odds-ratio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/07_stan_normal_normal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Inferenza bayesiana su una media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/08_stan_two_groups.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Confronto tra due gruppi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/09_stan_poisson_model_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">Modello di Poisson (1)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/10_stan_poisson_model_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Modello di Poisson (2)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/11_stan_gaussian_mixture.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">Modelli Mistura Gaussiani</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/12_stan_nuisance_parameters.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">Modelli con più di un parametro</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/13_stan_hier_beta_binom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">Modello gerarchico beta-binomiale con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/14_stan_categorical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Modello categoriale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/15_cmdstanr_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">59</span>&nbsp; <span class="chapter-title">Introduzione a CmdStanR</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="false">
 <span class="menu-text">Regressione</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/introduction_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/01_reglin_bayesian.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">60</span>&nbsp; <span class="chapter-title">Modello bayesiano di regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/02_non_centered_param.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">61</span>&nbsp; <span class="chapter-title">Non-Centered Parameterization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/03_beauty_sex_power.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">62</span>&nbsp; <span class="chapter-title">Bellezza, sesso e potere</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/04_synt_sugar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">63</span>&nbsp; <span class="chapter-title">Zucchero sintattico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/05_two_means.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">64</span>&nbsp; <span class="chapter-title">Confronto tra le medie di due gruppi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/06_prediction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">65</span>&nbsp; <span class="chapter-title">Predizione e inferenza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/07_design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">66</span>&nbsp; <span class="chapter-title">Disegno della ricerca e potere statistico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/08_linear_algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">67</span>&nbsp; <span class="chapter-title">Elementi di algebra lineare</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/09_stan_multreg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">68</span>&nbsp; <span class="chapter-title">Il modello di regressione multipla</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/10_hier_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">69</span>&nbsp; <span class="chapter-title">Il modello lineare gerarchico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/11_stan_mixed_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">70</span>&nbsp; <span class="chapter-title">Modelli misti con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/12_specification_error.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">71</span>&nbsp; <span class="chapter-title">Errore di specificazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/13_causal_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">Inferenza causale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/14_interactions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">Interazioni statistiche</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/15_ate_att_atu.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Concetti di ATE, ATT e ATU</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/16_missing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Dati mancanti</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="false">
 <span class="menu-text">GLM</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/introduction_glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/01_robust_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">Regressione robusta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/02_stan_binomial_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Regressione binomiale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/03_stan_logistic_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">78</span>&nbsp; <span class="chapter-title">Regressione logistica con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/04_stan_poisson_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">79</span>&nbsp; <span class="chapter-title">Regressione di Poisson con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/05_simchon_2023.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">80</span>&nbsp; <span class="chapter-title">Tweets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/06_stan_rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">81</span>&nbsp; <span class="chapter-title">Incorporare dati storici di controllo in una RCT</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/07_stan_mediation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">82</span>&nbsp; <span class="chapter-title">Modello di mediazione con Stan</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true">
 <span class="menu-text">Entropia</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/introduction_entropy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/01_entropy.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">Entropia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/02_kl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">84</span>&nbsp; <span class="chapter-title">Divergenza KL, LPPD, ELPD e LOO-CV</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/03_loo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">85</span>&nbsp; <span class="chapter-title">Validazione Incrociata Leave-One-Out</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/04_cognitive_modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">86</span>&nbsp; <span class="chapter-title">Dall’analisi descrittiva alla modellazione cognitiva</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/05_inductive_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">87</span>&nbsp; <span class="chapter-title">Limiti dell’inferenza induttiva</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="false">
 <span class="menu-text">Dinamiche</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/introduction_dynamic_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/01_canoeing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">88</span>&nbsp; <span class="chapter-title">Dinamiche post-errore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/02_change_across_time.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">89</span>&nbsp; <span class="chapter-title">Modellare il cambiamento nel tempo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/03_rescorla_wagner.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">90</span>&nbsp; <span class="chapter-title">Apprendimento per rinforzo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/04_affect.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">91</span>&nbsp; <span class="chapter-title">Le emozioni influenzano le emozioni</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/05_sequential_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">92</span>&nbsp; <span class="chapter-title">Analisi dinamica delle sequenze di apprendimento</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/07_bipolar_disorder.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">93</span>&nbsp; <span class="chapter-title">Modello di Markov nascosto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/dynamic_models/08_haslbeck.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">94</span>&nbsp; <span class="chapter-title">Recuperare le dinamiche intra-personali dalle serie temporali psicologiche</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" aria-expanded="false">
 <span class="menu-text">Modelli cognitivi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/cognitive_models/introduction_cognitive_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/cognitive_models/01_ddm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">95</span>&nbsp; <span class="chapter-title">Drift Diffusion Model</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" aria-expanded="false">
 <span class="menu-text">Frequentismo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/introduction_frequentist_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/01_intro_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">96</span>&nbsp; <span class="chapter-title">Introduzione all’inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/02_conf_interv.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">97</span>&nbsp; <span class="chapter-title">Intervallo di confidenza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/03_test_ipotesi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">98</span>&nbsp; <span class="chapter-title">Significatività statistica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/frequentist_inference/04_two_ind_samples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">99</span>&nbsp; <span class="chapter-title">Test t di Student per campioni indipendenti</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" aria-expanded="false">
 <span class="menu-text">Crisi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/introduction_replication_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/01_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">100</span>&nbsp; <span class="chapter-title">La Crisi della Replicazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/02_limits_stat_freq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">101</span>&nbsp; <span class="chapter-title">Limiti dell’inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/03_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">102</span>&nbsp; <span class="chapter-title">La grandezza dell’effetto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/04_s_m_errors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">103</span>&nbsp; <span class="chapter-title">Errori di segno e errori di grandezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/05_changes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">104</span>&nbsp; <span class="chapter-title">Proposte di cambiamento</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/06_integrity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">105</span>&nbsp; <span class="chapter-title">Integrità della ricerca</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" aria-expanded="false">
 <span class="menu-text">Epilogo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/epiloque/epiloque.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Considerazioni Conclusive</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../99-references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliografia</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" aria-expanded="false">
 <span class="menu-text">Appendici</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a00_installation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Ambiente di lavoro</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01_markdown.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Jupyter Notebook</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a02_shell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">La Shell</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a03_colab_tutorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Colab: un breve tutorial</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a04_virtual_env.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Ambienti virtuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a10_math_symbols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Simbologia di base</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a11_numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Numeri e intervalli</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a12_sum_notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">H</span>&nbsp; <span class="chapter-title">Sommatorie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a13_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">I</span>&nbsp; <span class="chapter-title">Insiemi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a14_combinatorics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">J</span>&nbsp; <span class="chapter-title">Calcolo combinatorio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a15_calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">K</span>&nbsp; <span class="chapter-title">Per liberarvi dai terrori preliminari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a20_kde_plot.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">L</span>&nbsp; <span class="chapter-title">Kernel Density Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a30_prob_tutorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">M</span>&nbsp; <span class="chapter-title">Esercizi di probabilità discreta</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a40_rng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">N</span>&nbsp; <span class="chapter-title">Generazione di numeri casuali</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a44_montecarlo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">O</span>&nbsp; <span class="chapter-title">Simulazione Monte Carlo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a46_stan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">P</span>&nbsp; <span class="chapter-title">Linguaggio Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a47_first_order_markov.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">Q</span>&nbsp; <span class="chapter-title">Catene di Markov</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a48_hidden_markov_processes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">R</span>&nbsp; <span class="chapter-title">Processi di Markov Nascosti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a50_lin_fun.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">S</span>&nbsp; <span class="chapter-title">La funzione lineare</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a51_r_squared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">T</span>&nbsp; <span class="chapter-title">Teorema della scomposizione della devianza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a55_rescorla_wagner.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">U</span>&nbsp; <span class="chapter-title">Apprendimento per rinforzo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a60_ttest_exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">V</span>&nbsp; <span class="chapter-title">Esercizi sull’inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a70_predict_counts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">W</span>&nbsp; <span class="chapter-title">La predizione delle frequenze</span></span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" aria-expanded="false">
 <span class="menu-text">Soluzioni degli esercizi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-16" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/solutions_probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">X</span>&nbsp; <span class="chapter-title">Probabilità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/solutions_bayesian_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">Y</span>&nbsp; <span class="chapter-title">Inferenza bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/solutions_causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">Z</span>&nbsp; <span class="chapter-title">Causalità</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/solutions_mult_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/solutions_rescorla_wagner.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">\</span>&nbsp; <span class="chapter-title">Modello Rescorla-Wagner</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/solutions_entropia.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">]</span>&nbsp; <span class="chapter-title">Entropia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/solutions_replication_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">^</span>&nbsp; <span class="chapter-title">Crisi della replicazione</span></span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Indice</h2>
   
  <ul>
  <li><a href="#introduzione" id="toc-introduzione" class="nav-link active" data-scroll-target="#introduzione">Introduzione</a></li>
  <li><a href="#la-generalizzabilità-dei-modelli-e-il-metodo-scientifico" id="toc-la-generalizzabilità-dei-modelli-e-il-metodo-scientifico" class="nav-link" data-scroll-target="#la-generalizzabilità-dei-modelli-e-il-metodo-scientifico"><span class="header-section-number">83.1</span> La Generalizzabilità dei Modelli e il Metodo Scientifico</a></li>
  <li><a href="#cosè-lentropia-dellinformazione" id="toc-cosè-lentropia-dellinformazione" class="nav-link" data-scroll-target="#cosè-lentropia-dellinformazione"><span class="header-section-number">83.2</span> Cos’è l’Entropia dell’Informazione?</a>
  <ul class="collapse">
  <li><a href="#definizione-di-sorpresa-e-entropia" id="toc-definizione-di-sorpresa-e-entropia" class="nav-link" data-scroll-target="#definizione-di-sorpresa-e-entropia"><span class="header-section-number">83.2.1</span> Definizione di Sorpresa e Entropia</a></li>
  <li><a href="#calcolo-dellentropia" id="toc-calcolo-dellentropia" class="nav-link" data-scroll-target="#calcolo-dellentropia"><span class="header-section-number">83.2.2</span> Calcolo dell’Entropia</a></li>
  <li><a href="#caratteristiche-dellentropia" id="toc-caratteristiche-dellentropia" class="nav-link" data-scroll-target="#caratteristiche-dellentropia"><span class="header-section-number">83.2.3</span> Caratteristiche dell’Entropia</a></li>
  <li><a href="#additività-dellentropia-per-eventi-indipendenti" id="toc-additività-dellentropia-per-eventi-indipendenti" class="nav-link" data-scroll-target="#additività-dellentropia-per-eventi-indipendenti"><span class="header-section-number">83.2.4</span> Additività dell’Entropia per Eventi Indipendenti</a></li>
  <li><a href="#entropia-di-variabili-casuali" id="toc-entropia-di-variabili-casuali" class="nav-link" data-scroll-target="#entropia-di-variabili-casuali"><span class="header-section-number">83.2.5</span> Entropia di Variabili Casuali</a></li>
  <li><a href="#applicazioni-psicologiche" id="toc-applicazioni-psicologiche" class="nav-link" data-scroll-target="#applicazioni-psicologiche"><span class="header-section-number">83.2.6</span> Applicazioni Psicologiche</a></li>
  </ul></li>
  <li><a href="#la-codifica-huffman" id="toc-la-codifica-huffman" class="nav-link" data-scroll-target="#la-codifica-huffman"><span class="header-section-number">83.3</span> La Codifica Huffman</a>
  <ul class="collapse">
  <li><a href="#come-funziona-la-codifica-huffman" id="toc-come-funziona-la-codifica-huffman" class="nav-link" data-scroll-target="#come-funziona-la-codifica-huffman"><span class="header-section-number">83.3.1</span> Come Funziona la Codifica Huffman</a></li>
  <li><a href="#esempio-di-codifica-huffman" id="toc-esempio-di-codifica-huffman" class="nav-link" data-scroll-target="#esempio-di-codifica-huffman"><span class="header-section-number">83.3.2</span> Esempio di Codifica Huffman</a></li>
  <li><a href="#perché-la-codifica-huffman-è-efficiente" id="toc-perché-la-codifica-huffman-è-efficiente" class="nav-link" data-scroll-target="#perché-la-codifica-huffman-è-efficiente"><span class="header-section-number">83.3.3</span> Perché la Codifica Huffman è Efficiente</a></li>
  </ul></li>
  <li><a href="#lentropia-come-lunghezza-media-del-codice-binario" id="toc-lentropia-come-lunghezza-media-del-codice-binario" class="nav-link" data-scroll-target="#lentropia-come-lunghezza-media-del-codice-binario"><span class="header-section-number">83.4</span> L’Entropia come Lunghezza Media del Codice Binario</a></li>
  <li><a href="#lentropia-relativa" id="toc-lentropia-relativa" class="nav-link" data-scroll-target="#lentropia-relativa"><span class="header-section-number">83.5</span> L’Entropia Relativa</a>
  <ul class="collapse">
  <li><a href="#concetto-di-divergenza-di-kullback-leibler" id="toc-concetto-di-divergenza-di-kullback-leibler" class="nav-link" data-scroll-target="#concetto-di-divergenza-di-kullback-leibler"><span class="header-section-number">83.5.1</span> Concetto di Divergenza di Kullback-Leibler</a></li>
  <li><a href="#definizione-matematica-della-divergenza-di-kullback-leibler" id="toc-definizione-matematica-della-divergenza-di-kullback-leibler" class="nav-link" data-scroll-target="#definizione-matematica-della-divergenza-di-kullback-leibler"><span class="header-section-number">83.5.2</span> Definizione Matematica della Divergenza di Kullback-Leibler</a></li>
  <li><a href="#entropia-entropia-incrociata-e-divergenza-kl" id="toc-entropia-entropia-incrociata-e-divergenza-kl" class="nav-link" data-scroll-target="#entropia-entropia-incrociata-e-divergenza-kl"><span class="header-section-number">83.5.3</span> Entropia, Entropia Incrociata e Divergenza KL</a></li>
  <li><a href="#interpretazione-della-divergenza-kl" id="toc-interpretazione-della-divergenza-kl" class="nav-link" data-scroll-target="#interpretazione-della-divergenza-kl"><span class="header-section-number">83.5.4</span> Interpretazione della Divergenza KL</a></li>
  </ul></li>
  <li><a href="#applicazione-della-divergenza-d_textkl-nella-selezione-di-modelli" id="toc-applicazione-della-divergenza-d_textkl-nella-selezione-di-modelli" class="nav-link" data-scroll-target="#applicazione-della-divergenza-d_textkl-nella-selezione-di-modelli"><span class="header-section-number">83.6</span> Applicazione della Divergenza <span class="math inline">\(D_{\text{KL}}\)</span> nella Selezione di Modelli</a>
  <ul class="collapse">
  <li><a href="#proprietà-chiave" id="toc-proprietà-chiave" class="nav-link" data-scroll-target="#proprietà-chiave"><span class="header-section-number">83.6.1</span> Proprietà Chiave</a></li>
  <li><a href="#selezione-dei-modelli-statistici" id="toc-selezione-dei-modelli-statistici" class="nav-link" data-scroll-target="#selezione-dei-modelli-statistici"><span class="header-section-number">83.6.2</span> Selezione dei Modelli Statistici</a></li>
  </ul></li>
  <li><a href="#riflessioni-conclusive" id="toc-riflessioni-conclusive" class="nav-link" data-scroll-target="#riflessioni-conclusive"><span class="header-section-number">83.7</span> Riflessioni Conclusive</a></li>
  <li><a href="#esercizi" id="toc-esercizi" class="nav-link" data-scroll-target="#esercizi"><span class="header-section-number">83.8</span> Esercizi</a></li>
  <li><a href="#informazioni-sullambiente-di-sviluppo" id="toc-informazioni-sullambiente-di-sviluppo" class="nav-link" data-scroll-target="#informazioni-sullambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.dev/ccaudek/psicometria/blob/main/chapters/entropy/01_entropy.ipynb" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/entropy/introduction_entropy.html">Entropia</a></li><li class="breadcrumb-item"><a href="../../chapters/entropy/01_entropy.html"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">Entropia</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-entropy" class="quarto-section-identifier"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">Entropia</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><strong>Prerequisiti</strong></p>
<p><strong>Concetti e competenze chiave</strong></p>
<ul>
<li>Comprendere il concetto di entropia.</li>
<li>Comprendere il concetto di divervenza di Kullback-Leibler (<span class="math inline">\(D_{\text{KL}}\)</span>).</li>
<li>Calcolare la divergenza <span class="math inline">\(D_{\text{KL}}\)</span> dall’entropia;</li>
</ul>
<p><strong>Preparazione del Notebook</strong></p>
<div id="83330a93-8ca1-4a1a-8077-9339c6123f4c" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-03-28T12:22:36.008220Z&quot;,&quot;start_time&quot;:&quot;2024-03-28T12:22:32.642274Z&quot;}" data-execution_count="10">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats <span class="im">as</span> stats</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> kl_div</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> arviz <span class="im">as</span> az</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="3bf59906-1847-4aa2-8d25-1e9f6e158df7" class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-03-28T12:22:38.469877Z&quot;,&quot;start_time&quot;:&quot;2024-03-28T12:22:38.462529Z&quot;}" data-execution_count="11">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>config InlineBackend.figure_format <span class="op">=</span> <span class="st">'retina'</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>RANDOM_SEED <span class="op">=</span> <span class="dv">8927</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(RANDOM_SEED)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>az.style.use(<span class="st">"arviz-darkgrid"</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="introduzione" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="introduzione">Introduzione</h2>
<p>Nel campo della statistica bayesiana, è fondamentale confrontare diversi modelli predittivi per determinare quale si adatta meglio ai dati disponibili. In questo capitolo, esploreremo il concetto di entropia, una misura essenziale per quantificare l’incertezza nelle distribuzioni di probabilità. L’entropia di una variabile casuale indica il grado medio della sua imprevedibilità.</p>
<p>Analizzeremo anche come l’entropia può essere utilizzata per valutare la “distanza” tra un modello teorico e i dati osservati, introducendo la divergenza di Kullback-Leibler (<span class="math inline">\(D_{\text{KL}}\)</span>). Questa misura quantifica le discrepanze tra due distribuzioni di probabilità, fornendo un indicatore di quanto accuratamente un modello rappresenti le osservazioni empiriche.</p>
<p>Nel capitolo successivo, presenteremo la tecnica della Validazione Incrociata Leave-One-Out (LOO-CV), che viene utilizzata per calcolare un’approssimazione della divergenza <span class="math inline">\(D_{\text{KL}}\)</span>. Questa tecnica consente di stimare quanto bene un modello generalizzi i dati, offrendo un’ulteriore metrica per la valutazione dei modelli statistici.</p>
</section>
<section id="la-generalizzabilità-dei-modelli-e-il-metodo-scientifico" class="level2" data-number="83.1">
<h2 data-number="83.1" class="anchored" data-anchor-id="la-generalizzabilità-dei-modelli-e-il-metodo-scientifico"><span class="header-section-number">83.1</span> La Generalizzabilità dei Modelli e il Metodo Scientifico</h2>
<p>La generalizzabilità dei modelli è un concetto fondamentale nella scienza e uno dei pilastri del metodo scientifico. Questo concetto si riferisce alla capacità di un modello di essere applicato con successo e di produrre risultati accurati al di fuori del contesto specifico o del set di dati per cui è stato inizialmente sviluppato o testato. In altre parole, il valore scientifico di un modello dipende fortemente dalla sua capacità di generalizzare a nuovi dati.</p>
<p>Nella pratica, la generalizzabilità di un modello può essere compromessa da due problemi principali: il sotto-adattamento e il sovra-adattamento.</p>
<ul>
<li><p><strong>Sotto-adattamento</strong>: Si verifica quando un modello è troppo semplice per rappresentare adeguatamente la complessità dei dati. Un modello sotto-adattato non riesce a catturare le relazioni essenziali nei dati, portando a prestazioni scadenti sia sul set di dati di addestramento che su nuovi dati. Questo limita gravemente la sua utilità in applicazioni pratiche.</p></li>
<li><p><strong>Sovra-adattamento</strong>: Si manifesta quando un modello è eccessivamente complesso e si adatta troppo fedelmente al rumore o alle particolarità specifiche del set di dati di addestramento. Un modello sovra-adattato può mostrare ottime prestazioni sui dati di addestramento, ma fallisce nel generalizzare a nuovi dati, riducendo la sua efficacia predittiva.</p></li>
</ul>
<p>L’approccio bayesiano alla modellazione offre un modo efficace per bilanciare la complessità del modello con l’adattamento ai dati. Come descritto da <span class="citation" data-cites="McElreath_rethinking">McElreath (<a href="../../99-references.html#ref-McElreath_rethinking" role="doc-biblioref">2020</a>)</span>, la selezione del modello è un processo che richiede un equilibrio tra la semplicità del modello e la sua capacità di rappresentare accuratamente la realtà dei dati.</p>
<p>Una pratica comune nella scelta tra modelli alternativi si basa sul principio del rasoio di Ockham, che favorisce le spiegazioni più semplici quando ci sono più teorie equivalenti per un fenomeno. Tuttavia, questo principio da solo non è sufficiente; è fondamentale che il modello scelto descriva accuratamente i dati.</p>
<p>Tradizionalmente, la selezione dei modelli è stata spesso basata sull’uso dei valori-p, ma, come evidenziato da <span class="citation" data-cites="McElreath_rethinking">McElreath (<a href="../../99-references.html#ref-McElreath_rethinking" role="doc-biblioref">2020</a>)</span>, questo approccio presenta numerosi problemi e manca di una giustificazione teorica solida.</p>
<p>Un metodo più robusto e teoricamente fondato è l’uso della divergenza di Kullback-Leibler, che misura quanto un modello riesca ad approssimare efficacemente la distribuzione reale dei dati. Questa misura fornisce una stima quantitativa della precisione del modello nel rappresentare il processo generativo sottostante.</p>
<p>Questo capitolo introduce il concetto di entropia, che è essenziale per comprendere la divergenza di Kullback-Leibler, e le sue applicazioni nella selezione dei modelli, tema che verrà approfondito nel prossimo capitolo.</p>
</section>
<section id="cosè-lentropia-dellinformazione" class="level2" data-number="83.2">
<h2 data-number="83.2" class="anchored" data-anchor-id="cosè-lentropia-dellinformazione"><span class="header-section-number">83.2</span> Cos’è l’Entropia dell’Informazione?</h2>
<p>L’entropia dell’informazione, introdotta da Claude Shannon, è un concetto fondamentale della teoria dell’informazione. Questa grandezza matematica misura l’incertezza o la sorpresa associata alla ricezione di un messaggio. In termini semplici, quanto più un evento è improbabile, tanto maggiore sarà la sorpresa associata alla sua osservazione.</p>
<section id="definizione-di-sorpresa-e-entropia" class="level3" data-number="83.2.1">
<h3 data-number="83.2.1" class="anchored" data-anchor-id="definizione-di-sorpresa-e-entropia"><span class="header-section-number">83.2.1</span> Definizione di Sorpresa e Entropia</h3>
<p>La sorpresa di un evento, determinata dalla sua probabilità <span class="math inline">\(p\)</span>, è calcolata utilizzando la formula:</p>
<p><span class="math display">\[ H(p) = -\log_2(p) = \log_2 \left(\frac{1}{p}\right). \]</span></p>
<p>L’uso del logaritmo in base 2 in questa formula si basa su alcune considerazioni fondamentali:</p>
<ul>
<li><strong>Unità di misura</strong>: La base 2 è utilizzata perché l’informazione viene misurata in “bit”. Un bit rappresenta la quantità di informazione derivante da una scelta binaria (ad esempio, il risultato di un lancio di moneta). Ad esempio, l’entropia associata al lancio di una moneta bilanciata è pari a 1 bit.</li>
<li><strong>Scala logaritmica</strong>: La scala logaritmica rappresenta più accuratamente la percezione umana dell’informazione e della sorpresa. Ad esempio, eventi con probabilità molto basse hanno un impatto informativo significativamente maggiore rispetto a piccoli cambiamenti nelle probabilità di eventi più probabili.</li>
</ul>
</section>
<section id="calcolo-dellentropia" class="level3" data-number="83.2.2">
<h3 data-number="83.2.2" class="anchored" data-anchor-id="calcolo-dellentropia"><span class="header-section-number">83.2.2</span> Calcolo dell’Entropia</h3>
<p>Per capire meglio il concetto di entropia, consideriamo alcuni esempi pratici. La funzione per calcolare l’entropia di un evento con probabilità <span class="math inline">\(p\)</span> è la seguente:</p>
<div id="cell-7" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calcola_entropia(p):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> p <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> p <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">0</span>  <span class="co"># Non c'è incertezza se l'evento è certo o impossibile</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span>p <span class="op">*</span> math.log2(p)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Esempi di probabilità</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>probabilità <span class="op">=</span> [<span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.9</span>, <span class="fl">1.0</span>]</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcolo dell'entropia per ciascuna probabilità</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>entropie <span class="op">=</span> {p: calcola_entropia(p) <span class="cf">for</span> p <span class="kw">in</span> probabilità}</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(entropie)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{0.0: 0, 0.1: 0.33219280948873625, 0.5: 0.5, 0.9: 0.13680278410054494, 1.0: 0}</code></pre>
</div>
</div>
<p>L’output di questo script mostra che l’entropia è massima per eventi con probabilità intermedia (0.5) e minima (zero) per eventi certi (probabilità 1) o impossibili (probabilità 0). Questo riflette il fatto che l’incertezza è massima quando tutti gli esiti sono ugualmente probabili e minima quando l’esito è noto con certezza.</p>
</section>
<section id="caratteristiche-dellentropia" class="level3" data-number="83.2.3">
<h3 data-number="83.2.3" class="anchored" data-anchor-id="caratteristiche-dellentropia"><span class="header-section-number">83.2.3</span> Caratteristiche dell’Entropia</h3>
<ul>
<li><p><strong>Massima Entropia</strong>: L’entropia raggiunge il suo valore massimo quando tutti gli esiti possibili di un evento hanno la stessa probabilità di verificarsi. Questa condizione rappresenta il massimo grado di imprevedibilità, poiché non esistono indizi che possano aiutare a prevedere quale esito si verificherà.</p></li>
<li><p><strong>Minima Entropia</strong>: L’entropia è minima quando l’esito di un evento è completamente certo (probabilità di 1) o impossibile (probabilità di 0). In tali casi, non c’è alcuna incertezza o sorpresa, e quindi non c’è alcuna informazione da guadagnare osservando l’evento.</p></li>
</ul>
</section>
<section id="additività-dellentropia-per-eventi-indipendenti" class="level3" data-number="83.2.4">
<h3 data-number="83.2.4" class="anchored" data-anchor-id="additività-dellentropia-per-eventi-indipendenti"><span class="header-section-number">83.2.4</span> Additività dell’Entropia per Eventi Indipendenti</h3>
<p>L’entropia ha una proprietà di additività nel caso di eventi indipendenti. Se due o più eventi indipendenti si verificano, l’entropia totale associata alla loro combinazione è uguale alla somma delle entropie di ciascun evento preso singolarmente. Questa proprietà deriva dal fatto che i logaritmi sono additivi, permettendo così di sommare le entropie individuali per ottenere l’entropia complessiva.</p>
</section>
<section id="entropia-di-variabili-casuali" class="level3" data-number="83.2.5">
<h3 data-number="83.2.5" class="anchored" data-anchor-id="entropia-di-variabili-casuali"><span class="header-section-number">83.2.5</span> Entropia di Variabili Casuali</h3>
<p>L’entropia può essere estesa dal concetto di singolo evento a quello di variabile casuale. In questo contesto, l’entropia misura l’incertezza complessiva associata alla realizzazione di una variabile casuale, che può essere discreta o continua.</p>
<section id="entropia-di-una-variabile-casuale-discreta" class="level4" data-number="83.2.5.1">
<h4 data-number="83.2.5.1" class="anchored" data-anchor-id="entropia-di-una-variabile-casuale-discreta"><span class="header-section-number">83.2.5.1</span> Entropia di una Variabile Casuale Discreta</h4>
<p>Per una variabile casuale discreta <span class="math inline">\(X\)</span> con un insieme di possibili valori <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span> e una funzione di massa di probabilità <span class="math inline">\(p(x) = \Pr\{X = x\}\)</span>, l’entropia <span class="math inline">\(H(X)\)</span> è definita come:</p>
<p><span class="math display">\[ H(X) = -\sum_{x \in X} p(x) \log_2 p(x). \]</span></p>
<p>Questa formula rappresenta la somma pesata delle sorprese di ciascun possibile esito. Il segno negativo è necessario perché i logaritmi delle probabilità, essendo numeri minori di 1, sono negativi. Il segno negativo inverte questi valori, trasformandoli in quantità positive che rappresentano correttamente l’informazione o la sorpresa.</p>
<p>In sintesi, l’entropia <span class="math inline">\(H(X)\)</span> misura l’incertezza complessiva di una variabile casuale discreta, considerando le probabilità di tutti i suoi possibili esiti. Ogni termine della somma <span class="math inline">\(-p(x) \log_2(p(x))\)</span> rappresenta la quantità di sorpresa o informazione associata a ciascun esito, ponderata dalla probabilità di quell’esito.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 83.1</strong></span> Sia <span class="math inline">\(X\)</span> una variabile casuale discreta che può assumere i valori <span class="math inline">\(a, b, c,\)</span> e <span class="math inline">\(d\)</span> con una distribuzione di probabilità di massa <span class="math inline">\(p(a) = \frac{1}{2}\)</span>, <span class="math inline">\(p(b) = \frac{1}{4}\)</span>, <span class="math inline">\(p(c) = \frac{1}{8}\)</span>, e <span class="math inline">\(p(d) = \frac{1}{8}\)</span>, rispettivamente. L’entropia di <span class="math inline">\(X\)</span>, che misura l’incertezza associata alla distribuzione di probabilità, è calcolata come:</p>
<p><span class="math display">\[
H(X) = -\left(\frac{1}{2} \log_2 \frac{1}{2} + \frac{1}{4} \log_2 \frac{1}{4} + \frac{1}{8} \log_2 \frac{1}{8} + \frac{1}{8} \log_2 \frac{1}{8}\right).
\]</span></p>
<p>Calcolando i singoli termini, otteniamo:</p>
<p><span class="math display">\[
H(X) = -\left(\frac{1}{2} \cdot (-1) + \frac{1}{4} \cdot (-2) + \frac{1}{8} \cdot (-3) + \frac{1}{8} \cdot (-3)\right) = \frac{7}{4} \text{ bits}.
\]</span></p>
<p>È importante notare che l’entropia <span class="math inline">\(H(X)\)</span> dipende esclusivamente dalla distribuzione di probabilità dei valori di <span class="math inline">\(X\)</span> e non dai valori stessi.</p>
</div>
</section>
<section id="entropia-di-una-variabile-casuale-continua" class="level4" data-number="83.2.5.2">
<h4 data-number="83.2.5.2" class="anchored" data-anchor-id="entropia-di-una-variabile-casuale-continua"><span class="header-section-number">83.2.5.2</span> Entropia di una Variabile Casuale Continua</h4>
<p>Nel caso delle variabili casuali continue, il concetto di entropia viene generalizzato sostituendo la somma con un integrale. Questo è necessario perché le variabili continue possono assumere un numero infinito di valori all’interno di un intervallo.</p>
<p>Per una variabile casuale continua <span class="math inline">\(X\)</span> con una funzione di densità di probabilità <span class="math inline">\(p(x)\)</span>, l’entropia (nota anche come entropia differenziale) è definita dalla seguente formula:</p>
<p><span class="math display">\[ H(X) = -\int p(x) \log_2(p(x)) \, dx, \]</span></p>
<p>dove:</p>
<ul>
<li><span class="math inline">\(p(x)\)</span> è la funzione di densità di probabilità di <span class="math inline">\(X\)</span>,</li>
<li>l’integrale è calcolato su tutto il dominio di <span class="math inline">\(X\)</span>.</li>
</ul>
<p>L’entropia di una variabile casuale continua fornisce una misura dell’incertezza o della sorpresa associata alla distribuzione della variabile. Come nel caso discreto, l’entropia continua quantifica l’incertezza associata a <span class="math inline">\(X\)</span>. Una PDF molto concentrata (ad esempio, una distribuzione con picchi stretti) implica bassa entropia, poiché l’evento è più prevedibile. Una PDF distribuita uniformemente implica alta entropia, poiché l’evento è meno prevedibile.</p>
<p>Il segno negativo assicura che l’entropia sia una quantità positiva, in quanto <span class="math inline">\(\log_2(p(x))\)</span> è negativo per <span class="math inline">\(p(x)\)</span> compreso tra 0 e 1.</p>
</section>
</section>
<section id="applicazioni-psicologiche" class="level3" data-number="83.2.6">
<h3 data-number="83.2.6" class="anchored" data-anchor-id="applicazioni-psicologiche"><span class="header-section-number">83.2.6</span> Applicazioni Psicologiche</h3>
<p>L’entropia dell’informazione trova applicazioni anche in psicologia, per esempio nello studio dell’effetto della sorpresa sull’umore. La sorpresa, o entropia, è stata documentata sia in laboratorio che in contesti naturali come un fattore significativo che influenza le emozioni.</p>
<p>Ad esempio, <span class="citation" data-cites="spector1956expectations">Spector (<a href="../../99-references.html#ref-spector1956expectations" role="doc-biblioref">1956</a>)</span> osservò l’effetto della probabilità a priori sulla soddisfazione dei soggetti in risposta a una promozione lavorativa. I risultati indicano che gli esiti meno probabili a priori (e quindi più sorprendenti quando si verificano) hanno un impatto maggiore sull’umore. In altre parole, quando un evento inatteso e sorprendente si verifica, esso tende a influenzare l’umore in modo più forte rispetto a eventi previsti e probabili.</p>
</section>
</section>
<section id="la-codifica-huffman" class="level2" data-number="83.3">
<h2 data-number="83.3" class="anchored" data-anchor-id="la-codifica-huffman"><span class="header-section-number">83.3</span> La Codifica Huffman</h2>
<p>La codifica Huffman è un metodo utilizzato per rappresentare gli esiti di una variabile casuale in un formato binario, riducendo al minimo l’incertezza e ottimizzando la lunghezza del codice necessario per descrivere questi esiti. Questo algoritmo crea una rappresentazione binaria che permette di comprimere i dati senza perdita di informazioni, utilizzando una codifica efficiente basata sulla frequenza o probabilità dei simboli.</p>
<p>Nella <em>codifica Huffman</em>, i simboli più frequenti sono rappresentati da codici binari più brevi, mentre i simboli meno frequenti sono rappresentati da codici più lunghi. Questo approccio assicura che la lunghezza media del codice per rappresentare una sequenza di simboli sia la più corta possibile, ottimizzando così lo spazio necessario per la memorizzazione o la trasmissione dei dati. In questo modo, la codifica Huffman riesce a ridurre la quantità di bit necessari per codificare una variabile casuale, rispettando il principio della teoria dell’informazione che lega la probabilità di un simbolo alla lunghezza del codice a esso associato.</p>
<section id="come-funziona-la-codifica-huffman" class="level3" data-number="83.3.1">
<h3 data-number="83.3.1" class="anchored" data-anchor-id="come-funziona-la-codifica-huffman"><span class="header-section-number">83.3.1</span> Come Funziona la Codifica Huffman</h3>
<p>La codifica Huffman è un esempio di <strong>codifica prefissa</strong>, il che significa che nessun codice assegnato a un simbolo è prefisso del codice assegnato a un altro simbolo. Questa proprietà garantisce che la decodifica dei bit sia univoca e senza ambiguità.</p>
<p>Ecco i passaggi per creare una codifica Huffman:</p>
<ol type="1">
<li><p><strong>Creare una lista di simboli e le loro frequenze o probabilità</strong>: Si inizia con un insieme di simboli, ognuno associato alla sua frequenza o probabilità di apparizione.</p></li>
<li><p><strong>Costruire un albero binario</strong>:</p>
<ul>
<li>Ogni simbolo viene rappresentato come un nodo foglia in un albero binario, con la sua frequenza come “peso” del nodo.</li>
<li>Si trovano i due nodi con i pesi minori e si uniscono per formare un nuovo nodo padre. Il peso del nuovo nodo padre è la somma dei pesi dei due nodi figli.</li>
<li>Si ripete il passaggio precedente fino a quando rimane un solo nodo, che diventa la radice dell’albero. Questo processo costruisce l’albero di Huffman.</li>
</ul></li>
<li><p><strong>Assegnare i codici</strong>:</p>
<ul>
<li>A partire dalla radice dell’albero, si assegnano 0 e 1 ai rami dell’albero. Generalmente, si assegna “0” a un ramo e “1” all’altro.</li>
<li>Il codice di ciascun simbolo è la sequenza di bit che si ottiene attraversando l’albero dalla radice alla foglia corrispondente al simbolo.</li>
</ul></li>
</ol>
</section>
<section id="esempio-di-codifica-huffman" class="level3" data-number="83.3.2">
<h3 data-number="83.3.2" class="anchored" data-anchor-id="esempio-di-codifica-huffman"><span class="header-section-number">83.3.2</span> Esempio di Codifica Huffman</h3>
<p>Consideriamo un esempio semplice con quattro simboli: <span class="math inline">\(A, B, C,\)</span> e <span class="math inline">\(D\)</span> con le seguenti probabilità:</p>
<ul>
<li><span class="math inline">\(p(A) = 0.4\)</span></li>
<li><span class="math inline">\(p(B) = 0.3\)</span></li>
<li><span class="math inline">\(p(C) = 0.2\)</span></li>
<li><span class="math inline">\(p(D) = 0.1\)</span></li>
</ul>
<section id="passaggio-1-costruire-lalbero" class="level4" data-number="83.3.2.1">
<h4 data-number="83.3.2.1" class="anchored" data-anchor-id="passaggio-1-costruire-lalbero"><span class="header-section-number">83.3.2.1</span> Passaggio 1: Costruire l’Albero</h4>
<ol type="1">
<li><p><strong>Lista iniziale dei nodi foglia con le loro probabilità</strong>:</p>
<ul>
<li><span class="math inline">\(A: 0.4\)</span></li>
<li><span class="math inline">\(B: 0.3\)</span></li>
<li><span class="math inline">\(C: 0.2\)</span></li>
<li><span class="math inline">\(D: 0.1\)</span></li>
</ul></li>
<li><p><strong>Unire i due nodi con le probabilità minori (C e D)</strong>:</p>
<ul>
<li>Creiamo un nodo padre con probabilità <span class="math inline">\(0.2 + 0.1 = 0.3\)</span>.</li>
</ul></li>
<li><p><strong>Aggiornare la lista e ripetere</strong>:</p>
<ul>
<li>Nuova lista: <span class="math inline">\(A: 0.4, B: 0.3, (CD): 0.3\)</span></li>
<li>Unire <span class="math inline">\(B\)</span> e <span class="math inline">\(CD\)</span>: Creiamo un nodo padre con probabilità <span class="math inline">\(0.3 + 0.3 = 0.6\)</span>.</li>
</ul></li>
<li><p><strong>Unire i due nodi rimanenti (A e (BCD))</strong>:</p>
<ul>
<li>Creiamo un nodo padre con probabilità <span class="math inline">\(0.4 + 0.6 = 1.0\)</span>, che diventa la radice dell’albero.</li>
</ul></li>
</ol>
<p>L’albero risultante ha questa struttura:</p>
<pre><code>          Root (1.0)
         /         \
       A (0.4)   Internal (0.6)
                 /          \
               B (0.3)    Internal (0.3)
                          /         \
                        C (0.2)     D (0.1)</code></pre>
</section>
<section id="passaggio-2-assegnare-i-codici" class="level4" data-number="83.3.2.2">
<h4 data-number="83.3.2.2" class="anchored" data-anchor-id="passaggio-2-assegnare-i-codici"><span class="header-section-number">83.3.2.2</span> Passaggio 2: Assegnare i Codici</h4>
<ul>
<li>Dal nodo radice:
<ul>
<li>Per scendere a <span class="math inline">\(A\)</span>, assegniamo “0”.</li>
<li>Per scendere all’altro ramo (contenente <span class="math inline">\(B, C,\)</span> e <span class="math inline">\(D\)</span>), assegniamo “1”.</li>
</ul></li>
<li>Continuando l’assegnazione lungo ogni ramo:
<ul>
<li><span class="math inline">\(A\)</span> = “0”</li>
<li><span class="math inline">\(B\)</span> = “10”</li>
<li><span class="math inline">\(C\)</span> = “110”</li>
<li><span class="math inline">\(D\)</span> = “111”</li>
</ul></li>
</ul>
</section>
<section id="risultato-della-codifica-huffman" class="level4" data-number="83.3.2.3">
<h4 data-number="83.3.2.3" class="anchored" data-anchor-id="risultato-della-codifica-huffman"><span class="header-section-number">83.3.2.3</span> Risultato della Codifica Huffman</h4>
<p>Il risultato finale assegna i seguenti codici binari ai simboli:</p>
<ul>
<li><span class="math inline">\(A\)</span> = “0”</li>
<li><span class="math inline">\(B\)</span> = “10”</li>
<li><span class="math inline">\(C\)</span> = “110”</li>
<li><span class="math inline">\(D\)</span> = “111”</li>
</ul>
<p>Questa distribuzione di codici è ottimale in termini di lunghezza media del codice, data la distribuzione di probabilità dei simboli.</p>
</section>
</section>
<section id="perché-la-codifica-huffman-è-efficiente" class="level3" data-number="83.3.3">
<h3 data-number="83.3.3" class="anchored" data-anchor-id="perché-la-codifica-huffman-è-efficiente"><span class="header-section-number">83.3.3</span> Perché la Codifica Huffman è Efficiente</h3>
<p>La codifica Huffman è efficiente perché:</p>
<ul>
<li><strong>Minimizza la lunghezza media del codice</strong>: Assegna codici più corti ai simboli più frequenti e codici più lunghi a quelli meno frequenti, riducendo così la lunghezza complessiva del messaggio codificato.</li>
<li><strong>È una codifica prefissa</strong>: Nessun codice è prefisso di un altro, il che elimina le ambiguità durante la decodifica. Questo significa che, quando si legge un flusso di bit codificati, è sempre possibile identificare in modo univoco quale simbolo rappresentano.</li>
</ul>
<p>In conclusione, la codifica Huffman è un metodo di compressione dei dati che utilizza le proprietà della probabilità e della teoria dell’informazione per creare una rappresentazione binaria ottimizzata, riducendo così la quantità di dati necessari per rappresentare una sequenza di simboli.</p>
</section>
</section>
<section id="lentropia-come-lunghezza-media-del-codice-binario" class="level2" data-number="83.4">
<h2 data-number="83.4" class="anchored" data-anchor-id="lentropia-come-lunghezza-media-del-codice-binario"><span class="header-section-number">83.4</span> L’Entropia come Lunghezza Media del Codice Binario</h2>
<p>L’entropia, in termini di teoria dell’informazione, rappresenta la quantità media di informazione necessaria per descrivere gli esiti di una variabile casuale. In altre parole, può essere interpretata come la lunghezza media del codice binario utilizzato per rappresentare questi esiti, tenendo conto delle loro probabilità.</p>
<p>Consideriamo una variabile casuale discreta <span class="math inline">\(X\)</span> che può assumere quattro valori: <span class="math inline">\(A, B, C,\)</span> e <span class="math inline">\(D\)</span>, con le seguenti probabilità:</p>
<ul>
<li><span class="math inline">\(p(A) = 0.4\)</span></li>
<li><span class="math inline">\(p(B) = 0.3\)</span></li>
<li><span class="math inline">\(p(C) = 0.2\)</span></li>
<li><span class="math inline">\(p(D) = 0.1\)</span></li>
</ul>
<p>Utilizzando la codifica Huffman per questa variabile casuale, otteniamo i seguenti codici binari:</p>
<ul>
<li><span class="math inline">\(A\)</span> = “0”</li>
<li><span class="math inline">\(B\)</span> = “10”</li>
<li><span class="math inline">\(C\)</span> = “110”</li>
<li><span class="math inline">\(D\)</span> = “111”</li>
</ul>
<p>La lunghezza media del codice in bit può essere calcolata come segue:</p>
<p><span class="math display">\[
\begin{align}
\text{Lunghezza media} &amp;= p(A) \times \text{lunghezza di } A + p(B) \times \text{lunghezza di } B\notag \\
&amp;\quad + p(C) \times \text{lunghezza di } C\notag + p(D) \times \text{lunghezza di } D\notag.
\end{align}
\]</span></p>
<p>Sostituendo i valori delle probabilità e le lunghezze dei codici:</p>
<p><span class="math display">\[
\begin{align}
\text{Lunghezza media} &amp;= (0.4 \times 1) + (0.3 \times 2) + (0.2 \times 3) + (0.1 \times 3)\notag\\
&amp;= 0.4 + 0.6 + 0.6 + 0.3 = 1.9 \text{ bit}.\notag
\end{align}
\]</span></p>
<p>Calcoliamo ora l’entropia <span class="math inline">\(H(X)\)</span> della variabile casuale <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[
\begin{align}
H(X) &amp;= -\sum p(x) \log_2 p(x) \notag\\
     &amp;= -(0.4 \log_2 0.4 + 0.3 \log_2 0.3 + 0.2 \log_2 0.2 + 0.1 \log_2 0.1)\notag\\
     &amp;= 1.8465 \text{ bit}.
\end{align}
\]</span></p>
<p>In questo esempio, l’entropia <span class="math inline">\(H(X) = 1.8465\)</span> bit rappresenta la lunghezza media minima teorica del codice binario necessario per rappresentare la variabile casuale <span class="math inline">\(X\)</span>. La lunghezza media del codice Huffman che abbiamo calcolato è <span class="math inline">\(1.9\)</span> bit, che è molto vicina all’entropia. Questo dimostra che la codifica Huffman è quasi ottimale, avvicinandosi molto al limite inferiore determinato dall’entropia. Pertanto, l’entropia misura effettivamente la lunghezza media minima del codice binario necessaria per descrivere la distribuzione di una variabile casuale.</p>
</section>
<section id="lentropia-relativa" class="level2" data-number="83.5">
<h2 data-number="83.5" class="anchored" data-anchor-id="lentropia-relativa"><span class="header-section-number">83.5</span> L’Entropia Relativa</h2>
<p>Una volta compresa la relazione tra l’entropia e la lunghezza media minima del codice binario necessario per descrivere la distribuzione di una variabile casuale, possiamo introdurre il concetto di <strong>entropia relativa</strong>, conosciuta anche come <strong>divergenza di Kullback-Leibler</strong> (KL-divergence). L’entropia relativa è una misura della “distanza” tra due distribuzioni di probabilità, che quantifica quanto una distribuzione si discosta da un’altra.</p>
<section id="concetto-di-divergenza-di-kullback-leibler" class="level3" data-number="83.5.1">
<h3 data-number="83.5.1" class="anchored" data-anchor-id="concetto-di-divergenza-di-kullback-leibler"><span class="header-section-number">83.5.1</span> Concetto di Divergenza di Kullback-Leibler</h3>
<p>La divergenza di Kullback-Leibler, denotata come <span class="math inline">\(D(P \parallel Q)\)</span>, misura l’inefficienza di assumere che la distribuzione dei dati sia <span class="math inline">\(Q\)</span> quando, in realtà, la distribuzione vera è <span class="math inline">\(P\)</span>. In termini di teoria dell’informazione, <span class="math inline">\(D(P \parallel Q)\)</span> indica quanto è meno efficiente, in termini di lunghezza media del codice, utilizzare una distribuzione errata <span class="math inline">\(Q\)</span> rispetto alla distribuzione corretta <span class="math inline">\(P\)</span>.</p>
<p>Se conoscessimo la vera distribuzione <span class="math inline">\(P\)</span> di una variabile casuale, potremmo costruire un codice ottimale con una lunghezza media pari all’entropia <span class="math inline">\(H(P)\)</span>. Tuttavia, se utilizzassimo un codice basato su una distribuzione diversa <span class="math inline">\(Q\)</span>, la lunghezza media necessaria per descrivere la variabile casuale sarebbe maggiore. Precisamente, questa lunghezza media del codice sarebbe <span class="math inline">\(H(P) + D(P \parallel Q)\)</span> bit. L’incremento rappresentato da <span class="math inline">\(D(P \parallel Q)\)</span> riflette l’inefficienza aggiuntiva derivante dall’utilizzo della distribuzione <span class="math inline">\(Q\)</span> invece della distribuzione vera <span class="math inline">\(P\)</span>.</p>
</section>
<section id="definizione-matematica-della-divergenza-di-kullback-leibler" class="level3" data-number="83.5.2">
<h3 data-number="83.5.2" class="anchored" data-anchor-id="definizione-matematica-della-divergenza-di-kullback-leibler"><span class="header-section-number">83.5.2</span> Definizione Matematica della Divergenza di Kullback-Leibler</h3>
<p>Matematicamente, la divergenza di Kullback-Leibler è definita come:</p>
<p><span class="math display">\[
D(P \parallel Q) = \sum_{x} p(x) \log_2 \left( \frac{p(x)}{q(x)} \right),
\]</span></p>
<p>dove:</p>
<ul>
<li><span class="math inline">\(p(x)\)</span> è la probabilità associata al valore <span class="math inline">\(x\)</span> secondo la distribuzione vera <span class="math inline">\(P\)</span>.</li>
<li><span class="math inline">\(q(x)\)</span> è la probabilità associata al valore <span class="math inline">\(x\)</span> secondo la distribuzione approssimata <span class="math inline">\(Q\)</span>.</li>
</ul>
<p>Questa formula rappresenta la somma, pesata dalle probabilità <span class="math inline">\(p(x)\)</span>, dei logaritmi del rapporto tra le probabilità delle due distribuzioni.</p>
</section>
<section id="entropia-entropia-incrociata-e-divergenza-kl" class="level3" data-number="83.5.3">
<h3 data-number="83.5.3" class="anchored" data-anchor-id="entropia-entropia-incrociata-e-divergenza-kl"><span class="header-section-number">83.5.3</span> Entropia, Entropia Incrociata e Divergenza KL</h3>
<p>Per chiarire ulteriormente il concetto, consideriamo due misure fondamentali: l’entropia di Shannon e l’entropia incrociata.</p>
<ol type="1">
<li><p><strong>Entropia di Shannon</strong>: Se <span class="math inline">\(X\)</span> è una variabile casuale che segue la distribuzione <span class="math inline">\(P\)</span>, l’entropia <span class="math inline">\(H(P)\)</span> misura l’incertezza intrinseca di <span class="math inline">\(P\)</span> ed è definita come:</p>
<p><span class="math display">\[
H(P) = -\sum_x p(x) \log_2(p(x)).
\]</span></p></li>
<li><p><strong>Entropia incrociata</strong>: L’entropia incrociata <span class="math inline">\(H(P, Q)\)</span> misura l’incertezza media quando si usa la distribuzione <span class="math inline">\(Q\)</span> per descrivere la variabile casuale <span class="math inline">\(X\)</span> distribuita secondo <span class="math inline">\(P\)</span>:</p>
<p><span class="math display">\[
H(P, Q) = -\sum_x p(x) \log_2(q(x)).
\]</span></p>
<p>Questa misura rappresenta la sorpresa media se si assume che i dati siano distribuiti secondo <span class="math inline">\(Q\)</span> quando in realtà seguono <span class="math inline">\(P\)</span>.</p></li>
<li><p><strong>Divergenza KL</strong>: La divergenza di Kullback-Leibler può essere vista come la differenza tra l’entropia di <span class="math inline">\(P\)</span> e l’entropia incrociata tra <span class="math inline">\(P\)</span> e <span class="math inline">\(Q\)</span>:</p>
<p><span class="math display">\[
D_{\text{KL}}(P \parallel Q) = H(P, Q) - H(P).
\]</span></p>
<p>Alternativamente, si può esprimere come:</p>
<p><span class="math display">\[
D_{\text{KL}}(P \parallel Q) = \sum_x p(x) \log_2 \left(\frac{p(x)}{q(x)}\right).
\]</span></p></li>
</ol>
</section>
<section id="interpretazione-della-divergenza-kl" class="level3" data-number="83.5.4">
<h3 data-number="83.5.4" class="anchored" data-anchor-id="interpretazione-della-divergenza-kl"><span class="header-section-number">83.5.4</span> Interpretazione della Divergenza KL</h3>
<p>La divergenza <span class="math inline">\(D_{\text{KL}}(P \parallel Q)\)</span> rappresenta il “costo” aggiuntivo di sorpresa o inefficienza quando si utilizza la distribuzione <span class="math inline">\(Q\)</span> per modellare i dati che in realtà seguono la distribuzione <span class="math inline">\(P\)</span>. Questo “costo” è espresso in bit e rappresenta l’informazione che viene “persa” quando <span class="math inline">\(Q\)</span> è usata al posto di <span class="math inline">\(P\)</span>.</p>
<p>È importante notare che la divergenza di Kullback-Leibler non è simmetrica, il che significa che <span class="math inline">\(D(P \parallel Q) \neq D(Q \parallel P)\)</span>. Nonostante non sia una vera distanza matematica, fornisce un’indicazione quantitativa di quanto una distribuzione si discosti dall’altra in termini di inefficienza di codifica.</p>
<p>In sintesi, l’entropia relativa o divergenza di Kullback-Leibler è una misura chiave nella teoria dell’informazione. Essa valuta l’efficacia di un modello probabilistico confrontando una distribuzione teorica con la distribuzione vera dei dati. La divergenza KL fornisce un quadro per quantificare la perdita di informazione e l’incremento di incertezza quando si utilizza una distribuzione approssimativa per descrivere dati reali.</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>Un commento sulla notazione. Le lettere maiuscole <span class="math inline">\(P\)</span> e <span class="math inline">\(Q\)</span> sono utilizzate per indicare le distribuzioni di probabilità e lettere minuscole <span class="math inline">\(p(x)\)</span> e <span class="math inline">\(q(x)\)</span> per le funzioni di probabilità specifiche dei valori discreti.</p>
</div>
</div>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 83.2</strong></span> Per fare un esempio, supponiamo che <span class="math inline">\(P\)</span> e <span class="math inline">\(Q\)</span> siano due distribuzioni di probabilità su un insieme finito di possibili esiti, ad esempio {0, 1, 2}. Per semplicità, consideriamo che <span class="math inline">\(P\)</span> e <span class="math inline">\(Q\)</span> siano definite come segue:</p>
<ul>
<li><span class="math inline">\(P\)</span> è la distribuzione “vera”: <span class="math inline">\(P = [0.1, 0.6, 0.3]\)</span>;</li>
<li><span class="math inline">\(Q\)</span> è una distribuzione alternativa che usiamo per la stima: <span class="math inline">\(Q = [0.2, 0.5, 0.3]\)</span>.</li>
</ul>
<div id="76a89be7-d054-4e98-ba4f-63d58130c4ab" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Definizione delle distribuzioni</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.6</span>, <span class="fl">0.3</span>])</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> np.array([<span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.3</span>])</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcolo della divergenza KL da P a Q</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>KL_divergence <span class="op">=</span> np.<span class="bu">sum</span>(kl_div(P, Q))</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Divergenza KL da P a Q: </span><span class="sc">{</span>KL_divergence<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Divergenza KL da P a Q: 0.0401</code></pre>
</div>
</div>
<p>Nel codice precedente, <code>kl_div(P, Q)</code> calcola la divergenza <span class="math inline">\(D_{\text{KL}}\)</span> elemento per elemento dell’array. Essa calcola <span class="math inline">\(\sum_x p(x) \log \left(\frac{p(x)}{q(x)}\right)\)</span> per ogni esito <span class="math inline">\(x\)</span>, che è esattamente il termine <span class="math inline">\(p(x) \log \left(\frac{p(x)}{q(x)}\right)\)</span> descritto nella formula della divergenza <span class="math inline">\(D_{\text{KL}}\)</span>. Utilizziamo poi <code>np.sum</code> per sommare tutti i contributi individuali e ottenere il valore totale della divergenza <span class="math inline">\(D_{\text{KL}}\)</span>.</p>
<p>Questo esempio fornisce un calcolo diretto della divergenza <span class="math inline">\(D_{\text{KL}}\)</span> tra due distribuzioni, mostrando come una distribuzione <span class="math inline">\(Q\)</span> possa essere inadeguata nel modellare una distribuzione <span class="math inline">\(P\)</span>, con un focus sul “costo” di sorpresa per ogni esito.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Esempio 83.3</strong></span> In un due altri esempi, rendiamo via via <span class="math inline">\(Q\)</span> più diverso da <span class="math inline">\(P\)</span>. Notiamo come la divergenza <span class="math inline">\(D_{\text{KL}}\)</span> aumenta.</p>
<div id="87554350-4f0b-4a86-b75f-a477782430bb" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.6</span>, <span class="fl">0.3</span>])</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> np.array([<span class="fl">0.35</span>, <span class="fl">0.3</span>, <span class="fl">0.35</span>])</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>KL_divergence <span class="op">=</span> np.<span class="bu">sum</span>(kl_div(P, Q))</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Divergenza KL da P a Q: </span><span class="sc">{</span>KL_divergence<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Divergenza KL da P a Q: 0.2444</code></pre>
</div>
</div>
<div id="18870510-8d44-47dc-b1cf-90a779796e4e" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.6</span>, <span class="fl">0.3</span>])</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> np.array([<span class="fl">0.6</span>, <span class="fl">0.3</span>, <span class="fl">0.1</span>])</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>KL_divergence <span class="op">=</span> np.<span class="bu">sum</span>(kl_div(P, Q))</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Divergenza KL da P a Q: </span><span class="sc">{</span>KL_divergence<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Divergenza KL da P a Q: 0.5663</code></pre>
</div>
</div>
</div>
</section>
</section>
<section id="applicazione-della-divergenza-d_textkl-nella-selezione-di-modelli" class="level2" data-number="83.6">
<h2 data-number="83.6" class="anchored" data-anchor-id="applicazione-della-divergenza-d_textkl-nella-selezione-di-modelli"><span class="header-section-number">83.6</span> Applicazione della Divergenza <span class="math inline">\(D_{\text{KL}}\)</span> nella Selezione di Modelli</h2>
<p>La divergenza di Kullback-Leibler, <span class="math inline">\(D_{\text{KL}}\)</span>, è uno strumento fondamentale nella selezione dei modelli statistici. L’obiettivo è identificare il modello <span class="math inline">\(Q\)</span> che minimizza <span class="math inline">\(D_{\text{KL}}(P \parallel Q)\)</span>, riducendo al minimo la differenza tra l’entropia della distribuzione vera <span class="math inline">\(P\)</span> e l’entropia incrociata tra <span class="math inline">\(P\)</span> e <span class="math inline">\(Q\)</span>. In altre parole, si cerca di minimizzare l’errore introdotto nell’approssimare la distribuzione reale <span class="math inline">\(P\)</span> con il modello <span class="math inline">\(Q\)</span>.</p>
<section id="proprietà-chiave" class="level3" data-number="83.6.1">
<h3 data-number="83.6.1" class="anchored" data-anchor-id="proprietà-chiave"><span class="header-section-number">83.6.1</span> Proprietà Chiave</h3>
<ul>
<li><p><strong>Non-negatività:</strong> La divergenza <span class="math inline">\(D_{\text{KL}}(P \parallel Q)\)</span> è sempre maggiore o uguale a zero. Questo valore è pari a zero solo quando le distribuzioni <span class="math inline">\(P\)</span> e <span class="math inline">\(Q\)</span> sono identiche, indicando una perfetta corrispondenza tra il modello e la distribuzione vera.</p></li>
<li><p><strong>Asimmetria:</strong> <span class="math inline">\(D_{\text{KL}}(P \parallel Q) \neq D_{\text{KL}}(Q \parallel P)\)</span>. Questa proprietà mostra che la “distanza” percepita tra <span class="math inline">\(P\)</span> e <span class="math inline">\(Q\)</span> dipende dalla direzione in cui è misurata. La divergenza di <span class="math inline">\(Q\)</span> rispetto a <span class="math inline">\(P\)</span> non è la stessa della divergenza di <span class="math inline">\(P\)</span> rispetto a <span class="math inline">\(Q\)</span>.</p></li>
</ul>
</section>
<section id="selezione-dei-modelli-statistici" class="level3" data-number="83.6.2">
<h3 data-number="83.6.2" class="anchored" data-anchor-id="selezione-dei-modelli-statistici"><span class="header-section-number">83.6.2</span> Selezione dei Modelli Statistici</h3>
<p>Nel contesto della selezione dei modelli statistici, l’obiettivo principale è scegliere il modello <span class="math inline">\(Q\)</span> che minimizzi la divergenza <span class="math inline">\(D_{\text{KL}}(P \parallel Q)\)</span> rispetto alla distribuzione reale <span class="math inline">\(P\)</span> dei dati. Tuttavia, poiché la distribuzione vera <span class="math inline">\(P\)</span> è spesso sconosciuta o non direttamente osservabile, non è possibile calcolare la divergenza <span class="math inline">\(D_{\text{KL}}\)</span> in modo diretto.</p>
<p>Per superare questa limitazione, i ricercatori e gli statistici utilizzano criteri approssimativi per stimare indirettamente la divergenza <span class="math inline">\(D_{\text{KL}}\)</span>. Questi criteri considerano sia la bontà di adattamento del modello ai dati osservati sia la complessità del modello stesso, cercando un equilibrio tra accuratezza e parsimonia. Nel capitolo successivo, esploreremo questi criteri in dettaglio e discuteremo come vengono utilizzati per valutare e selezionare i modelli statistici migliori.</p>
</section>
</section>
<section id="riflessioni-conclusive" class="level2" data-number="83.7">
<h2 data-number="83.7" class="anchored" data-anchor-id="riflessioni-conclusive"><span class="header-section-number">83.7</span> Riflessioni Conclusive</h2>
<p>In questo capitolo, abbiamo esaminato il concetto di entropia, evidenziando il suo ruolo fondamentale nel quantificare l’incertezza all’interno delle distribuzioni di probabilità. Abbiamo anche affrontato la questione di come l’entropia possa essere impiegata per valutare la “distanza” tra un modello teorico e i dati reali. A tale scopo, abbiamo introdotto la divergenza <span class="math inline">\(\mathbb{KL}\)</span>, una misura che quantifica le discrepanze tra due distribuzioni di probabilità.</p>
<p>Nel capitolo successivo, approfondiremo ulteriormente il tema della divergenza <span class="math inline">\(\mathbb{KL}\)</span>. Esploreremo come questo strumento possa essere utilizzato per confrontare modelli teorici con dati empirici e ci concentreremo su come possa fornirci una comprensione più dettagliata dell’adattamento di un modello alla realtà che intende rappresentare. Questa esplorazione ci permetterà di valutare più accuratamente la validità e la generalizzabilità dei modelli scientifici nel loro tentativo di catturare e interpretare la complessità dei fenomeni oggetto di studio.</p>
</section>
<section id="esercizi" class="level2" data-number="83.8">
<h2 data-number="83.8" class="anchored" data-anchor-id="esercizi"><span class="header-section-number">83.8</span> Esercizi</h2>
<div id="exr-entropy-1" class="theorem exercise">
<p><span class="theorem-title"><strong>Esercizio 83.1</strong></span> Cosideriamo due distribuzioni di probabilità discrete, <span class="math inline">\(p\)</span> e <span class="math inline">\(q\)</span>:</p>
<pre><code>p = np.array([0.2, 0.5, 0.3])
q = np.array([0.1, 0.2, 0.7])</code></pre>
<p>Si calcoli l’entropia di <span class="math inline">\(p\)</span>, l’entropia incrociata tra <span class="math inline">\(p\)</span> e <span class="math inline">\(q\)</span>, la divergenza di Kullback-Leibler da <span class="math inline">\(p\)</span> a <span class="math inline">\(q\)</span>.</p>
<p>Si consideri <code>q = np.array([0.2, 0.55, 0.25])</code> e si calcoli di nuovo a divergenza di Kullback-Leibler da <span class="math inline">\(p\)</span> a <span class="math inline">\(q\)</span>. Si confronti con il risultato precedente e si interpreti.</p>
</div>
<div id="exr-entropy-2" class="theorem exercise">
<p><span class="theorem-title"><strong>Esercizio 83.2</strong></span> Sia <span class="math inline">\(p\)</span> una distribuzione binomiale di parametri <span class="math inline">\(\theta = 0.2\)</span> e <span class="math inline">\(n = 5\)</span>. Sia <span class="math inline">\(q_1\)</span> una approssimazione a <span class="math inline">\(p\)</span>: <code>q1 = np.array([0.46, 0.42, 0.10, 0.01, 0.01])</code>. Sia <span class="math inline">\(q_2\)</span> una distribuzione uniforme: <code>q2 = [0.2] * 5</code>. Si calcoli la divergenza <span class="math inline">\(\mathbb{KL}\)</span> di <span class="math inline">\(q_1\)</span> da <span class="math inline">\(p\)</span> e da <span class="math inline">\(q_2\)</span> da <span class="math inline">\(p\)</span> e si interpretino i risultati.</p>
</div>
<div id="exr-entropy-3" class="theorem exercise">
<p><span class="theorem-title"><strong>Esercizio 83.3</strong></span> La Divergenza <span class="math inline">\(\mathbb{KL}\)</span> è spesso paragonata a una “distanza” tra due distribuzioni di probabilità, ma è fondamentale capire che non è simmetrica. Questo significa che la misura di quanto <span class="math inline">\(p\)</span> è diversa da <span class="math inline">\(q\)</span> non è la stessa di quanto <span class="math inline">\(q\)</span> è diversa da <span class="math inline">\(p\)</span>. Questa asimmetria riflette la differenza nella perdita di informazione quando si sostituisce una distribuzione con l’altra.</p>
<p>Per le seguenti distribuzioni</p>
<pre><code>p = np.array([0.01, 0.99])
q = np.array([0.7, 0.3])</code></pre>
<p>si calcoli l’entropia di p, l’entropia incrociata da p a q, la divergenza KL da p a q, l’entropia di q, l’entropia incrociata da q a p, e la divergenza KL da q a p.&nbsp;Si commenti.</p>
</div>
</section>
<section id="informazioni-sullambiente-di-sviluppo" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="informazioni-sullambiente-di-sviluppo">Informazioni sull’Ambiente di Sviluppo</h2>
<div id="84e18249-4a1a-44f1-b835-74c3b8996eff" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext watermark</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>watermark <span class="op">-</span>n <span class="op">-</span>u <span class="op">-</span>v <span class="op">-</span>iv <span class="op">-</span>w <span class="op">-</span>m</span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Last updated: Fri Jul 26 2024

Python implementation: CPython
Python version       : 3.12.4
IPython version      : 8.26.0

Compiler    : Clang 16.0.6 
OS          : Darwin
Release     : 23.5.0
Machine     : arm64
Processor   : arm
CPU cores   : 8
Architecture: 64bit

numpy     : 1.26.4
matplotlib: 3.9.1
scipy     : 1.14.0
pandas    : 2.2.2
arviz     : 0.18.0

Watermark: 2.4.3
</code></pre>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-McElreath_rethinking" class="csl-entry" role="listitem">
McElreath, Richard. 2020. <em>Statistical rethinking: <span>A</span> <span>Bayesian</span> course with examples in <span>R</span> and <span>Stan</span></em>. 2nd Edition. Boca Raton, Florida: CRC Press.
</div>
<div id="ref-spector1956expectations" class="csl-entry" role="listitem">
Spector, Aaron J. 1956. <span>«Expectations, fulfillment, and morale»</span>. <em>The Journal of Abnormal and Social Psychology</em> 52 (1): 51–56.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiato!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiato!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/ccaudek\.github\.io\/psicometria\/intro\.html");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../chapters/entropy/introduction_entropy.html" class="pagination-link" aria-label="Introduzione">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Introduzione</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/entropy/02_kl.html" class="pagination-link" aria-label="Divergenza KL, LPPD, ELPD e LOO-CV">
        <span class="nav-page-text"><span class="chapter-number">84</span>&nbsp; <span class="chapter-title">Divergenza KL, LPPD, ELPD e LOO-CV</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Data Science per Psicologi è stato scritto da Corrado Caudek.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.dev/ccaudek/psicometria/blob/main/chapters/entropy/01_entropy.ipynb" class="toc-action"><i class="bi bi-github"></i>Modifica questa pagina</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Questo libro è stato realizzato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>